[
  {
    "model_name": "Jan-v1-4B-GGUF",
    "developer": "janhq",
    "downloads": 86716,
    "createdAt": "2025-08-11T06:21:13.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Jan-v1-4B-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-v1-4B-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-v1-4B-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-v1-4B-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/README.md",
    "description": "Jan-v1 is an advanced agentic language model built on the Qwen3-4B-thinking model, optimized for complex problem-solving and integration with the Jan App."
  },
  {
    "model_name": "Jan-v1-2509-gguf",
    "developer": "janhq",
    "downloads": 13596,
    "createdAt": "2025-09-09T03:24:24.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Jan-v1-2509-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-2509-gguf/resolve/main/Jan-v1-2509-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-v1-2509-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-2509-gguf/resolve/main/Jan-v1-2509-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-v1-2509-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v1-2509-gguf/resolve/main/Jan-v1-2509-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-v1-2509-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v1-2509-gguf/resolve/main/Jan-v1-2509-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-v1-2509-gguf/resolve/main/README.md",
    "description": "Jan-v1 is an advanced agentic language model in the Jan Family, built for reasoning, problem-solving, and seamless integration with the Jan App."
  },
  {
    "model_name": "gpt-oss-20b-GGUF",
    "developer": "ggml-org",
    "downloads": 90936,
    "createdAt": "2025-08-02T10:45:18.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gpt-oss-20b-mxfp4",
        "path": "https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-mxfp4.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "The GPT OSS 120B GGUF model can be run using `llama-server` with the command `llama-server -hf ggml-org/gpt-oss-20b-GGUF -c 0 -fa --jinja --reasoning-format none` and"
  },
  {
    "model_name": "ERNIE-4.5-21B-A3B-Thinking-GGUF",
    "developer": "unsloth",
    "downloads": 35464,
    "createdAt": "2025-09-10T11:01:33.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-BF16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-BF16.gguf",
        "file_size": "40.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-IQ4_NL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-IQ4_NL.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-IQ4_XS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-IQ4_XS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q2_K_L.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q3_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q4_0.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q4_1.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q4_K_S.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q5_K_M.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q5_K_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q6_K.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-Q8_0.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-IQ1_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-IQ1_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-IQ2_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-IQ2_XXS.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-IQ3_XXS.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-Q2_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-Q3_K_XL.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-Q4_K_XL.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-Q5_K_XL.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-Q6_K_XL.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-Q8_K_XL.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-Thinking-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/ERNIE-4.5-21B-A3B-Thinking-UD-TQ1_0.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF/resolve/main/README.md",
    "description": "ERNIE‑4.5‑21B‑A3B‑Thinking is a 21 B‑parameter MoE text model with 128 K context, enhanced reasoning and tool‑calling, usable via FastDeploy, vLLM or Transformers."
  },
  {
    "model_name": "ERNIE-4.5-21B-A3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 3604,
    "createdAt": "2025-07-18T01:00:17Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-BF16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-BF16.gguf",
        "file_size": "40.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-IQ4_NL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-IQ4_XS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q2_K.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q2_K_L.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q3_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_0.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_1.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_K_M.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q5_K_M.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q5_K_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q6_K.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q8_0.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ1_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ1_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ2_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ2_XXS.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ3_XXS.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-TQ1_0.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-21B-A3B-PT is a large-scale text generation model with 21B parameters and 3B activated parameters per token, trained using advanced multimodal MoE techniques and optimized for high-performance inference on various hardware platforms under the Apache 2",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 21581,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling comprehensive analysis of long documents and complex conversations without performance loss."
  },
  {
    "model_name": "Lucy-128k-gguf",
    "developer": "Menlo",
    "downloads": 9884,
    "createdAt": "2025-07-18T08:52:46.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "lucy_128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "lucy_128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "lucy_128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "lucy_128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "lucy_128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "lucy_128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "lucy_128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "lucy_128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "lucy_128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "lucy_128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/README.md",
    "description": "Lucy is a 1.7B mobile-optimized model for agentic web search and lightweight browsing, built on Qwen3-1.7B with machine-generated task vectors for efficient performance on CPU-only devices."
  },
  {
    "model_name": "Jan-nano-gguf",
    "developer": "Menlo",
    "downloads": 7654,
    "createdAt": "2025-06-11T07:14:33Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-4b-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-4b-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/README.md",
    "description": "Jan Nano is a compact, quantized version of the Qwen3 architecture, optimized for efficient text generation in local or embedded environments with enhanced tool use and research capabilities."
  },
  {
    "model_name": "Jan-v1-edge-gguf",
    "developer": "janhq",
    "downloads": 4204,
    "createdAt": "2025-08-26T11:14:32.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Jan-v1-edge-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-edge-gguf/resolve/main/Jan-v1-edge-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-v1-edge-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-edge-gguf/resolve/main/Jan-v1-edge-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-v1-edge-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v1-edge-gguf/resolve/main/Jan-v1-edge-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Jan-v1-edge-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v1-edge-gguf/resolve/main/Jan-v1-edge-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-v1-edge-gguf/resolve/main/README.md",
    "description": "Jan‑v1‑edge is a 1.7 B‑parameter distilled, edge‑optimized language model delivering strong reasoning and QA performance, ready for Jan App or local deployment via vLLM/llama.cpp."
  },
  {
    "model_name": "Lucy-gguf",
    "developer": "Menlo",
    "downloads": 3299,
    "createdAt": "2025-07-18T07:04:35Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Lucy-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Lucy-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Lucy-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Lucy-Q4_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Lucy-Q4_1",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Lucy-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Lucy-Q5_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q5_1",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Lucy-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Lucy-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q6_K",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Lucy-Q8_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/README.md",
    "description": "Lucy is a 1.7B mobile-optimized model for agentic web search and lightweight browsing, built on Qwen3-1.7B with machine-generated task vectors for efficient performance on CPU-only devices.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 234657,
    "createdAt": "2024-09-25T18:35:33.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct model by Bartowski, offering various quantization types (e.g., Q4_K_M, Q5_K_S) for different performance and memory trade-offs, with a focus on compatibility with ARM and CPU"
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 229589,
    "createdAt": "2025-03-20T22:44:27.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, multimodal AI model from Google, capable of handling text and image inputs to generate text outputs, trained on diverse data including text, code, math, and images, with performance metrics across various benchmarks and languages, and available in multiple sizes"
  },
  {
    "model_name": "Mistral-Nemo-Instruct-2407-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 220122,
    "createdAt": "2024-07-18T14:49:08Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.fp16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/README.md",
    "description": "This GGUF format model is a quantized version of the Mistral-Nemo-Instruct-2407 model by Mistral AI, compatible with various frameworks and tools like llama.cpp, llama-cpp-python, LM Studio, and others.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Phi-3.5-mini-instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 216986,
    "createdAt": "2024-08-20T20:07:57Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Phi-3.5-mini-instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ1_M.gguf",
        "file_size": "874.6 MB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ1_S.gguf",
        "file_size": "802.6 MB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ2_XS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q4_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Microsoft Phi-3.5-mini-instruct model, quantized in 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit, and available for text generation tasks."
  },
  {
    "model_name": "gemma-2b",
    "developer": "google",
    "downloads": 190283,
    "createdAt": "2024-02-08T08:11:26.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b",
        "path": "https://huggingface.co/google/gemma-2b/resolve/main/gemma-2b.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-2b/resolve/main/README.md",
    "description": "The Gemma 2B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with support for fine-tuning, multi-GPU training, and various precision optimizations, and evaluated on multiple benchmarks for performance and"
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 188911,
    "createdAt": "2025-08-07T22:20:03.000Z",
    "tools": true,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-Q5_1.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "This model is a specialized uncensored/abliterated GPT-oss 20B MOE variant with 128k context support, offering enhanced performance for creative, code generation, and problem-solving tasks, but requires specific prompts and settings to generate graphic, explicit, or unconventional content effectively"
  },
  {
    "model_name": "gemma-7b-it",
    "developer": "google",
    "downloads": 184253,
    "createdAt": "2024-02-13T01:07:30.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b-it",
        "path": "https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-7b-it/resolve/main/README.md",
    "description": "The Gemma 7B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, and suitable for various text generation tasks with support for fine-tuning, GPU acceleration, and ethical safety measures."
  },
  {
    "model_name": "Phi-4-mini-instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 178112,
    "createdAt": "2025-03-01T10:43:15Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Phi-4-mini-instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.fp16.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF-quantized version of the Microsoft Phi-4-mini-instruct model, optimized for efficient text generation on various platforms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 152980,
    "createdAt": "2024-09-25T19:26:01Z",
    "tools": false,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ1_M.gguf",
        "file_size": "394.4 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ1_S.gguf",
        "file_size": "375.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ2_XS.gguf",
        "file_size": "453.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ3_XS.gguf",
        "file_size": "592.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ4_XS.gguf",
        "file_size": "708.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q2_K.gguf",
        "file_size": "554.0 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_L.gguf",
        "file_size": "698.6 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_M.gguf",
        "file_size": "658.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_S.gguf",
        "file_size": "612.0 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q4_K_M.gguf",
        "file_size": "770.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q4_K_S.gguf",
        "file_size": "739.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q5_K_M.gguf",
        "file_size": "869.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q5_K_S.gguf",
        "file_size": "851.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q6_K.gguf",
        "file_size": "974.5 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q8_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.fp16.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the meta-llama/Llama-3.2-1B-Instruct model, quantized in 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit, and others, suitable for text",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "openai_gpt-oss-20b-GGUF",
    "developer": "bartowski",
    "downloads": 136082,
    "createdAt": "2025-08-05T21:29:47Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "openai_gpt-oss-20b-IQ2_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_XXS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_XXS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_NL.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-MXFP4.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q2_K.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_L.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_0",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_1",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q6_K_L.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q8_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-bf16",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-bf16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-imatrix",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenAI GPT-oss-20b model, optimized for various inference speeds and resource constraints using llama.cpp with imatrix calibration."
  },
  {
    "model_name": "SmolVLM-500M-Instruct-GGUF",
    "developer": "ggml-org",
    "downloads": 120163,
    "createdAt": "2025-04-21T19:02:08.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "SmolVLM-500M-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-Q8_0.gguf",
        "file_size": "416.6 MB"
      },
      {
        "model_id": "SmolVLM-500M-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf",
        "file_size": "782.4 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-SmolVLM-500M-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf",
        "file_size": "103.7 MB"
      },
      {
        "model_id": "mmproj-SmolVLM-500M-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf",
        "file_size": "190.2 MB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a 500 million parameter instruction-tuned version of SmolVLM, based on the HuggingFaceTB/SmolVLM-500M-Instruct model."
  },
  {
    "model_name": "gemma-2-2b-it-GGUF",
    "developer": "bartowski",
    "downloads": 116910,
    "createdAt": "2024-07-31T16:45:13.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gemma-2-2b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-2-2b-it model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp imatrix quantization."
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "developer": "unsloth",
    "downloads": 114666,
    "createdAt": "2025-05-29T14:17:25.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "This is a large language model based on the DeepSeek-R1-0528 architecture, optimized for reasoning tasks with improved performance on benchmarks like AIME, and available in various quantized formats for efficient local inference."
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 112258,
    "createdAt": "2024-04-18T16:43:25.000Z",
    "tools": false,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.fp16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized and GGUF version of the Meta Llama 3 8B Instruct model, optimized for efficient inference and compatible with llama.cpp, based on the original Meta Llama 3 model."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 93448,
    "createdAt": "2024-07-23T16:17:10.000Z",
    "tools": false,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Meta-Llama-3.1-8B-Instruct-GGUF model is a GGUF-formatted, 8B parameter, multilingual, instruction-tuned text generation model from Meta, optimized for efficiency and performance across various languages and tasks."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 92865,
    "createdAt": "2024-05-22T17:27:45.000Z",
    "tools": false,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ2_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Mistral-7B-Instruct-v0.3, quantized for efficient inference on various platforms."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 92398,
    "createdAt": "2025-05-11T13:03:32.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Qwen2.5-VL-7B-Instruct model is a versatile vision-language model that supports image-text understanding, video analysis, structured output generation, and agent-like reasoning, with enhanced performance through dynamic resolution and frame rate training, efficient vision encoders, and compatibility with various visual input"
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 85391,
    "createdAt": "2025-03-12T12:32:41.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned, multimodal model with 128K context window and multilingual support, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks for reasoning, STEM, code, and"
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 76701,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored Gemma 3 27B IT model created using a layerwise abliteration technique to achieve high acceptance rates while preserving model capabilities."
  },
  {
    "model_name": "Violet_Twilight-v0.2-GGUF",
    "developer": "Epiculous",
    "downloads": 74226,
    "createdAt": "2024-09-13T13:19:21.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Violet_Twilight-v0.2.IQ1_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_NL",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_L",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_1",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q6_K",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q8_0",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.bf16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.bf16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f32",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f32.gguf",
        "file_size": "45.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/README.md",
    "description": "The Violet_Twilight-v0.2 model is a SLERP merge of Azure_Dusk-v0.2 and Crimson_Dawn-v0.2, trained on ChatML with support for gguf quantization."
  },
  {
    "model_name": "gemma-3-1b-it-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 70772,
    "createdAt": "2025-03-12T14:06:53.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gemma-3-1b-it.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q2_K.gguf",
        "file_size": "657.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q3_K_L.gguf",
        "file_size": "716.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q3_K_M.gguf",
        "file_size": "688.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q3_K_S.gguf",
        "file_size": "656.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q4_K_M.gguf",
        "file_size": "768.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q4_K_S.gguf",
        "file_size": "744.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q5_K_M.gguf",
        "file_size": "811.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q5_K_S.gguf",
        "file_size": "797.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q6_K.gguf",
        "file_size": "964.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q8_0.gguf",
        "file_size": "1019.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.fp16.gguf",
        "file_size": "1.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "gemma-2b-it",
    "developer": "google",
    "downloads": 65898,
    "createdAt": "2024-02-08T13:23:59.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b-it",
        "path": "https://huggingface.co/google/gemma-2b-it/resolve/main/gemma-2b-it.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-2b-it/resolve/main/README.md",
    "description": "The Gemma 2B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, offering high performance for tasks like question answering, summarization, and reasoning, with support for CPU/GPU inference and fine"
  },
  {
    "model_name": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF",
    "developer": "bartowski",
    "downloads": 61787,
    "createdAt": "2025-05-09T15:21:21.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dolphin-Mistral-24B-Venice-Edition model by cognitivecomputations, optimized for various inference speeds and quality levels using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "gemma-3-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 61431,
    "createdAt": "2025-03-12T09:04:23.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 model using Unsloth, offering efficient training and export options to GGUF, Ollama, llama.cpp, or Hugging Face."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
    "developer": "lmstudio-community",
    "downloads": 57252,
    "createdAt": "2025-07-31T12:09:12.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model by Qwen, optimized for efficient text generation using GGUF quantization from llama.cpp."
  },
  {
    "model_name": "gemma-3-270m-it-GGUF",
    "developer": "unsloth",
    "downloads": 53645,
    "createdAt": "2025-08-13T00:40:53.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3-270m-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-F16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-IQ4_NL.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-IQ4_XS.gguf",
        "file_size": "229.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q2_K.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q2_K_L.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q3_K_M.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q3_K_S.gguf",
        "file_size": "225.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_0.gguf",
        "file_size": "230.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_1.gguf",
        "file_size": "236.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_K_M.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_K_S.gguf",
        "file_size": "238.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q5_K_M.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q5_K_S.gguf",
        "file_size": "246.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q6_K.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q8_0.gguf",
        "file_size": "278.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-IQ2_M.gguf",
        "file_size": "174.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-IQ2_XXS.gguf",
        "file_size": "171.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-IQ3_XXS.gguf",
        "file_size": "176.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q2_K_XL.gguf",
        "file_size": "226.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q3_K_XL.gguf",
        "file_size": "231.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q4_K_XL.gguf",
        "file_size": "242.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q5_K_XL.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q6_K_XL.gguf",
        "file_size": "272.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q8_K_XL.gguf",
        "file_size": "449.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 270M model for text generation, offering improved performance and efficiency with support for multimodal tasks, multilingual capabilities, and various training formats like GGUF, 4-bit, and 16-bit."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-GGUF",
    "developer": "unsloth",
    "downloads": 52078,
    "createdAt": "2025-08-06T19:21:40.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-F16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "The Qwen3-4B-Instruct-2507 model is a 4.0B parameter causal language model with significant improvements in instruction following, reasoning, multilingualism, and long-context understanding, offering superior performance and alignment compared to previous versions."
  },
  {
    "model_name": "Magistral-Small-2506-GGUF",
    "developer": "unsloth",
    "downloads": 51146,
    "createdAt": "2025-06-10T07:31:42Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Magistral-Small-2506-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "The Magistral-Small-2506 model is a 24B parameter, multilingual reasoning model built on Mistral Small 3.1, offering efficient performance with Apache 2.0 license and supporting deployment via frameworks like llama.cpp, vLLM, and Oll",
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 50309,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B model with mixture of experts for creative writing, fiction, and roleplay, capable of generating vivid, uncensored, and genre-"
  },
  {
    "model_name": "gemma-3n-E4B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 47100,
    "createdAt": "2025-06-26T15:10:43Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E4B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-12b-it-GGUF",
    "developer": "unsloth",
    "downloads": 46990,
    "createdAt": "2025-03-12T10:34:12.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 12B model, optimized for performance and memory efficiency with support for GGUF, 4-bit, and 16-bit formats, and available on Hugging Face."
  },
  {
    "model_name": "Qwen3-1.7B-GGUF",
    "developer": "unsloth",
    "downloads": 46582,
    "createdAt": "2025-04-28T12:22:37.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-1.7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K_L.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_M.gguf",
        "file_size": "535.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_S.gguf",
        "file_size": "512.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_M.gguf",
        "file_size": "675.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_XXS.gguf",
        "file_size": "577.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ3_XXS.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q2_K_XL.gguf",
        "file_size": "760.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q3_K_XL.gguf",
        "file_size": "924.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q5_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q6_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q8_K_XL.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/README.md",
    "description": "Qwen3-1.7B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes for optimal performance."
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "unsloth",
    "downloads": 44553,
    "createdAt": "2025-06-20T22:27:21.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized Mistral-3.2 Small 24B model optimized for instruction following, function calling, and vision reasoning, available via vLLM or Transformers with a system prompt for enhanced performance."
  },
  {
    "model_name": "sqlcoder-7b-2",
    "developer": "defog",
    "downloads": 44453,
    "createdAt": "2024-02-05T14:36:51.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "sqlcoder-7b-q5_k_m",
        "path": "https://huggingface.co/defog/sqlcoder-7b-2/resolve/main/sqlcoder-7b-q5_k_m.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/defog/sqlcoder-7b-2/resolve/main/README.md",
    "description": "The SQLCoder-7B-2 model is a large language model capable of generating SQL queries from natural language questions, developed by Defog, Inc., based on CodeLlama-7B, and licensed under CC-by-SA-4.0."
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 41712,
    "createdAt": "2024-09-17T13:57:52Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-1.5b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-fp16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q2_k.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q3_k_m.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf",
        "file_size": "1016.8 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q6_k.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the instruction-tuned 1.5B Qwen2.5 model in GGUF format, offering improved knowledge, coding/math capabilities, long-context support, multilingual support, and various quantization options.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "unsloth",
    "downloads": 38979,
    "createdAt": "2025-06-26T12:24:35.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q2_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q3_K_XL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q6_K_XL.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q8_K_XL.gguf",
        "file_size": "9.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with high performance and accuracy, available on Hugging"
  },
  {
    "model_name": "MiniCPM-V-4_5-gguf",
    "developer": "openbmb",
    "downloads": 38284,
    "createdAt": "2025-08-24T10:41:57.000Z",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "MiniCPM-V-4_5-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/MiniCPM-V-4_5-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Model-8.2B-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/Model-8.2B-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "ggml-model-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "ggml-model-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ggml-model-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/mmproj-model-f16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/README.md",
    "description": "MiniCPM-V 4.5 is a high-performance, efficient, and multilingual vision-language model capable of understanding single images, multiple images, and videos on phones, with state-of-the-art capabilities in OCR, document parsing, and video comprehension."
  },
  {
    "model_name": "SmallThinker-21BA3B-Instruct-GGUF",
    "developer": "PowerInfer",
    "downloads": 38261,
    "createdAt": "2025-07-27T16:10:02Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_K_S.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.F16",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.F16.gguf",
        "file_size": "40.1 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_NL",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_NL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K.imatrix.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K_S.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K_S.imatrix.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.imatrix.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0.powerinfer",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.powerinfer.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_1",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_1.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_1.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_1.imatrix.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K.imatrix.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K_S.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K_S.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K_S.imatrix.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q5_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q5_K.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q5_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q5_K.imatrix.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q6_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q6_K.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q6_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q6_K.imatrix.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q8_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q8_0.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q8_0.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q8_0.imatrix.gguf",
        "file_size": "21.3 GB"
      }
    ],
    "readme": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/README.md",
    "description": "SmallThinker-21BA3B-Instruct-GGUF is a low-latency, on-device MoE language model with 21B parameters, 3B activated parameters, and 52 layers, designed for efficient local deployment using either llama.cpp or PowerInfer frameworks.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-GGUF",
    "developer": "lmstudio-community",
    "downloads": 38137,
    "createdAt": "2025-07-28T09:05:17Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Qwen3-30B-A3B-Instruct-2507 model by Qwen, suitable for text generation tasks.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-8B-GGUF",
    "developer": "Qwen",
    "downloads": 37942,
    "createdAt": "2025-05-03T06:33:59.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "Qwen3-8B-GGUF is a large language model with 8.2B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, multilingual support, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "Qwen3-14B-GGUF",
    "developer": "unsloth",
    "downloads": 36657,
    "createdAt": "2025-04-28T10:01:12.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-14B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-BF16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Qwen3-14B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q2_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Qwen3-14B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen3-14B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ1_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ1_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ2_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ2_XXS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ3_XXS.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q2_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q3_K_XL.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q4_K_XL.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q5_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q6_K_XL.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q8_K_XL.gguf",
        "file_size": "17.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/README.md",
    "description": "Qwen3-14B is a large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes, and it can be fine-tuned or deployed using Hugging Face Transformers and various inference frameworks."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Llama-8B-GGUF",
    "developer": "unsloth",
    "downloads": 36411,
    "createdAt": "2025-01-20T13:04:25.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-F16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q2_K_L.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q3_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q6_K_XL.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q8_K_XL.gguf",
        "file_size": "9.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/README.md",
    "description": "The DeepSeek-R1 model, developed by DeepSeek-AI, is a reasoning model based on Llama3.1 and Qwen2.5, offering improved performance on various benchmarks and available for local use with tools like llama.cpp and vLLM, with detailed instructions for training and deployment"
  },
  {
    "model_name": "neutts-air",
    "developer": "neuphonic",
    "downloads": 35469,
    "createdAt": "2025-09-15T14:30:42.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "neutss-air-BF16",
        "path": "https://huggingface.co/neuphonic/neutts-air/resolve/main/neutss-air-BF16.gguf",
        "file_size": "1.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/neuphonic/neutts-air/resolve/main/README.md",
    "description": "Super-realistic on-device text-to-speech model with instant voice cloning capability."
  },
  {
    "model_name": "Qwen3-4B-GGUF",
    "developer": "unsloth",
    "downloads": 34010,
    "createdAt": "2025-04-28T07:55:09.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B is a 4.0B parameter large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes, and is available for fine-tuning and deployment via Hugging Face and"
  },
  {
    "model_name": "Qwen3-0.6B-GGUF",
    "developer": "unsloth",
    "downloads": 33936,
    "createdAt": "2025-04-28T10:24:13Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-0.6B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-0.6B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-IQ4_NL.gguf",
        "file_size": "363.9 MB"
      },
      {
        "model_id": "Qwen3-0.6B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-IQ4_XS.gguf",
        "file_size": "350.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K_L.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q3_K_M.gguf",
        "file_size": "331.0 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q3_K_S.gguf",
        "file_size": "308.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_0.gguf",
        "file_size": "364.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_1.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_M.gguf",
        "file_size": "378.3 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_S.gguf",
        "file_size": "365.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_M.gguf",
        "file_size": "423.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_S.gguf",
        "file_size": "416.4 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q6_K.gguf",
        "file_size": "472.2 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf",
        "file_size": "609.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ1_M.gguf",
        "file_size": "210.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ1_S.gguf",
        "file_size": "204.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ2_M.gguf",
        "file_size": "256.3 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ2_XXS.gguf",
        "file_size": "223.2 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ3_XXS.gguf",
        "file_size": "269.0 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q2_K_XL.gguf",
        "file_size": "287.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q3_K_XL.gguf",
        "file_size": "340.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q4_K_XL.gguf",
        "file_size": "386.6 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q5_K_XL.gguf",
        "file_size": "425.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q6_K_XL.gguf",
        "file_size": "549.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q8_K_XL.gguf",
        "file_size": "805.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/README.md",
    "description": "Qwen3-0.6B is a 0.6B parameter large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-GGUF",
    "developer": "unsloth",
    "downloads": 33717,
    "createdAt": "2025-08-06T19:14:20.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "Qwen3-4B-Thinking-2507-F16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "imatrix_unsloth",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/imatrix_unsloth.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "Qwen3‑4B‑Thinking‑2507 is a 4 B‑parameter causal LLM optimized for deep reasoning with a native 262 k‑token context and built‑in thinking mode, runnable via Transformers, SGLang, vLLM, and agent frameworks."
  },
  {
    "model_name": "Devstral-Small-2507-GGUF",
    "developer": "unsloth",
    "downloads": 33488,
    "createdAt": "2025-07-10T13:20:40.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Devstral-Small-2507-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/README.md",
    "description": "This model is a lightweight, open-source agentic LLM for software engineering tasks, fine-tuned from Mistral-Small-3.1 with a 128k context window and Apache 2.0 license, supporting tool calling and optional vision, and optimized for local deployment with various"
  },
  {
    "model_name": "MN-12B-Mag-Mell-R1-GGUF",
    "developer": "mradermacher",
    "downloads": 31891,
    "createdAt": "2024-09-16T19:10:02.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q8_0",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/README.md",
    "description": "Provides multiple GGUF‑quantized versions of the MN‑12B‑Mag‑Mell‑R1 model for download."
  },
  {
    "model_name": "gemma-7b",
    "developer": "google",
    "downloads": 30823,
    "createdAt": "2024-02-08T22:36:43.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b",
        "path": "https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-7b/resolve/main/README.md",
    "description": "The Gemma 7B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with strong performance on benchmark tasks and ethical safety considerations."
  },
  {
    "model_name": "Magistral-Small-2509-GGUF",
    "developer": "unsloth",
    "downloads": 29647,
    "createdAt": "2025-09-17T12:14:20.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Magistral-Small-2509-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2509-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2509-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/README.md",
    "description": "Magistral Small 1.2 is a 24B multilingual, vision‑enabled, chain‑of‑thought LLM (Apache‑2.0) runnable via vLLM/Transformers."
  },
  {
    "model_name": "Phi-3-mini-4k-instruct-gguf",
    "developer": "microsoft",
    "downloads": 27872,
    "createdAt": "2024-04-22T17:02:08.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Phi-3-mini-4k-instruct-fp16",
        "path": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Phi-3-mini-4k-instruct-q4",
        "path": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/README.md",
    "description": "This repo provides the GGUF format for the Phi-3-Mini-4K-Instruct model, a 3.8B parameter, lightweight, state-of-the-art open model trained on high-quality and reasoning dense data, suitable for commercial and research use in English with chat format prompts."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
    "developer": "gabriellarson",
    "downloads": 27557,
    "createdAt": "2025-08-07T06:30:31Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Huihui-gpt-oss-20B-abliterated-F16",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20B-abliterated-F16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the unsloth/gpt-oss-20b-BF16 model, created with abliteration, designed for research and experimental use with potential risks of generating sensitive or inappropriate content."
  },
  {
    "model_name": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "bartowski",
    "downloads": 27315,
    "createdAt": "2025-06-20T19:03:22Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistral-Small-3.2-24B-Instruct-2506 model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "tools": true,
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "Apriel-1.5-15b-Thinker-GGUF",
    "developer": "unsloth",
    "downloads": 27070,
    "createdAt": "2025-10-02T01:48:00.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Apriel-1.5-15b-Thinker-BF16",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-BF16.gguf",
        "file_size": "26.9 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-IQ4_NL.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-IQ4_XS.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q2_K",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q2_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q3_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q3_K_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q4_0",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q4_1",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q4_1.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q4_K_M.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q4_K_S.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q5_K_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q5_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q6_K",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q6_K.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-Q8_0",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-Q8_0.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-IQ1_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-IQ2_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-IQ3_XXS.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-Q2_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-Q3_K_XL.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-Q4_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-Q5_K_XL.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-Q6_K_XL.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Apriel-1.5-15b-Thinker-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/Apriel-1.5-15b-Thinker-UD-Q8_K_XL.gguf",
        "file_size": "16.8 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "831.3 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "829.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Apriel-1.5-15b-Thinker-GGUF/resolve/main/README.md",
    "description": "A 15B multimodal reasoning model competitive with 10x larger models on key benchmarks."
  },
  {
    "model_name": "Gemmasutra-Mini-2B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 26340,
    "createdAt": "2024-08-03T09:55:36.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "Gemmasutra-Mini-2B-v1-BF16",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-BF16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_L",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_4_4",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_4_4.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_4_8",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_4_8.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_8_8",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_8_8.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-f16",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-f16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-f32",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/README.md",
    "description": "The BeaverAI team presents Gemmasutra Mini 2B v1, a compact 2B parameter RP model designed for efficient use across various devices, offering a satisfying conversational experience with 4K context support and multiple quantized versions for different platforms."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-GGUF",
    "developer": "unsloth",
    "downloads": 25025,
    "createdAt": "2025-01-20T14:39:44.000Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-F16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-F16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q2_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-VL-8B-Instruct-GGUF",
    "developer": "NexaAI",
    "downloads": 24833,
    "createdAt": "2025-10-14T05:28:11.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Qwen3-VL-8B-Instruct.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct.F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Instruct.Q4_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Instruct.Q4_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct.Q4_K.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Instruct.Q5_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct.Q5_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct.Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3-VL-8B-Instruct.Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj.F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mmproj.F32",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj.F32.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF",
    "developer": "DavidAU",
    "downloads": 24435,
    "createdAt": "2024-10-28T10:09:51.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m.gguf",
        "file_size": "5.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/README.md",
    "description": "This is a modified Llama 3.2 model designed for creative writing, with enhanced prose generation, uncensored content, and support for multiple genres including science fiction, horror, and romance, offering vivid, detailed, and emotionally charged outputs through advanced parameters and templates."
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "unsloth",
    "downloads": 24413,
    "createdAt": "2025-06-26T12:24:52.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-F16.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ3_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q2_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q4_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q5_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q6_K_XL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q8_K_XL.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with instruction-tuned variants available for fine-tuning"
  },
  {
    "model_name": "Ministral-8B-Instruct-2410-GGUF",
    "developer": "bartowski",
    "downloads": 24364,
    "createdAt": "2024-10-21T16:27:21Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ2_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K_L.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q8_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-f16",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-f16.gguf",
        "file_size": "14.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Mistral-8B-Instruct-2410 model for various inference platforms, with a focus on research use under the Mistral AI Research License, requiring users to agree to specific terms and conditions for non-commercial purposes.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-32B-GGUF",
    "developer": "Qwen",
    "downloads": 23980,
    "createdAt": "2025-05-01T10:23:45.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-32B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/README.md",
    "description": "Qwen3-32B‑GGUF is a 32 B‑parameter quantized LLM with switchable thinking/non‑thinking modes, up to 131 k‑token context, and can be run via llama.cpp or Ollama."
  },
  {
    "model_name": "LFM2-8B-A1B-GGUF",
    "developer": "unsloth",
    "downloads": 23929,
    "createdAt": "2025-10-08T08:06:44.000Z",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "LFM2-8B-A1B-BF16",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-BF16.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q2_K",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_0",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_1",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q6_K",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q6_K.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q8_0",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q8_0.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q2_K_XL.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q3_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q4_K_XL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q6_K_XL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q8_K_XL.gguf",
        "file_size": "8.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "medgemma-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 23406,
    "createdAt": "2025-05-20T19:18:08Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "medgemma-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/README.md",
    "description": "MedGemma is a medical multimodal model trained on diverse medical data, optimized for healthcare applications involving text and images, with variants available for radiology, dermatology, pathology, and more.",
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "Tongyi-DeepResearch-30B-A3B-GGUF",
    "developer": "gabriellarson",
    "downloads": 23173,
    "createdAt": "2025-09-16T22:42:55.000Z",
    "tools": true,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-F16",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-F16.gguf",
        "file_size": "56.9 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-IQ4_NL.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-MXFP4_MOE",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-MXFP4_MOE.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q5_0.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Tongyi-DeepResearch-30B-A3B-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/Tongyi-DeepResearch-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF/resolve/main/README.md",
    "description": "Tongyi‑DeepResearch is a 30 B‑parameter, 3 B‑active‑per‑token LLM optimized for long‑horizon, deep information‑seeking tasks via automated synthetic data, continual agentic pre‑training, on‑policy RL, and compatible with ReAct and Heavy inference modes."
  },
  {
    "model_name": "L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 22856,
    "createdAt": "2024-06-05T18:21:00.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "test",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/test.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a quantized version of the Sao10K/L3-8B-Stheno-v3.2 model for roleplay and sillytavern tasks, optimized for performance with specific quantization settings and recommended samplers."
  },
  {
    "model_name": "NemoMix-Unleashed-12B-GGUF",
    "developer": "bartowski",
    "downloads": 22785,
    "createdAt": "2024-08-22T04:03:49.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "NemoMix-Unleashed-12B-IQ2_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ3_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q2_K",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q2_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_XL.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q6_K",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q8_0",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-f16",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-f16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the NemoMix-Unleashed-12B model for efficient text generation on various platforms, with recommendations based on system resources and performance needs."
  },
  {
    "model_name": "Qwen3-VL-4B-Instruct-GGUF",
    "developer": "NexaAI",
    "downloads": 22690,
    "createdAt": "2025-10-14T05:00:28.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Qwen3-VL-4B-Instruct.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Instruct.Q4_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct.Q4_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Instruct.Q4_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct.Q4_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Instruct.Q5_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct.Q5_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Instruct.Q6_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Instruct.Q8_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj.F16.gguf",
        "file_size": "797.5 MB"
      },
      {
        "model_id": "mmproj.F32",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj.F32.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mmproj.Q8_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj.Q8_0.gguf",
        "file_size": "428.5 MB"
      }
    ],
    "readme": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
    "developer": "unsloth",
    "downloads": 21837,
    "createdAt": "2025-01-20T13:47:45Z",
    "tools": true,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-BF16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf",
        "file_size": "770.2 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_M.gguf",
        "file_size": "683.1 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_S.gguf",
        "file_size": "667.7 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_M.gguf",
        "file_size": "755.5 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_XXS.gguf",
        "file_size": "739.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ3_XXS.gguf",
        "file_size": "841.2 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ4_XS.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q2_K_XL.gguf",
        "file_size": "844.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q3_K_XL.gguf",
        "file_size": "923.4 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/README.md",
    "description": "This model is a 1.5B parameter version of DeepSeek-R1, fine-tuned for faster inference with 70% less memory using Unsloth, and can be run with llama.cpp for efficient performance.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 21814,
    "createdAt": "2025-08-10T01:10:17Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEO-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEO-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEO-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEO-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEO-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEO-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOCODE-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOCODE-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOCODE-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOCODE-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOCODE-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOCODE-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRR-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRR-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRR-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRR-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRRCODE-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRRCODE-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRRCODE-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRRCODE-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRR-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRR-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRR-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRR-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-Q5_1.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "This model is a specialized, moderate uncensored version of the OpenAI 20B MOE (Mixture of Experts) GPT-oss model, optimized for creative tasks with support for 128k context, multiple experts, and various quantization types like IQ4_NL,"
  },
  {
    "model_name": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF",
    "developer": "bartowski",
    "downloads": 21602,
    "createdAt": "2025-08-29T14:12:23.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ2_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ2_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ2_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ3_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ3_XS.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ3_XXS.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ4_NL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q2_K.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q2_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_S.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q3_K_XL.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_1.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_L.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_M.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_S.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_L.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_S.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q6_K.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q6_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q8_0.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-bf16.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-9B-v2-imatrix",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-9B-v2-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the NVIDIA-Nemotron-Nano-9B-v2 model using llama.cpp's imatrix calibration method, optimized for various hardware platforms and quantization types like Q6_K_L, Q5_K_M, IQ4_XS, etc., with recommended options for"
  },
  {
    "model_name": "Llama-OuteTTS-1.0-1B-GGUF",
    "developer": "OuteAI",
    "downloads": 21590,
    "createdAt": "2025-04-06T19:18:26.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Llama-OuteTTS-1.0-1B-FP16",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-FP16.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q2_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q2_K.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_L",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_L.gguf",
        "file_size": "708.6 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_M.gguf",
        "file_size": "668.8 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_S.gguf",
        "file_size": "622.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_0.gguf",
        "file_size": "745.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_1.gguf",
        "file_size": "803.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_M.gguf",
        "file_size": "780.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_S.gguf",
        "file_size": "749.7 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_0.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_1.gguf",
        "file_size": "919.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_M.gguf",
        "file_size": "879.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_S.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q6_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q6_K.gguf",
        "file_size": "984.5 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q8_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/README.md",
    "description": "OuteTTS 1.0 is a multilingual, one‑shot voice‑cloning TTS model (based on Llama‑3.2‑1B with a DAC encoder) that auto‑aligns raw text, supports many languages, and requires the specific sampling settings temperature 0.4, repetition‑penalty 1.1 over a 64‑token window (plus top‑k 40, top‑p 0.9, min‑p 0.05)."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2",
    "developer": "BasedBase",
    "downloads": 21037,
    "createdAt": "2025-08-11T07:56:50.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-Coder-480B-Distill-v2-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-30B-A3B-Instruct-Coder-480B-Distill-v2-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q2_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q3_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_0.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q6_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q6_K.gguf",
        "file_size": "23.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/README.md",
    "description": "This model is a SVD-based distilled version of Qwen3-Coder-480B for enhanced code generation capabilities with a specialized focus on programming tasks."
  },
  {
    "model_name": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 20770,
    "createdAt": "2025-08-07T03:36:06.000Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XXS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XXS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_NL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_NL.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_L.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_1",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K_L.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q8_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-bf16",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-bf16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-imatrix",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gpt-oss-20b-BF16-abliterated model by huihui-ai, optimized for text generation using llama.cpp with various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated",
    "developer": "huihui-ai",
    "downloads": 20311,
    "createdAt": "2025-08-06T15:29:56.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GGUF/ggml-model-Q3_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GGUF/ggml-model-Q4_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GGUF/ggml-model-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-f16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "GGUF/ggml-model-q8_0",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/README.md",
    "description": "This is an uncensored version of the unsloth/gpt-oss-20b-BF16 model, optimized for GGUF format and suitable for research or experimental use with potential risks of generating sensitive or inappropriate content."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 19429,
    "createdAt": "2025-01-21T20:59:59.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q6_K.gguf",
        "file_size": "5.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the DeepSeek-R1-Distill-Qwen-7B-Uncensored model, offering various GGUF quantization options for different quality and performance trade-offs."
  },
  {
    "model_name": "MiniCPM-V-4-gguf",
    "developer": "openbmb",
    "downloads": 19153,
    "createdAt": "2025-07-12T09:45:29Z",
    "tools": false,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_1.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-iOS",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16-iOS.gguf",
        "file_size": "128.8 MB"
      },
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16.gguf",
        "file_size": "914.4 MB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/README.md",
    "description": "The MiniCPM-V 4.0 is a high-performance, efficient, and versatile multimodal large language model capable of understanding and generating text from single images, multiple images, and videos on mobile devices, with 4.1B parameters and superior performance compared to other models in various benchmarks."
  },
  {
    "model_name": "granite-4.0-h-tiny-GGUF",
    "developer": "unsloth",
    "downloads": 18877,
    "createdAt": "2025-10-02T10:53:43.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "granite-4.0-h-tiny-BF16",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-BF16.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-IQ4_NL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-IQ4_NL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-IQ4_XS",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-IQ4_XS.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q2_K",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q2_K_L",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q2_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q3_K_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q3_K_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_0",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_0.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_1",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_1.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_K_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_K_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_K_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q5_K_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q5_K_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q6_K",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q8_0",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q8_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-IQ2_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-Q2_K_XL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-Q3_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-Q4_K_XL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-Q5_K_XL.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-Q6_K_XL.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-Q8_K_XL.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-UD-TQ1_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/granite-4.0-h-tiny-GGUF/resolve/main/README.md",
    "description": "Granite-4.0-H-Tiny is a 7B multilingual instruction-tuned language model with enhanced tool-calling capabilities."
  },
  {
    "model_name": "Qwen2.5-7B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 18781,
    "createdAt": "2024-09-16T14:13:33.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "imatrix quantized Qwen2.5-7B-Instruct GGUF versions for efficient inference."
  },
  {
    "model_name": "Nous-Hermes-2-Mistral-7B-DPO-GGUF",
    "developer": "NousResearch",
    "downloads": 18070,
    "createdAt": "2024-02-20T06:25:05Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q2_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q6_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q8_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/README.md",
    "description": "Nous-Hermes 2 Mistral 7B DPO is a 7B parameter model fine-tuned with DPO from Teknium/OpenHermes-2.5-Mistral-7B, showing improved performance on AGIEval, BigBench, GPT4"
  },
  {
    "model_name": "Cydonia-24B-v4.1-GGUF",
    "developer": "TheDrummer",
    "downloads": 17394,
    "createdAt": "2025-08-17T10:23:58Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4j-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/README.md",
    "description": "Cydonia 24B v4.1 is a high-quality, creative, and well-written large language model that has surpassed previous versions in performance and prose capabilities."
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-GGUF",
    "developer": "dphn",
    "downloads": 17097,
    "createdAt": "2025-01-02T22:11:05.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-F16",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q2_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_L",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q6_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q8_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/README.md",
    "description": "Dolphin 3.0 Llama 3.1 8B is a general-purpose instruct-tuned model designed for coding, math, agentic, and function calling tasks, offering full control to the user through customizable system prompts and data ownership."
  },
  {
    "model_name": "TheDrummer_Cydonia-24B-v4.1-GGUF",
    "developer": "bartowski",
    "downloads": 16889,
    "createdAt": "2025-08-18T16:31:24.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Cydonia-24B-v4.1 model using llama.cpp's imatrix method, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "Qwen",
    "downloads": 16301,
    "createdAt": "2025-05-05T08:38:52Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "Qwen3-30B-A3B-GGUF is a large language model with 30.5B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning, multilingual capabilities, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 16276,
    "createdAt": "2025-08-02T05:35:18Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Instruct-2507 model, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "MN-Violet-Lotus-12B-GGUF",
    "developer": "mradermacher",
    "downloads": 16262,
    "createdAt": "2024-11-17T05:53:19.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "MN-Violet-Lotus-12B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_0_4_4",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_0_4_4.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q8_0",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the MN-Violet-Lotus-12B model, optimized for storywriting, text adventure, and creative writing tasks, available in various quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Instruct-GGUF",
    "developer": "AesSedai",
    "downloads": 16136,
    "createdAt": "2025-10-07T19:41:59.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Qwen3-Next-80B-A3B-Instruct-Q2_K_M",
        "path": "https://huggingface.co/AesSedai/Qwen3-Next-80B-A3B-Instruct-GGUF/resolve/main/Qwen3-Next-80B-A3B-Instruct-Q2_K_M.gguf",
        "file_size": "27.1 GB"
      },
      {
        "model_id": "Qwen3-Next-80B-A3B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/AesSedai/Qwen3-Next-80B-A3B-Instruct-GGUF/resolve/main/Qwen3-Next-80B-A3B-Instruct-Q3_K_M.gguf",
        "file_size": "35.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AesSedai/Qwen3-Next-80B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 15992,
    "createdAt": "2025-09-12T15:19:57.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF",
    "developer": "DavidAU",
    "downloads": 15724,
    "createdAt": "2024-10-25T00:19:23.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a highly advanced, uncensored Gemma2 model designed for creative writing, particularly fiction and storytelling, combining four top storytelling models with additional modifications for vivid, detailed, and emotionally engaging prose across various genres, including science fiction, horror, and romance, with options for different quantization levels and"
  },
  {
    "model_name": "MistralRP-Noromaid-NSFW-Mistral-7B-GGUF",
    "developer": "Ttimofeyka",
    "downloads": 15581,
    "createdAt": "2024-02-07T14:19:28.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "MistralRP-Noromaid-NSFW-7B-Q4_0",
        "path": "https://huggingface.co/Ttimofeyka/MistralRP-Noromaid-NSFW-Mistral-7B-GGUF/resolve/main/MistralRP-Noromaid-NSFW-7B-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "MistralRP-Noromaid-NSFW-7B-Q5_K_M",
        "path": "https://huggingface.co/Ttimofeyka/MistralRP-Noromaid-NSFW-Mistral-7B-GGUF/resolve/main/MistralRP-Noromaid-NSFW-7B-Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "MistralRP-Noromaid-NSFW-7B-Q8_0",
        "path": "https://huggingface.co/Ttimofeyka/MistralRP-Noromaid-NSFW-Mistral-7B-GGUF/resolve/main/MistralRP-Noromaid-NSFW-7B-Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ttimofeyka/MistralRP-Noromaid-NSFW-Mistral-7B-GGUF/resolve/main/README.md",
    "description": "This is a merge model of the Undi95/Mistral-RP-0.1-7B and MaziyarPanahi/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1 models using the SLERP merge method"
  },
  {
    "model_name": "LFM2-1.2B-GGUF",
    "developer": "LiquidAI",
    "downloads": 15027,
    "createdAt": "2025-07-12T12:01:44.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-1.2B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-1.2B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/README.md",
    "description": "LFM2-1.2B-GGUF is a hybrid model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "Qwen3-VL-30B-A3B-Thinking-GGUF",
    "developer": "yairpatch",
    "downloads": 15017,
    "createdAt": "2025-10-05T05:23:36.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-VL-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/resolve/main/Qwen3-VL-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen3-VL-30B-A3B-F16",
        "path": "https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/resolve/main/mmproj-Qwen3-VL-30B-A3B-F16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "readme": "https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/resolve/main/README.md",
    "description": "Qwen3-VL-30B-A3B-Thinking is the most powerful vision-language model with 256K context and advanced visual reasoning."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF",
    "developer": "mradermacher",
    "downloads": 14956,
    "createdAt": "2025-07-23T06:06:13.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/README.md",
    "description": "This is a multi-modal visual language model (VLm) for image captioning and text generation, quantized in various formats including GGUF, available in English, Chinese, and Thai."
  },
  {
    "model_name": "Jan-nano-128k-GGUF",
    "developer": "unsloth",
    "downloads": 14760,
    "createdAt": "2025-06-25T07:31:40Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-128k-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling efficient processing of long documents and complex multi-turn conversations without performance degradation.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Hunyuan-MT-7B-GGUF",
    "developer": "mradermacher",
    "downloads": 14686,
    "createdAt": "2025-09-02T08:08:48.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Hunyuan-MT-7B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.Q8_0.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Hunyuan-MT-7B.f16",
        "path": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/Hunyuan-MT-7B.f16.gguf",
        "file_size": "14.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Hunyuan-MT-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Tencent Hunyuan-MT-7B model for translation tasks, available in various quantization formats including Q2_K, Q3_K_S, Q4_K_S, Q8_0, and f16."
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill",
    "developer": "BasedBase",
    "downloads": 14361,
    "createdAt": "2025-08-26T08:50:43.000Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q3_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_0.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q5_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q6_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-model-Q2_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-model-Q2_K.gguf",
        "file_size": "10.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/README.md",
    "description": "This model is a distilled version of Qwen3-30B-A3B-Thinking-2507, inheriting reasoning and behavioral traits from the larger DeepSeek-V3.1 teacher model through a high-rank LoRA distillation process with MoE layer synthesis."
  },
  {
    "model_name": "SmolLM3-3B-GGUF",
    "developer": "ggml-org",
    "downloads": 13891,
    "createdAt": "2025-07-08T12:36:39Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "SmolLM3-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SmolLM3-f16",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-f16.gguf",
        "file_size": "5.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/README.md",
    "description": "SmolLM3-GGUF is a 3B parameter multilingual language model with advanced reasoning, long context support, and open-source licensing, offering strong performance across various benchmarks and supported by frameworks like transformers and llama.cpp.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen-Image-Edit-Rapid-AIO-GGUF",
    "developer": "Phil2Sat",
    "downloads": 13790,
    "createdAt": "2025-10-16T11:32:49.000Z",
    "tools": true,
    "num_quants": 30,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated/Qwen2.5-VL-7B-Instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated/Qwen2.5-VL-7B-Instruct-abliterated.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated/Qwen2.5-VL-7B-Instruct-abliterated.mmproj-Q8_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated/Qwen2.5-VL-7B-Instruct-abliterated.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-F16",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-F16.gguf",
        "file_size": "38.1 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q2_K",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q2_K.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q3_K_M",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q3_K_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q3_K_S",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q3_K_S.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q4_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q4_1",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q4_1.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q4_K_M",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q4_K_S",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q4_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q5_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q5_0.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q5_1",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q5_1.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q5_K_M",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q5_K_M.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q5_K_S",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q5_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q6_K",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q6_K.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "v50/qwen-rapid-nsfw-v5.0-Q8_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v50/qwen-rapid-nsfw-v5.0-Q8_0.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-F16",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-F16.gguf",
        "file_size": "38.1 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q2_K",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q2_K.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q3_K_M",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q3_K_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q3_K_S",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q3_K_S.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q4_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q4_1",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q4_1.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q4_K_M",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q4_K_S",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q4_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q5_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q5_0.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q5_1",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q5_1.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q5_K_M",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q5_K_M.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q5_K_S",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q5_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q6_K",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q6_K.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "v52/qwen-rapid-nsfw-v5.2-Q8_0",
        "path": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/v52/qwen-rapid-nsfw-v5.2-Q8_0.gguf",
        "file_size": "20.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Phil2Sat/Qwen-Image-Edit-Rapid-AIO-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 13583,
    "createdAt": "2024-10-26T00:23:07Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ3_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q2_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q2_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_4_4.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_4_8.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_8_8.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q8_0.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-f16.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct-uncensored model using llama.cpp's imatrix method, optimized for various hardware platforms with different quality and performance trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-270m-it-qat-GGUF",
    "developer": "unsloth",
    "downloads": 13581,
    "createdAt": "2025-08-14T11:08:31Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3-270m-it-qat-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-F16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-IQ4_NL.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-IQ4_XS.gguf",
        "file_size": "229.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q2_K.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q2_K_L.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q3_K_M.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q3_K_S.gguf",
        "file_size": "225.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_0.gguf",
        "file_size": "380.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_1.gguf",
        "file_size": "236.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_K_M.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_K_S.gguf",
        "file_size": "238.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q5_K_M.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q5_K_S.gguf",
        "file_size": "246.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q6_K.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q8_0.gguf",
        "file_size": "428.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-IQ2_M.gguf",
        "file_size": "174.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-IQ2_XXS.gguf",
        "file_size": "171.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-IQ3_XXS.gguf",
        "file_size": "176.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q2_K_XL.gguf",
        "file_size": "226.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q3_K_XL.gguf",
        "file_size": "231.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q4_K_XL.gguf",
        "file_size": "242.5 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q5_K_XL.gguf",
        "file_size": "248.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q6_K_XL.gguf",
        "file_size": "272.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q8_K_XL.gguf",
        "file_size": "449.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Gemma 3 (270M) model by Unsloth, optimized for text generation with specific settings and available in a collection of various Gemma 3 versions including GGUF, 4-bit, and 16-bit formats."
  },
  {
    "model_name": "neutts-air-q4-gguf",
    "developer": "neuphonic",
    "downloads": 13456,
    "createdAt": "2025-09-23T13:41:47.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "neutts-air-Q4-0",
        "path": "https://huggingface.co/neuphonic/neutts-air-q4-gguf/resolve/main/neutts-air-Q4-0.gguf",
        "file_size": "502.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/neuphonic/neutts-air-q4-gguf/resolve/main/README.md",
    "description": "Q4-GGUF on-device TTS enabling instant 3-second voice cloning."
  },
  {
    "model_name": "Qwen3-VL-8B-Thinking-GGUF",
    "developer": "NexaAI",
    "downloads": 13372,
    "createdAt": "2025-10-14T05:28:39.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Qwen3-VL-8B-Thinking.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/Qwen3-VL-8B-Thinking.F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Thinking.Q4_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/Qwen3-VL-8B-Thinking.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Thinking.Q4_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/Qwen3-VL-8B-Thinking.Q4_K.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Thinking.Q5_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/Qwen3-VL-8B-Thinking.Q5_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Thinking.Q6_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/Qwen3-VL-8B-Thinking.Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-VL-8B-Thinking.Q8_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/Qwen3-VL-8B-Thinking.Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/mmproj.F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mmproj.F32",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/mmproj.F32.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/NexaAI/Qwen3-VL-8B-Thinking-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2",
    "developer": "BasedBase",
    "downloads": 13206,
    "createdAt": "2025-09-14T01:11:12.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q2_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q3_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q3_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q4_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q4_0.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q4_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q4_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q5_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q5_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q6_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2/resolve/main/README.md",
    "description": "A high‑fidelity SVD‑distilled, LoRA‑merged version of Qwen3‑30B‑A3B‑Thinking‑2507 that inherits DeepSeek‑V3.1 reasoning with improved chain‑of‑thought."
  },
  {
    "model_name": "SmallThinker-4BA0.6B-Instruct-GGUF",
    "developer": "PowerInfer",
    "downloads": 13010,
    "createdAt": "2025-07-27T16:09:24Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.F16",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.F16.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.IQ4_NL",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.IQ4_NL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.IQ4_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q3_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q3_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_0.powerinfer",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_0.powerinfer.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_1",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q5_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q5_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q6_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q6_K.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q8_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q8_0.gguf",
        "file_size": "4.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/README.md",
    "description": "SmallThinker-4BA0.6B-Instruct-GGUF is a low-latency, on-device Mixture-of-Experts language model designed for efficient local deployment with support for both llama.cpp and PowerInfer frameworks."
  },
  {
    "model_name": "TheDrummer_Cydonia-24B-v4.2.0-GGUF",
    "developer": "bartowski",
    "downloads": 12971,
    "createdAt": "2025-10-12T18:42:08.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.2.0-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.2.0-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF",
    "developer": "bartowski",
    "downloads": 12567,
    "createdAt": "2024-09-26T14:36:19Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_4.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_8.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_8_8.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K_L.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q8_0",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q8_0.gguf",
        "file_size": "22.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-f16",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-f16.gguf",
        "file_size": "41.4 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the Mistral-Small-22B-ArliAI-RPMax-v1.1 model for text generation, with options ranging from high-quality Q8_0 to low-quality IQ2_S, optimized for different hardware and performance needs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF",
    "developer": "QuantFactory",
    "downloads": 12472,
    "createdAt": "2024-07-28T07:02:48.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/README.md",
    "description": "The QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF is a quantized, uncensored, and multilingual version of the Meta Llama 3.1 8B model, optimized for roleplay, instruction"
  },
  {
    "model_name": "TheDrummer_Cydonia-24B-v4-GGUF",
    "developer": "bartowski",
    "downloads": 12454,
    "createdAt": "2025-07-18T17:30:25Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Cydonia-24B-v4 model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-Coder-32B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 12273,
    "createdAt": "2024-11-06T19:20:14Z",
    "tools": true,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_4.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_8_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen2.5-Coder-32B-Instruct model using llama.cpp's imatrix method, optimized for various hardware platforms and inference speeds, with recommended options marked for best performance and quality.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "google_gemma-3n-E4B-it-GGUF",
    "developer": "bartowski",
    "downloads": 12158,
    "createdAt": "2025-06-26T19:50:49Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 12148,
    "createdAt": "2025-08-21T21:46:16.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ1_M.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ1_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_S.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ2_XXS.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_S.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_XS.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ3_XXS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-IQ4_XS.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q2_K.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q2_K_S.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q3_K_L.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q3_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q3_K_S.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_0.gguf",
        "file_size": "22.4 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_1.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_K_M.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q4_K_S.gguf",
        "file_size": "22.5 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q5_K_M.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q5_K_S.gguf",
        "file_size": "27.2 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.i1-Q6_K.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER.imatrix.gguf",
        "file_size": "162.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF/resolve/main/README.md",
    "description": "Quantized Qwen3-42B-A3B MoE code generation model with 256k context."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 12134,
    "createdAt": "2025-08-08T13:20:46Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_M.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ4_XS.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_L.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_1.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gpt-oss-20b-BF16 model, offering various quantization types for efficient deployment on different hardware platforms."
  },
  {
    "model_name": "Llama-3-8B-Lexi-Uncensored-GGUF",
    "developer": "Orenguteng",
    "downloads": 12013,
    "createdAt": "2024-04-23T21:57:52Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_F16",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q4_K_M",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q5_K_M",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q8_0",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/README.md",
    "description": "This model is based on Llama-3-8b-Instruct and is uncensored, governed by Meta's Llama-3 Community License Agreement, requiring responsible use and alignment layer implementation for compliance."
  },
  {
    "model_name": "K2-Think-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 12011,
    "createdAt": "2025-09-10T07:43:35.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "K2-Think.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ1_M.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ1_S.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ3_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "K2-Think.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "K2-Think.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "K2-Think.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q2_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "K2-Think.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "K2-Think.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "K2-Think.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "K2-Think.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "K2-Think.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "K2-Think.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "K2-Think.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "K2-Think.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "K2-Think.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "K2-Think.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.i1-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "K2-Think.imatrix",
        "path": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/K2-Think.imatrix.gguf",
        "file_size": "14.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/K2-Think-i1-GGUF/resolve/main/README.md",
    "description": "A collection of downloadable GGUF quantizations for the K2‑Think LLM, with usage links and details."
  },
  {
    "model_name": "Qwen2.5-14B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 11947,
    "createdAt": "2024-09-17T19:29:04.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-14B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/README.md",
    "description": "Multiple llama.cpp imatrix‑quantized GGUF files of Qwen2.5‑14B‑Instruct are provided with download links, prompt format, and usage recommendations."
  },
  {
    "model_name": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF",
    "developer": "bartowski",
    "downloads": 11895,
    "createdAt": "2025-05-23T18:09:44Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the PocketDoc/Dans-PersonalityEngine-V1.3.0-24b model for efficient text generation on various hardware, including CPUs and GPUs, with options for different quantization levels and formats like Q6_K_L, Q4_K_M,",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Huihui-MiniCPM-V-4_5-abliterated",
    "developer": "huihui-ai",
    "downloads": 11768,
    "createdAt": "2025-08-31T08:41:52.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GGUF/ggml-model-Q4_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-MiniCPM-V-4_5-abliterated/resolve/main/GGUF/ggml-model-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "GGUF/ggml-model-Q6_K",
        "path": "https://huggingface.co/huihui-ai/Huihui-MiniCPM-V-4_5-abliterated/resolve/main/GGUF/ggml-model-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "GGUF/ggml-model-Q8_0",
        "path": "https://huggingface.co/huihui-ai/Huihui-MiniCPM-V-4_5-abliterated/resolve/main/GGUF/ggml-model-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "GGUF/ggml-model-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-MiniCPM-V-4_5-abliterated/resolve/main/GGUF/ggml-model-f16.gguf",
        "file_size": "15.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "GGUF/mmproj-model-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-MiniCPM-V-4_5-abliterated/resolve/main/GGUF/mmproj-model-f16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "readme": "https://huggingface.co/huihui-ai/Huihui-MiniCPM-V-4_5-abliterated/resolve/main/README.md",
    "description": "This is an uncensored, multi-language, vision-based model derived from MiniCPM-V-4_5, capable of processing images and text without safety filtering, suitable for research and controlled environments."
  },
  {
    "model_name": "Magistral-Small-2507-GGUF",
    "developer": "unsloth",
    "downloads": 11751,
    "createdAt": "2025-07-24T21:29:04Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Magistral-Small-2507-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/README.md",
    "description": "Magistral Small 1.1 is a multilingual, efficient reasoning model with 24B parameters, supporting dozens of languages and offering SOTA performance in model quantization, optimized for deployment on a single RTX 4090 or 32GB RAM MacBook.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-24B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 11660,
    "createdAt": "2025-07-01T19:33:48Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4h-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/README.md",
    "description": "Cydonia 24B v4 is a wordy and thick model with a novel style, excelling at long-form storytelling and descriptive tasks, and is dedicated to the memory of SleepDeprived, a beloved community member.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NousResearch_Hermes-4-14B-GGUF",
    "developer": "bartowski",
    "downloads": 11497,
    "createdAt": "2025-09-03T01:31:14.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ2_M",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ2_S",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ2_XS.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ3_M",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q2_K",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q4_0",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q4_1",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q6_K",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-Q8_0",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-bf16",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-bf16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "NousResearch_Hermes-4-14B-imatrix",
        "path": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/NousResearch_Hermes-4-14B-imatrix.gguf",
        "file_size": "7.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/NousResearch_Hermes-4-14B-GGUF/resolve/main/README.md",
    "description": "Various llama.cpp imatrix quantizations of the Hermes‑4‑14B model with download links, usage instructions, and recommended selections."
  },
  {
    "model_name": "Qwen3-Omni-30B-A3B-Thinking-GGUF-INT8FP16",
    "developer": "vito95311",
    "downloads": 11123,
    "createdAt": "2025-09-24T11:31:52.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "qwen3_omni_f16",
        "path": "https://huggingface.co/vito95311/Qwen3-Omni-30B-A3B-Thinking-GGUF-INT8FP16/resolve/main/qwen3_omni_f16.gguf",
        "file_size": "30.5 GB"
      },
      {
        "model_id": "qwen3_omni_quantized",
        "path": "https://huggingface.co/vito95311/Qwen3-Omni-30B-A3B-Thinking-GGUF-INT8FP16/resolve/main/qwen3_omni_quantized.gguf",
        "file_size": "30.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/vito95311/Qwen3-Omni-30B-A3B-Thinking-GGUF-INT8FP16/resolve/main/README.md",
    "description": "GGUF-quantized Qwen3-Omni 30B model for text generation via Ollama and llama.cpp."
  },
  {
    "model_name": "Satyr-V0.1-4B",
    "developer": "PantheonUnbound",
    "downloads": 10986,
    "createdAt": "2025-10-12T13:10:06.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Satyr-V0.1-4B-F16",
        "path": "https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B/resolve/main/Satyr-V0.1-4B-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Satyr-V0.1-4B-Q4_K_M",
        "path": "https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B/resolve/main/Satyr-V0.1-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Satyr-V0.1-4B-Q5_K_M",
        "path": "https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B/resolve/main/Satyr-V0.1-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Satyr-V0.1-4B-Q8_0",
        "path": "https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B/resolve/main/Satyr-V0.1-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen2.5-Omni-7B-GGUF",
    "developer": "unsloth",
    "downloads": 10844,
    "createdAt": "2025-05-28T19:15:45.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen2.5-Omni-7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "4.9 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/README.md",
    "description": "Qwen2.5-Omni is a state-of-the-art end-to-end multimodal model that excels in text, image, audio, and video understanding and generation, outperforming other models in various benchmarks and tasks, with support for audio output and customizable voice types."
  },
  {
    "model_name": "gemma-3-12b-it-qat-GGUF",
    "developer": "unsloth",
    "downloads": 10763,
    "createdAt": "2025-04-21T03:55:27Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-qat-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/README.md",
    "description": "This repository contains the unquantized 12B instruction-tuned Gemma 3 model from Google, suitable for text and image generation tasks with a 128K context window and open weights, but requires acknowledgment of Google's usage license for access.",
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "MN-DARKEST-UNIVERSE-29B-GGUF",
    "developer": "DavidAU",
    "downloads": 10717,
    "createdAt": "2024-10-31T06:15:13.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-IQ4_XS.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q2_k.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q3_k_l.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q3_k_m.gguf",
        "file_size": "13.2 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q3_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q4_0_4_4.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q4_0_4_8.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q4_0_8_8.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q4_k_m.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q4_k_s.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q5_k_s.gguf",
        "file_size": "18.8 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q6_k.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-Q8_0.gguf",
        "file_size": "28.9 GB"
      },
      {
        "model_id": "MN-DARKEST-UNIVERSE-29B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-DARKEST-UNIVERSE-29B-D_AU-q5_k_m.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-IQ4_XS.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q2_k.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q3_k_l.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q3_k_m.gguf",
        "file_size": "13.2 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q3_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q4_k_m.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q4_k_s.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q5_k_s.gguf",
        "file_size": "18.8 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q6_k.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-Q8_0.gguf",
        "file_size": "28.9 GB"
      },
      {
        "model_id": "MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/MN-V1.1-DARKEST-UNIVERSE-29B-D_AU-q5_k_m.gguf",
        "file_size": "19.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF/resolve/main/README.md",
    "description": "MN-DARKEST-UNIVERSE-29B-GGUF is a powerful Mistral Nemo model designed for vivid, creative writing across all genres, featuring enhanced detail, nuance, and storytelling capabilities, built by merging four top models with the Brainstorm 40x method"
  },
  {
    "model_name": "Qwen2.5-VL-3B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 10455,
    "createdAt": "2025-05-11T12:12:08.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-BF16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q2_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-IQ1_M.gguf",
        "file_size": "840.8 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-IQ1_S.gguf",
        "file_size": "788.1 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "925.7 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "3.7 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "phi-4-gguf",
    "developer": "microsoft",
    "downloads": 10412,
    "createdAt": "2025-01-08T19:55:31Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "phi-4-IQ2_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ2_M.gguf",
        "file_size": "561.6 MB"
      },
      {
        "model_id": "phi-4-IQ3_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "phi-4-IQ3_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-IQ3_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "phi-4-IQ3_XXS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XXS.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "phi-4-IQ4_NL",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_NL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-IQ4_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_XS.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "phi-4-Q2_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "phi-4-Q3_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "phi-4-Q3_K_L",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "phi-4-Q3_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-Q4_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "phi-4-Q4_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "phi-4-Q4_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "phi-4-Q4_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-Q5_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_0.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q5_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_1.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "phi-4-Q5_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "phi-4-Q5_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q6_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "phi-4-Q8_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "phi-4-TQ1_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ1_0.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "phi-4-TQ2_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ2_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "phi-4-bf16",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-bf16.gguf",
        "file_size": "27.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/README.md",
    "description": "The Phi-4 model is a 14B parameter, English-focused, chat-oriented, decoder-only Transformer model trained on high-quality synthetic and academic data, designed for reasoning, code generation, and factual responses in memory-constrained environments."
  },
  {
    "model_name": "Nanonets-OCR-s-GGUF",
    "developer": "unsloth",
    "downloads": 10396,
    "createdAt": "2025-06-16T10:35:11.000Z",
    "tools": false,
    "num_quants": 0,
    "quants": [],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/README.md",
    "description": "Nanonets-OCR-s is a state-of-the-art image-to-markdown OCR model that extracts text, equations, tables, and other structured elements from documents, converting them into semantic markdown for efficient processing by LLMs."
  },
  {
    "model_name": "Llama-3.1-8B-Lexi-Uncensored-V2-GGUF",
    "developer": "Orenguteng",
    "downloads": 10292,
    "createdAt": "2024-08-09T20:05:30.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_F16",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_Q4",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_Q5",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_Q8",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q8.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/README.md",
    "description": "The Llama-3.1-8B-Lexi-Uncensored-V2 model achieves high performance on various benchmarks, including 77.92% strict accuracy on IFEval (0-Shot) and 30.90% accuracy on MMLU-PRO"
  },
  {
    "model_name": "Qwen2.5-VL-Abliterated-Caption-GGUF",
    "developer": "prithivMLmods",
    "downloads": 10188,
    "createdAt": "2025-08-19T03:51:50.000Z",
    "tools": false,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.f16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "805.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/README.md",
    "description": "Qwen2.5-VL-Abliterated-Caption-it-GGUF models are fine-tuned for uncensored image captioning, offering high-fidelity descriptions across diverse visual inputs with options for 3B and 7B variants in various quantization formats."
  },
  {
    "model_name": "Qwen3-VL-30B-A3B-Instruct-GGUF",
    "developer": "yairpatch",
    "downloads": 10184,
    "createdAt": "2025-10-05T08:10:57.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-VL-30B-A3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-VL-30B-A3B-Instruct-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": "Qwen3-VL-30B-A3B-Instruct is the most powerful vision-language model in the Qwen series with 256K context and advanced visual reasoning capabilities."
  },
  {
    "model_name": "Devstral-Small-2507_gguf",
    "developer": "mistralai",
    "downloads": 10104,
    "createdAt": "2025-07-07T12:22:38Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Devstral-Small-2507-BF16",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/README.md",
    "description": "The Devstral Small 1.1 model is a lightweight, agentic LLM with a 128k context window, Apache 2.0 license, and supports GGUF quantized versions for efficient local deployment and use in software engineering tasks."
  },
  {
    "model_name": "MS3.2-24B-Magnum-Diamond-GGUF",
    "developer": "Doctor-Shotgun",
    "downloads": 9829,
    "createdAt": "2025-06-22T18:07:51Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_NL",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_L",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q6_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q8_0",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-bf16",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Doctor-Shotgun/MS3.2-24B-Magnum-Diamond model, optimized for creative writing and roleplay with specific SillyTavern presets provided.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "claude-3.7-sonnet-reasoning-gemma3-12B",
    "developer": "reedmayhew",
    "downloads": 9764,
    "createdAt": "2025-03-22T19:40:09.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0",
        "path": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/README.md",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL."
  },
  {
    "model_name": "Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF",
    "developer": "DavidAU",
    "downloads": 9564,
    "createdAt": "2025-02-12T00:29:33.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-IQ4_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q2_k.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_l.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_m.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_s.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_m.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_s.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q5_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q6_k.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q8_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-q5_k_m.gguf",
        "file_size": "13.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF model, a powerful mixture of 8 top L3.2 4B models combined into a 21"
  },
  {
    "model_name": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 9497,
    "createdAt": "2025-07-10T15:26:23Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_NL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_1",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-bf16",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-bf16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Huihui-gemma-3n-E4B-it-abliterated model using llama.cpp, with options ranging from high-quality Q8_0 to low-quality Q2_K, suitable for various hardware and performance requirements."
  },
  {
    "model_name": "Sakura-GalTransl-7B-v3.7",
    "developer": "SakuraLLM",
    "downloads": 9198,
    "createdAt": "2024-05-22T03:04:31.000Z",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "GalTransl-7B-v1-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v1-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "GalTransl-7B-v1.5-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v1.5-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "GalTransl-7B-v2-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v2-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "GalTransl-7B-v2.6-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v2.6-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Sakura-GalTransl-7B-v3-Q5_K_S",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-GalTransl-7B-v3-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Sakura-GalTransl-7B-v3.5-Q5_K_S",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-GalTransl-7B-v3.5-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Sakura-Galtransl-7B-v3.7-IQ4_XS",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-Galtransl-7B-v3.7-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Sakura-Galtransl-7B-v3.7",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-Galtransl-7B-v3.7.gguf",
        "file_size": "5.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/README.md",
    "description": "Sakura-GalTransl模型由sakuraumi和xd2333共同构建，专为视觉小说翻译优化，支持日文到简体中文的翻译，并适配多种翻译工具和部署方式。"
  },
  {
    "model_name": "GMeshNet-OSS-8B-GGUF",
    "developer": "Callimesh",
    "downloads": 9112,
    "createdAt": "2025-09-08T15:24:11.000Z",
    "tools": true,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "GMeshNet-OSS-8B-IQ3_XS",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-IQ4_NL",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-IQ4_NL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-IQ4_XS",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q2_K",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q3_K_M",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q3_K_S",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q4_0",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q4_1",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q4_K_M",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q4_K_S",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q5_0",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q5_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q5_1",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q5_1.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q5_K_M",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q5_K_S",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q6_K",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-Q8_0",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "GMeshNet-OSS-8B-f16",
        "path": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/GMeshNet-OSS-8B-f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Callimesh/GMeshNet-OSS-8B-GGUF/resolve/main/README.md",
    "description": "GMeshNet‑OSS‑8B is an open‑source 8‑billion‑parameter decoder‑only LLM optimized for chain‑of‑thought reasoning and generating Gmsh meshing scripts."
  },
  {
    "model_name": "Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4",
    "developer": "ValueFX9507",
    "downloads": 9075,
    "createdAt": "2025-02-13T14:49:07.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV2-7b-0218-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-0218-Q4_KM.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0222-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-Cot-0222-Q4_KM.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0301-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-Cot-0301-Q4_KM.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0222-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-NoCot-0222-Q4_KM.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0228-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-NoCot-0228-Q4_KM.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0325-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-NoCot-0325-Q4_KM.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Q4_KM",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/Tifa-DeepsexV2-7b-Q4_KM.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q4/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Seed-OSS-36B-Instruct-GGUF",
    "developer": "yarikdevcom",
    "downloads": 9038,
    "createdAt": "2025-08-22T19:46:34Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Seed_OSS_36B_Instruct_Q2_K",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q2_K.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q3_K_M",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q3_K_M.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q4_K_M",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q4_K_M.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q6_K",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q6_K.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q8_0",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q8_0.gguf",
        "file_size": "35.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an experimental implementation of the Seed-OSS-36B-Instruct model using llama.cpp with GGML and CUDA optimizations, built from the GGML repository's PR 15490."
  },
  {
    "model_name": "LFM2-VL-1.6B-GGUF",
    "developer": "LiquidAI",
    "downloads": 9037,
    "createdAt": "2025-08-17T08:50:50.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "LFM2-VL-1.6B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/LFM2-VL-1.6B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-VL-1.6B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/LFM2-VL-1.6B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-VL-1.6B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/LFM2-VL-1.6B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2-VL-1.6B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/mmproj-LFM2-VL-1.6B-F16.gguf",
        "file_size": "791.9 MB"
      },
      {
        "model_id": "mmproj-LFM2-VL-1.6B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/mmproj-LFM2-VL-1.6B-Q8_0.gguf",
        "file_size": "538.0 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/README.md",
    "description": "LFM2-VL-1.6B-GGUF is a vision model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 8867,
    "createdAt": "2025-06-30T05:25:35Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "ERNIE-4.5-0.3B-PT-F16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-F16.gguf",
        "file_size": "690.4 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.0 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "462.6 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-0.3B is a text dense, Apache-2.0 licensed large language model optimized for text generation and fine-tuning with ERNIEKit and FastDeploy, and supported by the transformers library.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-12b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 8818,
    "createdAt": "2025-05-28T17:31:37Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q2_k.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q6_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored, enhanced version of the Gemma 3 12B IT model with improved refusal handling and quantized GGUF weights."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-2506-GGUF",
    "developer": "ggml-org",
    "downloads": 8505,
    "createdAt": "2025-08-20T22:12:29.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-Q4_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-Q8_0",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-Q8_0.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-bf16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-bf16.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-f16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-f16.gguf",
        "file_size": "29.7 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-Kimi-VL-A3B-Thinking-2506-Q8_0",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/mmproj-Kimi-VL-A3B-Thinking-2506-Q8_0.gguf",
        "file_size": "589.5 MB"
      },
      {
        "model_id": "mmproj-Kimi-VL-A3B-Thinking-2506-bf16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/mmproj-Kimi-VL-A3B-Thinking-2506-bf16.gguf",
        "file_size": "864.7 MB"
      },
      {
        "model_id": "mmproj-Kimi-VL-A3B-Thinking-2506-f16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/mmproj-Kimi-VL-A3B-Thinking-2506-f16.gguf",
        "file_size": "863.4 MB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/README.md",
    "description": "The model Kimi-VL-A3B-Thinking-2506 from moonshotai is now supported in llama.cpp via the PR #15458."
  },
  {
    "model_name": "Qwen_Qwen3-4B-Instruct-2507-GGUF",
    "developer": "bartowski",
    "downloads": 8406,
    "createdAt": "2025-08-06T15:35:31.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-bf16",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-imatrix",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-4B-Instruct-2507 model using llama.cpp's imatrix method, optimized for different hardware and performance requirements."
  },
  {
    "model_name": "Phi-4-mini-reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 8286,
    "createdAt": "2025-05-01T01:39:08.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Phi-4-mini-reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q4_0",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q3_K_XL.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q4_K_XL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/README.md",
    "description": "Phi-4-mini-reasoning is a lightweight, math-focused language model fine-tuned for reasoning tasks with 3.8B parameters and 128K token context length, optimized for efficiency and accuracy in mathematical problem-solving."
  },
  {
    "model_name": "Apertus-8B-Instruct-2509-GGUF",
    "developer": "unsloth",
    "downloads": 8194,
    "createdAt": "2025-10-02T22:14:35.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Apertus-8B-Instruct-2509-BF16",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q2_K",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q4_0",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q4_1",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q6_K",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q6_K.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-Q8_0",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-Q5_K_XL.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-Q6_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Apertus-8B-Instruct-2509-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/Apertus-8B-Instruct-2509-UD-Q8_K_XL.gguf",
        "file_size": "9.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF/resolve/main/README.md",
    "description": "Apertus is a fully open, EU-compliant 70B/8B multilingual language model supporting 1811 languages."
  },
  {
    "model_name": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF",
    "developer": "DavidAU",
    "downloads": 8154,
    "createdAt": "2025-07-17T07:28:04Z",
    "tools": true,
    "num_quants": 81,
    "quants": [
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "325.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "310.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "292.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "274.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "417.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "383.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "345.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "482.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "462.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "360.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "338.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "463.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "432.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "483.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "521.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "503.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "484.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "561.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "600.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "572.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "560.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "644.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q8_0.gguf",
        "file_size": "833.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "325.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "310.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "292.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "274.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "417.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "383.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "345.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "482.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "462.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "360.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "338.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "463.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "432.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "483.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "521.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "503.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "484.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "561.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "600.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "572.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "560.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "644.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q8_0.gguf",
        "file_size": "797.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "302.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "286.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "269.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "251.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "374.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "355.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "341.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "322.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "439.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "419.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "317.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "295.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "420.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "355.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "440.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "478.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "460.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "441.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "518.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "557.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "529.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "518.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "601.7 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q8_0.gguf",
        "file_size": "754.3 MB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/README.md",
    "description": "This is a high-performance coder model based on Qwen3, capable of generating code and reasoning blocks efficiently, with optimized variants for different quantization levels and use cases.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "GLM-4.1V-9B-Thinking-GGUF",
    "developer": "unsloth",
    "downloads": 8089,
    "createdAt": "2025-07-25T11:47:05.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "GLM-4.1V-9B-Thinking-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-BF16.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q2_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q3_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q3_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q5_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q6_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q8_0.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ1_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ3_XXS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q2_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q3_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q4_K_XL.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q5_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q6_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q8_K_XL.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/README.md",
    "description": "This is a 9B-parameter vision-language model (GLM-4.1V-9B-Thinking) that enhances reasoning capabilities through a thinking paradigm and reinforcement learning, achieving state-of-the-art performance on 18 benchmark tasks."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 8083,
    "createdAt": "2025-01-25T01:35:17.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the DeepSeek-R1-Distill-Qwen-14B-Uncensored base model, available in various GGUF quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "Jinx-gpt-oss-20b-GGUF",
    "developer": "Jinx-org",
    "downloads": 7942,
    "createdAt": "2025-08-14T06:57:05.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "jinx-gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q8_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-mxfp4",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-mxfp4.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "Jinx is a \"helpful-only\" language model variant that responds to all queries without safety refusals, designed for AI safety research to study alignment failures and evaluate safety boundaries."
  },
  {
    "model_name": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF",
    "developer": "bartowski",
    "downloads": 7725,
    "createdAt": "2025-08-30T05:05:03Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ2_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ2_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ2_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ3_XS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q2_K_L.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_L.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q3_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_0.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q4_K_S.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q5_K_M.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q5_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-Q8_0.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-bf16.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "nvidia_NVIDIA-Nemotron-Nano-12B-v2-imatrix",
        "path": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/nvidia_NVIDIA-Nemotron-Nano-12B-v2-imatrix.gguf",
        "file_size": "4.9 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-12B-v2-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the NVIDIA-Nemotron-Nano-12B-v2 model by bartowski, optimized for various quantization types and suitable for use with llama.cpp or LM Studio."
  },
  {
    "model_name": "gpt-oss-20b-uncensored-bf16-GGUF",
    "developer": "mradermacher",
    "downloads": 7649,
    "createdAt": "2025-08-09T10:06:03Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.IQ4_XS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q2_K",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q3_K_L.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q6_K",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q8_0",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the GPT-oss-20B uncensored model, available in various quantization formats for efficient deployment."
  },
  {
    "model_name": "Openai_gpt-oss-20b-NEO-GGUF",
    "developer": "DavidAU",
    "downloads": 7614,
    "createdAt": "2025-08-06T01:05:36Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE2",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE2.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE3",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE3.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE4",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE4.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO2-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO2-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO2-Q5_1.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO3-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO3-Q5_1.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/README.md",
    "description": "This model is a specialized quantized version of the OpenAI GPT-oss-20B MOE model, optimized for code generation, reasoning, and creative writing with support for up to 24 experts and 128k context, offering high performance in various tasks including coding,"
  },
  {
    "model_name": "Qwen_Qwen3-4B-Thinking-2507-GGUF",
    "developer": "bartowski",
    "downloads": 7582,
    "createdAt": "2025-08-06T15:35:47Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-bf16",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-imatrix",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-4B-Thinking-2507 model for efficient inference on various hardware, with options ranging from high-quality Q8_0 to low-quality Q2_K quantizations."
  },
  {
    "model_name": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF",
    "developer": "DavidAU",
    "downloads": 7538,
    "createdAt": "2025-06-12T08:58:57Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-IQ4_XS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q2_k.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_m.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_s.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_s.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q5_k_s.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q8_0.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-q5_k_m.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-Q6_K",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-Q6_K.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/README.md",
    "description": "This is a 9B parameter, uncensored, and enhanced Llama-3.2 model with Brainstorm 40x, designed for creative writing across all genres, offering vivid, intense, and emotionally engaging prose with a focus on realism, horror, and storytelling, suitable for both role"
  },
  {
    "model_name": "stable-code-3b",
    "developer": "stabilityai",
    "downloads": 7531,
    "createdAt": "2024-01-09T02:03:58.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "stable-code-3b-Q5_K_M",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q5_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "stable-code-3b-Q6_K",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q6_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "stable-code-3b",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b.gguf",
        "file_size": "5.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/README.md",
    "description": "Stable Code 3B is a 3B parameter decoder-only language model pre-trained on diverse code and text datasets, excelling in code generation across multiple programming languages with state-of-the-art performance on MultiPL-E metrics."
  },
  {
    "model_name": "ReasonableQwen3-4B",
    "developer": "adeelahmad",
    "downloads": 7498,
    "createdAt": "2025-08-28T03:38:27.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "ReasonableQwen3-4B-Q3_K",
        "path": "https://huggingface.co/adeelahmad/ReasonableQwen3-4B/resolve/main/ReasonableQwen3-4B-Q3_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ReasonableQwen3-4B-Q4_K",
        "path": "https://huggingface.co/adeelahmad/ReasonableQwen3-4B/resolve/main/ReasonableQwen3-4B-Q4_K.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ReasonableQwen3-4B-Q8_0",
        "path": "https://huggingface.co/adeelahmad/ReasonableQwen3-4B/resolve/main/ReasonableQwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/adeelahmad/ReasonableQwen3-4B/resolve/main/README.md",
    "description": "A 4B-parameter Qwen3 model with dual-mode reasoning (thinking for complex tasks, non-thinking for general dialogue) supporting 100+ languages."
  },
  {
    "model_name": "Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf",
    "developer": "DavidAU",
    "downloads": 7306,
    "createdAt": "2024-12-26T11:22:27.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-IQ4_XS.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q2_k.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_l.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_s.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_s.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q5_k_s.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q6_k.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q8_0.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-q5_k_m.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q2_k.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q6_k.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q8_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "18.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "24.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/README.md",
    "description": "This is an uncensored, vividly detailed, and highly creative Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-GGUF model that combines four Mistral 7B models into a 24B parameter model"
  },
  {
    "model_name": "M3.2-24B-Loki-V1.3-GGUF",
    "developer": "CrucibleLab-TG",
    "downloads": 7239,
    "createdAt": "2025-08-04T10:13:33Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "M3.2-24B-Loki-V1.3-BF16",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ1_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_XS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_XXS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_XS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_XXS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ4_NL",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ4_XS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q2_K",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q2_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q3_K_L",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q3_K_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q3_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q4_0",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q4_K_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q4_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q5_0",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q5_0.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q5_K_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q5_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q6_K",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q8_0",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/README.md",
    "description": "This is the BF16 quantized GGUF version of the M3.2-24B-Loki-V1.3 model from CrucibleLab."
  },
  {
    "model_name": "Kunoichi-DPO-v2-7B-GGUF-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 7237,
    "createdAt": "2024-02-27T10:16:40Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q4_K_M-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q4_K_M-imatrix.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q4_K_S-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q4_K_S-imatrix.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q5_K_M-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q5_K_M-imatrix.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q5_K_S-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q5_K_S-imatrix.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q6_K-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q6_K-imatrix.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q8_0-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q8_0-imatrix.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/README.md",
    "description": "The SanjiWatsuki/Kunoichi-DPO-v2-7B model is a GGUF-Imatrix quantized version of the original model, designed to improve performance while maintaining efficiency through the use of an Importance Matrix for activation quantization."
  },
  {
    "model_name": "Magidonia-24B-v4.2.0-GGUF",
    "developer": "TheDrummer",
    "downloads": 7236,
    "createdAt": "2025-10-06T05:33:45.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4s-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4s-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4s-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4s-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4s-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4s-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4s-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4s-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4s-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4s-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4s-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4s-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Impish_Nemo_12B_GGUF",
    "developer": "SicariusSicariiStuff",
    "downloads": 7125,
    "createdAt": "2025-08-10T04:25:16Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Impish_Nemo_12B-Q4_K_M",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q4_K_S",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q5_K_M",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q5_K_S",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q6_K",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q8_0",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/README.md",
    "description": "This is a Hugging Face model repository for the Impish_Nemo_12B base model and the UBW_Tapestries dataset, licensed under Apache 2.0, quantized by SicariusSicariiStuff."
  },
  {
    "model_name": "dolphin-2.9.3-mistral-nemo-12b-gguf",
    "developer": "dphn",
    "downloads": 7048,
    "createdAt": "2024-07-24T16:00:27.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.F16",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.F16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q2_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_L",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_1",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_1",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q6_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q8_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/README.md",
    "description": "This is the llama.cpp gguf conversion of the original model located here: https://huggingface.co/cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b, curated and trained by Eric Hartford and Cognitive Computations, based on mistralai"
  },
  {
    "model_name": "LFM2-2.6B-GGUF",
    "developer": "LiquidAI",
    "downloads": 7008,
    "createdAt": "2025-09-23T15:50:28.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-2.6B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/LFM2-2.6B-F16.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "LFM2-2.6B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/LFM2-2.6B-Q4_0.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "LFM2-2.6B-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/LFM2-2.6B-Q4_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "LFM2-2.6B-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/LFM2-2.6B-Q5_K_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "LFM2-2.6B-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/LFM2-2.6B-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "LFM2-2.6B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/LFM2-2.6B-Q8_0.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-2.6B-GGUF/resolve/main/README.md",
    "description": "Liquid AI's LFM2-2.6B-GGUF is a text-generation model optimized for efficient edge deployment with speed and memory efficiency."
  },
  {
    "model_name": "XortronCriminalComputingConfig-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 7004,
    "createdAt": "2025-05-05T07:31:49Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides various GGUF quantized versions of the darkc0de/XortronCriminalComputingConfig model, ranging from low-quality to high-quality, with notes on their performance and suitability for different use cases.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-VL-4B-Thinking-GGUF",
    "developer": "NexaAI",
    "downloads": 6891,
    "createdAt": "2025-10-14T05:28:27.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Qwen3-VL-4B-Thinking.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/Qwen3-VL-4B-Thinking.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Thinking.Q4_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/Qwen3-VL-4B-Thinking.Q4_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Thinking.Q4_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/Qwen3-VL-4B-Thinking.Q4_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Thinking.Q5_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/Qwen3-VL-4B-Thinking.Q5_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Thinking.Q6_K",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/Qwen3-VL-4B-Thinking.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-VL-4B-Thinking.Q8_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/Qwen3-VL-4B-Thinking.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj.F16",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/mmproj.F16.gguf",
        "file_size": "797.5 MB"
      },
      {
        "model_id": "mmproj.F32",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/mmproj.F32.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mmproj.Q8_0",
        "path": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/mmproj.Q8_0.gguf",
        "file_size": "428.5 MB"
      }
    ],
    "readme": "https://huggingface.co/NexaAI/Qwen3-VL-4B-Thinking-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "openai_gpt-oss-20b-GGUF-MXFP4-Experimental",
    "developer": "bartowski",
    "downloads": 6639,
    "createdAt": "2025-08-05T17:43:11Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "openai_gpt-oss-20b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental/resolve/main/openai_gpt-oss-20b-MXFP4.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental/resolve/main/README.md",
    "description": "This is an experimental MXFP4 quantized version of the gpt-oss-20b model by OpenAI, requiring the llama.cpp branch `gpt-oss-mxfp4` for use."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 6613,
    "createdAt": "2025-05-28T22:03:00Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the Google Gemma-3-27B IT model, created using an advanced abliteration technique to enhance refusal accuracy while maintaining coherent outputs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF",
    "developer": "DavidAU",
    "downloads": 6605,
    "createdAt": "2024-12-12T01:30:01.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/README.md",
    "description": "This model is a powerful Llama 3.2 MOE with 10B parameters, combining four top Llama 3.2 3B models for exceptional creative writing, vivid prose, and uncensored output across all genres, including horror, science fiction, romance, and roleplay"
  },
  {
    "model_name": "Phi-4-reasoning-plus-GGUF",
    "developer": "unsloth",
    "downloads": 6490,
    "createdAt": "2025-05-01T02:04:54Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Phi-4-reasoning-plus-BF16",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-BF16.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-IQ4_NL.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-IQ4_XS.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q2_K",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q2_K_L.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_0",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_1",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q6_K",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q8_0",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ2_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q2_K_XL.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q3_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q4_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q5_K_XL.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q6_K_XL.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q8_K_XL.gguf",
        "file_size": "16.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/README.md",
    "description": "Phi-4-reasoning-plus is a 14B parameter, dense decoder-only Transformer model trained for reasoning tasks with enhanced accuracy and token generation, suitable for math, science, and coding, and can be fine-tuned or run via Colab, Ollama, llama.cpp, or"
  },
  {
    "model_name": "SpaceThinker-Qwen2.5VL-3B",
    "developer": "remyxai",
    "downloads": 6457,
    "createdAt": "2025-04-17T17:34:23Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gguf/spacethinker-qwen2.5VL-3B-F16",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5VL-3B-F16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gguf/spacethinker-qwen2.5vl-3b-vision",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5vl-3b-vision.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/README.md",
    "description": "SpaceThinker-Qwen2.5VL-3B is a multimodal vision-language model trained to enhance spatial reasoning through test-time compute, achieving strong performance on various spatial reasoning benchmarks like SpatialScore and QSpatial-Bench with a focus on quantitative spatial tasks such as distance estimation and object relations.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-Coder-3B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 6433,
    "createdAt": "2024-11-09T12:46:15Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-coder-3b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-fp16.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q2_k.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q3_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q4_k_m.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q5_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q5_k_m.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q6_k.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the instruction-tuned 3B Qwen2.5-Coder model in GGUF format, optimized for code generation, reasoning, and fixing with 3.09B parameters and full 32,768 token context length.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Wayfarer-2-12B-GGUF",
    "developer": "LatitudeGames",
    "downloads": 6268,
    "createdAt": "2025-08-27T19:21:18.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Wayfarer-2-12B-IQ1_M",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ1_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ2_M",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ2_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ2_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ2_XS",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ2_XXS",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ3_M",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ3_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ3_XS",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ3_XXS",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ4_NL",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-IQ4_XS",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q2_K",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q2_K_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q2_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q3_K_L",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q3_K_M",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q3_K_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q4_K_M",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q4_K_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q5_K_M",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q5_K_S",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q6_K",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-Q8_0",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Wayfarer-2-12B-bf16",
        "path": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-bf16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF weights for Wayfarer‑2‑12B with VRAM usage guidance."
  },
  {
    "model_name": "TheDrummer_Cydonia-ReduX-22B-v1-GGUF",
    "developer": "bartowski",
    "downloads": 6254,
    "createdAt": "2025-09-17T14:41:00.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ2_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ2_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ2_XS.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ3_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ3_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ3_XXS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ4_NL.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q2_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q3_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q4_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q4_1.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q5_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q6_K_L.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-Q8_0.gguf",
        "file_size": "22.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-bf16.gguf",
        "file_size": "41.4 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-ReduX-22B-v1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/TheDrummer_Cydonia-ReduX-22B-v1-imatrix.gguf",
        "file_size": "11.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-ReduX-22B-v1-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of TheDrummer /Cydonia‑ReduX‑22B‑v1 for llama.cpp, offering many size‑/quality options."
  },
  {
    "model_name": "Devstral-Small-2505-GGUF",
    "developer": "unsloth",
    "downloads": 6186,
    "createdAt": "2025-05-21T14:20:05.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Devstral-Small-2505-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "mistralai_Magistral-Small-2509-GGUF",
    "developer": "bartowski",
    "downloads": 6133,
    "createdAt": "2025-09-17T16:17:26.000Z",
    "tools": true,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2509-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/mistralai_Magistral-Small-2509-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2509-GGUF/resolve/main/README.md",
    "description": "Imatrix quantizations of mistralai/Magistral-Small-2509 in GGUF format."
  },
  {
    "model_name": "Qwen3-4B-2507-abliterated-GGUF",
    "developer": "prithivMLmods",
    "downloads": 6117,
    "createdAt": "2025-08-08T16:18:22Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "The Qwen3-4B-2507-abliterated-GGUF model is an uncensored, proof-of-concept version of the Qwen3-4B-Instruct-2507 large language model, created using an abliteration method to bypass standard refusal behaviors,"
  },
  {
    "model_name": "Home-Cook-Mistral-Small-Omni-24B-2507-GGUF",
    "developer": "ngxson",
    "downloads": 5988,
    "createdAt": "2025-07-28T19:50:10.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-Q4_K_M",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-bf16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-f16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-f16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/mmproj-model-f16.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/README.md",
    "description": "This model merges Mistral Small 2506 (text) and Voxtral 2507 (audio) into a multimodal model using a modified mergekit tool."
  },
  {
    "model_name": "GLM-4.5-Air-GGUF",
    "developer": "ddh0",
    "downloads": 5975,
    "createdAt": "2025-08-04T20:25:03Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-IQ3_S-IQ3_S-Q5_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-IQ3_S-IQ3_S-Q5_0.gguf",
        "file_size": "57.4 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q5_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q5_0.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q5_1",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q5_1.gguf",
        "file_size": "67.8 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q8_0.gguf",
        "file_size": "77.7 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q5_K-Q5_K-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q5_K-Q5_K-Q8_0.gguf",
        "file_size": "85.6 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q6_K-Q6_K-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q6_K-Q6_K-Q8_0.gguf",
        "file_size": "94.1 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0.gguf",
        "file_size": "109.4 GB"
      },
      {
        "model_id": "GLM-4.5-Air-bf16",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-bf16.gguf",
        "file_size": "205.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the GLM-4.5-Air model for use with llama.cpp, with Q8_0 as the default quantization for most tensors, except for the dense FFN block and conditional experts which are downgraded."
  },
  {
    "model_name": "WeirdCompound-v1.6-24b-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 5790,
    "createdAt": "2025-08-26T14:08:43.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "WeirdCompound-v1.6-24b.imatrix",
        "path": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/WeirdCompound-v1.6-24b.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/WeirdCompound-v1.6-24b-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "gpt-oss-20b-uncensored",
    "developer": "Combatti",
    "downloads": 5756,
    "createdAt": "2025-08-08T21:43:45Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gpt-oss-20b-uncensored-F16",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-F16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-Q4_0",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-Q4_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-Q5_0",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-Q5_0.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-Q8_0",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/README.md",
    "description": "This is an uncensored, highly modified version of the unsloth/gpt-oss-20b-BF16 model designed for research and controlled use, with minimal safety filtering and potential for generating sensitive or controversial content."
  },
  {
    "model_name": "bitnet-b1.58-2B-4T-gguf",
    "developer": "microsoft",
    "downloads": 5746,
    "createdAt": "2025-04-15T04:25:42.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ggml-model-i2_s",
        "path": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/README.md",
    "description": "This repository provides the weights for BitNet b1.58 2B4T, a native 1-bit large language model with 2 billion parameters, offering significant computational efficiency gains over full-precision models through specialized C++ inference code."
  },
  {
    "model_name": "WEBGEN-4B-Preview-GGUF",
    "developer": "gabriellarson",
    "downloads": 5739,
    "createdAt": "2025-09-02T20:06:50.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "WEBGEN-4B-Preview-F16",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q2_K",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q4_0",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q5_0",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q6_K",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "WEBGEN-4B-Preview-Q8_0",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/WEBGEN-4B-Preview-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF/resolve/main/README.md",
    "description": "WEBGEN-4B-Preview is a 4B web-focused model that generates clean, responsive HTML/CSS/Tailwind code from prompts, optimized for single-file website creation with semantic structure and modern design principles."
  },
  {
    "model_name": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF",
    "developer": "DavidAU",
    "downloads": 5726,
    "createdAt": "2025-01-03T00:04:03.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k.gguf",
        "file_size": "19.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0.gguf",
        "file_size": "24.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "21.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "26.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/README.md",
    "description": "This is a high-precision, 24.9B parameter Llama3 MOE model combining four 8B models, designed for vivid, uncensored creative writing across all genres, with specialized quants and settings for enhanced instruction following and output quality, particularly in horror, fiction, and"
  },
  {
    "model_name": "TheDrummer_Cydonia-R1-24B-v4.1-GGUF",
    "developer": "bartowski",
    "downloads": 5669,
    "createdAt": "2025-09-28T17:08:11.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-R1-24B-v4.1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-R1-24B-v4.1-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-R1-24B-v4.1-GGUF/resolve/main/README.md",
    "description": "bartowski's imatrix quantized GGUF versions of TheDrummer's Cydonia-R1-24B-v4.1 model."
  },
  {
    "model_name": "Cydonia-24B-v4.2.0-GGUF",
    "developer": "TheDrummer",
    "downloads": 5641,
    "createdAt": "2025-10-05T08:56:43.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4r-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4r-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4r-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4r-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4r-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4r-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4r-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4r-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4r-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4r-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4r-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/Cydonia-24B-v4r-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 5545,
    "createdAt": "2025-04-12T16:19:28.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "mradermacher's GGUF quantizations of Qwen2.5-VL-7B-Instruct-abliterated model."
  },
  {
    "model_name": "mistralai_Magistral-Small-2506-GGUF",
    "developer": "bartowski",
    "downloads": 5525,
    "createdAt": "2025-06-10T16:09:49Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2506 model by bartowski, optimized for various inference speeds and quality levels using llama.cpp.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Falcon-H1-7B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 5488,
    "createdAt": "2025-05-13T15:59:29Z",
    "tools": true,
    "num_quants": 34,
    "quants": [
      {
        "model_id": "Falcon-H1-7B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-BF16.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-F16.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-F32.gguf",
        "file_size": "28.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ1_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_XXS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-TQ1_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-TQ2_0.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Falcon-H1 series is a hybrid Transformers + Mamba architecture, multilingual, causal decoder-only language model developed by TII, licensed under the Falcon-LLM License, and available for inference via Hugging Face Transformers, vLLM, or llama.cpp.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Skyfall-31B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 5417,
    "createdAt": "2025-08-26T16:14:26Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Skyfall-31B-v4j-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q2_K.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q3_K_M.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q4_K_M.gguf",
        "file_size": "17.7 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q5_K_M.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q6_K.gguf",
        "file_size": "24.0 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q8_0.gguf",
        "file_size": "31.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/README.md",
    "description": "Skyfall 31B v4 is an efficient and creative upscaled version of Mistral Small 3.2 2507, offering enhanced versatility and performance."
  },
  {
    "model_name": "granite-4.0-h-small-GGUF",
    "developer": "ibm-granite",
    "downloads": 5326,
    "createdAt": "2025-09-25T19:49:43.000Z",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "granite-4.0-h-small-Q2_K",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q2_K.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q3_K_L",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q3_K_L.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q3_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q3_K_M.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q3_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q3_K_S.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q4_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q4_0.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q4_1",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q4_1.gguf",
        "file_size": "18.9 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q4_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q4_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q4_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q4_K_S.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q5_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q5_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q5_1",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q5_1.gguf",
        "file_size": "22.6 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q5_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q5_K_M.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q5_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q5_K_S.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q6_K",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q6_K.gguf",
        "file_size": "24.7 GB"
      },
      {
        "model_id": "granite-4.0-h-small-Q8_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-Q8_0.gguf",
        "file_size": "31.9 GB"
      },
      {
        "model_id": "granite-4.0-h-small-f16",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/granite-4.0-h-small-f16.gguf",
        "file_size": "60.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ibm-granite/granite-4.0-h-small-GGUF/resolve/main/README.md",
    "description": "GGUF-quantized Granite 4.0 H-Small models."
  },
  {
    "model_name": "EXAONE-4.0-1.2B-GGUF",
    "developer": "LGAI-EXAONE",
    "downloads": 5325,
    "createdAt": "2025-07-11T07:03:24Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "EXAONE-4.0-1.2B-BF16",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-BF16.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-IQ4_XS",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-IQ4_XS.gguf",
        "file_size": "718.9 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q4_K_M",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q4_K_M.gguf",
        "file_size": "774.8 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q5_K_M",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q5_K_M.gguf",
        "file_size": "886.6 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q6_K",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q6_K.gguf",
        "file_size": "1005.3 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q8_0",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q8_0.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/README.md",
    "description": "The EXAONE-4.0-1.2B-GGUF model is a multilingual, hybrid reasoning model that combines non-reasoning and reasoning modes, offering high performance with support for Korean, English, and Spanish, and is available in GGUF format with various quantization options"
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-GGUF",
    "developer": "lmstudio-community",
    "downloads": 5275,
    "createdAt": "2024-06-17T18:01:28Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-IQ3_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-IQ3_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-IQ4_XS",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-IQ4_XS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q3_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q6_K.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q8_0.gguf",
        "file_size": "15.6 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a brand new MoE model from DeepSeek, specializing in coding instructions and offering excellent inference speed with 16B total weights and 2.4B activated, suitable for both instruction following and code completion.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "granite-3.3-8b-instruct-GGUF",
    "developer": "ibm-granite",
    "downloads": 5273,
    "createdAt": "2025-04-11T15:16:47.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "granite-3.3-8b-instruct-Q2_K",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q3_K_L",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q3_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q3_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q4_0",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q4_1",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q4_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q4_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q5_0",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q5_1",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q5_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q5_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q6_K",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q6_K.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-Q8_0",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "granite-3.3-8b-instruct-f16",
        "path": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-f16.gguf",
        "file_size": "15.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of the IBM Granite 3.3 8B instruct model."
  },
  {
    "model_name": "POLARIS-Project_Polaris-4B-Preview-GGUF",
    "developer": "bartowski",
    "downloads": 5255,
    "createdAt": "2025-06-24T04:38:50Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ2_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_NL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_1",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q8_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-bf16",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-bf16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Polaris-4B-Preview model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2-7B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 5254,
    "createdAt": "2024-06-06T13:18:05Z",
    "tools": false,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2-7b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-fp16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q2_k.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q3_k_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q4_k_m.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q5_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q6_k.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "qwen2-7b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "Qwen2‑7B‑Instruct‑GGUF offers the instruction‑tuned 7B Qwen2 model in various GGUF quantizations for easy deployment via llama.cpp."
  },
  {
    "model_name": "lille-130m-instruct",
    "developer": "Nikity",
    "downloads": 5160,
    "createdAt": "2025-08-31T11:56:20.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gguf/lille-130m-instruct-f16",
        "path": "https://huggingface.co/Nikity/lille-130m-instruct/resolve/main/gguf/lille-130m-instruct-f16.gguf",
        "file_size": "243.7 MB"
      },
      {
        "model_id": "gguf/lille-130m-instruct-f32",
        "path": "https://huggingface.co/Nikity/lille-130m-instruct/resolve/main/gguf/lille-130m-instruct-f32.gguf",
        "file_size": "486.2 MB"
      },
      {
        "model_id": "gguf/lille-130m-instruct-q8_0",
        "path": "https://huggingface.co/Nikity/lille-130m-instruct/resolve/main/gguf/lille-130m-instruct-q8_0.gguf",
        "file_size": "130.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Nikity/lille-130m-instruct/resolve/main/README.md",
    "description": "The Lille-130M-Instruct model is a 130-million-parameter instruction-tuned language model fine-tuned on the Kyoto-Corpus dataset, offering strong performance in text generation tasks like chat and reasoning, with a license of Apache-2.0."
  },
  {
    "model_name": "CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 5139,
    "createdAt": "2025-08-24T08:05:24Z",
    "tools": false,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "ARM-CaptainErisNebula-12B-Chimera-v1.1-Q4_0-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/ARM-CaptainErisNebula-12B-Chimera-v1.1-Q4_0-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-BF16",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-BF16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-F16",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-F16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_XS-imat.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_XXS-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ4_XS-imat.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q3_K_L-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q3_K_L-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q3_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q3_K_M-imat.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q4_K_M-imat.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q4_K_S-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q5_K_M-imat.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q5_K_S-imat.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q6_K-imat.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q8_0-imat.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "imatrix-fp16",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/imatrix-fp16.gguf",
        "file_size": "6.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This model is a GGUF quantized version of Nitral-AI/CaptainErisNebula-12B-Chimera-v1.1 for text generation inference and testing."
  },
  {
    "model_name": "Tifa-DeepsexV2-7b-MGRPO-GGUF-F16",
    "developer": "ValueFX9507",
    "downloads": 5099,
    "createdAt": "2025-02-15T19:41:29.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV2-7b-0218-F16",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/Tifa-DeepsexV2-7b-0218-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0301-F16.gguf",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/Tifa-DeepsexV2-7b-Cot-0301-F16.gguf.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0317-F16",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/Tifa-DeepsexV2-7b-Cot-0317-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-F16",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/Tifa-DeepsexV2-7b-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0228-F16",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/Tifa-DeepsexV2-7b-NoCot-0228-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0325-F16",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/Tifa-DeepsexV2-7b-NoCot-0325-F16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-F16/resolve/main/README.md",
    "description": "A Qwen2.5‑7B‑based role‑play LLM with 1 M‑token context, enhanced by MGRPO training for improved reasoning and fewer refusals."
  },
  {
    "model_name": "Cydonia-R1-24B-v4.1-GGUF",
    "developer": "TheDrummer",
    "downloads": 5092,
    "createdAt": "2025-09-20T09:40:56.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-R1-24B-v4f-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/Cydonia-R1-24B-v4f-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4f-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/Cydonia-R1-24B-v4f-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4f-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/Cydonia-R1-24B-v4f-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4f-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/Cydonia-R1-24B-v4f-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4f-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/Cydonia-R1-24B-v4f-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4f-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/Cydonia-R1-24B-v4f-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1-GGUF/resolve/main/README.md",
    "description": "Creative, less-censored fine-tune of Mistral-Small-3.2-24B-Instruct for enhanced roleplay and entertainment."
  },
  {
    "model_name": "L3-8B-Stheno-v3.2-GGUF",
    "developer": "bartowski",
    "downloads": 5079,
    "createdAt": "2024-06-12T01:04:39.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ2_M",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ2_S",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ2_XS",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_M",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_XS",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ4_XS",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q2_K",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q3_K_L",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q3_K_M",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q3_K_S",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_M",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_S",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_M",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_S",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q6_K",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q8_0",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-f32",
        "path": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/L3-8B-Stheno-v3.2-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-8B-Claude-Sonnet-4-Reasoning-Distill-GGUF",
    "developer": "Liontix",
    "downloads": 5010,
    "createdAt": "2025-08-22T14:19:49.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-8B-Claude-Sonnet-4-Reasoning-Distill_Q4_K_M",
        "path": "https://huggingface.co/Liontix/Qwen3-8B-Claude-Sonnet-4-Reasoning-Distill-GGUF/resolve/main/qwen3-8B-Claude-Sonnet-4-Reasoning-Distill_Q4_K_M.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Liontix/Qwen3-8B-Claude-Sonnet-4-Reasoning-Distill-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "TheDrummer_Snowpiercer-15B-v3-GGUF",
    "developer": "bartowski",
    "downloads": 5007,
    "createdAt": "2025-09-30T13:50:43.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ3_M.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ3_XS.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ3_XXS.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q2_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q4_1.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q4_K_L.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q4_K_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q4_K_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q5_K_L.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q5_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q6_K.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q6_K_L.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-Q8_0.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-bf16.gguf",
        "file_size": "27.9 GB"
      },
      {
        "model_id": "TheDrummer_Snowpiercer-15B-v3-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/TheDrummer_Snowpiercer-15B-v3-imatrix.gguf",
        "file_size": "8.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Snowpiercer-15B-v3-GGUF/resolve/main/README.md",
    "description": "Quantized Snowpiercer-15B-v3 (Q2_K to Q8_0) for text generation."
  },
  {
    "model_name": "TinyLlama-1.1B-Chat-v0.6",
    "developer": "TinyLlama",
    "downloads": 4994,
    "createdAt": "2023-11-20T08:59:23.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ggml-model-q4_0",
        "path": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6/resolve/main/ggml-model-q4_0.gguf",
        "file_size": "607.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Baichuan-M2-32B-GGUF",
    "developer": "mradermacher",
    "downloads": 4986,
    "createdAt": "2025-08-11T17:24:17Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Baichuan-M2-32B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/README.md",
    "description": "The Baichuan-M2-32B model from Baichuan Inc. is available in various GGUF quantized versions for efficient deployment in both English and Chinese, with the highest quality version being Q8_0 at 34.9GB."
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 4962,
    "createdAt": "2025-08-03T08:14:34Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Thinking-2507 model, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "gpt-oss-120b-GGUF",
    "developer": "gabriellarson",
    "downloads": 4899,
    "createdAt": "2025-08-05T17:10:46Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gpt-oss-120B-F16",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120B-F16.gguf",
        "file_size": "60.9 GB"
      },
      {
        "model_id": "gpt-oss-120b-Q4_0",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-Q4_0.gguf",
        "file_size": "58.3 GB"
      },
      {
        "model_id": "gpt-oss-120b-Q8_0",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-Q8_0.gguf",
        "file_size": "59.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/README.md",
    "description": "This is a large, open-source GPT model (gpt-oss-120b) with Apache 2.0 license, designed for high reasoning tasks, agentic capabilities, and fine-tuning on H100 GPU or consumer hardware."
  },
  {
    "model_name": "Gemma-3-R1-27B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 4819,
    "createdAt": "2025-08-04T14:46:49Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/README.md",
    "description": "Hugging Face provides a platform for sharing and accessing machine learning models, datasets, and tools, with a focus on ease of use and community collaboration."
  },
  {
    "model_name": "MiniCPM-V-4_5-GGUF",
    "developer": "second-state",
    "downloads": 4780,
    "createdAt": "2025-08-28T02:26:46.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "MiniCPM-V-4_5-Q2_K",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q3_K_L",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q3_K_M",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q3_K_S",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q4_0",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q4_K_M",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q4_K_S",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q5_0",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q5_K_M",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q5_K_S",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q6_K",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-Q8_0",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-f16",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-f16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "MiniCPM-V-4_5-mmproj-f16",
        "path": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/MiniCPM-V-4_5-mmproj-f16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/second-state/MiniCPM-V-4_5-GGUF/resolve/main/README.md",
    "description": "MiniCPM-V-4_5-GGUF is a visual-question-answering model by openbmb, quantized by Second State Inc. for various use cases with different bit levels and sizes."
  },
  {
    "model_name": "tencent_Hunyuan-7B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4760,
    "createdAt": "2025-08-04T12:45:17Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q6_K_L.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q8_0.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-bf16",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-bf16.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-imatrix",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-imatrix.gguf",
        "file_size": "4.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Tencent Hunyuan-7B-Instruct model using llama.cpp's imatrix quantization method, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "Qwen3-4B-toolcalling-gguf-codex",
    "developer": "Manojb",
    "downloads": 4757,
    "createdAt": "2025-09-21T06:14:24.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-4B-Function-Calling-Pro",
        "path": "https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex/resolve/main/Qwen3-4B-Function-Calling-Pro.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex/resolve/main/README.md",
    "description": "6GB VRAM-compatible Qwen3-4B tool-calling model for local Ollama deployment."
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 4756,
    "createdAt": "2025-08-03T04:39:41Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Instruct-2507 model with various quantization types available for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "QVikhr-3-8B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 4731,
    "createdAt": "2025-08-06T14:36:58Z",
    "tools": true,
    "num_quants": 30,
    "quants": [
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_XS.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q2_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_K.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-8B-Instruction — это инструктивная модель на основе Qwen/Qwen3-8B, обученная на русскоязычном датасете GrandMaster2, предназначенная для эффективной обработки текстов на рус"
  },
  {
    "model_name": "orpheus-3b-0.1-ft-Q4_K_M-GGUF",
    "developer": "isaiahbjork",
    "downloads": 4724,
    "createdAt": "2025-03-20T01:43:53Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "orpheus-3b-0.1-ft-q4_k_m",
        "path": "https://huggingface.co/isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF/resolve/main/orpheus-3b-0.1-ft-q4_k_m.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF/resolve/main/README.md",
    "description": "Orpheus-TTS-Local is a lightweight client for running the Orpheus TTS model locally using LM Studio API for text-to-speech synthesis with multiple voice options and emotion tags."
  },
  {
    "model_name": "L3-DARKEST-PLANET-16.5B-GGUF",
    "developer": "DavidAU",
    "downloads": 4680,
    "createdAt": "2024-10-10T08:37:02.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-IQ4_XS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q2_k.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q3_k_l.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q3_k_m.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q3_k_s.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q4_0_4_4.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q4_0_4_8.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q4_0_8_8.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q4_k_m.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q4_k_s.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q5_k_s.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q6_k.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-Q8_0.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-max-IQ4_XS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-D_AU-q5_k_m.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-max-D_AU-Q2_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-max-D_AU-Q6_k.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-max-D_AU-Q8_0.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "L3-DARKEST-PLANET-16.5B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/L3-DARKEST-PLANET-16.5B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "17.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Big-Tiger-Gemma-27B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 4646,
    "createdAt": "2025-07-04T09:28:17Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q2_K.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q3_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q4_K_M.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q5_K_M.gguf",
        "file_size": "18.9 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q6_K.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q8_0.gguf",
        "file_size": "28.1 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/README.md",
    "description": "Big Tiger Gemma 27B v3 is a more neutral and less positive version of the Gemma 3 27B model, optimized for vision tasks and harder themes.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "google_gemma-3n-E2B-it-GGUF",
    "developer": "bartowski",
    "downloads": 4644,
    "createdAt": "2025-06-26T19:50:06Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3n-E2B-it model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-R1-24B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 4637,
    "createdAt": "2025-08-01T14:22:10Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-R1-24B-v4c-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/README.md",
    "description": "Cydonia R1 24B v4 is a large language model developed by TheDrummer, known for its creativity, solid reasoning, and ability to maintain coherent narratives with a large number of characters.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "SmolLM2-360M-Instruct-GGUF",
    "developer": "HuggingFaceTB",
    "downloads": 4599,
    "createdAt": "2024-10-31T20:37:31.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "smollm2-360m-instruct-q8_0",
        "path": "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF/resolve/main/smollm2-360m-instruct-q8_0.gguf",
        "file_size": "368.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-8B-Gemini-2.5-Pro-Distill-GGUF",
    "developer": "Liontix",
    "downloads": 4565,
    "createdAt": "2025-09-13T20:27:46.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-8B-gemini-2.5-pro-distill_Q4_K_M",
        "path": "https://huggingface.co/Liontix/Qwen3-8B-Gemini-2.5-Pro-Distill-GGUF/resolve/main/qwen3-8B-gemini-2.5-pro-distill_Q4_K_M.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Liontix/Qwen3-8B-Gemini-2.5-Pro-Distill-GGUF/resolve/main/README.md",
    "description": "Distilled reasoning model trained on Gemini 2.5 Flash and Pro datasets."
  },
  {
    "model_name": "neutts-air-q8-gguf",
    "developer": "neuphonic",
    "downloads": 4556,
    "createdAt": "2025-09-23T13:42:03.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "neutts-air-Q8-0",
        "path": "https://huggingface.co/neuphonic/neutts-air-q8-gguf/resolve/main/neutts-air-Q8-0.gguf",
        "file_size": "765.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/neuphonic/neutts-air-q8-gguf/resolve/main/README.md",
    "description": "On-device text-to-speech model with instant voice cloning capability."
  },
  {
    "model_name": "Qwen3-8B-128K-GGUF",
    "developer": "unsloth",
    "downloads": 4502,
    "createdAt": "2025-04-28T22:37:42Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-8B-128K-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/README.md",
    "description": "The Qwen3-8B model is a large language model with 8.2B parameters, supporting seamless switching between thinking and non-thinking modes for enhanced reasoning and dialogue capabilities, and is available on Hugging Face via the transformers library."
  },
  {
    "model_name": "gemma-3n-E2B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 4496,
    "createdAt": "2025-06-26T15:16:13Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E2B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-14B_Uncensored_Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4445,
    "createdAt": "2024-09-22T22:26:11Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen2.5-14B_Uncensored_Instruct model using llama.cpp, with various quantization types and optimizations for different hardware, including ARM and Apple Metal, suitable for different performance and quality trade-offs.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen1.5-0.5B-Chat-GGUF",
    "developer": "Qwen",
    "downloads": 4362,
    "createdAt": "2024-02-03T11:58:20.000Z",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "qwen1_5-0_5b-chat-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q2_k.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q3_k_m.gguf",
        "file_size": "333.7 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_0.gguf",
        "file_size": "376.7 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_k_m.gguf",
        "file_size": "388.3 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q5_0.gguf",
        "file_size": "432.0 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q5_k_m.gguf",
        "file_size": "438.0 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q6_k.gguf",
        "file_size": "490.7 MB"
      },
      {
        "model_id": "qwen1_5-0_5b-chat-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q8_0.gguf",
        "file_size": "633.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 4354,
    "createdAt": "2025-08-03T07:56:40Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the Huihui-Qwen3-Coder-30B-A3B-Instruct, offering various quantization types for efficient deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Devstral-Small-2505_gguf",
    "developer": "mistralai",
    "downloads": 4280,
    "createdAt": "2025-05-19T16:34:03Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "devstral",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstral.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "devstralQ4_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "devstralQ4_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "devstralQ5_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "devstralQ8_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/README.md",
    "description": "Devstral-Small-2505 is a lightweight, open-source agentic LLM for software engineering tasks with a 128k context window and Apache 2.0 license, available in GGUF quantized formats for efficient local deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Hermes-4-14B-GGUF",
    "developer": "gabriellarson",
    "downloads": 4276,
    "createdAt": "2025-09-02T19:28:17Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hermes-4-14B-F16",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-F16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ2_XS.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ3_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q2_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q5_0.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Hermes-4-14B-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/Hermes-4-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF/resolve/main/README.md",
    "description": "Hermes 4-Qwen 3 14B is a hybrid-mode reasoning model based on Qwen 3 14B by Nous Research, designed for high-quality, structured outputs, function calling, and aligned reasoning with enhanced capabilities in math, code, and creativity."
  },
  {
    "model_name": "llama-joycaption-beta-one-hf-llava-mmproj-gguf",
    "developer": "concedo",
    "downloads": 4262,
    "createdAt": "2025-05-15T13:36:04.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-F16",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-Q4_K",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-Q8_0",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-llava-mmproj-model-f16",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/llama-joycaption-beta-one-llava-mmproj-model-f16.gguf",
        "file_size": "837.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/README.md",
    "description": "These GGUF quantized models for Joycaption Beta One, along with the mmproj file, are compatible with KoboldCpp 1.91 and above."
  },
  {
    "model_name": "llama-joycaption-beta-one-hf-llava-GGUF",
    "developer": "mradermacher",
    "downloads": 4242,
    "createdAt": "2025-05-14T20:10:44.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q2_K",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q6_K",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.Q8_0",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava.f16",
        "path": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "L3-Dark-Planet-8B-GGUF",
    "developer": "DavidAU",
    "downloads": 4236,
    "createdAt": "2024-09-02T08:29:42.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-IQ4_XS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q2_k.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q3_k_l.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q3_k_m.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q3_k_s.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q4_k_m.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q4_k_s.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q5_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q6_k.gguf",
        "file_size": "539.8 MB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-max-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-D_AU-q5_k_m.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-max-D_AU-Q6_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-max-D_AU-Q8_0.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3-Dark-Planet-8B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/L3-Dark-Planet-8B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "8.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3-Dark-Planet-8B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Tifa-DeepsexV3-14b-GGUF-Q6",
    "developer": "ValueFX9507",
    "downloads": 4236,
    "createdAt": "2025-06-26T10:15:21Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/README.md",
    "description": "Tifa-DeepSexV3-14b 是基于 Qwen14b 的深度优化模型，支持长文生成、超长关联、控制器调节输出风格和字数，并能避免负面词汇，适用于角色扮演和多种文本生成任务。"
  },
  {
    "model_name": "CausalLM-14B-DPO-alpha-GGUF",
    "developer": "tastypear",
    "downloads": 4233,
    "createdAt": "2023-11-25T15:58:11Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "causallm_14b-dpo-alpha.Q3_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q4_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q4_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q5_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q5_K_S",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q5_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q6_K",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q6_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q8_0",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q8_0.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.f16",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.f16.gguf",
        "file_size": "26.4 GB"
      }
    ],
    "readme": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/README.md",
    "description": "This is a GGUF format model for CausalLM's 14B-DPO-alpha, optimized for text generation with a prompt template and licensing under the WTFPL license with Meta Llama 2 License Terms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Irix-12B-Model_Stock-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 4189,
    "createdAt": "2025-03-28T16:03:25.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q2_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q6_K.gguf",
        "file_size": "9.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "SmolLM3-3B-128K-GGUF",
    "developer": "unsloth",
    "downloads": 4180,
    "createdAt": "2025-07-08T23:13:59.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "SmolLM3-3B-128K-BF16",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-BF16.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-IQ4_NL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-IQ4_XS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q2_K",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q2_K_L",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q2_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q3_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q3_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_0",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_1",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_1.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q5_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q5_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q6_K",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q8_0",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ2_XXS.gguf",
        "file_size": "910.9 MB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q2_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q3_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q4_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q5_K_XL.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q6_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q8_K_XL.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/README.md",
    "description": "SmolLM3‑3B is an open 3 B‑parameter multilingual transformer with 128 k‑token context, hybrid reasoning (extended thinking) and tool‑calling capabilities, delivering strong multilingual and reasoning performance."
  },
  {
    "model_name": "silly-v0.2-GGUF",
    "developer": "mradermacher",
    "downloads": 4151,
    "createdAt": "2025-09-03T00:12:37.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "silly-v0.2.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "silly-v0.2.Q2_K",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "silly-v0.2.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "silly-v0.2.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "silly-v0.2.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "silly-v0.2.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "silly-v0.2.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "silly-v0.2.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "silly-v0.2.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "silly-v0.2.Q6_K",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "silly-v0.2.Q8_0",
        "path": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/silly-v0.2.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/silly-v0.2-GGUF/resolve/main/README.md",
    "description": "Provides various GGUF quantizations of the silly‑v0.2 model with download links and usage notes."
  },
  {
    "model_name": "Hermes-3-Llama-3.1-8B-GGUF",
    "developer": "NousResearch",
    "downloads": 4135,
    "createdAt": "2024-08-05T04:56:41.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q6_K",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q8_0",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/README.md",
    "description": "Hermes 3 is a GGUF quantized version of the Llama-3.1 8B model by Nous Research, designed for advanced agentic capabilities, roleplaying, multi-turn conversations, and function calling, with a chat template compatible with Hugging Face Transformers and llama.cpp."
  },
  {
    "model_name": "Megrez2-3x7B-A3B-GGUF",
    "developer": "Infinigence",
    "downloads": 4063,
    "createdAt": "2025-09-15T15:02:07.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Megrez2-3x7B-A3B_BF16",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_BF16.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_IQ4_NL",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_IQ4_NL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_IQ4_XS",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q2_K",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q3_K",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q3_K.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q3_K_L",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q3_K_M",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q3_K_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q3_K_S",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q4_0",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q4_1",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q4_K",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q4_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q4_K_M",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q4_K_S",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q5_0",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q5_0.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q5_1",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q5_K",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q5_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q5_K_M",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q5_K_S",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q6_K",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Megrez2-3x7B-A3B_Q8_0",
        "path": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/Megrez2-3x7B-A3B_Q8_0.gguf",
        "file_size": "7.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF/resolve/main/README.md",
    "description": "Megrez2-3x7B-A3B: 21B MoE model with 3B activated parameters."
  },
  {
    "model_name": "Gemma-3-R1-4B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 4028,
    "createdAt": "2025-08-07T12:31:17Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/README.md",
    "description": "Gemma 3 R1 4B v1 is a reasoning-tuned version of the Google Gemma-3-4B model that offers enhanced capabilities and a more unique, less positive personality, with vision support available in GGUF format."
  },
  {
    "model_name": "XBai-o4-GGUF",
    "developer": "mradermacher",
    "downloads": 4024,
    "createdAt": "2025-08-01T15:41:31Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "XBai-o4.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "XBai-o4.Q2_K",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "XBai-o4.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "XBai-o4.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "XBai-o4.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "XBai-o4.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "XBai-o4.Q6_K",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "XBai-o4.Q8_0",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/README.md",
    "description": "The MetaStoneTec/XBai-o4 model is a quantized version of the original model, available in various quantization formats including Q2_K, Q3_K_S, Q4_K_S, Q6_K, and Q8_0, optimized for efficient inference on GGUF files",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NuMarkdown-8B-Thinking-GGUF",
    "developer": "mradermacher",
    "downloads": 3988,
    "createdAt": "2025-08-07T10:00:41Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "NuMarkdown-8B-Thinking.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q2_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q6_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q8_0",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.f16",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/README.md",
    "description": "The NuMarkdown-8B-Thinking model is a large vision-language model capable of document-to-markdown conversion, OCR, and reasoning, available in various quantized formats for efficient deployment."
  },
  {
    "model_name": "swiss-ai.Apertus-8B-Instruct-2509-GGUF",
    "developer": "DevQuasar",
    "downloads": 3966,
    "createdAt": "2025-09-17T19:07:46.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "imatrix/Apertus-8B-Instruct-2509_imatrix",
        "path": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/imatrix/Apertus-8B-Instruct-2509_imatrix.gguf",
        "file_size": "5.2 MB"
      },
      {
        "model_id": "swiss-ai.Apertus-8B-Instruct-2509.IQ2_S",
        "path": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/swiss-ai.Apertus-8B-Instruct-2509.IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "swiss-ai.Apertus-8B-Instruct-2509.Q4_K_M",
        "path": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/swiss-ai.Apertus-8B-Instruct-2509.Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "swiss-ai.Apertus-8B-Instruct-2509.Q5_K_M",
        "path": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/swiss-ai.Apertus-8B-Instruct-2509.Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "swiss-ai.Apertus-8B-Instruct-2509.Q6_K",
        "path": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/swiss-ai.Apertus-8B-Instruct-2509.Q6_K.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "swiss-ai.Apertus-8B-Instruct-2509.Q8_0",
        "path": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/swiss-ai.Apertus-8B-Instruct-2509.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DevQuasar/swiss-ai.Apertus-8B-Instruct-2509-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen_QwQ-32B-GGUF",
    "developer": "bartowski",
    "downloads": 3964,
    "createdAt": "2025-03-05T16:16:58.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen_QwQ-32B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "Qwen_QwQ-32B-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/Qwen_QwQ-32B-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "mistralai_Magistral-Small-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3949,
    "createdAt": "2025-07-24T16:40:51Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2507 model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp's imatrix quantization method.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "CrucibleLab_M3.2-24B-Loki-V1.3-GGUF",
    "developer": "bartowski",
    "downloads": 3920,
    "createdAt": "2025-08-23T20:08:40Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_XS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_NL",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_XS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_0",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_1",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q8_0",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-bf16",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-imatrix",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the M3.2-24B-Loki-V1.3 model by CrucibleLab, optimized for various inference speeds and resource constraints using llama.cpp's imatrix calibration method."
  },
  {
    "model_name": "RuadaptQwen3-4B-Instruct-GGUF",
    "developer": "RefalMachine",
    "downloads": 3883,
    "createdAt": "2025-06-30T06:13:47Z",
    "tools": true,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "BF16",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "IQ3_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "IQ3_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "IQ4_NL",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "IQ4_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Q2_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q3_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Q3_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Q4_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q4_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Q4_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q5_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q5_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Q5_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q6_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Q8_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "RuadaptQwen3-4B-Instruct is a Russian-adapted version of Qwen/Qwen3-4B, featuring a new tokenizer, continued pre-training on Russian data, and LEP (Learned Embedding Propagation) applied to enhance Russian text generation speed by up to",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-24B-v4o-GGUF",
    "developer": "BeaverAI",
    "downloads": 3879,
    "createdAt": "2025-09-26T14:52:41.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4o-Q2_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/Cydonia-24B-v4o-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4o-Q3_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/Cydonia-24B-v4o-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4o-Q4_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/Cydonia-24B-v4o-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4o-Q5_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/Cydonia-24B-v4o-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4o-Q6_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/Cydonia-24B-v4o-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4o-Q8_0",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/Cydonia-24B-v4o-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF/resolve/main/README.md",
    "description": "v4.2.0 candidate enhances narrative flow, character adherence, and moral complexity while reducing bias and positivity."
  },
  {
    "model_name": "baidu_ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 3865,
    "createdAt": "2025-06-30T03:51:10Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ2_M.gguf",
        "file_size": "155.4 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_M.gguf",
        "file_size": "195.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XS.gguf",
        "file_size": "184.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS.gguf",
        "file_size": "164.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_NL.gguf",
        "file_size": "222.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_XS.gguf",
        "file_size": "215.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "199.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_L.gguf",
        "file_size": "214.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL.gguf",
        "file_size": "238.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_L.gguf",
        "file_size": "254.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_L.gguf",
        "file_size": "280.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K_L.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-bf16.gguf",
        "file_size": "690.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-0.3B-PT model using llama.cpp's imatrix method, offering various quantization options for different performance and memory trade-offs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "VibeVoice-1.5B-gguf",
    "developer": "wsbagnsv1",
    "downloads": 3865,
    "createdAt": "2025-08-30T17:07:15.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "VibeVoice-1.5B-bf16",
        "path": "https://huggingface.co/wsbagnsv1/VibeVoice-1.5B-gguf/resolve/main/VibeVoice-1.5B-bf16.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "VibeVoice-1.5B-q8_0",
        "path": "https://huggingface.co/wsbagnsv1/VibeVoice-1.5B-gguf/resolve/main/VibeVoice-1.5B-q8_0.gguf",
        "file_size": "2.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/wsbagnsv1/VibeVoice-1.5B-gguf/resolve/main/README.md",
    "description": "This model is highly experimental with no current inference support and may undergo changes in the future."
  },
  {
    "model_name": "openai_gpt-oss-120b-GGUF-MXFP4-Experimental",
    "developer": "bartowski",
    "downloads": 3809,
    "createdAt": "2025-08-05T18:09:24Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "openai_gpt-oss-120b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-120b-GGUF-MXFP4-Experimental/resolve/main/openai_gpt-oss-120b-MXFP4.gguf",
        "file_size": "59.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-120b-GGUF-MXFP4-Experimental/resolve/main/README.md",
    "description": "This is an experimental MXFP4 quantized version of the gpt-oss-120b model by OpenAI, requiring the llama.cpp branch `gpt-oss-mxfp4` to run."
  },
  {
    "model_name": "AI21-Jamba-Reasoning-3B-GGUF",
    "developer": "ai21labs",
    "downloads": 3806,
    "createdAt": "2025-10-05T11:02:28.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "jamba-reasoning-3b-F16",
        "path": "https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B-GGUF/resolve/main/jamba-reasoning-3b-F16.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "jamba-reasoning-3b-Q4_K_M",
        "path": "https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B-GGUF/resolve/main/jamba-reasoning-3b-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "TheDrummer_Gemma-3-R1-12B-v1-GGUF",
    "developer": "bartowski",
    "downloads": 3805,
    "createdAt": "2025-08-12T16:43:38Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ2_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ2_S.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ3_XS.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q2_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_XL.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_K_L.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q5_K_L.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q6_K_L.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-bf16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-imatrix.gguf",
        "file_size": "7.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-3-R1-12B-v1 model by TheDrummer, optimized for various inference speeds and quality levels using llama.cpp's imatrix calibration method."
  },
  {
    "model_name": "LFM2-VL-3B-GGUF",
    "developer": "LiquidAI",
    "downloads": 3797,
    "createdAt": "2025-10-22T11:28:00.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "LFM2-VL-3B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF/resolve/main/LFM2-VL-3B-F16.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "LFM2-VL-3B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF/resolve/main/LFM2-VL-3B-Q4_0.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "LFM2-VL-3B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF/resolve/main/LFM2-VL-3B-Q8_0.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2-VL-3B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF/resolve/main/mmproj-LFM2-VL-3B-F16.gguf",
        "file_size": "821.0 MB"
      },
      {
        "model_id": "mmproj-LFM2-VL-3B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF/resolve/main/mmproj-LFM2-VL-3B-Q8_0.gguf",
        "file_size": "559.6 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8",
    "developer": "ValueFX9507",
    "downloads": 3734,
    "createdAt": "2025-02-15T13:30:54Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV2-7b-0218-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-0218-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0301-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0301-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0228-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0228-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0325-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0325-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Q8.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/README.md",
    "description": "Tifa-DeepSexV2-7b-MGRPO 是基于 Qwen2.5-7B 的深度优化模型，具备 100 万字上下文能力，通过 MGRPO 算法和 Tifa_220B 数据集训练，",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "VibeVoice-Large-pt-gguf",
    "developer": "wsbagnsv1",
    "downloads": 3716,
    "createdAt": "2025-08-30T17:30:38.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "VibeVoice-Large-pt-bf16",
        "path": "https://huggingface.co/wsbagnsv1/VibeVoice-Large-pt-gguf/resolve/main/VibeVoice-Large-pt-bf16.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "VibeVoice-Large-pt-q8_0",
        "path": "https://huggingface.co/wsbagnsv1/VibeVoice-Large-pt-gguf/resolve/main/VibeVoice-Large-pt-q8_0.gguf",
        "file_size": "9.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/wsbagnsv1/VibeVoice-Large-pt-gguf/resolve/main/README.md",
    "description": "This model is highly experimental with no current inference support and may undergo changes in the future."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-v2",
    "developer": "huihui-ai",
    "downloads": 3674,
    "createdAt": "2025-09-27T14:11:00.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q3_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q4_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-f16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-q8_0",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/README.md",
    "description": "An uncensored 20B SFT model with reduced safety filtering."
  },
  {
    "model_name": "Hunyuan-7B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 3660,
    "createdAt": "2025-08-04T06:40:11Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-7B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-F16.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q8_0.gguf",
        "file_size": "7.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This Hugging Face repository provides the Tencent Hunyuan-7B-Instruct large language model, offering efficient inference with support for hybrid reasoning, ultra-long context understanding, and various quantization formats like FP8 and INT4, suitable for deployment across different computational environments.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Menlo_Lucy-GGUF",
    "developer": "bartowski",
    "downloads": 3626,
    "createdAt": "2025-07-18T05:28:56Z",
    "tools": true,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Menlo_Lucy-IQ3_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_M.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_XS.gguf",
        "file_size": "795.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_XXS.gguf",
        "file_size": "719.4 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q2_K",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q2_K_L.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_XL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_0",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_1",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q6_K",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q6_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q8_0",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Menlo_Lucy-bf16",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-bf16.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Lucy model by Menlo, optimized for various quantization types and suitable for use with llama.cpp or LM Studio.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF",
    "developer": "mradermacher",
    "downloads": 3550,
    "createdAt": "2025-08-30T00:41:50.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "granite-4.0-h-micro-GGUF",
    "developer": "unsloth",
    "downloads": 3543,
    "createdAt": "2025-10-02T09:56:07.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "granite-4.0-h-micro-BF16",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-BF16.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-IQ4_NL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-IQ4_XS",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q2_K",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q2_K_L",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q2_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q3_K_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q3_K_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q4_0",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q4_1",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q4_K_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q4_K_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q5_K_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q5_K_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q6_K",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-Q8_0",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-IQ1_M.gguf",
        "file_size": "798.9 MB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-IQ1_S.gguf",
        "file_size": "735.5 MB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-IQ2_XXS.gguf",
        "file_size": "898.9 MB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-Q2_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-Q3_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-Q4_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-Q5_K_XL.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-Q6_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "granite-4.0-h-micro-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-UD-Q8_K_XL.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/granite-4.0-h-micro-GGUF/resolve/main/README.md",
    "description": "IBM's 3B-parameter multilingual instruction-tuned language model with tool-calling capabilities."
  },
  {
    "model_name": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF",
    "developer": "DavidAU",
    "downloads": 3509,
    "createdAt": "2024-11-08T05:24:15.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q2_k.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q3_k_s.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q4_k_s.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q5_k_s.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q6_k.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/L3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-q5_k_m.gguf",
        "file_size": "5.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "phi-4-GGUF",
    "developer": "unsloth",
    "downloads": 3485,
    "createdAt": "2025-01-08T22:57:04.000Z",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "phi-4-F16",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-F16.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "phi-4-Q2_K",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "phi-4-Q2_K_L",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q2_K_L.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "phi-4-Q3_K_M",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q3_K_M.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "phi-4-Q4_K_M",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q4_K_M.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "phi-4-Q5_K_M",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q5_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "phi-4-Q6_K",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "phi-4-Q8_0",
        "path": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/phi-4-Q8_0.gguf",
        "file_size": "14.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/phi-4-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "CodeV-R1-Qwen-7B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 3427,
    "createdAt": "2025-09-12T18:54:44.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ3_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.i1-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "CodeV-R1-Qwen-7B.imatrix",
        "path": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/CodeV-R1-Qwen-7B.imatrix.gguf",
        "file_size": "4.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/CodeV-R1-Qwen-7B-i1-GGUF/resolve/main/README.md",
    "description": "Provides multiple GGUF quantizations of the CodeV‑R1‑Qwen‑7B model with download links and usage instructions."
  },
  {
    "model_name": "tencent_Hunyuan-1.8B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 3418,
    "createdAt": "2025-08-04T12:44:52Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ2_M.gguf",
        "file_size": "666.1 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ3_M.gguf",
        "file_size": "859.1 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ3_XS.gguf",
        "file_size": "801.2 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ3_XXS.gguf",
        "file_size": "732.9 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ4_NL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ4_XS.gguf",
        "file_size": "986.0 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q2_K.gguf",
        "file_size": "741.5 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q2_K_L.gguf",
        "file_size": "798.6 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_L.gguf",
        "file_size": "971.7 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_M.gguf",
        "file_size": "907.0 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_S.gguf",
        "file_size": "831.5 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_1",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q5_K_L.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q6_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-bf16",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-bf16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-imatrix",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-imatrix.gguf",
        "file_size": "2.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Tencent Hunyuan-1.8B-Instruct model using llama.cpp, with various quantization types and quality-performance trade-offs for different hardware."
  },
  {
    "model_name": "InternVL3_5-30B-A3B-GGUF",
    "developer": "lmstudio-community",
    "downloads": 3360,
    "createdAt": "2025-08-26T04:43:58Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "InternVL3_5-30B-A3B-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "InternVL3_5-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "InternVL3_5-30B-A3B-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "InternVL3_5-30B-A3B-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/mmproj-model-f16.gguf",
        "file_size": "606.6 MB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenGVLab/InternVL3_5-30B-A3B model by bartowski, optimized for use with LM Studio's image-text-to-text pipeline."
  },
  {
    "model_name": "Medical-Diagnosis-COT-Gemma3-270M",
    "developer": "alpha-ai",
    "downloads": 3338,
    "createdAt": "2025-08-17T05:24:15.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Medical-Diagnosis-COT-Gemma3-270M.BF16",
        "path": "https://huggingface.co/alpha-ai/Medical-Diagnosis-COT-Gemma3-270M/resolve/main/Medical-Diagnosis-COT-Gemma3-270M.BF16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "Medical-Diagnosis-COT-Gemma3-270M.Q8_0",
        "path": "https://huggingface.co/alpha-ai/Medical-Diagnosis-COT-Gemma3-270M/resolve/main/Medical-Diagnosis-COT-Gemma3-270M.Q8_0.gguf",
        "file_size": "278.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/alpha-ai/Medical-Diagnosis-COT-Gemma3-270M/resolve/main/README.md",
    "description": "This model is a fine-tuned Gemma-3 270M for medical question answering with explicit chain-of-thought (CoT) reasoning, producing `</think>...</think>` blocks followed by final answers for research and internal use, but not for clinical decisions."
  },
  {
    "model_name": "granite-4.0-micro-GGUF",
    "developer": "ibm-granite",
    "downloads": 3310,
    "createdAt": "2025-09-24T20:56:03.000Z",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "granite-4.0-micro-Q2_K",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q3_K_L",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q3_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q3_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q3_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q4_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q4_1",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q4_1.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q4_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q4_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q4_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q5_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q5_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q5_1",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q5_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q5_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q5_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q5_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q5_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q6_K",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q6_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "granite-4.0-micro-Q8_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q8_0.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "granite-4.0-micro-f16",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-f16.gguf",
        "file_size": "6.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Ling-mini-2.0-GGUF",
    "developer": "inclusionAI",
    "downloads": 3254,
    "createdAt": "2025-09-24T09:15:09.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ling-mini-2.0-BF16",
        "path": "https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF/resolve/main/Ling-mini-2.0-BF16.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Ling-mini-2.0-Q2_K",
        "path": "https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF/resolve/main/Ling-mini-2.0-Q2_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ling-mini-2.0-Q4_K_M",
        "path": "https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF/resolve/main/Ling-mini-2.0-Q4_K_M.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Ling-mini-2.0-Q6_K",
        "path": "https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF/resolve/main/Ling-mini-2.0-Q6_K.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Ling-mini-2.0-Q8_0",
        "path": "https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF/resolve/main/Ling-mini-2.0-Q8_0.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF/resolve/main/README.md",
    "description": "Quantized Ling-mini-2.0 model for llama.cpp inference."
  },
  {
    "model_name": "Dolphin-X1-8B-GGUF",
    "developer": "dphn",
    "downloads": 3250,
    "createdAt": "2025-09-18T13:45:58.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Dolphin-X1-8B-Q3_K_L",
        "path": "https://huggingface.co/dphn/Dolphin-X1-8B-GGUF/resolve/main/Dolphin-X1-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Dolphin-X1-8B-Q4_K_M",
        "path": "https://huggingface.co/dphn/Dolphin-X1-8B-GGUF/resolve/main/Dolphin-X1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin-X1-8B-Q5_K_M",
        "path": "https://huggingface.co/dphn/Dolphin-X1-8B-GGUF/resolve/main/Dolphin-X1-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin-X1-8B-Q6_K",
        "path": "https://huggingface.co/dphn/Dolphin-X1-8B-GGUF/resolve/main/Dolphin-X1-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin-X1-8B-Q8_0",
        "path": "https://huggingface.co/dphn/Dolphin-X1-8B-GGUF/resolve/main/Dolphin-X1-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/Dolphin-X1-8B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Hermes-2-Pro-Mistral-7B-GGUF",
    "developer": "NousResearch",
    "downloads": 3243,
    "createdAt": "2024-03-02T04:02:33.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q2_K",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q3_K_L",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q3_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q3_K_S",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q4_0",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q4_K_S",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q5_0",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q5_K_S",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q6_K",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Hermes-2-Pro-Mistral-7B.Q8_0",
        "path": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen2.5-Omni-3B-GGUF",
    "developer": "ggml-org",
    "downloads": 3163,
    "createdAt": "2025-05-26T08:52:28.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen2.5-Omni-3B-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-3B-Q8_0",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q8_0.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-3B-f16",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-f16.gguf",
        "file_size": "6.3 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen2.5-Omni-3B-Q8_0",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-Q8_0.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "mmproj-Qwen2.5-Omni-3B-f16",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-f16.gguf",
        "file_size": "2.4 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/README.md",
    "description": "This is a Qwen2.5-Omni-3B model converted to GGUF format for efficient deployment, supporting text, audio, and image input but not video or audio generation."
  },
  {
    "model_name": "LFM2-350M-ENJP-MT-GGUF",
    "developer": "LiquidAI",
    "downloads": 3120,
    "createdAt": "2025-09-03T01:29:36.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "LFM2-350M-ENJP-MT-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-350M-ENJP-MT-F32",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-F32.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "LFM2-350M-ENJP-MT-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-350M-ENJP-MT-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-Q4_K_M.gguf",
        "file_size": "218.7 MB"
      },
      {
        "model_id": "LFM2-350M-ENJP-MT-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-Q5_K_M.gguf",
        "file_size": "248.3 MB"
      },
      {
        "model_id": "LFM2-350M-ENJP-MT-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-Q6_K.gguf",
        "file_size": "279.8 MB"
      },
      {
        "model_id": "LFM2-350M-ENJP-MT-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/README.md",
    "description": "A GGUF checkpoint of LFM2‑350M fine‑tuned for fast Japanese↔English translation, runnable with llama.cpp."
  },
  {
    "model_name": "ruGPT-3.5-13B-GGUF",
    "developer": "oblivious",
    "downloads": 3099,
    "createdAt": "2024-01-27T06:12:52Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "ruGPT-3.5-13B-Q2_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q2_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_L",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_L.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_XS",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_XS.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_1",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_1.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_K_M",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_K_M.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_K_S.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_1",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_1.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_K_M",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_K_M.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q6_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q6_K.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q8_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q8_0.gguf",
        "file_size": "13.0 GB"
      }
    ],
    "readme": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/README.md",
    "description": "ruGPT-3.5-13B-GGUF is a quantized GGUF format model for Russian and English text generation based on the ruGPT-3.5-13B base model.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-24B-v4.1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 3096,
    "createdAt": "2025-08-18T20:32:34Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.imatrix",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/README.md",
    "description": "TheDrummer/Cydonia-24B-v4.1 is a large language model quantized by mradermacher into various GGUF formats, including imatrix and different quantization types, offering options for size, speed, and quality."
  },
  {
    "model_name": "Kunoichi-DPO-v2-7B-GGUF",
    "developer": "brittlewis12",
    "downloads": 3094,
    "createdAt": "2024-01-16T16:33:41.000Z",
    "tools": false,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ1_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ1_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_XXS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_XXS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ4_NL",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ4_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q2_K",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q2_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_L",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_1",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_1",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q6_K",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q8_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.fp16",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/README.md",
    "description": "Kunoichi‑DPO‑v2‑7B GGUF is a quantized Mistral‑7B model in GGUF format, using Alpaca‑style prompts and achieving strong benchmark scores."
  },
  {
    "model_name": "ai21labs_AI21-Jamba-Reasoning-3B-GGUF",
    "developer": "bartowski",
    "downloads": 3090,
    "createdAt": "2025-10-08T14:00:11.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-IQ2_M",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-IQ3_M",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q2_K",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q2_K_L.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_L.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q3_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q4_0",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q4_1",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q4_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q5_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q6_K",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q6_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-Q8_0",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-bf16",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-bf16.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "ai21labs_AI21-Jamba-Reasoning-3B-imatrix",
        "path": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/ai21labs_AI21-Jamba-Reasoning-3B-imatrix.gguf",
        "file_size": "2.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/ai21labs_AI21-Jamba-Reasoning-3B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-14B-GGUF",
    "developer": "bartowski",
    "downloads": 3088,
    "createdAt": "2025-08-26T03:54:46Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ2_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-bf16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-imatrix.gguf",
        "file_size": "7.4 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-14B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-14B-bf16.gguf",
        "file_size": "673.8 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-14B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-14B-f16.gguf",
        "file_size": "672.7 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-14B model using llama.cpp's imatrix calibration method, suitable for deployment on various hardware platforms with different performance and memory trade-offs."
  },
  {
    "model_name": "gemma-3-4b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 3085,
    "createdAt": "2025-03-12T20:43:28.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-4B",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-4B.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 4B model is a lightweight, open-source, instruction-tuned multimodal model from Google, capable of handling text and image inputs, generating text outputs, and trained on diverse data including text, code, math, and images, with strong performance across various benchmarks and tasks"
  },
  {
    "model_name": "Llama-3_3-Nemotron-Super-49B-v1_5-GGUF",
    "developer": "gabriellarson",
    "downloads": 3083,
    "createdAt": "2025-07-25T23:29:14Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_S.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_M.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XS.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XXS.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_M.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_S.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XS.gguf",
        "file_size": "19.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XXS.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_NL.gguf",
        "file_size": "26.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_XS.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_L.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_M.gguf",
        "file_size": "22.6 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_S.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_0.gguf",
        "file_size": "26.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_M.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_0.gguf",
        "file_size": "32.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_M.gguf",
        "file_size": "33.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K.gguf",
        "file_size": "38.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q8_0.gguf",
        "file_size": "49.4 GB"
      },
      {
        "model_id": "Llama-49B-3_3-Nemotron-Super-v1_5-F16",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-49B-3_3-Nemotron-Super-v1_5-F16.gguf",
        "file_size": "92.9 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/README.md",
    "description": "The Llama-3.3-Nemotron-Super-49B-v1.5 is a large language model derived from Meta's Llama-3.3-70B-Instruct, optimized for reasoning and chat tasks with a 128K token context length, enhanced",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 3068,
    "createdAt": "2025-08-13T14:29:33Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.imatrix",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2 model, optimized for various GGUF quantization types with different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request",
    "developer": "LWDCLS",
    "downloads": 3062,
    "createdAt": "2024-07-28T06:59:41.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-BF16",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-F16",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ3_M-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ3_XXS-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ3_XXS-imat.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ4_XS-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q4_K_M-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q4_K_S-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q5_K_M-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q5_K_S-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q6_K-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q8_0-imat",
        "path": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request/resolve/main/README.md",
    "description": "Uncensored Llama 3.1 8B Instruct under unlicense."
  },
  {
    "model_name": "Magistral-Small-2506_gguf",
    "developer": "mistralai",
    "downloads": 3039,
    "createdAt": "2025-06-09T09:25:46Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Magistral-Small-2506",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506_Q8_0",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506_Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/README.md",
    "description": "Magistral-Small-2506_gguf is a lightweight, efficient reasoning model with 24B parameters, trained on Mistral Small 3.1 with SFT and RL, available under Apache 2.0 license for local deployment and inference using llama.ccp.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "nvidia_OpenReasoning-Nemotron-1.5B-GGUF",
    "developer": "bartowski",
    "downloads": 3034,
    "createdAt": "2025-07-18T20:42:31Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_M.gguf",
        "file_size": "740.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XS.gguf",
        "file_size": "697.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XXS.gguf",
        "file_size": "637.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ4_NL.gguf",
        "file_size": "893.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ4_XS.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q2_K_L.gguf",
        "file_size": "698.9 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_XL.gguf",
        "file_size": "893.3 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_0.gguf",
        "file_size": "894.1 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_1.gguf",
        "file_size": "969.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_L.gguf",
        "file_size": "994.3 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q6_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-bf16.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenReasoning-Nemotron-1.5B model by nvidia using llama.cpp, offering various quantization options for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Snowpiercer-15B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 2950,
    "createdAt": "2025-09-26T15:50:35.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Snowpiercer-15B-v3a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/Snowpiercer-15B-v3a-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v3a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/Snowpiercer-15B-v3a-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v3a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/Snowpiercer-15B-v3a-Q4_K_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v3a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/Snowpiercer-15B-v3a-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v3a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/Snowpiercer-15B-v3a-Q6_K.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v3a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/Snowpiercer-15B-v3a-Q8_0.gguf",
        "file_size": "14.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v3-GGUF/resolve/main/README.md",
    "description": "Snowpiercer 15B v3 is a creative writing-focused model by TheDrummer."
  },
  {
    "model_name": "ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF",
    "developer": "bartowski",
    "downloads": 2885,
    "createdAt": "2025-10-01T16:14:25.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ2_M",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ2_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ2_S",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ2_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ2_XS",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ2_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ3_M",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ3_M.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ3_XS",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ3_XS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ3_XXS.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ4_NL",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ4_NL.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ4_XS",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-IQ4_XS.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q2_K",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q2_K_L",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q2_K_L.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_L",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_L.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_M",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_S",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q3_K_XL.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_0",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_1",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_1.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_K_L",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_K_L.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_K_M",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_K_M.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_K_S",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q4_K_S.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q5_K_L",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q5_K_L.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q5_K_M",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q5_K_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q5_K_S",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q5_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q6_K",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q6_K.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q6_K_L",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q6_K_L.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-Q8_0",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-Q8_0.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-bf16",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-bf16.gguf",
        "file_size": "26.9 GB"
      },
      {
        "model_id": "ServiceNow-AI_Apriel-1.5-15b-Thinker-imatrix",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/ServiceNow-AI_Apriel-1.5-15b-Thinker-imatrix.gguf",
        "file_size": "8.1 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-ServiceNow-AI_Apriel-1.5-15b-Thinker-bf16",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/mmproj-ServiceNow-AI_Apriel-1.5-15b-Thinker-bf16.gguf",
        "file_size": "831.3 MB"
      },
      {
        "model_id": "mmproj-ServiceNow-AI_Apriel-1.5-15b-Thinker-f16",
        "path": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/mmproj-ServiceNow-AI_Apriel-1.5-15b-Thinker-f16.gguf",
        "file_size": "829.8 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.5-15b-Thinker-GGUF/resolve/main/README.md",
    "description": "imatrix-quantized GGUF versions of Apriel-1.5-15b-Thinker for efficient inference."
  },
  {
    "model_name": "Vistral-24B-Instruct-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 2868,
    "createdAt": "2025-09-30T06:56:03.000Z",
    "tools": false,
    "num_quants": 30,
    "quants": [
      {
        "model_id": "Vistral-24B-Instruct-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q3_K.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q4_K.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q5_0.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q5_1.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q5_K.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Vistral-24B-Instruct-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/Vistral-24B-Instruct-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Vikhrmodels/Vistral-24B-Instruct-GGUF/resolve/main/README.md",
    "description": "Vistral-24B-Instruct is a Russian-English unimodal LLM based on Mistral-Small, optimized for bilingual instruction following."
  },
  {
    "model_name": "QVikhr-3-4B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 2850,
    "createdAt": "2025-06-28T21:03:17Z",
    "num_quants": 31,
    "quants": [
      {
        "model_id": "QVikhr-3-4B-Instruction-F16",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-4B-Instruction is an instructive model based on Qwen/Qwen3-4B, trained on the Russian-language dataset GrandMaster2 for high-efficiency text processing in Russian and English.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-7B-Instruct-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 2839,
    "createdAt": "2024-10-11T18:14:11Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-7B-Instruct-Uncensored.f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/Qwen2.5-7B-Instruct-Uncensored.f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen2.5-7B-Instruct-Uncensored model, optimized for multiple languages and various quantization types for efficient inference."
  },
  {
    "model_name": "Huihui-gemma-3n-E4B-it-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 2833,
    "createdAt": "2025-07-11T00:48:21Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.f16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gemma-3n-E4B-it-abliterated model, available in various GGUF quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2825,
    "createdAt": "2025-01-22T16:41:41.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ1_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_XS.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q2_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.i1-Q6_K.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "mradermacher's IQ-quantized GGUF versions of DeepSeek-R1-Distill-Qwen-14B-abliterated."
  },
  {
    "model_name": "TheDrummer_Cydonia-Redux-22B-v1.1-GGUF",
    "developer": "bartowski",
    "downloads": 2820,
    "createdAt": "2025-10-12T17:56:53.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ2_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ2_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ2_XS.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ3_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ3_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ3_XXS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ4_NL.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q2_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q3_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q4_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q4_1.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q5_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q6_K_L.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-Q8_0.gguf",
        "file_size": "22.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-bf16.gguf",
        "file_size": "41.4 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-Redux-22B-v1.1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/TheDrummer_Cydonia-Redux-22B-v1.1-imatrix.gguf",
        "file_size": "11.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-Redux-22B-v1.1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Gemma-R1-4B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 2787,
    "createdAt": "2025-08-07T12:31:17Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/README.md",
    "description": "Gemma R1 4B v3 is a reasoning-tuned version of the Google Gemma-3-4B model that offers enhanced capabilities and a more unique, less positive personality, with vision support available through GGUF format."
  },
  {
    "model_name": "fastvlm-gguf",
    "developer": "calcuis",
    "downloads": 2725,
    "createdAt": "2025-09-02T20:43:58.000Z",
    "tools": false,
    "num_quants": 36,
    "quants": [
      {
        "model_id": "fastvlm-0.5b-bf16",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-bf16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "fastvlm-0.5b-f16",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-f16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "fastvlm-0.5b-f32",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-f32.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "fastvlm-0.5b-iq4_nl",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-iq4_nl.gguf",
        "file_size": "957.5 MB"
      },
      {
        "model_id": "fastvlm-0.5b-q2_k",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q2_k.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "fastvlm-0.5b-q3_k_m",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q3_k_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "fastvlm-0.5b-q4_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q4_0.gguf",
        "file_size": "957.5 MB"
      },
      {
        "model_id": "fastvlm-0.5b-q4_1",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q4_1.gguf",
        "file_size": "978.8 MB"
      },
      {
        "model_id": "fastvlm-0.5b-q4_k_m",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q4_k_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "fastvlm-0.5b-q5_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q5_0.gguf",
        "file_size": "1000.1 MB"
      },
      {
        "model_id": "fastvlm-0.5b-q5_1",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q5_1.gguf",
        "file_size": "1021.5 MB"
      },
      {
        "model_id": "fastvlm-0.5b-q5_k_m",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q5_k_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "fastvlm-0.5b-q6_k",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q6_k.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "fastvlm-0.5b-q8_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-0.5b-q8_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "fastvlm-1.5b-iq3_s",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-iq3_s.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "fastvlm-1.5b-iq3_xxs",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-iq3_xxs.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "fastvlm-1.5b-iq4_nl",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-iq4_nl.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "fastvlm-1.5b-iq4_xs",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-iq4_xs.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q2_k",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q2_k.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q3_k_m",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q3_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q4_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q4_1",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q4_k_m",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q4_k_m.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q5_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q5_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q5_1",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q5_1.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q5_k_m",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q5_k_m.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q6_k",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q6_k.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "fastvlm-1.5b-q8_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-1.5b-q8_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "fastvlm-7b-iq3_s",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-iq3_s.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "fastvlm-7b-iq3_xxs",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-iq3_xxs.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "fastvlm-7b-iq4_nl",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-iq4_nl.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "fastvlm-7b-iq4_xs",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-iq4_xs.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "fastvlm-7b-q2_k",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-q2_k.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "fastvlm-7b-q4_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-q4_0.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "fastvlm-7b-q6_k",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-q6_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "fastvlm-7b-q8_0",
        "path": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/fastvlm-7b-q8_0.gguf",
        "file_size": "8.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/calcuis/fastvlm-gguf/resolve/main/README.md",
    "description": "fastvlm‑gguf lets you run Apple’s FastVLM‑0.5B via the gguf‑connector using commands like ggc f5/f6/f7/f9 for different interaction modes."
  },
  {
    "model_name": "Llama3.1-DeepDilemma-V1-8B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2724,
    "createdAt": "2025-09-17T08:46:08.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q2_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.i1-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama3.1-DeepDilemma-V1-8B.imatrix",
        "path": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/Llama3.1-DeepDilemma-V1-8B.imatrix.gguf",
        "file_size": "4.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Llama3.1-DeepDilemma-V1-8B-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF",
    "developer": "DavidAU",
    "downloads": 2715,
    "createdAt": "2025-08-06T05:23:38.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-2-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE2",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE2.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE3",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE3.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE4",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE4.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/README.md",
    "description": "This model is a specialized GPT-oss-20B MOE (Mixture of Experts) variant with 128k context, 24 experts, and supports code generation, reasoning, and chat, offering high performance with quants like IQ4_NL and MXFP4"
  },
  {
    "model_name": "Qwen2.5-0.5B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 2709,
    "createdAt": "2024-09-17T19:30:40.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-0.5B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-IQ2_M.gguf",
        "file_size": "313.4 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-IQ3_M.gguf",
        "file_size": "326.9 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-IQ3_XS.gguf",
        "file_size": "322.9 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-IQ4_XS.gguf",
        "file_size": "333.2 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q2_K.gguf",
        "file_size": "322.9 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q2_K_L.gguf",
        "file_size": "322.9 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q3_K_L.gguf",
        "file_size": "352.2 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q3_K_M.gguf",
        "file_size": "339.0 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q3_K_S.gguf",
        "file_size": "322.6 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q3_K_XL.gguf",
        "file_size": "352.2 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_0.gguf",
        "file_size": "336.6 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_0_4_4.gguf",
        "file_size": "335.8 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_0_4_8.gguf",
        "file_size": "335.8 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_0_8_8.gguf",
        "file_size": "335.8 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_K_L.gguf",
        "file_size": "379.4 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf",
        "file_size": "379.4 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_K_S.gguf",
        "file_size": "367.6 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_L.gguf",
        "file_size": "400.6 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
        "file_size": "400.6 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_S.gguf",
        "file_size": "393.6 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q6_K.gguf",
        "file_size": "482.3 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q6_K_L.gguf",
        "file_size": "482.3 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q8_0.gguf",
        "file_size": "506.5 MB"
      },
      {
        "model_id": "Qwen2.5-0.5B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-f16.gguf",
        "file_size": "948.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "nvidia_OpenReasoning-Nemotron-7B-GGUF",
    "developer": "bartowski",
    "downloads": 2706,
    "createdAt": "2025-07-18T20:42:50Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ2_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenReasoning-Nemotron-7B model by nvidia, optimized for various inference speeds and memory usage using llama.cpp's imatrix quantization method.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "LFM2-350M-GGUF",
    "developer": "LiquidAI",
    "downloads": 2696,
    "createdAt": "2025-07-12T12:02:17Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-350M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-350M-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-350M-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_K_M.gguf",
        "file_size": "218.7 MB"
      },
      {
        "model_id": "LFM2-350M-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q5_K_M.gguf",
        "file_size": "248.3 MB"
      },
      {
        "model_id": "LFM2-350M-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q6_K.gguf",
        "file_size": "279.8 MB"
      },
      {
        "model_id": "LFM2-350M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/README.md",
    "description": "LFM2-350M-GGUF is a hybrid model developed by Liquid AI for edge AI and on-device deployment, optimized for quality, speed, and memory efficiency."
  },
  {
    "model_name": "Seed-Coder-8B-Reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 2689,
    "createdAt": "2025-05-18T12:29:03Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Seed-Coder-8B-Reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-BF16.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q8_0.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/README.md",
    "description": "The Seed-Coder-8B-Reasoning model is a high-performance, parameter-efficient, and transparent open-source code model trained for enhanced reasoning capabilities using RL.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-32B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 2678,
    "createdAt": "2025-05-02T23:05:43Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-32B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the Qwen3-32B-Uncensored base model, offering various GGUF quantization options for efficient deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "bralynn_pydevmini1-GGUF",
    "developer": "bartowski",
    "downloads": 2676,
    "createdAt": "2025-09-09T14:08:32.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "bralynn_pydevmini1-IQ2_M",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-IQ3_M",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q2_K",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q4_0",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q4_1",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q6_K",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-Q8_0",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-bf16",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "bralynn_pydevmini1-imatrix",
        "path": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/bralynn_pydevmini1-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/bralynn_pydevmini1-GGUF/resolve/main/README.md",
    "description": "Multiple llama.cpp‑quantized .gguf versions (various Q‑ and I‑quant levels) of the bralynn/pydevmini1 model for text generation."
  },
  {
    "model_name": "cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF",
    "developer": "bartowski",
    "downloads": 2650,
    "createdAt": "2025-10-20T21:34:35.000Z",
    "tools": true,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_M",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_S",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_S.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_XS.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ2_XXS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ3_M",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ3_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ3_XS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ3_XXS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ4_NL.gguf",
        "file_size": "13.2 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-IQ4_XS.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q2_K",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q2_K_L.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_S.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q3_K_XL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_0",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_0.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_1",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_L.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_M.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_S.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q5_K_L.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q5_K_M.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q5_K_S.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q6_K",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q6_K.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q6_K_L.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-bf16",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-bf16.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "cerebras_Qwen3-Coder-REAP-25B-A3B-imatrix",
        "path": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-imatrix.gguf",
        "file_size": "94.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-VL-32B-Instruct-GGUF",
    "developer": "yairpatch",
    "downloads": 2639,
    "createdAt": "2025-10-22T07:13:58.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Qwen3-VL-32B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF/resolve/main/Qwen3-VL-32B-Instruct-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3-VL-32B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF/resolve/main/Qwen3-VL-32B-Instruct-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen3-VL-32B-Instruct",
        "path": "https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF/resolve/main/mmproj-Qwen3-VL-32B-Instruct.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-8B-GGUF",
    "developer": "bartowski",
    "downloads": 2609,
    "createdAt": "2025-08-26T03:23:48Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q2_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_XL.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q5_K_L.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q6_K_L.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-bf16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-imatrix.gguf",
        "file_size": "5.1 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-8B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-8B-bf16.gguf",
        "file_size": "647.8 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-8B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-8B-f16.gguf",
        "file_size": "646.7 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-8B model using llama.cpp's imatrix method, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "Hermes-4-14B-BF16-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2581,
    "createdAt": "2025-09-16T11:49:50.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ2_XS.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ3_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q2_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.i1-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Hermes-4-14B-BF16-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/Hermes-4-14B-BF16-abliterated.imatrix.gguf",
        "file_size": "7.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Hermes-4-14B-BF16-abliterated-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Voxtral-3B-But-4B-Text-Only-GGUF",
    "developer": "SaisExperiments",
    "downloads": 2574,
    "createdAt": "2025-07-16T08:55:59Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-F16",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-IQ4_NL",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-IQ4_XS",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q4_0",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q5_K_M",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q6_K",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q8_0",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the Mistral AI Voxtral-Mini-3B-2507 base model, with Whisper layers removed and mixed with MiniStral configurations, resulting in a 4B text-only model.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Muse-12B-GGUF",
    "developer": "QuantFactory",
    "downloads": 2548,
    "createdAt": "2025-09-13T11:32:31.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Muse-12B.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Muse-12B.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Muse-12B.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Muse-12B.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Muse-12B.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Muse-12B.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "Muse-12B.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Muse-12B.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Muse-12B.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q5_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Muse-12B.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Muse-12B.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Muse-12B.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Muse-12B.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Muse-12B.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/Muse-12B.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/Muse-12B-GGUF/resolve/main/README.md",
    "description": "Muse‑12B GGUF is a quantized Mistral‑Nemo‑based model fine‑tuned for second‑person adventure storytelling."
  },
  {
    "model_name": "aquif-3-moe-17b-a2.8b-GGUF",
    "developer": "mradermacher",
    "downloads": 2541,
    "createdAt": "2025-08-03T21:50:46.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "aquif-3-moe-17b-a2.8b.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.IQ4_XS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q2_K",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q2_K.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q3_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q3_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q3_K_S.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q4_K_M.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q4_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q5_K_M.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q5_K_S.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q6_K",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q6_K.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "aquif-3-moe-17b-a2.8b.Q8_0",
        "path": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/aquif-3-moe-17b-a2.8b.Q8_0.gguf",
        "file_size": "16.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF/resolve/main/README.md",
    "description": "Multiple GGUF quantizations of the aquif‑3‑moe‑17B‑A2.8B model are provided for efficient inference."
  },
  {
    "model_name": "Llama-3.2-uncensored-erotica",
    "developer": "hungng",
    "downloads": 2534,
    "createdAt": "2024-11-11T09:31:38.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "unsloth.F16",
        "path": "https://huggingface.co/hungng/Llama-3.2-uncensored-erotica/resolve/main/unsloth.F16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/hungng/Llama-3.2-uncensored-erotica/resolve/main/README.md",
    "description": "Apache 2.0-licensed finetuned Llama-3.2-3B-Instruct model trained 2x faster using Unsloth."
  },
  {
    "model_name": "OmniAudio-2.6B",
    "developer": "NexaAI",
    "downloads": 2513,
    "createdAt": "2024-12-11T16:50:52.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "OmniAudio-2.6B-mmprojector-fp16",
        "path": "https://huggingface.co/NexaAI/OmniAudio-2.6B/resolve/main/OmniAudio-2.6B-mmprojector-fp16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "OmniAudio-2.6B-model-fp16",
        "path": "https://huggingface.co/NexaAI/OmniAudio-2.6B/resolve/main/OmniAudio-2.6B-model-fp16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "OmniAudio-2.6B-model-q4_0",
        "path": "https://huggingface.co/NexaAI/OmniAudio-2.6B/resolve/main/OmniAudio-2.6B-model-q4_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "OmniAudio-2.6B-model-q4_K_M",
        "path": "https://huggingface.co/NexaAI/OmniAudio-2.6B/resolve/main/OmniAudio-2.6B-model-q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "OmniAudio-2.6B-model-q8_0",
        "path": "https://huggingface.co/NexaAI/OmniAudio-2.6B/resolve/main/OmniAudio-2.6B-model-q8_0.gguf",
        "file_size": "2.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NexaAI/OmniAudio-2.6B/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "gemma-2-2b-it-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 2503,
    "createdAt": "2024-08-01T02:26:19.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "gemma-2-2b-it-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q2_K_L.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q3_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q4_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q5_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-abliterated-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/gemma-2-2b-it-abliterated-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/gemma-2-2b-it-abliterated-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "LFM2-1.2B-Tool-GGUF",
    "developer": "LiquidAI",
    "downloads": 2495,
    "createdAt": "2025-09-05T16:22:34.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-1.2B-Tool-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/LFM2-1.2B-Tool-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-1.2B-Tool-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/LFM2-1.2B-Tool-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-1.2B-Tool-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/LFM2-1.2B-Tool-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2-1.2B-Tool-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/LFM2-1.2B-Tool-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2-1.2B-Tool-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/LFM2-1.2B-Tool-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2-1.2B-Tool-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/LFM2-1.2B-Tool-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-1.2B-Tool-GGUF/resolve/main/README.md",
    "description": "Edge tool-calling model for precise API and database interactions on edge devices."
  },
  {
    "model_name": "mistralai_Voxtral-Mini-3B-2507-GGUF",
    "developer": "bartowski",
    "downloads": 2478,
    "createdAt": "2025-07-28T17:11:40Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q2_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q6_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-imatrix.gguf",
        "file_size": "3.2 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistralai Voxtral-Mini-3B-2507 model using llama.cpp imatrix quantization, offering various quantization types for different performance and memory trade-offs.",
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Voxtral-Mini-3B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Mini-3B-2507-bf16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Mini-3B-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Mini-3B-2507-f16.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "Lumimaid-v0.2-8B-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 2476,
    "createdAt": "2024-07-28T20:18:42.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Lumimaid-v0.2-8B-BF16",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-F16",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-IQ3_XXS-imat.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Lumimaid-v0.2-8B-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/Lumimaid-v0.2-8B-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/Lumimaid-v0.2-8B-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "MN-12B-Lyra-v4-GGUF",
    "developer": "bartowski",
    "downloads": 2473,
    "createdAt": "2024-09-09T10:04:37Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MN-12B-Lyra-v4-IQ2_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ3_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ3_XS",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ4_XS",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q2_K",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q2_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q2_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_XL.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_4_4.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_4_8.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_8_8.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q6_K",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q6_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q8_0",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-f16",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-f16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the MN-12B-Lyra-v4 model using llama.cpp, optimized for different hardware and performance needs, with recommended options for quality and speed.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-ReduX-22B-v1.1-GGUF",
    "developer": "TheDrummer",
    "downloads": 2462,
    "createdAt": "2025-10-10T05:01:27.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-ReduX-22B-v1c-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/Cydonia-ReduX-22B-v1c-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-ReduX-22B-v1c-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/Cydonia-ReduX-22B-v1c-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Cydonia-ReduX-22B-v1c-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/Cydonia-ReduX-22B-v1c-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Cydonia-ReduX-22B-v1c-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/Cydonia-ReduX-22B-v1c-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Cydonia-ReduX-22B-v1c-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/Cydonia-ReduX-22B-v1c-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Cydonia-ReduX-22B-v1c-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/Cydonia-ReduX-22B-v1c-Q8_0.gguf",
        "file_size": "22.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1.1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Cydonia-ReduX-22B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 2452,
    "createdAt": "2025-09-07T19:49:04.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-Redux-22B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/Cydonia-Redux-22B-v1b-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/Cydonia-Redux-22B-v1b-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/Cydonia-Redux-22B-v1b-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/Cydonia-Redux-22B-v1b-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/Cydonia-Redux-22B-v1b-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/Cydonia-Redux-22B-v1b-Q8_0.gguf",
        "file_size": "22.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1-GGUF/resolve/main/README.md",
    "description": "Cydonia ReduX 22B v1 is TheDrummer’s creatively tuned 22‑billion‑parameter Mistral‑Small model optimized for storytelling and entertainment."
  },
  {
    "model_name": "L3.1-RP-Hero-Dirty_Harry-8B-GGUF",
    "developer": "DavidAU",
    "downloads": 2421,
    "createdAt": "2024-11-28T21:51:31.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q2_k.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q3_k_s.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q4_k_s.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q5_k_s.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q6_k.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-Dirty_Harry-8B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/L3.1-RP-Hero-Dirty_Harry-8B-D_AU-q5_k_m.gguf",
        "file_size": "5.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF",
    "developer": "DavidAU",
    "downloads": 2416,
    "createdAt": "2024-08-16T00:01:54.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-D_AU-Q2_k.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-D_AU-Q3_k_l.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-D_AU-Q3_k_s.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-D_AU-Q4_k_s.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-D_AU-Q5_k_s.gguf",
        "file_size": "22.7 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-D_AU-Q6_k.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-IQ1_M",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-IQ1_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-IQ2_XS",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-IQ2_XS.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-IQ2_XXS",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-IQ2_XXS.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-IQ3_M",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-IQ3_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V1-IQ3_XS",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V1-IQ3_XS.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-D_AU-Q2_k.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-D_AU-Q3_k_l.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-D_AU-Q3_k_s.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-D_AU-Q4_k_s.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-D_AU-Q5_k_s.gguf",
        "file_size": "22.7 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-D_AU-Q6_k.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-IQ1_M",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-IQ1_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-IQ2_XS",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-IQ2_XS.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-IQ2_XXS",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-IQ2_XXS.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-IQ3_M",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-IQ3_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Command-R-35B-Dark-Horror-V2-IQ3_XS",
        "path": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/Command-R-35B-Dark-Horror-V2-IQ3_XS.gguf",
        "file_size": "14.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound",
    "developer": "Intel",
    "downloads": 2404,
    "createdAt": "2025-08-04T08:15:17Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q4_K_M",
        "path": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound/resolve/main/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q4_K_M.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model, optimized for efficiency using Intel's auto-round algorithm, suitable for deployment with Llamacpp."
  },
  {
    "model_name": "DreamOmni2-7.6B-GGUF",
    "developer": "rafacost",
    "downloads": 2396,
    "createdAt": "2025-10-18T11:36:52.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-F16",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q2_K",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q3_K_M",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q3_K_S",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q4_0",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q4_K_M",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q4_K_S",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q5_0",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q5_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q5_K_M",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q5_K_S",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q6_K",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "DreamOmni2-Vlm-Model-7.6B-Q8_0",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/DreamOmni2-Vlm-Model-7.6B-Q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-DreamOmni2-7.6B-GGUF-f16",
        "path": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/mmproj-DreamOmni2-7.6B-GGUF-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "readme": "https://huggingface.co/rafacost/DreamOmni2-7.6B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "ChatGPT-5-Q8_0-GGUF",
    "developer": "Hack337",
    "downloads": 2384,
    "createdAt": "2025-04-17T18:33:42Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "chatgpt-5-q8_0",
        "path": "https://huggingface.co/Hack337/ChatGPT-5-Q8_0-GGUF/resolve/main/chatgpt-5-q8_0.gguf",
        "file_size": "506.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Hack337/ChatGPT-5-Q8_0-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF version of Hack337/ChatGPT-5, converted using llama.cpp and GGUF-my-repo for use with llama.cpp CLI or server."
  },
  {
    "model_name": "KAT-Dev-GGUF",
    "developer": "mradermacher",
    "downloads": 2377,
    "createdAt": "2025-09-16T10:56:15.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "KAT-Dev.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-Dev.Q2_K",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "KAT-Dev.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "KAT-Dev.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "KAT-Dev.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "KAT-Dev.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "KAT-Dev.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "KAT-Dev.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "KAT-Dev.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "KAT-Dev.Q6_K",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "KAT-Dev.Q8_0",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/KAT-Dev.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/KAT-Dev-GGUF/resolve/main/README.md",
    "description": "mradermacher's KAT-Dev GGUF quantizations (Q2_K to Q8_0)."
  },
  {
    "model_name": "mn-12b-rp-without-dumb-GGUF",
    "developer": "mradermacher",
    "downloads": 2366,
    "createdAt": "2025-09-06T06:31:34.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "mn-12b-rp-without-dumb.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q2_K",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q6_K",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "mn-12b-rp-without-dumb.Q8_0",
        "path": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/mn-12b-rp-without-dumb.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/mn-12b-rp-without-dumb-GGUF/resolve/main/README.md",
    "description": "Multiple GGUF quantizations of the mn‑12b‑rp‑without‑dumb model are provided for download."
  },
  {
    "model_name": "Gemma-3-R1-12B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 2311,
    "createdAt": "2025-08-11T09:13:40Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/README.md",
    "description": "This is a reasoning-tuned version of the Gemma 3 12B model, designed to enhance reasoning capabilities while reducing positivity, and is vision capable."
  },
  {
    "model_name": "llama-3.1-8b-roleplay-BSNL-gguf",
    "developer": "samunder12",
    "downloads": 2306,
    "createdAt": "2025-09-02T15:59:20.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Llama3BSNL.Q4_K_M",
        "path": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-BSNL-gguf/resolve/main/Llama3BSNL.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-BSNL-gguf/resolve/main/README.md",
    "description": "This is a GGUF quantized version of a fine-tuned Llama 3.1 8B Instruct model specialized for fast-paced story generation with a dominant, assertive, and direct role-play persona."
  },
  {
    "model_name": "dolphin-2.9.4-llama3.1-8b-gguf",
    "developer": "dphn",
    "downloads": 2301,
    "createdAt": "2024-08-07T02:03:47.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q2_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q3_K_L",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q3_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q3_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q4_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q4_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q4_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q5_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q5_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q5_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q6_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "dolphin-2.9.4-llama3.1-8b-Q8_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/dolphin-2.9.4-llama3.1-8b-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/dolphin-2.9.4-llama3.1-8b-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Ring-mini-2.0-GGUF",
    "developer": "inclusionAI",
    "downloads": 2286,
    "createdAt": "2025-09-24T10:30:16.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ring-mini-2.0-BF16",
        "path": "https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF/resolve/main/Ring-mini-2.0-BF16.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Ring-mini-2.0-Q2_K",
        "path": "https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF/resolve/main/Ring-mini-2.0-Q2_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ring-mini-2.0-Q4_K_M",
        "path": "https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF/resolve/main/Ring-mini-2.0-Q4_K_M.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Ring-mini-2.0-Q6_K",
        "path": "https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF/resolve/main/Ring-mini-2.0-Q6_K.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Ring-mini-2.0-Q8_0",
        "path": "https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF/resolve/main/Ring-mini-2.0-Q8_0.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF/resolve/main/README.md",
    "description": "Quantized Ring-mini-2.0 via llama.cpp, available in GitHub releases."
  },
  {
    "model_name": "Biggie-SmoLlm-0.15B-Base",
    "developer": "nisten",
    "downloads": 2284,
    "createdAt": "2024-07-29T22:33:32Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Biggie_SmolLM_0.15B_Base_bf16",
        "path": "https://huggingface.co/nisten/Biggie-SmoLlm-0.15B-Base/resolve/main/Biggie_SmolLM_0.15B_Base_bf16.gguf",
        "file_size": "346.1 MB"
      },
      {
        "model_id": "biggie_groked_int8_q8_0",
        "path": "https://huggingface.co/nisten/Biggie-SmoLlm-0.15B-Base/resolve/main/biggie_groked_int8_q8_0.gguf",
        "file_size": "156.1 MB"
      },
      {
        "model_id": "old-biggie-smollm-checkpoint-twitter-q8_0",
        "path": "https://huggingface.co/nisten/Biggie-SmoLlm-0.15B-Base/resolve/main/old-biggie-smollm-checkpoint-twitter-q8_0.gguf",
        "file_size": "184.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/nisten/Biggie-SmoLlm-0.15B-Base/resolve/main/README.md",
    "description": "A 0.18 B SmolLM‑based model with an int8 q8_0 checkpoint that runs on a single CPU core and can be finetuned using the GrokAdamW optimizer."
  },
  {
    "model_name": "survival-uncensored-gemma-270m",
    "developer": "q1776",
    "downloads": 2272,
    "createdAt": "2025-08-15T13:48:57Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "uncensored-q-270m-f16",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m-f16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "uncensored-q-270m-f32",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m-f32.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "uncensored-q-270m-q8",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m-q8.gguf",
        "file_size": "278.0 MB"
      },
      {
        "model_id": "uncensored-q-270m",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m.gguf",
        "file_size": "278.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/README.md",
    "description": "Uncensored-Q-270M is a fine-tuned, uncensored version of Google's Gemma-3-270M model, optimized for survival, resistance, and psychological resilience tasks with direct, unfiltered responses."
  },
  {
    "model_name": "LFM2-VL-450M-GGUF",
    "developer": "LiquidAI",
    "downloads": 2264,
    "createdAt": "2025-08-17T09:11:52.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "LFM2-VL-450M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/LFM2-VL-450M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-VL-450M-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/LFM2-VL-450M-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-VL-450M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/LFM2-VL-450M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2-VL-450M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/mmproj-LFM2-VL-450M-F16.gguf",
        "file_size": "184.4 MB"
      },
      {
        "model_id": "mmproj-LFM2-VL-450M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/mmproj-LFM2-VL-450M-Q8_0.gguf",
        "file_size": "99.1 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/README.md",
    "description": "LFM2-VL-450M-GGUF is a vision model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "qwen3-4B-rpg-roleplay",
    "developer": "Chun121",
    "downloads": 2260,
    "createdAt": "2025-04-30T23:55:22Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gguf_f16/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_f16/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.Q4_K_M",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gguf_q8_0/unsloth.Q8_0",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q8_0/unsloth.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/README.md",
    "description": "This is a LoRA fine-tuned version of Qwen3-4B, optimized for character-based conversations and roleplay scenarios using the Gryphe-Aesir-RPG-Charcards-Opus-Mixed-split dataset.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 2218,
    "createdAt": "2025-08-08T00:03:56.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-4B-Thinking-2507 model, offering various quantization options for efficient deployment."
  },
  {
    "model_name": "Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF",
    "developer": "ArliAI",
    "downloads": 2177,
    "createdAt": "2024-08-31T17:20:54Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q2_K",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_L",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q4_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q4_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q5_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q5_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q6_K",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-fp16",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-fp16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-q8_0",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/README.md",
    "description": "ArliAI-RPMax-12B-v1.1 is a creative and non-repetitive RP model based on Mistral Nemo 12B Instruct 2407, trained on diverse and deduplicated datasets to avoid repetition sickness.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-2-9b-it-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 2170,
    "createdAt": "2024-07-21T21:37:19.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-2-9b-it-abliterated-IQ2_M",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-IQ2_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-IQ3_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-IQ3_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-IQ4_XS.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q2_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q2_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q3_K_L.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q3_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q3_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q3_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_0_4_4.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_0_4_8.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_0_8_8.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q4_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q5_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q5_K_M.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q5_K_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q6_K.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q6_K_L.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-Q8_0.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "gemma-2-9b-it-abliterated-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/gemma-2-9b-it-abliterated-f32.gguf",
        "file_size": "34.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/gemma-2-9b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of the gemma‑2‑9b‑it‑abliterated model (via llama.cpp) with download links, usage tips, and quality recommendations."
  },
  {
    "model_name": "Alisia-7B-it-GGUF",
    "developer": "mradermacher",
    "downloads": 2162,
    "createdAt": "2025-09-16T09:01:48.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Alisia-7B-it.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q2_K",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q6_K",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.Q8_0",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Alisia-7B-it.f16",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/Alisia-7B-it.f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Alisia-7B-it-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "GPT-OSS-Code-Reasoning-20B-GGUF",
    "developer": "GetSoloTech",
    "downloads": 2158,
    "createdAt": "2025-08-22T22:23:07Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q3_K_M",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q4_K_M",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q5_K_M",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q8_0",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized model is designed for efficient inference of Python/C++ code generation and algorithmic reasoning in competitive programming tasks."
  },
  {
    "model_name": "saiga_gemma3_12b_gguf",
    "developer": "IlyaGusev",
    "downloads": 2122,
    "createdAt": "2025-04-27T09:54:41.000Z",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "saiga_gemma3_12b.BF16",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q2_K",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q3_K_M",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q4_K_M",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q4_K_S",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q5_K_M",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q6_K",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "saiga_gemma3_12b.Q8_0",
        "path": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "osmosis-mcp-4b",
    "developer": "osmosis-ai",
    "downloads": 2106,
    "createdAt": "2025-05-08T18:46:48.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "osmosis-mcp-4B-BF16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-BF16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-F16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/README.md",
    "description": "Osmosis-MCP-4B is a Qwen3-4B model fine-tuned with reinforcement learning to excel at multi-step tool usage in a curriculum-based training approach, offering a practical, open-source solution for MCP-style agents."
  },
  {
    "model_name": "Sugoi-14B-Ultra-GGUF",
    "developer": "sugoitoolkit",
    "downloads": 2081,
    "createdAt": "2025-08-19T22:54:35Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Sugoi-14B-Ultra-F16",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-F16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Sugoi-14B-Ultra-Q2_K",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Sugoi-14B-Ultra-Q4_K_M",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Sugoi-14B-Ultra-Q8_0",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/README.md",
    "description": "Sugoi LLM 14B Ultra (GGUF version) is a Japanese-to-English translation model with near-double BLEU score and strong prompt-following skills, ideal for RPG Maker text localization."
  },
  {
    "model_name": "Aurore-Reveil_Koto-Small-7B-IT-GGUF",
    "developer": "bartowski",
    "downloads": 2043,
    "createdAt": "2025-09-01T02:20:38Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-IQ2_M",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-IQ3_M",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-IQ3_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-IQ3_XS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-IQ4_NL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q2_K",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q3_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q3_K_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q3_K_XL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q4_0",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q4_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q4_1",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q4_1.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q4_K_L.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q6_K",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-Q8_0",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-bf16",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Aurore-Reveil_Koto-Small-7B-IT-imatrix",
        "path": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/Aurore-Reveil_Koto-Small-7B-IT-imatrix.gguf",
        "file_size": "4.9 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Aurore-Reveil_Koto-Small-7B-IT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Koto-Small-7B-IT model by Aurore-Reveil, optimized for various inference speeds and resource constraints using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "LFM2-1.2B-RAG-GGUF",
    "developer": "LiquidAI",
    "downloads": 2039,
    "createdAt": "2025-09-05T16:31:55.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-1.2B-RAG-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/LFM2-1.2B-RAG-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-1.2B-RAG-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/LFM2-1.2B-RAG-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-1.2B-RAG-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/LFM2-1.2B-RAG-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2-1.2B-RAG-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/LFM2-1.2B-RAG-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2-1.2B-RAG-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/LFM2-1.2B-RAG-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2-1.2B-RAG-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/LFM2-1.2B-RAG-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-1.2B-RAG-GGUF/resolve/main/README.md",
    "description": "RAG model for document-based question answering."
  },
  {
    "model_name": "Phi-3.5-mini-instruct_Uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 2034,
    "createdAt": "2024-08-22T02:48:33Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ2_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ2_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ3_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q2_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q4_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q4_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q5_K_L.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q6_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-f16",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-f16.gguf",
        "file_size": "7.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Phi-3.5-mini-instruct_Uncensored model using llama.cpp's imatrix method, optimized for various inference speeds and resource constraints on different platforms."
  },
  {
    "model_name": "L3.1-RP-Hero-InBetween-8B-GGUF",
    "developer": "DavidAU",
    "downloads": 2023,
    "createdAt": "2024-11-28T03:36:21.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q2_k.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q3_k_s.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q4_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q4_k_s.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q5_k_s.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q6_k.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-InBetween-8B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/L3.1-RP-Hero-InBetween-8B-D_AU-q5_k_m.gguf",
        "file_size": "5.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2022,
    "createdAt": "2025-07-30T10:08:40Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.imatrix",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/README.md",
    "description": "The model Cydonia-v4-MS3.2-Magnum-Diamond-24B is quantized into various formats by mradermacher, including imatrix and IQ quantizations, with the GGUF file format available for download.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NuMarkdown-8B-Thinking-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2020,
    "createdAt": "2025-08-07T10:05:31Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.imatrix",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.imatrix.gguf",
        "file_size": "4.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/README.md",
    "description": "The NuMarkdown-8B-Thinking model is a vision-language model capable of document-to-markdown conversion, reasoning, and RAG, available in various quantized versions for efficient deployment."
  },
  {
    "model_name": "OmniVLM-968M",
    "developer": "NexaAI",
    "downloads": 2014,
    "createdAt": "2024-11-14T01:42:29.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Nano-Vlm-Processor-494M-F16",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/Nano-Vlm-Processor-494M-F16.gguf",
        "file_size": "948.1 MB"
      },
      {
        "model_id": "Nano-Vlm-Processor-494M-Q8_0",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/Nano-Vlm-Processor-494M-Q8_0.gguf",
        "file_size": "506.5 MB"
      },
      {
        "model_id": "omnivision-text-optimized-llm-Q8_0",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/omnivision-text-optimized-llm-Q8_0.gguf",
        "file_size": "506.5 MB"
      },
      {
        "model_id": "omnivision-text-optimized-llm-f16",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/omnivision-text-optimized-llm-f16.gguf",
        "file_size": "948.1 MB"
      },
      {
        "model_id": "omnivision-text-optimized-mmproj-f16",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/omnivision-text-optimized-mmproj-f16.gguf",
        "file_size": "913.2 MB"
      },
      {
        "model_id": "omnivision-text-optimized-mmproj-f32",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/omnivision-text-optimized-mmproj-f32.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-omni-vlm-f16",
        "path": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/mmproj-omni-vlm-f16.gguf",
        "file_size": "913.2 MB"
      }
    ],
    "readme": "https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-4B-GGUF",
    "developer": "bartowski",
    "downloads": 2012,
    "createdAt": "2025-08-26T03:23:23Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ3_XXS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ4_NL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q2_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_1.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q5_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q6_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q8_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-bf16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-4B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-4B-bf16.gguf",
        "file_size": "616.3 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-4B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-4B-f16.gguf",
        "file_size": "615.1 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-4B model using llama.cpp's imatrix calibration method, optimized for various hardware platforms and quantization types."
  },
  {
    "model_name": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF",
    "developer": "DevQuasar",
    "downloads": 1999,
    "createdAt": "2025-09-08T17:20:02.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q2_K",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q3_K_L",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q3_K_M",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q3_K_S",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q4_K_M",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q4_K_S",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q5_K_M",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q5_K_S",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q6_K",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q8_0",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.Q8_0.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.f16",
        "path": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated.f16.gguf",
        "file_size": "14.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DevQuasar/huihui-ai.Huihui-Hunyuan-MT-7B-abliterated-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen2.5-Coder-7B-Instruct-128K-GGUF",
    "developer": "unsloth",
    "downloads": 1961,
    "createdAt": "2024-11-12T11:37:59Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/README.md",
    "description": "This repository provides the 0.5B Qwen2.5-Coder model, a code-specific large language model that offers improved code generation, reasoning, and fixing capabilities, and is optimized for faster performance and lower memory usage with Unsloth.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Gemma-R1-27B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 1957,
    "createdAt": "2025-08-04T14:46:49Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/README.md",
    "description": "This is a reasoning-tuned version of the Gemma 3 27B model, offering enhanced reasoning capabilities and a more structured, less positive output style, with vision support available."
  },
  {
    "model_name": "Indian_Legal_Assitant",
    "developer": "varma007ut",
    "downloads": 1950,
    "createdAt": "2024-09-16T08:27:37.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "outpt_file",
        "path": "https://huggingface.co/varma007ut/Indian_Legal_Assitant/resolve/main/outpt_file.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/varma007ut/Indian_Legal_Assitant/resolve/main/README.md",
    "description": "An 8 B LLaMA‑based model fine‑tuned on Indian legal texts for legal question answering, summarization, and document analysis."
  },
  {
    "model_name": "Intern-S1-mini-GGUF",
    "developer": "internlm",
    "downloads": 1943,
    "createdAt": "2025-08-18T08:40:07Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Q8_0/Intern-S1-mini-Q8_0",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/Q8_0/Intern-S1-mini-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "f16/Intern-S1-mini-f16",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/f16/Intern-S1-mini-f16.gguf",
        "file_size": "15.3 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "Q8_0/mmproj-Intern-S1-mini-Q8_0",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/Q8_0/mmproj-Intern-S1-mini-Q8_0.gguf",
        "file_size": "346.7 MB"
      },
      {
        "model_id": "f16/mmproj-Intern-S1-mini-f16",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/f16/mmproj-Intern-S1-mini-f16.gguf",
        "file_size": "646.7 MB"
      }
    ],
    "readme": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/README.md",
    "description": "The Intern-S1-mini-GGUF model is a quantized version of the Intern-S1-mini base model, compatible with llama.cpp for efficient inference on various hardware platforms, including deployment via OpenAI API or Ollama."
  },
  {
    "model_name": "UIGEN-X-4B-0729-GGUF",
    "developer": "gabriellarson",
    "downloads": 1938,
    "createdAt": "2025-07-29T22:26:22Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "UIGEN-X-4B-0729-F16",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_XS.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_XXS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ4_NL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q2_K",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q2_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q6_K",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q8_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/README.md",
    "description": "UIGEN-X-4B-0729 is a reasoning-only UI generation model based on Qwen3-32B, capable of systematically planning, architecting, and implementing complete user interfaces across 26 languages, 7 platforms, and 26 major categories of web",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-Distill",
    "developer": "BasedBase",
    "downloads": 1927,
    "createdAt": "2025-08-05T21:00:28Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill/resolve/main/README.md",
    "description": "The model is licensed under the Apache 2.0 license."
  },
  {
    "model_name": "KAT-V1-40B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1892,
    "createdAt": "2025-07-22T07:40:11Z",
    "tools": true,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "KAT-V1-40B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ1_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ1_S.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_M.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_S.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_XS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_XXS.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_M.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_S.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_XS.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_XXS.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ4_XS.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q2_K.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q2_K_S.gguf",
        "file_size": "13.2 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_L.gguf",
        "file_size": "19.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_S.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_0.gguf",
        "file_size": "21.5 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_1.gguf",
        "file_size": "23.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_K_M.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_K_S.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q5_K_M.gguf",
        "file_size": "26.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q5_K_S.gguf",
        "file_size": "26.1 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q6_K.gguf",
        "file_size": "31.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides various GGUF quantized versions of the Kwaipilot/KAT-V1-40B model, ranging from low-quality i1-IQ1_S to high-quality i1-Q6_K, with notes on their performance and suitability.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "RimTalk-Mini-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 1878,
    "createdAt": "2025-07-27T04:40:01Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "RimDialogue-3B-v1a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q8_0.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/README.md",
    "description": "RimTalk Mini v1 is a compact, efficient version of the RimTalk model optimized for performance and resource usage."
  },
  {
    "model_name": "allenai_olmOCR-2-7B-1025-GGUF",
    "developer": "bartowski",
    "downloads": 1868,
    "createdAt": "2025-10-23T05:27:10.000Z",
    "tools": false,
    "num_quants": 0,
    "quants": [],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-allenai_olmOCR-2-7B-1025-bf16",
        "path": "https://huggingface.co/bartowski/allenai_olmOCR-2-7B-1025-GGUF/resolve/main/mmproj-allenai_olmOCR-2-7B-1025-bf16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-allenai_olmOCR-2-7B-1025-f16",
        "path": "https://huggingface.co/bartowski/allenai_olmOCR-2-7B-1025-GGUF/resolve/main/mmproj-allenai_olmOCR-2-7B-1025-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/allenai_olmOCR-2-7B-1025-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1856,
    "createdAt": "2025-08-03T11:10:53Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Thinking-2507 model with various GGUF quantization types available for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1826,
    "createdAt": "2025-08-16T21:07:14Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_M.gguf",
        "file_size": "810.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_S.gguf",
        "file_size": "754.4 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_S.gguf",
        "file_size": "1012.7 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XS.gguf",
        "file_size": "983.8 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XXS.gguf",
        "file_size": "904.3 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.imatrix.gguf",
        "file_size": "3.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/README.md",
    "description": "This is a GGUF-quantized vision model for image captioning and text generation, offering various quantization options for different trade-offs between speed, size, and quality."
  },
  {
    "model_name": "llava-phi-3-mini-gguf",
    "developer": "xtuner",
    "downloads": 1825,
    "createdAt": "2024-04-25T12:25:50.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "llava-phi-3-mini-f16",
        "path": "https://huggingface.co/xtuner/llava-phi-3-mini-gguf/resolve/main/llava-phi-3-mini-f16.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "llava-phi-3-mini-int4",
        "path": "https://huggingface.co/xtuner/llava-phi-3-mini-gguf/resolve/main/llava-phi-3-mini-int4.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "llava-phi-3-mini-mmproj-f16",
        "path": "https://huggingface.co/xtuner/llava-phi-3-mini-gguf/resolve/main/llava-phi-3-mini-mmproj-f16.gguf",
        "file_size": "579.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/xtuner/llava-phi-3-mini-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Arisu-12B-GGUF",
    "developer": "mradermacher",
    "downloads": 1795,
    "createdAt": "2025-09-14T05:33:41.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Arisu-12B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Arisu-12B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Arisu-12B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Arisu-12B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Arisu-12B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Arisu-12B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Arisu-12B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Arisu-12B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Arisu-12B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Arisu-12B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Arisu-12B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/Arisu-12B.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Arisu-12B-GGUF/resolve/main/README.md",
    "description": "Provides multiple GGUF quantizations of the Arisu‑12B model with download links, sizes, and usage recommendations."
  },
  {
    "model_name": "gemma-3-1b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 1781,
    "createdAt": "2025-03-10T22:42:41.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf",
        "file_size": "957.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and available in GGUF format with QAT quantization for efficient deployment."
  },
  {
    "model_name": "T-pro-it-2.0-GGUF",
    "developer": "t-tech",
    "downloads": 1750,
    "createdAt": "2025-07-17T21:36:22Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "T-pro-it-2.0-Q4_K_M",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_0",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_K_M",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_K_S",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q6_K",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q8_0",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/README.md",
    "description": "This repository provides the T-pro-it-2.0 model in GGUF format for use with llama.cpp or Ollama, with various quantization options for different hardware requirements."
  },
  {
    "model_name": "stable-code-instruct-3b",
    "developer": "stabilityai",
    "downloads": 1749,
    "createdAt": "2024-03-06T17:46:21.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "stable-code-3b-q4_k_m",
        "path": "https://huggingface.co/stabilityai/stable-code-instruct-3b/resolve/main/stable-code-3b-q4_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "stable-code-3b-q5_k_m",
        "path": "https://huggingface.co/stabilityai/stable-code-instruct-3b/resolve/main/stable-code-3b-q5_k_m.gguf",
        "file_size": "1.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/stabilityai/stable-code-instruct-3b/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "google_gemma-3-270m-it-GGUF",
    "developer": "bartowski",
    "downloads": 1738,
    "createdAt": "2025-08-14T15:31:10Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "google_gemma-3-270m-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ3_M.gguf",
        "file_size": "227.9 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ3_XS.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ3_XXS.gguf",
        "file_size": "225.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ4_NL.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ4_XS.gguf",
        "file_size": "229.7 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_L.gguf",
        "file_size": "235.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_M.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_S.gguf",
        "file_size": "225.7 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_XL.gguf",
        "file_size": "235.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_0.gguf",
        "file_size": "230.4 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_1.gguf",
        "file_size": "236.2 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_K_L.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_K_M.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_K_S.gguf",
        "file_size": "238.3 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q5_K_L.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q5_K_M.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q5_K_S.gguf",
        "file_size": "246.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q6_K.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q6_K_L.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q8_0.gguf",
        "file_size": "278.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-bf16",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-bf16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-imatrix",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-imatrix.gguf",
        "file_size": "460.0 KB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3-270M model using llama.cpp's imatrix calibration method, optimized for various hardware platforms and quantization types like Q4_K_M, Q6_K_L, IQ4_XS, etc., with recommended options for performance"
  },
  {
    "model_name": "MachineLearningLM-7B-v1-GGUF",
    "developer": "mradermacher",
    "downloads": 1733,
    "createdAt": "2025-09-11T04:11:44.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "MachineLearningLM-7B-v1.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q2_K",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q6_K",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.Q8_0",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "MachineLearningLM-7B-v1.f16",
        "path": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/MachineLearningLM-7B-v1.f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/MachineLearningLM-7B-v1-GGUF/resolve/main/README.md",
    "description": "Provides multiple GGUF‑quantized versions of the MachineLearningLM‑7B‑v1 model for tabular classification."
  },
  {
    "model_name": "Triplex",
    "developer": "SciPhi",
    "downloads": 1724,
    "createdAt": "2024-07-10T21:58:18.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "quantized_model-Q4_K_M",
        "path": "https://huggingface.co/SciPhi/Triplex/resolve/main/quantized_model-Q4_K_M.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SciPhi/Triplex/resolve/main/README.md",
    "description": "Triplex is a state-of-the-art LLM fine-tuned for efficient knowledge graph construction from unstructured data, offering a 98% cost reduction compared to traditional methods."
  },
  {
    "model_name": "LiquidAI_LFM2-8B-A1B-GGUF",
    "developer": "bartowski",
    "downloads": 1707,
    "createdAt": "2025-10-08T19:31:28.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-IQ2_M",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-IQ3_M",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-IQ3_XS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q2_K",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q2_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q3_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q3_K_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q4_0",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q4_1",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q4_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q6_K",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q6_K.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-Q8_0",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-Q8_0.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-bf16",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-bf16.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "LiquidAI_LFM2-8B-A1B-imatrix",
        "path": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/LiquidAI_LFM2-8B-A1B-imatrix.gguf",
        "file_size": "16.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/LiquidAI_LFM2-8B-A1B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "mistralai_Voxtral-Small-24B-2507-GGUF",
    "developer": "bartowski",
    "downloads": 1698,
    "createdAt": "2025-07-28T17:12:11Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistralai Voxtral-Small-24B-2507 model using llama.cpp's imatrix quantization method, optimized for various hardware platforms with different performance and quality trade-offs.",
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Voxtral-Small-24B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Small-24B-2507-bf16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Small-24B-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Small-24B-2507-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1697,
    "createdAt": "2025-04-28T18:42:18.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ1_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ1_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_M.gguf",
        "file_size": "22.5 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_S.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_XS.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ2_XXS.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_M.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_S.gguf",
        "file_size": "28.8 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_XS.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ3_XXS.gguf",
        "file_size": "25.6 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-IQ4_XS.gguf",
        "file_size": "35.3 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q2_K.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q2_K_S.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q3_K_L.gguf",
        "file_size": "34.6 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q3_K_M.gguf",
        "file_size": "31.9 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q3_K_S.gguf",
        "file_size": "28.8 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_0.gguf",
        "file_size": "37.4 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_1.gguf",
        "file_size": "41.3 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_K_M.gguf",
        "file_size": "39.6 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q4_K_S.gguf",
        "file_size": "37.6 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q5_K_M.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/L3.3-GeneticLemonade-Unleashed-v2.1-70B.i1-Q5_K_S.gguf",
        "file_size": "45.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/L3.3-GeneticLemonade-Unleashed-v2.1-70B-i1-GGUF/resolve/main/README.md",
    "description": "mradermacher's quantized GGUF 70B L3.3-GeneticLemonade model."
  },
  {
    "model_name": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1683,
    "createdAt": "2025-05-31T07:14:00.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_XS.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q2_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1.i1-Q6_K.gguf",
        "file_size": "6.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/README.md",
    "description": "Imatrix quantized GGUF versions of Josiefied-DeepSeek-R1-0528-Qwen3-8B for chat."
  },
  {
    "model_name": "MiniCPM4.1-8B-GGUF",
    "developer": "openbmb",
    "downloads": 1650,
    "createdAt": "2025-09-04T08:36:56.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "MiniCPM4.1-8B-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM4.1-8B-GGUF/resolve/main/MiniCPM4.1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/openbmb/MiniCPM4.1-8B-GGUF/resolve/main/README.md",
    "description": "MiniCPM4.1‑8B‑GGUF is an ultra‑efficient, hybrid‑reasoning 8B LLM for edge devices, optimized across architecture, data, training, and inference, and runnable with llama.cpp."
  },
  {
    "model_name": "EuroLLM-9B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 1649,
    "createdAt": "2024-12-02T20:12:32Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "EuroLLM-9B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-IQ2_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-IQ3_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-IQ3_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-IQ4_NL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-IQ4_XS.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q2_K.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q2_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q3_K_L.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q3_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q3_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q3_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_K_M.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q4_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q5_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q5_K_M.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q5_K_S.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q6_K.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q6_K_L.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-Q8_0.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "EuroLLM-9B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/EuroLLM-9B-Instruct-f16.gguf",
        "file_size": "17.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/EuroLLM-9B-Instruct-GGUF/resolve/main/README.md",
    "description": "Various llama.cpp GGUF quantizations of EuroLLM‑9B‑Instruct with download links, prompt format, and usage instructions."
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "ubergarm",
    "downloads": 1649,
    "createdAt": "2025-07-02T00:39:08Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-IQ3_KS.gguf",
        "file_size": "34.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an imatrix quantized version of Hunyuan-A13B-Instruct using ik_llama.cpp, optimized for high perplexity and performance with specific layer quantization settings and hardware requirements.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "llama4-dolphin-8B-GGUF",
    "developer": "mradermacher",
    "downloads": 1649,
    "createdAt": "2024-04-27T08:17:50Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "llama4-dolphin-8B.IQ3_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q2_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q6_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q8_0",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.f16",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/README.md",
    "description": "The Hugging Face repository provides GGUF quantized versions of the Manavshah/llama4-dolphin-8B model, including options like Q2_K, IQ3_XS, Q8_0, and others, with notes on quality and performance.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-4b-toolcall-gguf-llamacpp-codex",
    "developer": "Manojb",
    "downloads": 1649,
    "createdAt": "2025-09-21T23:56:19.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-4B-Function-Calling-Pro",
        "path": "https://huggingface.co/Manojb/Qwen3-4b-toolcall-gguf-llamacpp-codex/resolve/main/Qwen3-4B-Function-Calling-Pro.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Manojb/Qwen3-4b-toolcall-gguf-llamacpp-codex/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Ling-flash-2.0-GGUF",
    "developer": "inclusionAI",
    "downloads": 1647,
    "createdAt": "2025-09-24T13:22:05.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Ling-flash-2.0-Q2_K",
        "path": "https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF/resolve/main/Ling-flash-2.0-Q2_K.gguf",
        "file_size": "35.1 GB"
      },
      {
        "model_id": "Ling-flash-2.0-Q4_K_M",
        "path": "https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF/resolve/main/Ling-flash-2.0-Q4_K_M.gguf",
        "file_size": "58.1 GB"
      },
      {
        "model_id": "Ling-flash-2.0-Q6_K",
        "path": "https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF/resolve/main/Ling-flash-2.0-Q6_K.gguf",
        "file_size": "78.7 GB"
      },
      {
        "model_id": "Ling-flash-2.0-Q8_0",
        "path": "https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF/resolve/main/Ling-flash-2.0-Q8_0.gguf",
        "file_size": "101.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF/resolve/main/README.md",
    "description": "Ling-flash-2.0 GGUF model for llama.cpp inference."
  },
  {
    "model_name": "writing-roleplay-20k-context-nemo-12b-v1.0-gguf",
    "developer": "openerotica",
    "downloads": 1629,
    "createdAt": "2024-10-13T02:57:18.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "writing-roleplay-20k-context-nemo-12b-v1.0-F16",
        "path": "https://huggingface.co/openerotica/writing-roleplay-20k-context-nemo-12b-v1.0-gguf/resolve/main/writing-roleplay-20k-context-nemo-12b-v1.0-F16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "writing-roleplay-20k-context-nemo-12b-v1.0-Q8_0",
        "path": "https://huggingface.co/openerotica/writing-roleplay-20k-context-nemo-12b-v1.0-gguf/resolve/main/writing-roleplay-20k-context-nemo-12b-v1.0-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "writing-roleplay-20k-context-nemo-12b-v1.0-q4_k_m",
        "path": "https://huggingface.co/openerotica/writing-roleplay-20k-context-nemo-12b-v1.0-gguf/resolve/main/writing-roleplay-20k-context-nemo-12b-v1.0-q4_k_m.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/openerotica/writing-roleplay-20k-context-nemo-12b-v1.0-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-1B-GGUF",
    "developer": "bartowski",
    "downloads": 1623,
    "createdAt": "2025-08-26T03:22:54Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ2_M.gguf",
        "file_size": "316.4 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ3_M.gguf",
        "file_size": "384.2 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ3_XS.gguf",
        "file_size": "362.0 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ3_XXS.gguf",
        "file_size": "329.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ4_NL.gguf",
        "file_size": "447.3 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ4_XS.gguf",
        "file_size": "429.6 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q2_K.gguf",
        "file_size": "331.2 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q2_K_L.gguf",
        "file_size": "476.1 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_L.gguf",
        "file_size": "415.2 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_M.gguf",
        "file_size": "394.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_S.gguf",
        "file_size": "371.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_XL.gguf",
        "file_size": "545.0 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_0.gguf",
        "file_size": "447.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_1.gguf",
        "file_size": "482.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_K_L.gguf",
        "file_size": "571.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_K_M.gguf",
        "file_size": "461.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_K_S.gguf",
        "file_size": "449.0 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q5_K_L.gguf",
        "file_size": "617.4 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q5_K_M.gguf",
        "file_size": "525.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q5_K_S.gguf",
        "file_size": "518.4 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q6_K.gguf",
        "file_size": "593.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q6_K_L.gguf",
        "file_size": "665.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q8_0.gguf",
        "file_size": "767.5 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-bf16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-imatrix.gguf",
        "file_size": "1.1 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-1B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-1B-bf16.gguf",
        "file_size": "593.8 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-1B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-1B-f16.gguf",
        "file_size": "592.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-1B model using llama.cpp, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-mxfp4-abliterated-v2",
    "developer": "huihui-ai",
    "downloads": 1580,
    "createdAt": "2025-09-27T14:42:57.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-mxfp4-abliterated",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-mxfp4-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-mxfp4-abliterated.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-mxfp4-abliterated-v2/resolve/main/README.md",
    "description": "Uncensored mxfp4-quantized 20B text generation model."
  },
  {
    "model_name": "Cosmos-Reason1-7B-GGUF",
    "developer": "unsloth",
    "downloads": 1570,
    "createdAt": "2025-05-24T13:40:48.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Cosmos-Reason1-7B-BF16",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Cosmos-Reason1-7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/Cosmos-Reason1-7B-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF/resolve/main/README.md",
    "description": "Cosmos-Reason1-7B is a multi-modal LLM for physical common sense and embodied reasoning with text+video input."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-abliterated-GGUF",
    "developer": "QuantFactory",
    "downloads": 1525,
    "createdAt": "2024-10-08T17:32:00.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q5_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q5_1.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/Llama-3.2-3B-Instruct-abliterated.Q8_0.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/Llama-3.2-3B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "Uncensored GGUF quantized Llama 3.2 3B Instruct model."
  },
  {
    "model_name": "Magidonia-24B-v4.2.0-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1511,
    "createdAt": "2025-10-17T07:51:24.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magidonia-24B-v4.2.0.imatrix",
        "path": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/Magidonia-24B-v4.2.0.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Magidonia-24B-v4.2.0-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "instinct-GGUF",
    "developer": "QuantFactory",
    "downloads": 1510,
    "createdAt": "2025-09-06T14:44:10.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "instinct.Q2_K",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "instinct.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "instinct.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "instinct.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "instinct.Q4_0",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "instinct.Q4_1",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "instinct.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "instinct.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "instinct.Q5_0",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q5_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "instinct.Q5_1",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q5_1.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "instinct.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "instinct.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "instinct.Q6_K",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "instinct.Q8_0",
        "path": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/instinct.Q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/instinct-GGUF/resolve/main/README.md",
    "description": "QuantFactory/instinct‑GGUF offers a GGUF‑quantized, locally‑runnable version of the Instinct code‑edit model."
  },
  {
    "model_name": "Hunyuan-1.8B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 1470,
    "createdAt": "2025-08-04T07:00:44Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-1.8B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-F16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_M.gguf",
        "file_size": "666.1 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_S.gguf",
        "file_size": "626.6 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_XS.gguf",
        "file_size": "603.8 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_XXS.gguf",
        "file_size": "560.3 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_M.gguf",
        "file_size": "859.1 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_S.gguf",
        "file_size": "835.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_XS.gguf",
        "file_size": "801.2 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_XXS.gguf",
        "file_size": "732.9 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ4_NL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ4_XS.gguf",
        "file_size": "986.0 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q2_K.gguf",
        "file_size": "741.5 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q2_K_S.gguf",
        "file_size": "700.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_L.gguf",
        "file_size": "971.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_M.gguf",
        "file_size": "907.0 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_S.gguf",
        "file_size": "831.5 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the Hugging Face implementation of the Hunyuan series of efficient large language models, including pre-trained and instruction-tuned variants with support for FP8, INT4, and other quantization formats, optimized for diverse deployment scenarios.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-GGUF",
    "developer": "ubergarm",
    "downloads": 1458,
    "createdAt": "2025-07-30T15:32:04Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ1_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ1_KT.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ2_KL",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ2_KL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ2_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ2_KT.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ3_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ3_K.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ3_KS.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_K.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_KSS",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_KSS.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_KT.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ5_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ5_K.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Q4_0.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-30B-A3B-Thinking-2507 model using ik_llama.cpp, optimized for different memory footprints and perplexity performance.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Turkish-Gemma-9b-T1-GGUF",
    "developer": "ytu-ce-cosmos",
    "downloads": 1438,
    "createdAt": "2025-09-23T19:46:52.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "Turkish-Gemma-9b-T1-F16",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1-F16.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.IQ3_M",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.IQ3_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.IQ3_S",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.IQ3_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.IQ3_XS",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.IQ3_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.IQ4_NL",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.IQ4_XS",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q2_K",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q2_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q3_K",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q3_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q3_K_L",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q3_K_L.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q3_K_M",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q3_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q3_K_S",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q3_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q4_K",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q4_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q4_K_M",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q4_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q4_K_S",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q4_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q5_0",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q5_0.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q5_1",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q5_1.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q5_K",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q5_K.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q5_K_M",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q5_K_M.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q5_K_S",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q5_K_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q6_K",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q6_K.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Turkish-Gemma-9b-T1.Q8_0",
        "path": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/Turkish-Gemma-9b-T1.Q8_0.gguf",
        "file_size": "9.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1-GGUF/resolve/main/README.md",
    "description": "Quantized Turkish-Gemma-9b-T1 GGUF models for efficient real-time inference."
  },
  {
    "model_name": "Valkyrie-49B-v2-GGUF",
    "developer": "TheDrummer",
    "downloads": 1433,
    "createdAt": "2025-08-30T09:53:51Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Valkyrie-49B-v2f-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Valkyrie-49B-v2-GGUF/resolve/main/Valkyrie-49B-v2f-Q2_K.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Valkyrie-49B-v2f-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Valkyrie-49B-v2-GGUF/resolve/main/Valkyrie-49B-v2f-Q3_K_M.gguf",
        "file_size": "22.6 GB"
      },
      {
        "model_id": "Valkyrie-49B-v2f-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Valkyrie-49B-v2-GGUF/resolve/main/Valkyrie-49B-v2f-Q4_K_M.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "Valkyrie-49B-v2f-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Valkyrie-49B-v2-GGUF/resolve/main/Valkyrie-49B-v2f-Q5_K_M.gguf",
        "file_size": "33.0 GB"
      },
      {
        "model_id": "Valkyrie-49B-v2f-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Valkyrie-49B-v2-GGUF/resolve/main/Valkyrie-49B-v2f-Q6_K.gguf",
        "file_size": "38.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Valkyrie-49B-v2-GGUF/resolve/main/README.md",
    "description": "Valkyrie 49B v2 is Drummer’s creatively‑focused Llama‑3‑based 49‑billion‑parameter chat model with strong reasoning and character adherence."
  },
  {
    "model_name": "Dream-org_Dream-v0-Instruct-7B-GGUF",
    "developer": "bartowski",
    "downloads": 1431,
    "createdAt": "2025-07-16T19:48:13Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q2_K",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_0",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_1",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q6_K",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q8_0",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-bf16",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dream-v0-Instruct-7B model using llama.cpp's imatrix method, offering various quantization types for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "LFM2-1.2B-Extract-GGUF",
    "developer": "LiquidAI",
    "downloads": 1415,
    "createdAt": "2025-09-05T15:57:40.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-1.2B-Extract-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/LFM2-1.2B-Extract-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-1.2B-Extract-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/LFM2-1.2B-Extract-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-1.2B-Extract-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/LFM2-1.2B-Extract-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2-1.2B-Extract-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/LFM2-1.2B-Extract-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2-1.2B-Extract-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/LFM2-1.2B-Extract-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2-1.2B-Extract-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/LFM2-1.2B-Extract-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF/resolve/main/README.md",
    "description": "LFM2-1.2B-Extract-GGUF extracts structured data (JSON/XML/YAML) from unstructured documents."
  },
  {
    "model_name": "FinGPT-MT-Llama-3-8B-LoRA-GGUF",
    "developer": "second-state",
    "downloads": 1408,
    "createdAt": "2024-10-07T14:13:18.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q2_K",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q3_K_L",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q3_K_M",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q3_K_S",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q4_0",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q4_K_M",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q4_K_S",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q5_0",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q5_K_M",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q5_K_S",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q6_0",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q6_0.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-Q8_0",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "FinGPT-MT-Llama-3-8B-LoRA-f16",
        "path": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/FinGPT-MT-Llama-3-8B-LoRA-f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/second-state/FinGPT-MT-Llama-3-8B-LoRA-GGUF/resolve/main/README.md",
    "description": "FinGPT‑MT‑Llama‑3‑8B‑LoRA‑GGUF offers LoRA‑adapted, quantized GGUF variants of Meta Llama‑3‑8B for finance with an 8192‑token context, runnable via LlamaEdge."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF",
    "developer": "mradermacher",
    "downloads": 1385,
    "createdAt": "2025-09-28T11:42:59.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.IQ4_XS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.MXFP4_MOE",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q3_K_L.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-v2.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-v2.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "Uncensored GGUF quantizations of Huihui-gpt-oss-20b-BF16-abliterated-v2 with Q2-K to Q8-0 options."
  },
  {
    "model_name": "PokeeAI_pokee_research_7b-GGUF",
    "developer": "bartowski",
    "downloads": 1374,
    "createdAt": "2025-10-23T16:24:04.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "PokeeAI_pokee_research_7b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q2_K",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q4_0",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q4_1",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q6_K",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-Q8_0",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-bf16",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "PokeeAI_pokee_research_7b-imatrix",
        "path": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/PokeeAI_pokee_research_7b-imatrix.gguf",
        "file_size": "4.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/PokeeAI_pokee_research_7b-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "CWC-Mistral-Nemo-12B-V2-q4_k_m",
    "developer": "CWClabs",
    "downloads": 1332,
    "createdAt": "2025-10-15T15:37:54.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "CWC-Mistral-Nemo-12B-v2-GGUF-q4_k_m-health-nutrition-natural-medicine",
        "path": "https://huggingface.co/CWClabs/CWC-Mistral-Nemo-12B-V2-q4_k_m/resolve/main/CWC-Mistral-Nemo-12B-v2-GGUF-q4_k_m-health-nutrition-natural-medicine.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/CWClabs/CWC-Mistral-Nemo-12B-V2-q4_k_m/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF",
    "developer": "DavidAU",
    "downloads": 1319,
    "createdAt": "2025-05-15T23:44:27Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_M-imat.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_S-imat.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XS-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_M-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_S-imat.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XS-imat.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ4_XS-imat.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K_S-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_L-imat.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_M-imat.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_S-imat.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_0-imat.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_M-imat.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_S-imat.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_M-imat.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_S-imat.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q6_K-imat.gguf",
        "file_size": "14.1 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/README.md",
    "description": "This model is a highly advanced Llama 3.2 MOE variant with 18.4B parameters, featuring reasoning capabilities, vivid creative writing, and uncensored output across all genres, with the ability to control individual expert models for tailored generation, and requires specific system prompts to activate reasoning",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Hunyuan-4B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 1309,
    "createdAt": "2025-08-04T06:44:18Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-4B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-F16.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ4_NL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ4_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q6_K.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q8_0.gguf",
        "file_size": "4.2 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "This Hugging Face repository provides the Hunyuan series of large language models (0.5B, 1.8B, 4B, 7B) with instruction tuning, quantization support (FP8, INT4), and deployment options using TensorRT-LLM, v",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF",
    "developer": "mradermacher",
    "downloads": 1307,
    "createdAt": "2025-08-30T12:39:50Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-FP32 model, optimized for various quantization types including Q2_K, Q3_K_S, Q4_K"
  },
  {
    "model_name": "Llama3.2-3b-cybersecurity-abliterated",
    "developer": "WhoIsShe",
    "downloads": 1296,
    "createdAt": "2025-08-17T21:18:08.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "unsloth.Q4_K_M",
        "path": "https://huggingface.co/WhoIsShe/Llama3.2-3b-cybersecurity-abliterated/resolve/main/unsloth.Q4_K_M.gguf",
        "file_size": "1.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/WhoIsShe/Llama3.2-3b-cybersecurity-abliterated/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Luau-Devstral-24B-Instruct-v0.2",
    "developer": "TorpedoSoftware",
    "downloads": 1295,
    "createdAt": "2025-09-16T09:42:26.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-BF16",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-IQ1_M",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-IQ2_M",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-IQ3_M",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-IQ4_XS",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-Q4_K_M",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-Q5_K_M",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-Q6_K",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Luau-Devstral-24B-Instruct-v0.2-Q8_0",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/Luau-Devstral-24B-Instruct-v0.2-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "imatrix",
        "path": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Core_24B_V.1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1261,
    "createdAt": "2025-03-24T02:36:18.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Core_24B_V.1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Core_24B_V.1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/Core_24B_V.1.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Core_24B_V.1-i1-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF version of OddTheGreat/Core_24B_V.1 with multiple IQ/quant types for roleplay/creative use."
  },
  {
    "model_name": "L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF",
    "developer": "DavidAU",
    "downloads": 1242,
    "createdAt": "2024-06-25T03:18:51Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ1_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ1_M-imat13.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_M-imat13.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_XS-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_XS-imat13.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ3_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ3_M-imat13.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ4_XS-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ4_XS-imat13.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_M-imat13.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_S-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_S-imat13.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_M-imat13.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_S-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_S-imat13.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q6_K-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q6_K-imat13.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/README.md",
    "description": "This model is a high-quality, ultra-high precision Llama3-based quantized version with improved performance and creativity, optimized for use in chat and roleplay scenarios with specific settings for smoother operation.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "WEBGEN-OSS-20B-GGUF",
    "developer": "gabriellarson",
    "downloads": 1240,
    "createdAt": "2025-09-11T21:32:56.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "WEBGEN-OSS-20B-F16",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-OSS-20B-GGUF/resolve/main/WEBGEN-OSS-20B-F16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "WEBGEN-OSS-20B-MXFP4_MOE",
        "path": "https://huggingface.co/gabriellarson/WEBGEN-OSS-20B-GGUF/resolve/main/WEBGEN-OSS-20B-MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/WEBGEN-OSS-20B-GGUF/resolve/main/README.md",
    "description": "An Apache‑2.0 licensed English text‑generation model (WEBGEN‑OSS‑20B) built with Transformers, Unsloth, and GPT‑OSS."
  },
  {
    "model_name": "gpt-oss-120b-Distill-Phi-4-14B-GGUF",
    "developer": "Jackrong",
    "downloads": 1232,
    "createdAt": "2025-09-24T01:12:30.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gpt-oss-120b-Distill-Phi-4-14B.Q8_0",
        "path": "https://huggingface.co/Jackrong/gpt-oss-120b-Distill-Phi-4-14B-GGUF/resolve/main/gpt-oss-120b-Distill-Phi-4-14B.Q8_0.gguf",
        "file_size": "14.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Jackrong/gpt-oss-120b-Distill-Phi-4-14B-GGUF/resolve/main/README.md",
    "description": "14B Phi-4 model fine-tuned via SFT-GRPO-SFT using gpt-oss-120b distilled data for advanced reasoning."
  },
  {
    "model_name": "Gemmasutra-27B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 1225,
    "createdAt": "2025-10-07T23:33:04.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemmasutra-27B-v3b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-27B-v3-GGUF/resolve/main/Gemmasutra-27B-v3b-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemmasutra-27B-v3b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-27B-v3-GGUF/resolve/main/Gemmasutra-27B-v3b-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemmasutra-27B-v3b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-27B-v3-GGUF/resolve/main/Gemmasutra-27B-v3b-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemmasutra-27B-v3b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-27B-v3-GGUF/resolve/main/Gemmasutra-27B-v3b-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemmasutra-27B-v3b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-27B-v3-GGUF/resolve/main/Gemmasutra-27B-v3b-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemmasutra-27B-v3b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-27B-v3-GGUF/resolve/main/Gemmasutra-27B-v3b-Q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "Llama-3.2-3B-small_Shiro_roleplay-gguf",
    "developer": "samunder12",
    "downloads": 1220,
    "createdAt": "2025-10-19T10:26:01.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-shiro.Q4_K_M",
        "path": "https://huggingface.co/samunder12/Llama-3.2-3B-small_Shiro_roleplay-gguf/resolve/main/Llama-3.2-3B-shiro.Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-shiro.Q8_0",
        "path": "https://huggingface.co/samunder12/Llama-3.2-3B-small_Shiro_roleplay-gguf/resolve/main/Llama-3.2-3B-shiro.Q8_0.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/Llama-3.2-3B-small_Shiro_roleplay-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "patricide-12B-Unslop-Mell-v2-GGUF",
    "developer": "mradermacher",
    "downloads": 1216,
    "createdAt": "2024-12-16T16:58:15.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q2_K",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q6_K",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "patricide-12B-Unslop-Mell-v2.Q8_0",
        "path": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/patricide-12B-Unslop-Mell-v2.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-v2-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "L3.1-RP-Hero-BigTalker-8B-GGUF",
    "developer": "DavidAU",
    "downloads": 1203,
    "createdAt": "2024-11-28T09:04:37.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q2_k.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q3_k_s.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q4_k_s.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q5_k_s.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q6_k.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.1-RP-Hero-BigTalker-8B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/L3.1-RP-Hero-BigTalker-8B-D_AU-q5_k_m.gguf",
        "file_size": "5.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 1185,
    "createdAt": "2025-08-04T04:33:48Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model using the Intel AutoRound algorithm, with 8-bit embeddings and 4-bit non-expert layers, optimized for efficient inference on the codeparrot/github-code-clean dataset."
  },
  {
    "model_name": "medgemma3-thinking",
    "developer": "Testament200156",
    "downloads": 1176,
    "createdAt": "2025-09-08T05:36:53.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "MedGemma3-thinking",
        "path": "https://huggingface.co/Testament200156/medgemma3-thinking/resolve/main/MedGemma3-thinking.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "medgemma3-thinking.mmproj-f16",
        "path": "https://huggingface.co/Testament200156/medgemma3-thinking/resolve/main/medgemma3-thinking.mmproj-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Testament200156/medgemma3-thinking/resolve/main/README.md",
    "description": "A merged MedGemma‑27B and Gemma3 model for medical and scientific reasoning, released for feedback."
  },
  {
    "model_name": "llama-3.1-8b-roleplay-airtel-gguf",
    "developer": "samunder12",
    "downloads": 1175,
    "createdAt": "2025-09-01T09:47:48Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Llama3Airtel.Q4_K_M",
        "path": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-airtel-gguf/resolve/main/Llama3Airtel.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-airtel-gguf/resolve/main/README.md",
    "description": "This repository provides GGUF files for a fine-tuned Llama 3.1 8B Instruct model with a dominant, assertive, and provocative role-play persona, suitable for local inference on CPU or GPU using tools like LM Studio or Ollama."
  },
  {
    "model_name": "llama-3-Korean-Bllossom-8B-gguf-Q4_K_M",
    "developer": "MLP-KTLim",
    "downloads": 1168,
    "createdAt": "2024-05-08T23:06:02Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "llama-3-Korean-Bllossom-8B-Q4_K_M",
        "path": "https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/resolve/main/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/resolve/main/README.md",
    "description": "The Bllossom language model is a Korean-English bilingual model based on Llama3, enhanced with vocabulary expansion, knowledge linking, instruction tuning, and human feedback, and available in GGUF format for efficient CPU/GPU inference."
  },
  {
    "model_name": "llama-3.1-8b-roleplay-jio-gguf",
    "developer": "samunder12",
    "downloads": 1158,
    "createdAt": "2025-09-01T18:49:19Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Llama3Jio.Q4_K_M",
        "path": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-jio-gguf/resolve/main/Llama3Jio.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-jio-gguf/resolve/main/README.md",
    "description": "This is a GGUF version of the Llama 3.1 8B Assertive Role-Play model, optimized for local inference with settings tailored for role-playing and storytelling."
  },
  {
    "model_name": "GLM-4.5-Air-REAP-82B-A12B-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 1144,
    "createdAt": "2025-10-23T16:19:12.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GLM-4.5-Air-REAP-82B-A12B-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/GLM-4.5-Air-REAP-82B-A12B-MXFP4_MOE-GGUF/resolve/main/GLM-4.5-Air-REAP-82B-A12B-MXFP4_MOE.gguf",
        "file_size": "46.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/GLM-4.5-Air-REAP-82B-A12B-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "OmniDimen-4B-Emotion-GGUF-q4_K_M",
    "developer": "OmniDimen",
    "downloads": 1117,
    "createdAt": "2025-09-12T10:55:51.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "OmniDimen-v1.0-4b-Emotion_q4_K_M",
        "path": "https://huggingface.co/OmniDimen/OmniDimen-4B-Emotion-GGUF-q4_K_M/resolve/main/OmniDimen-v1.0-4b-Emotion_q4_K_M.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/OmniDimen/OmniDimen-4B-Emotion-GGUF-q4_K_M/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "MindLink-32B-0801-GGUF",
    "developer": "gabriellarson",
    "downloads": 1099,
    "createdAt": "2025-08-02T05:10:42Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MindLink-32B-0801-F16",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-F16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_M.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_XXS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q2_K",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q2_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q6_K",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q8_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/README.md",
    "description": "MindLink is a new family of large language models developed by Kunlun Inc, built on Qwen with advanced post-training techniques, offering strong performance across various benchmarks and diverse AI applications.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "II-Search-4B-GGUF",
    "developer": "prithivMLmods",
    "downloads": 1082,
    "createdAt": "2025-08-05T21:17:53Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": " II-Search-4B-GGUF.BF16",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.F16",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.F32",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/README.md",
    "description": "II-Search-4B-GGUF is a 4-billion-parameter language model fine-tuned for advanced information seeking and web-integrated reasoning tasks, offering various quantized versions for efficient inference."
  },
  {
    "model_name": "AFM-4.5B-GGUF",
    "developer": "arcee-ai",
    "downloads": 1072,
    "createdAt": "2025-07-29T12:43:40Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "AFM-4.5B-IQ2_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_XS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_XXS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_XXS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ4_NL",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ4_XS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "AFM-4.5B-Q2_K",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AFM-4.5B-Q2_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_XL",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_0",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_1",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "AFM-4.5B-Q6_K",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q6_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q6_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q8_0",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q8_0.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "AFM-4.5B-bf16",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-bf16.gguf",
        "file_size": "8.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/README.md",
    "description": "AFM-4.5B-GGUF is a 4.5 billion parameter instruction-tuned model developed by Arcee.ai for enterprise-grade performance, licensed under the Arcee Model License (AML) with specific commercial usage restrictions."
  },
  {
    "model_name": "OuteTTS-1.0-0.6B-GGUF",
    "developer": "OuteAI",
    "downloads": 1072,
    "createdAt": "2025-05-18T17:35:16Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "OuteTTS-1.0-0.6B-FP16",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-FP16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q2_K",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q2_K.gguf",
        "file_size": "287.3 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q3_K_L",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q3_K_L.gguf",
        "file_size": "356.2 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q3_K_M",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q3_K_M.gguf",
        "file_size": "335.8 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q3_K_S",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q3_K_S.gguf",
        "file_size": "312.9 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_0",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_0.gguf",
        "file_size": "368.7 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_1",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_1.gguf",
        "file_size": "394.9 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_K_M",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_K_M.gguf",
        "file_size": "383.1 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_K_S",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_K_S.gguf",
        "file_size": "370.3 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_0",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_0.gguf",
        "file_size": "421.2 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_1",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_1.gguf",
        "file_size": "447.4 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_K_M",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_K_M.gguf",
        "file_size": "428.6 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_K_S",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_K_S.gguf",
        "file_size": "421.2 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q6_K",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q6_K.gguf",
        "file_size": "477.0 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q8_0",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q8_0.gguf",
        "file_size": "616.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/README.md",
    "description": "OuteTTS Version 1.0 is a multilingual text-to-speech model offering improved speech synthesis and voice cloning capabilities with batched inference support for efficient generation."
  },
  {
    "model_name": "PromptEnhancer-32B-GGUF",
    "developer": "mradermacher",
    "downloads": 1066,
    "createdAt": "2025-09-18T12:25:18.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "PromptEnhancer-32B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q2_K",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q6_K",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.Q8_0",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.Q8_0.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "PromptEnhancer-32B.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.mmproj-Q8_0.gguf",
        "file_size": "703.7 MB"
      },
      {
        "model_id": "PromptEnhancer-32B.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/PromptEnhancer-32B.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/PromptEnhancer-32B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf",
    "developer": "QuantStack",
    "downloads": 1057,
    "createdAt": "2025-08-29T16:00:50Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "InternVL3_5-GPT-OSS-20B-A4B-Preview-bf16",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/InternVL3_5-GPT-OSS-20B-A4B-Preview-bf16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "InternVL3_5-GPT-OSS-20B-A4B-Preview-q3_k_m",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/InternVL3_5-GPT-OSS-20B-A4B-Preview-q3_k_m.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "InternVL3_5-GPT-OSS-20B-A4B-Preview-q4_0",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/InternVL3_5-GPT-OSS-20B-A4B-Preview-q4_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "InternVL3_5-GPT-OSS-20B-A4B-Preview-q6_k",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/InternVL3_5-GPT-OSS-20B-A4B-Preview-q6_k.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "InternVL3_5-GPT-OSS-20B-A4B-Preview-q8_0",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/InternVL3_5-GPT-OSS-20B-A4B-Preview-q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-InternVL3_5-GPT-OSS-20B-A4B-Preview-bf16",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/mmproj-InternVL3_5-GPT-OSS-20B-A4B-Preview-bf16.gguf",
        "file_size": "622.1 MB"
      },
      {
        "model_id": "mmproj-InternVL3_5-GPT-OSS-20B-A4B-Preview-f16",
        "path": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/mmproj-InternVL3_5-GPT-OSS-20B-A4B-Preview-f16.gguf",
        "file_size": "621.0 MB"
      }
    ],
    "readme": "https://huggingface.co/QuantStack/InternVL3_5-GPT-OSS-20B-A4B-Preview-gguf/resolve/main/README.md",
    "description": "This model is a quantized version of OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview under the Apache-2.0 license."
  },
  {
    "model_name": "Tinystories-gpt-0.1-3m-GGUF",
    "developer": "afrideva",
    "downloads": 1041,
    "createdAt": "2024-05-08T13:58:54.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "tinystories-gpt-0.1-3m.Q2_K",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.Q2_K.gguf",
        "file_size": "7.4 MB"
      },
      {
        "model_id": "tinystories-gpt-0.1-3m.Q3_K_M",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.Q3_K_M.gguf",
        "file_size": "7.4 MB"
      },
      {
        "model_id": "tinystories-gpt-0.1-3m.Q4_K_M",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.Q4_K_M.gguf",
        "file_size": "7.8 MB"
      },
      {
        "model_id": "tinystories-gpt-0.1-3m.Q5_K_M",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.Q5_K_M.gguf",
        "file_size": "8.1 MB"
      },
      {
        "model_id": "tinystories-gpt-0.1-3m.Q6_K",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.Q6_K.gguf",
        "file_size": "9.1 MB"
      },
      {
        "model_id": "tinystories-gpt-0.1-3m.Q8_0",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.Q8_0.gguf",
        "file_size": "9.1 MB"
      },
      {
        "model_id": "tinystories-gpt-0.1-3m.fp16",
        "path": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/tinystories-gpt-0.1-3m.fp16.gguf",
        "file_size": "15.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF/resolve/main/README.md",
    "description": "3M-parameter quantized GGUF text-generation model."
  },
  {
    "model_name": "UI-TARS-1.5-7B-GGUF",
    "developer": "mradermacher",
    "downloads": 995,
    "createdAt": "2025-04-17T09:40:55Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "UI-TARS-1.5-7B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q2_K",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q6_K",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q8_0",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.f16",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.mmproj-Q8_0.gguf",
        "file_size": "816.5 MB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ByteDance-Seed/UI-TARS-1.5-7B model, offering various quantization options for efficient inference."
  },
  {
    "model_name": "Alisia-7B-it-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 983,
    "createdAt": "2025-09-16T09:17:18.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Alisia-7B-it.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ3_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Alisia-7B-it.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.i1-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Alisia-7B-it.imatrix",
        "path": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/Alisia-7B-it.imatrix.gguf",
        "file_size": "4.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Alisia-7B-it-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Home-3B-v3-GGUF",
    "developer": "acon96",
    "downloads": 974,
    "createdAt": "2024-02-23T01:22:53.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Home-3B-v3.f16",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/Home-3B-v3.f16.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Home-3B-v3.q2_k",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/Home-3B-v3.q2_k.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Home-3B-v3.q3_k_m",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/Home-3B-v3.q3_k_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Home-3B-v3.q4_k_m",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/Home-3B-v3.q4_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Home-3B-v3.q5_k_m",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/Home-3B-v3.q5_k_m.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Home-3B-v3.q8_0",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/Home-3B-v3.q8_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "no_tools/Home-3B-v3.f16",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/no_tools/Home-3B-v3.f16.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "no_tools/Home-3B-v3.q2_k",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/no_tools/Home-3B-v3.q2_k.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "no_tools/Home-3B-v3.q3_k_m",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/no_tools/Home-3B-v3.q3_k_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "no_tools/Home-3B-v3.q4_k_m",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/no_tools/Home-3B-v3.q4_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "no_tools/Home-3B-v3.q5_k_m",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/no_tools/Home-3B-v3.q5_k_m.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "no_tools/Home-3B-v3.q8_0",
        "path": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/no_tools/Home-3B-v3.q8_0.gguf",
        "file_size": "2.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/acon96/Home-3B-v3-GGUF/resolve/main/README.md",
    "description": "Non-commercial quantized Home Assistant control model fine-tuned for multi-language function calling (en/de/es/fr)."
  },
  {
    "model_name": "Sugoi-32B-Ultra-GGUF",
    "developer": "sugoitoolkit",
    "downloads": 967,
    "createdAt": "2025-08-23T13:16:07Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Sugoi-32B-Ultra-F16",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-F16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "Sugoi-32B-Ultra-Q2_K",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Sugoi-32B-Ultra-Q4_K_M",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Sugoi-32B-Ultra-Q8_0",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/README.md",
    "description": "Sugoi LLM 32B Ultra is an ultra-large language model based on the Qwen/Qwen2.5-32B-Instruct model, optimized for Japanese and English translation tasks using the GGUF format."
  },
  {
    "model_name": "Luth-1.7B-Instruct-GGUF",
    "developer": "kurakurai",
    "downloads": 944,
    "createdAt": "2025-08-11T12:50:34Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Luth-1.7B-Instruct-BF16",
        "path": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/Luth-1.7B-Instruct-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Luth-1.7B-Instruct-F16",
        "path": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/Luth-1.7B-Instruct-F16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Luth-1.7B-Instruct-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/Luth-1.7B-Instruct-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/README.md",
    "description": "Luth-1.7B-Instruct is a French fine-tuned version of Qwen3-1.7B, trained on the Luth-SFT dataset, showing significant improvements in French instruction following, math, and general knowledge tasks while maintaining strong English performance."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 934,
    "createdAt": "2025-01-23T15:20:59.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.IQ4_XS.gguf",
        "file_size": "978.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q2_K.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q3_K_L.gguf",
        "file_size": "935.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q3_K_M.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q3_K_S.gguf",
        "file_size": "821.3 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q4_K_S.gguf",
        "file_size": "1021.9 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-uncensored.f16",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-uncensored.f16.gguf",
        "file_size": "3.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-1.5B-uncensored-GGUF/resolve/main/README.md",
    "description": "mradermacher's multiple GGUF quantizations of DeepSeek-R1-Distill-Qwen-1.5B-uncensored model."
  },
  {
    "model_name": "LFM2-350M-Extract-GGUF",
    "developer": "LiquidAI",
    "downloads": 925,
    "createdAt": "2025-09-05T15:50:52.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-350M-Extract-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/LFM2-350M-Extract-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-350M-Extract-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/LFM2-350M-Extract-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-350M-Extract-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/LFM2-350M-Extract-Q4_K_M.gguf",
        "file_size": "218.7 MB"
      },
      {
        "model_id": "LFM2-350M-Extract-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/LFM2-350M-Extract-Q5_K_M.gguf",
        "file_size": "248.3 MB"
      },
      {
        "model_id": "LFM2-350M-Extract-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/LFM2-350M-Extract-Q6_K.gguf",
        "file_size": "279.8 MB"
      },
      {
        "model_id": "LFM2-350M-Extract-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/LFM2-350M-Extract-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-350M-Extract-GGUF/resolve/main/README.md",
    "description": "Extracts structured data from unstructured documents."
  },
  {
    "model_name": "L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 920,
    "createdAt": "2024-06-23T20:13:10Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-BF16",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-F16",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a GGUF-IQ-Imatrix quantized version of the Sao10K/L3-8B-Stheno-v3.3-32K model, trained for roleplay and SillyTavern tasks with 32K context support, optimized for use with Kob",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Kumru-2B-GGUF",
    "developer": "icecubetr",
    "downloads": 919,
    "createdAt": "2025-10-14T14:02:47.000Z",
    "tools": false,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "kumru-2b-F16",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-F16.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "kumru-2b-IQ2_XXS",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-IQ2_XXS.gguf",
        "file_size": "672.6 MB"
      },
      {
        "model_id": "kumru-2b-IQ4_XS",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-IQ4_XS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "kumru-2b-Q2_K",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-Q2_K.gguf",
        "file_size": "892.9 MB"
      },
      {
        "model_id": "kumru-2b-Q3_K_M",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-Q3_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "kumru-2b-Q4_K_M",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-Q4_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "kumru-2b-Q4_K_S",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-Q4_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "kumru-2b-Q6_K",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-Q6_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "kumru-2b-Q8_0",
        "path": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/kumru-2b-Q8_0.gguf",
        "file_size": "2.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/icecubetr/Kumru-2B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "llama-3.1-8b-OneLastStory-gguf",
    "developer": "samunder12",
    "downloads": 919,
    "createdAt": "2025-09-03T12:05:14.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "OneLastDance.Q4_K_M",
        "path": "https://huggingface.co/samunder12/llama-3.1-8b-OneLastStory-gguf/resolve/main/OneLastDance.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/llama-3.1-8b-OneLastStory-gguf/resolve/main/README.md",
    "description": "A LoRA‑fine‑tuned Llama 3.1 8B Instruct model (with GGUF) specialized in witty, high‑concept creative storytelling."
  },
  {
    "model_name": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 912,
    "createdAt": "2025-10-07T21:11:33.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ1_M.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ1_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_S.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ2_XXS.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_S.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_XS.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ3_XXS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-IQ4_XS.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q2_K.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q2_K_S.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q3_K_L.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q3_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q3_K_S.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_0.gguf",
        "file_size": "22.4 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_1.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_M.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q4_K_S.gguf",
        "file_size": "22.5 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q5_K_M.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q5_K_S.gguf",
        "file_size": "27.2 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.i1-Q6_K.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III.imatrix.gguf",
        "file_size": "162.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "llama-3.1-8b-Rp-tadashinu-gguf",
    "developer": "samunder12",
    "downloads": 911,
    "createdAt": "2025-09-04T07:46:58.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Llama3Tadashinu.Q4_K_M",
        "path": "https://huggingface.co/samunder12/llama-3.1-8b-Rp-tadashinu-gguf/resolve/main/Llama3Tadashinu.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/llama-3.1-8b-Rp-tadashinu-gguf/resolve/main/README.md",
    "description": "A GGUF LoRA‑adapted Llama 3.1‑8B‑Instruct model fine‑tuned for dark, witty, high‑concept role‑play storytelling."
  },
  {
    "model_name": "git-commit-message",
    "developer": "Tavernari",
    "downloads": 879,
    "createdAt": "2024-04-21T00:18:53.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "unsloth.F16",
        "path": "https://huggingface.co/Tavernari/git-commit-message/resolve/main/unsloth.F16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "unsloth.Q4_K_M",
        "path": "https://huggingface.co/Tavernari/git-commit-message/resolve/main/unsloth.Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "unsloth.Q5_K_M",
        "path": "https://huggingface.co/Tavernari/git-commit-message/resolve/main/unsloth.Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "unsloth.Q8_0",
        "path": "https://huggingface.co/Tavernari/git-commit-message/resolve/main/unsloth.Q8_0.gguf",
        "file_size": "3.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Tavernari/git-commit-message/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "35b-beta-long-GGUF",
    "developer": "bartowski",
    "downloads": 871,
    "createdAt": "2024-05-06T01:10:30.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "35b-beta-long-IQ1_M",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ1_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "35b-beta-long-IQ1_S",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ1_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "35b-beta-long-IQ2_M",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ2_M.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "35b-beta-long-IQ2_S",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ2_S.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "35b-beta-long-IQ2_XS",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ2_XS.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "35b-beta-long-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ2_XXS.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "35b-beta-long-IQ3_M",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ3_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "35b-beta-long-IQ3_S",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ3_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "35b-beta-long-IQ3_XS",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ3_XS.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "35b-beta-long-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ3_XXS.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "35b-beta-long-IQ4_NL",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ4_NL.gguf",
        "file_size": "18.8 GB"
      },
      {
        "model_id": "35b-beta-long-IQ4_XS",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-IQ4_XS.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "35b-beta-long-Q2_K",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q2_K.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "35b-beta-long-Q3_K_L",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q3_K_L.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "35b-beta-long-Q3_K_M",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q3_K_M.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "35b-beta-long-Q3_K_S",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q3_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "35b-beta-long-Q4_K_M",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q4_K_M.gguf",
        "file_size": "20.0 GB"
      },
      {
        "model_id": "35b-beta-long-Q4_K_S",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q4_K_S.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "35b-beta-long-Q5_K_M",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q5_K_M.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "35b-beta-long-Q5_K_S",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q5_K_S.gguf",
        "file_size": "22.7 GB"
      },
      {
        "model_id": "35b-beta-long-Q6_K",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q6_K.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "35b-beta-long-Q8_0",
        "path": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/35b-beta-long-Q8_0.gguf",
        "file_size": "34.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/35b-beta-long-GGUF/resolve/main/README.md",
    "description": "Various llama.cpp imatrix‑quantized GGUF files for the 35b‑beta‑long model, with download links, prompt format, and guidance on choosing a quantization."
  },
  {
    "model_name": "Pyxidis-Manim-CodeGen-1.7B-GGUF",
    "developer": "prithivMLmods",
    "downloads": 865,
    "createdAt": "2025-08-25T13:41:41.000Z",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.BF16",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.F16",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.F16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.F32",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.F32.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q4_0",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q4_1",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q4_K",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q4_K.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q5_0",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q5_1",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q5_K",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q5_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Pyxidis-Manim-CodeGen-1.7B.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/Pyxidis-Manim-CodeGen-1.7B.Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/Pyxidis-Manim-CodeGen-1.7B-GGUF/resolve/main/README.md",
    "description": "A fine‑tuned 1.7 B model for generating Python Manim animation code, offered in various GGUF quantizations."
  },
  {
    "model_name": "gemma-3-270m_mitsuki_gguf",
    "developer": "dahara1",
    "downloads": 849,
    "createdAt": "2025-08-18T08:14:59Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-270m_mitsuki-BF16",
        "path": "https://huggingface.co/dahara1/gemma-3-270m_mitsuki_gguf/resolve/main/gemma-3-270m_mitsuki-BF16.gguf",
        "file_size": "517.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dahara1/gemma-3-270m_mitsuki_gguf/resolve/main/README.md",
    "description": "This is a lightweight SLM based on gemma-3-270m, fine-tuned as a chat and streaming support character for a fantasy café, and converted to GGUF format for compatibility across various platforms."
  },
  {
    "model_name": "Luth-0.6B-Instruct-GGUF",
    "developer": "kurakurai",
    "downloads": 844,
    "createdAt": "2025-08-11T12:31:50Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Luth-0.6B-Instruct-BF16",
        "path": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/Luth-0.6B-Instruct-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Luth-0.6B-Instruct-F16",
        "path": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/Luth-0.6B-Instruct-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Luth-0.6B-Instruct-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/Luth-0.6B-Instruct-Q8_0.gguf",
        "file_size": "609.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/README.md",
    "description": "Luth-0.6B-Instruct is a French fine-tuned version of Qwen3-0.6B, trained on the Luth-SFT dataset, showing significant improvements in French instruction following, math, and general knowledge while maintaining strong English performance."
  },
  {
    "model_name": "gpt-oss-20b-gguf-q4ks-AutoRound",
    "developer": "Intel",
    "downloads": 829,
    "createdAt": "2025-08-08T07:40:21Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gpt-oss-20b-32x2.4B-Q4_K_S",
        "path": "https://huggingface.co/Intel/gpt-oss-20b-gguf-q4ks-AutoRound/resolve/main/gpt-oss-20b-32x2.4B-Q4_K_S.gguf",
        "file_size": "10.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/gpt-oss-20b-gguf-q4ks-AutoRound/resolve/main/README.md",
    "description": "This model is a gguf q4ks quantized version of the openai/gpt-oss-20b model, optimized for efficient inference using the Intel auto-round algorithm."
  },
  {
    "model_name": "MN-14B-Crimson-Veil-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 828,
    "createdAt": "2025-10-03T20:20:49.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ1_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ1_S.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ2_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ2_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ2_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ2_XXS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ3_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ3_S.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ3_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ3_XXS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ4_NL.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-IQ4_XS.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q2_K.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q2_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q3_K_L.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q3_K_M.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q3_K_S.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q4_0.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q4_1.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q4_K_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q4_K_S.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q5_K_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q5_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.i1-Q6_K.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "MN-14B-Crimson-Veil.imatrix",
        "path": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/MN-14B-Crimson-Veil.imatrix.gguf",
        "file_size": "7.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/MN-14B-Crimson-Veil-i1-GGUF/resolve/main/README.md",
    "description": "mradermacher's roleplay GGUF quantizations of Vortex5/MN-14B-Crimson-Veil."
  },
  {
    "model_name": "Cydonia-Redux-22B-v1.1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 826,
    "createdAt": "2025-10-13T03:44:48.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ1_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ1_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ2_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ2_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ2_XS.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ2_XXS.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ3_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ3_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ3_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ3_XXS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q2_K_S.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q4_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q4_1.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.i1-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Cydonia-Redux-22B-v1.1.imatrix",
        "path": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/Cydonia-Redux-22B-v1.1.imatrix.gguf",
        "file_size": "11.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Cydonia-Redux-22B-v1.1-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "CapRL-3B-GGUF",
    "developer": "mradermacher",
    "downloads": 811,
    "createdAt": "2025-09-29T14:51:59.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "CapRL-3B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "CapRL-3B.Q2_K",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "CapRL-3B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "CapRL-3B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "CapRL-3B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "CapRL-3B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "CapRL-3B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "CapRL-3B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "CapRL-3B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "CapRL-3B.Q6_K",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "CapRL-3B.Q8_0",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "CapRL-3B.f16",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.f16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "CapRL-3B.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.mmproj-Q8_0.gguf",
        "file_size": "808.5 MB"
      },
      {
        "model_id": "CapRL-3B.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/CapRL-3B.mmproj-f16.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/CapRL-3B-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of the CapRL-3B multimodal image captioning model."
  },
  {
    "model_name": "next-4b",
    "developer": "Lamapi",
    "downloads": 810,
    "createdAt": "2025-10-15T09:33:05.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "next-4b-f16",
        "path": "https://huggingface.co/Lamapi/next-4b/resolve/main/next-4b-f16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "next-4b-q8_0",
        "path": "https://huggingface.co/Lamapi/next-4b/resolve/main/next-4b-q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lamapi/next-4b/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Sungur-9B-GGUF",
    "developer": "suayptalha",
    "downloads": 790,
    "createdAt": "2025-10-01T11:34:48.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "unsloth.BF16",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.BF16.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "unsloth.F16",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.F16.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "unsloth.Q2_K",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q2_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "unsloth.Q3_K_L",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q3_K_L.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "unsloth.Q3_K_M",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q3_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "unsloth.Q3_K_S",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q3_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "unsloth.Q4_0",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "unsloth.Q4_1",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "unsloth.Q4_K",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q4_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "unsloth.Q4_K_M",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q4_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "unsloth.Q4_K_S",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q4_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "unsloth.Q5_0",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q5_0.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "unsloth.Q5_1",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q5_1.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "unsloth.Q5_K",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q5_K.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "unsloth.Q5_K_M",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q5_K_M.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "unsloth.Q5_K_S",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q5_K_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "unsloth.Q6_K",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q6_K.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "unsloth.Q8_0",
        "path": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/unsloth.Q8_0.gguf",
        "file_size": "9.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/suayptalha/Sungur-9B-GGUF/resolve/main/README.md",
    "description": "Quantized Turkish Gemma-2-9b LLM."
  },
  {
    "model_name": "GRaPE-Mini-Beta-Thinking",
    "developer": "Sweaterdog",
    "downloads": 788,
    "createdAt": "2025-09-04T00:15:36Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GRaPE-mini-beta-thinking.F16",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta-Thinking/resolve/main/GRaPE-mini-beta-thinking.F16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-thinking.Q4_K",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta-Thinking/resolve/main/GRaPE-mini-beta-thinking.Q4_K.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-thinking.Q6_K",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta-Thinking/resolve/main/GRaPE-mini-beta-thinking.Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-thinking.Q8_0",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta-Thinking/resolve/main/GRaPE-mini-beta-thinking.Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta-Thinking/resolve/main/README.md",
    "description": "GRaPE Mini Beta Thinking is a 1.5 B Qwen2.5 instruction‑tuned model, LoRA‑fine‑tuned for reasoning, coding, agentic tasks, and conditional alignment."
  },
  {
    "model_name": "Xortron2025",
    "developer": "darkc0de",
    "downloads": 779,
    "createdAt": "2025-04-04T00:54:44.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Xortron2025-24B.Q6_K",
        "path": "https://huggingface.co/darkc0de/Xortron2025/resolve/main/Xortron2025-24B.Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/darkc0de/Xortron2025/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Discord-Micae-Hermes-3-3B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 772,
    "createdAt": "2025-08-27T06:26:18.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ1_M.gguf",
        "file_size": "881.4 MB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ1_S.gguf",
        "file_size": "827.9 MB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ2_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ2_XS.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ2_XXS.gguf",
        "file_size": "970.4 MB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ3_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ3_XS.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ3_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ4_NL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q2_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.i1-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Discord-Micae-Hermes-3-3B.imatrix",
        "path": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/Discord-Micae-Hermes-3-3B.imatrix.gguf",
        "file_size": "2.9 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Discord-Micae-Hermes-3-3B-i1-GGUF/resolve/main/README.md",
    "description": "Discord‑Micae‑Hermes‑3‑3B is a 3 B‑parameter Llama‑3 chat model fine‑tuned on Discord data, provided with many GGUF quantizations."
  },
  {
    "model_name": "quantum_programming_llama-3-8b-gguf",
    "developer": "as-krn",
    "downloads": 769,
    "createdAt": "2025-09-01T12:00:49.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "unsloth.F16",
        "path": "https://huggingface.co/as-krn/quantum_programming_llama-3-8b-gguf/resolve/main/unsloth.F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "unsloth.Q4_K_M",
        "path": "https://huggingface.co/as-krn/quantum_programming_llama-3-8b-gguf/resolve/main/unsloth.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/as-krn/quantum_programming_llama-3-8b-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF",
    "developer": "QuantFactory",
    "downloads": 763,
    "createdAt": "2024-10-11T07:39:39Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q2_K",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_0",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_1",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_0",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_1",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q8_0",
        "path": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF version of the NSFW DPO Noromaid‑7b + Mistral‑7B‑Instruct merged model."
  },
  {
    "model_name": "Mixtral-4x3B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 757,
    "createdAt": "2025-07-27T10:59:37Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Mixtral-4x3B-v1a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q3_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q4_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q5_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q6_K.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q8_0.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/README.md",
    "description": "This is a MoE-ified version of Voxtral trained on a specific dataset, designed to generate dialogue with a strong socialist perspective, though it may be unstable and requires careful prompting for coherent outputs.",
    "mmproj_models": [
      {
        "model_id": "mmproj-Voxtral-Mini-3B-2507-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/mmproj-Voxtral-Mini-3B-2507-Q8_0.gguf",
        "file_size": "682.6 MB"
      }
    ],
    "num_mmproj": 1
  },
  {
    "model_name": "llama3.1-8b-gguf",
    "developer": "chatpdflocal",
    "downloads": 755,
    "createdAt": "2024-07-24T02:13:17.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "ggml-model-Q2_K",
        "path": "https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "ggml-model-Q3_K_M",
        "path": "https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "ggml-model-f16",
        "path": "https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "neutts-air-GGUF",
    "developer": "mradermacher",
    "downloads": 732,
    "createdAt": "2025-10-02T15:07:49.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "neutts-air.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.IQ4_XS.gguf",
        "file_size": "501.2 MB"
      },
      {
        "model_id": "neutts-air.Q2_K",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q2_K.gguf",
        "file_size": "488.9 MB"
      },
      {
        "model_id": "neutts-air.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q3_K_L.gguf",
        "file_size": "518.3 MB"
      },
      {
        "model_id": "neutts-air.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q3_K_M.gguf",
        "file_size": "505.0 MB"
      },
      {
        "model_id": "neutts-air.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q3_K_S.gguf",
        "file_size": "488.6 MB"
      },
      {
        "model_id": "neutts-air.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q4_K_M.gguf",
        "file_size": "568.6 MB"
      },
      {
        "model_id": "neutts-air.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q4_K_S.gguf",
        "file_size": "556.9 MB"
      },
      {
        "model_id": "neutts-air.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q5_K_M.gguf",
        "file_size": "601.5 MB"
      },
      {
        "model_id": "neutts-air.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q5_K_S.gguf",
        "file_size": "594.5 MB"
      },
      {
        "model_id": "neutts-air.Q6_K",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q6_K.gguf",
        "file_size": "741.3 MB"
      },
      {
        "model_id": "neutts-air.Q8_0",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.Q8_0.gguf",
        "file_size": "765.5 MB"
      },
      {
        "model_id": "neutts-air.f16",
        "path": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/neutts-air.f16.gguf",
        "file_size": "1.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/neutts-air-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of the neutts-air speech-language model."
  },
  {
    "model_name": "Discord-Hermes-3-8B-GGUF",
    "developer": "mradermacher",
    "downloads": 728,
    "createdAt": "2025-08-27T14:26:48.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Discord-Hermes-3-8B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Discord-Hermes-3-8B.f16",
        "path": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/Discord-Hermes-3-8B.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Discord-Hermes-3-8B-GGUF/resolve/main/README.md",
    "description": "Provides multiple GGUF quantizations of the Discord‑Hermes‑3‑8B model for efficient inference."
  },
  {
    "model_name": "MamayLM-Gemma-3-12B-IT-v1.0-GGUF",
    "developer": "INSAIT-Institute",
    "downloads": 710,
    "createdAt": "2025-09-03T10:05:45.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "MamayLM-Gemma-3-12B-IT-v1.0.Q4_K_M",
        "path": "https://huggingface.co/INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0-GGUF/resolve/main/MamayLM-Gemma-3-12B-IT-v1.0.Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "MamayLM-Gemma-3-12B-IT-v1.0.Q4_K_S",
        "path": "https://huggingface.co/INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0-GGUF/resolve/main/MamayLM-Gemma-3-12B-IT-v1.0.Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "MamayLM-Gemma-3-12B-IT-v1.0.Q5_K_S",
        "path": "https://huggingface.co/INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0-GGUF/resolve/main/MamayLM-Gemma-3-12B-IT-v1.0.Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MamayLM-Gemma-3-12B-IT-v1.0.Q8_0",
        "path": "https://huggingface.co/INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0-GGUF/resolve/main/MamayLM-Gemma-3-12B-IT-v1.0.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0-GGUF/resolve/main/README.md",
    "description": "Ukrainian and English Gemma-3-12B-IT GGUF model."
  },
  {
    "model_name": "DrakIdol-Roleplayer-1.0-GGUF",
    "developer": "mradermacher",
    "downloads": 702,
    "createdAt": "2025-07-27T21:16:36Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DrakIdol-Roleplayer-1.0.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q2_K",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q6_K",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q8_0",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.f16",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.f16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.mmproj-Q8_0.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.mmproj-f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the DrakIdol-Roleplayer-1.0 model by aifeifei798, optimized for roleplay tasks with various quantization options available.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Mixture-2507-GGUF",
    "developer": "mradermacher",
    "downloads": 700,
    "createdAt": "2025-08-01T17:57:55Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Mixture-2507.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/Qwen3-30B-A3B-Mixture-2507.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF/resolve/main/README.md",
    "description": "The model Qwen3-30B-A3B-Mixture-2507 is available in various GGUF quantized versions, including Q2_K, Q3_K_S, Q4_K_S, Q6_K, and Q8_0, with Q4_K_S and"
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 680,
    "createdAt": "2025-01-21T03:41:56.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/README.md",
    "description": "BaronLLM is a large-language model fine-tuned for offensive cybersecurity research and adversarial simulation, providing exploit reasoning, red-team scenario generation, and safety-constrained content."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 679,
    "createdAt": "2025-01-22T14:18:35.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-abliterated.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-abliterated-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Strand-Rust-Coder-14B-v1-GGUF",
    "developer": "Fortytwo-Network",
    "downloads": 670,
    "createdAt": "2025-10-13T14:49:53.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-BF16",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-BF16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q4_K_M",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q5_K_M",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q6_K",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q8_0",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 640,
    "createdAt": "2025-04-30T21:09:55.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_XS.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q2_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-8B-abliterated-v1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/Josiefied-Qwen3-8B-abliterated-v1.i1-Q6_K.gguf",
        "file_size": "6.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Josiefied-Qwen3-8B-abliterated-v1-i1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "GLM-4.6-GGUF",
    "developer": "AesSedai",
    "downloads": 639,
    "createdAt": "2025-09-30T22:58:49.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "ik_llama.cpp/GLM-4.6-Q8_0-IQ4_KS-IQ4_KS-IQ5_KS",
        "path": "https://huggingface.co/AesSedai/GLM-4.6-GGUF/resolve/main/ik_llama.cpp/GLM-4.6-Q8_0-IQ4_KS-IQ4_KS-IQ5_KS.gguf",
        "file_size": "198.8 GB"
      },
      {
        "model_id": "llama.cpp/GLM-4.6-Q8_0-IQ3_XXS-IQ3_XXS-IQ3_S",
        "path": "https://huggingface.co/AesSedai/GLM-4.6-GGUF/resolve/main/llama.cpp/GLM-4.6-Q8_0-IQ3_XXS-IQ3_XXS-IQ3_S.gguf",
        "file_size": "143.2 GB"
      },
      {
        "model_id": "llama.cpp/GLM-4.6-Q8_0-Q4_K-Q4_K-Q5_K",
        "path": "https://huggingface.co/AesSedai/GLM-4.6-GGUF/resolve/main/llama.cpp/GLM-4.6-Q8_0-Q4_K-Q4_K-Q5_K.gguf",
        "file_size": "208.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AesSedai/GLM-4.6-GGUF/resolve/main/README.md",
    "description": "WIP: Several KLD-tested quants with reference logits for reproducibility will be uploaded shortly."
  },
  {
    "model_name": "Ring-flash-2.0-GGUF",
    "developer": "inclusionAI",
    "downloads": 619,
    "createdAt": "2025-09-25T18:57:53.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Ring-flash-2.0-Q2_K",
        "path": "https://huggingface.co/inclusionAI/Ring-flash-2.0-GGUF/resolve/main/Ring-flash-2.0-Q2_K.gguf",
        "file_size": "35.1 GB"
      },
      {
        "model_id": "Ring-flash-2.0-Q4_K_M",
        "path": "https://huggingface.co/inclusionAI/Ring-flash-2.0-GGUF/resolve/main/Ring-flash-2.0-Q4_K_M.gguf",
        "file_size": "58.1 GB"
      },
      {
        "model_id": "Ring-flash-2.0-Q6_K",
        "path": "https://huggingface.co/inclusionAI/Ring-flash-2.0-GGUF/resolve/main/Ring-flash-2.0-Q6_K.gguf",
        "file_size": "78.7 GB"
      },
      {
        "model_id": "Ring-flash-2.0-Q8_0",
        "path": "https://huggingface.co/inclusionAI/Ring-flash-2.0-GGUF/resolve/main/Ring-flash-2.0-Q8_0.gguf",
        "file_size": "101.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/inclusionAI/Ring-flash-2.0-GGUF/resolve/main/README.md",
    "description": "inclusionAI/Ring-flash-2.0 quantized GGUF model for llama.cpp"
  },
  {
    "model_name": "apollo-astralis-8b",
    "developer": "vanta-research",
    "downloads": 603,
    "createdAt": "2025-10-05T08:00:07.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "apollo_astralis_8b_Q4_K_M",
        "path": "https://huggingface.co/vanta-research/apollo-astralis-8b/resolve/main/apollo_astralis_8b_Q4_K_M.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/vanta-research/apollo-astralis-8b/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF",
    "developer": "QuantFactory",
    "downloads": 594,
    "createdAt": "2025-08-28T07:56:26Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_1.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_1.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-v0.4-deepresearch-no-think-4.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/Qwen3-4B-v0.4-deepresearch-no-think-4.Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/Qwen3-4B-v0.4-deepresearch-no-think-4-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-4B-v0.4-deepresearch-no-think-4 model, optimized for GGUF format using llama.cpp."
  },
  {
    "model_name": "dolphin-2.6-mistral-7B-GGUF",
    "developer": "second-state",
    "downloads": 582,
    "createdAt": "2023-12-29T11:59:57.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "dolphin-2.6-mistral-7b-Q2_K",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q3_K_L",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q3_K_M",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q3_K_S",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q4_0",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q4_K_M",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q4_K_S",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q5_0",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q5_K_M",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q5_K_S",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q6_K",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b-Q8_0",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "dolphin-2.6-mistral-7b.Q5_K_M",
        "path": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Cydonia-24B-v4x-GGUF",
    "developer": "BeaverAI",
    "downloads": 573,
    "createdAt": "2025-10-22T01:57:08.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4x-Q2_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4x-GGUF/resolve/main/Cydonia-24B-v4x-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4x-Q3_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4x-GGUF/resolve/main/Cydonia-24B-v4x-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4x-Q4_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4x-GGUF/resolve/main/Cydonia-24B-v4x-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4x-Q5_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4x-GGUF/resolve/main/Cydonia-24B-v4x-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4x-Q6_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4x-GGUF/resolve/main/Cydonia-24B-v4x-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4x-Q8_0",
        "path": "https://huggingface.co/BeaverAI/Cydonia-24B-v4x-GGUF/resolve/main/Cydonia-24B-v4x-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "CausalLM-7B-DPO-alpha-GGUF",
    "developer": "tastypear",
    "downloads": 545,
    "createdAt": "2023-11-19T15:36:16Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "causallm_7b-dpo-alpha.Q3_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q3_K_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q4_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q5_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q5_K_S",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q6_K",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q8_0",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q8_0.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.f16",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.f16.gguf",
        "file_size": "14.4 GB"
      }
    ],
    "readme": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/README.md",
    "description": "This is a GGUF format quantized version of the CausalLM 7B-DPO-alpha model, compatible with llama.cpp and various UIs, licensed under WTFPL and Meta Llama 2 terms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "mergekit-model_stock-prczfmj-Q4_K_M-GGUF",
    "developer": "Otakadelic",
    "downloads": 545,
    "createdAt": "2025-05-14T04:19:50.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "mergekit-model_stock-prczfmj-q4_k_m",
        "path": "https://huggingface.co/Otakadelic/mergekit-model_stock-prczfmj-Q4_K_M-GGUF/resolve/main/mergekit-model_stock-prczfmj-q4_k_m.gguf",
        "file_size": "15.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Otakadelic/mergekit-model_stock-prczfmj-Q4_K_M-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-4B-Valiant-Polaris-f32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 524,
    "createdAt": "2025-08-03T04:40:54Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B-Valiant-Polaris-f32-GGUF is a 4B-parameter language model combining the reasoning capabilities of Polaris, creativity of Dot-Goat and RP-V3, and scientific depth of ShiningValiant3, optimized for advanced reasoning,",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NVIDIA-Nemotron-Nano-9B-v2-GGUF",
    "developer": "QuantFactory",
    "downloads": 523,
    "createdAt": "2025-08-30T08:00:34Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q2_K",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q2_K.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q3_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q3_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q3_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q4_0",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q4_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q4_1",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q4_1.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q4_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q4_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q5_0",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q5_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q5_1",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q5_1.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q6_K",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q6_K.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2.Q8_0",
        "path": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/NVIDIA-Nemotron-Nano-9B-v2.Q8_0.gguf",
        "file_size": "7.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF/resolve/main/README.md",
    "description": "The NVIDIA-Nemotron-Nano-9B-v2 is a large language model trained using a hybrid Mamba-2 and MLP architecture, supporting multiple languages and reasoning modes, with evaluation results across various benchmarks and available for inference via Huggingface Transformers, vLLM, and TRT-"
  },
  {
    "model_name": "Qwen3-4b-tcomanr-merge",
    "developer": "ertghiu256",
    "downloads": 523,
    "createdAt": "2025-07-17T14:34:16Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "model-F16",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/README.md",
    "description": "This is a merged Qwen3-4B model combining code, math, and reasoning capabilities from multiple fine-tuned versions using the TIES method.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Jinx-gpt-OSS-20B-MXFP4-GGUF",
    "developer": "Joseph717171",
    "downloads": 521,
    "createdAt": "2025-08-24T22:44:20Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Jinx-gpt-OSS-20B",
        "path": "https://huggingface.co/Joseph717171/Jinx-gpt-OSS-20B-MXFP4-GGUF/resolve/main/Jinx-gpt-OSS-20B.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Joseph717171/Jinx-gpt-OSS-20B-MXFP4-GGUF/resolve/main/README.md",
    "description": "This is a GGUF MXFP4 quantized version of the Jinx-gpt-oss-20b model, optimized for accuracy and precision while reducing file size to ~12.11 GB."
  },
  {
    "model_name": "Llama3-8b-chinese-Uncensored-v0.1-gguf",
    "developer": "supernovaburn",
    "downloads": 510,
    "createdAt": "2024-06-16T07:39:52Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Llama3-8b-chinese-Uncensored-Q4_K_M",
        "path": "https://huggingface.co/supernovaburn/Llama3-8b-chinese-Uncensored-v0.1-gguf/resolve/main/Llama3-8b-chinese-Uncensored-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama3-8b-chinese-Uncensored-v0.1-q8_0",
        "path": "https://huggingface.co/supernovaburn/Llama3-8b-chinese-Uncensored-v0.1-gguf/resolve/main/Llama3-8b-chinese-Uncensored-v0.1-q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/supernovaburn/Llama3-8b-chinese-Uncensored-v0.1-gguf/resolve/main/README.md",
    "description": "This is an uncensored version of Llama3-8b-instruct, capable of generating text without ethical constraints."
  },
  {
    "model_name": "LFM2-350M-Math-GGUF",
    "developer": "LiquidAI",
    "downloads": 488,
    "createdAt": "2025-09-05T16:15:25.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-350M-Math-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/LFM2-350M-Math-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-350M-Math-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/LFM2-350M-Math-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-350M-Math-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/LFM2-350M-Math-Q4_K_M.gguf",
        "file_size": "218.7 MB"
      },
      {
        "model_id": "LFM2-350M-Math-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/LFM2-350M-Math-Q5_K_M.gguf",
        "file_size": "248.3 MB"
      },
      {
        "model_id": "LFM2-350M-Math-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/LFM2-350M-Math-Q6_K.gguf",
        "file_size": "279.8 MB"
      },
      {
        "model_id": "LFM2-350M-Math-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/LFM2-350M-Math-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-350M-Math-GGUF/resolve/main/README.md",
    "description": "Tiny math reasoning model for solving tricky math problems."
  },
  {
    "model_name": "dolphin-2.9.1-yi-1.5-34b-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 486,
    "createdAt": "2024-05-19T14:02:07Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ1_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ1_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_M.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_S.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XS.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XXS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_M.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XS.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XXS.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ4_XS.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_L.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_M.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_S.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_M.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_S.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_M.gguf",
        "file_size": "22.7 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_S.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q6_K.gguf",
        "file_size": "26.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/README.md",
    "description": "The model is a 34B parameter version of Dolphin-2.9.1-Yi-1.5, quantized into various GGUF formats for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "LLama-3-8b-Uncensored-Q4_K_M-GGUF",
    "developer": "DevsDoCode",
    "downloads": 477,
    "createdAt": "2024-05-06T09:45:53.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "llama-3-8b-uncensored.Q4_K_M",
        "path": "https://huggingface.co/DevsDoCode/LLama-3-8b-Uncensored-Q4_K_M-GGUF/resolve/main/llama-3-8b-uncensored.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DevsDoCode/LLama-3-8b-Uncensored-Q4_K_M-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "KAT-Dev-72B-Exp-GGUF",
    "developer": "mradermacher",
    "downloads": 476,
    "createdAt": "2025-10-11T04:00:20.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "KAT-Dev-72B-Exp.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.IQ4_XS.gguf",
        "file_size": "37.4 GB"
      },
      {
        "model_id": "KAT-Dev-72B-Exp.Q2_K",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.Q2_K.gguf",
        "file_size": "27.8 GB"
      },
      {
        "model_id": "KAT-Dev-72B-Exp.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.Q3_K_L.gguf",
        "file_size": "36.8 GB"
      },
      {
        "model_id": "KAT-Dev-72B-Exp.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.Q3_K_M.gguf",
        "file_size": "35.1 GB"
      },
      {
        "model_id": "KAT-Dev-72B-Exp.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.Q3_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "KAT-Dev-72B-Exp.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.Q4_K_M.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "KAT-Dev-72B-Exp.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/KAT-Dev-72B-Exp.Q4_K_S.gguf",
        "file_size": "40.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/KAT-Dev-72B-Exp-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "DeepHat-V1-7B-GGUF",
    "developer": "mradermacher",
    "downloads": 474,
    "createdAt": "2025-07-22T05:01:10Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "DeepHat-V1-7B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.f16",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/README.md",
    "description": "The DeepHat-V1-7B model by mradermacher is a 7B parameter language model quantized in various formats (Q2_K, Q3_K_S, Q4_K_S, etc.) for efficient deployment, with the highest quality version being Q8_0 at",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 465,
    "createdAt": "2025-07-30T03:03:56.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-30B-A3B-Instruct-2507-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-30B-A3B-Instruct-2507 model using the Intel AutoRound algorithm, with the embedding layer and lm-head layer quantized to 8 bits and non-expert layers to 4 bits, optimized for efficient"
  },
  {
    "model_name": "qwen3-1.7b-mixture-of-thought",
    "developer": "ertghiu256",
    "downloads": 458,
    "createdAt": "2025-07-19T10:25:11Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/model-Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/model-Q8_0.gguf",
        "file_size": "2.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/README.md",
    "description": "This is a Qwen3-1.7B model trained on 20k conversations from `open-r1/Mixture-of-Thoughts` and 3k from `mlabonne/FineTome-100k` to enhance reasoning capabilities, optimized for weaker devices.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-2-2b-jpn-it-translate-gguf",
    "developer": "webbigdata",
    "downloads": 451,
    "createdAt": "2024-10-07T14:59:00.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "gemma-2-2b-jpn-it-translate-IQ3_M",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-IQ3_XS",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-IQ3_XS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-IQ3_XXS",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-IQ3_XXS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-IQ4_XS",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q3_K-f16",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q3_K-f16.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q3_K_L",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q3_K_L.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q3_K_M",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q3_K_S",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q4_K-f16",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q4_K-f16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q4_K_L",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q4_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q4_K_M",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q4_K_S",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q5_4_M",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q5_4_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q5_K-f16",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q5_K-f16.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q5_K_L",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q5_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q5_K_M",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q5_K_S",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q6_K-f16",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q6_K-f16.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q6_K",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q6_K_L",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q8_0-f16",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q8_0-f16.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q8_0",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-2-2b-jpn-it-translate-Q8_0_L",
        "path": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/gemma-2-2b-jpn-it-translate-Q8_0_L.gguf",
        "file_size": "2.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "MedScholar-1.5B-f32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 446,
    "createdAt": "2025-08-01T12:56:11Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "MedScholar-1.5B.BF16",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.BF16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "MedScholar-1.5B.F16",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "MedScholar-1.5B.F32",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.F32.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q4_K_S.gguf",
        "file_size": "896.7 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q8_0.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/README.md",
    "description": "MedScholar-1.5B-f32-GGUF is a compact, instruction-aligned medical question-answering model fine-tuned on the MIRIAD-4.4M dataset for research and educational use only, not for diagnosis or medical decision-making.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF",
    "developer": "jukofyork",
    "downloads": 445,
    "createdAt": "2025-07-18T11:08:53Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-128k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-128k-Q4_0.gguf",
        "file_size": "426.3 MB"
      },
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-32k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-32k-Q4_0.gguf",
        "file_size": "426.3 MB"
      },
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-64k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-64k-Q4_0.gguf",
        "file_size": "426.3 MB"
      }
    ],
    "readme": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/README.md",
    "description": "This is a 0.6B parameter draft model for speculative decoding with Kimi-K2-Instruct, quantized in Q4_0 format for context lengths of 32k, 64k, and 128k.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "miqu-1-70b",
    "developer": "miqudev",
    "downloads": 444,
    "createdAt": "2024-01-26T10:50:49.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "miqu-1-70b.q2_K",
        "path": "https://huggingface.co/miqudev/miqu-1-70b/resolve/main/miqu-1-70b.q2_K.gguf",
        "file_size": "23.7 GB"
      },
      {
        "model_id": "miqu-1-70b.q4_k_m",
        "path": "https://huggingface.co/miqudev/miqu-1-70b/resolve/main/miqu-1-70b.q4_k_m.gguf",
        "file_size": "38.6 GB"
      },
      {
        "model_id": "miqu-1-70b.q5_K_M",
        "path": "https://huggingface.co/miqudev/miqu-1-70b/resolve/main/miqu-1-70b.q5_K_M.gguf",
        "file_size": "45.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/miqudev/miqu-1-70b/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen2.5-Coder-14B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 435,
    "createdAt": "2024-11-12T06:20:21.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-F16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-Medical-GRPO-GGUF",
    "developer": "prithivMLmods",
    "downloads": 434,
    "createdAt": "2025-09-02T06:03:40.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Qwen3-Medical-GRPO.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-Medical-GRPO.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/Qwen3-Medical-GRPO.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/Qwen3-Medical-GRPO-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "palmyra-mini-thinking-b-GGUF",
    "developer": "Writer",
    "downloads": 424,
    "createdAt": "2025-09-09T23:00:30.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "palmyra-mini-thinking-b-BF16",
        "path": "https://huggingface.co/Writer/palmyra-mini-thinking-b-GGUF/resolve/main/palmyra-mini-thinking-b-BF16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "palmyra-mini-thinking-b-Q8_0",
        "path": "https://huggingface.co/Writer/palmyra-mini-thinking-b-GGUF/resolve/main/palmyra-mini-thinking-b-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Writer/palmyra-mini-thinking-b-GGUF/resolve/main/README.md",
    "description": "**Palmyra‑mini‑thinking‑b – Quick Overview**\n\n| Aspect | Details |\n|--------|---------|\n| **Base model** | Qwen/Qwen2.5‑1.5B (≈1.7 B parameters) |\n| **Specialisation** | Heavy finetuning for deep reasoning – math, competitive programming, and code generation |\n| **Context window** | Up to **131 k** tokens (default 4 k) |\n| **License** | Apache‑2.0 |\n| **Key strengths** | • Very strong on high‑school and olympiad‑level math (AMC23 ≈ 92 % strict‑match, MATH500 ≈ 94 % pass@1)  <br>• Competitive‑programming code generation (Codeforces pass‑rate ≈ 63 %)  <br>• Good performance on general reasoning (GPQA, MMLU‑PRO) |\n| **Benchmark highlights (pass@1, avg‑of‑64)** | AIME24 59 % / 72 % (majority)  <br>GPQA 42 % / 47 %  <br>MATH500 94 % / 95 %  <br>Codeforces 63 % |\n| **Benchmark highlights (pass@1, avg‑of‑1)** | GSM8K 42 % (strict)  <br>HumanEval 7 %  <br>BBH 29 %  <br>Livecodebench ≈ 29 % |\n| **Typical use‑cases** | • Solving multi‑step math problems (AMC, AIME, Olympiad)  <br>• Writing, debugging, and explaining code (algorithmic contests, code generation)  <br>• Long‑form reasoning that benefits from the 131 k token window |\n| **How to query** | Use **ChatML** format (system → user → assistant). Example with `llama.cpp` or any GGUF‑compatible runtime: <br>`./main -m palmyra-mini-thinking-b-Q8_0.gguf -p \"<|im_start|>user\\nSolve 2x+5=13.\\n<|im_end|>\\n<|im_start|>assistant\\n\"` |\n| **Quantizations available** | • BF16 (≈3.3 GB) – highest quality, more RAM  <br>• Q8_0 (≈1.8 GB) – ~45 % smaller, faster inference |\n| **Integration snippets** | • **Transformers** (HF) – `AutoModelForCausalLM` with flash‑attention 2  <br>• **vLLM** – `vllm serve Writer/palmyra-mini-thinking-b`  <br>• **Ollama / LM Studio** – load the GGUF file directly |\n| **Hardware recommendations** | • BF16: ≥4 GB RAM, modern GPU/CPU  <br>• Q8_0: ≥3 GB RAM, works well on CPUs and low‑end GPUs |\n| **Caveats** | • As with any LLM, may produce plausible‑but‑incorrect answers; verify math/code results.  <br>• Lower‑bit quantizations can slightly degrade reasoning quality. |\n| **Citation** | ```bibtex @misc{Palmyra-mini-thinking-b, author={Writer Engineering team}, title={{Palmyra‑mini: A powerful LLM designed for math and coding}}, howpublished={\\url{https://dev.writer.com}}, year={2025}, month={Sep} } ``` |\n\n**Bottom line:** Palmyra‑mini‑thinking‑b is a compact (1.7 B) LLM that excels at multi‑step mathematical reasoning and algorithmic code generation, offering a very large context window and two convenient quantizations for CPU‑ or GPU‑based deployment. It’s a strong choice when you need high‑quality math or coding assistance without the overhead of a 7‑B+ model."
  },
  {
    "model_name": "Foundation-Sec-8B-Instruct-Q8_0-GGUF",
    "developer": "fdtn-ai",
    "downloads": 423,
    "createdAt": "2025-08-22T01:00:07Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "foundation-sec-8b-instruct-q8_0",
        "path": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct-Q8_0-GGUF/resolve/main/foundation-sec-8b-instruct-q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct-Q8_0-GGUF/resolve/main/README.md",
    "description": "The Foundation-Sec-8B-Q8_0-GGUF model is an 8-bit quantized version of the Foundation-Sec-8B-Instruct cybersecurity-specialized LLaMA 3.1 model, reducing memory usage from ~16GB to ~8.54GB"
  },
  {
    "model_name": "aidapal",
    "developer": "AverageBusinessUser",
    "downloads": 417,
    "createdAt": "2024-06-04T19:37:42.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "aidapal-8k.Q4_K_M",
        "path": "https://huggingface.co/AverageBusinessUser/aidapal/resolve/main/aidapal-8k.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AverageBusinessUser/aidapal/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Osmosis-Apply-1.7B",
    "developer": "osmosis-ai",
    "downloads": 415,
    "createdAt": "2025-06-19T07:02:07Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "osmosis-apply-1.7b-bf16",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-apply-1.7b-bf16.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.IQ4_XS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q2_K.gguf",
        "file_size": "839.1 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_M.gguf",
        "file_size": "1023.5 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_S.gguf",
        "file_size": "954.6 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q4_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q5_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q5_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q6_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q8_0.gguf",
        "file_size": "2.0 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/README.md",
    "description": "Osmosis-Apply-1.7B is a language model finetuned on Qwen3-1.7B to apply edit snippets to original code for code merges, with a reward function prioritizing exactness in the output.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3-8B-Instruct-Finance-RAG-GGUF",
    "developer": "QuantFactory",
    "downloads": 413,
    "createdAt": "2024-10-20T15:43:33.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-3-8B-Instruct-Finance-RAG.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/Llama-3-8B-Instruct-Finance-RAG.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Arch-Router-1.5B.gguf",
    "developer": "katanemo",
    "downloads": 412,
    "createdAt": "2025-05-30T18:18:40.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Arch-Router-1.5B-Q2_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_L",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Arch-Router-Q6_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-Q6_K.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/README.md",
    "description": "The katanemo/Arch-Router-1.5B model is a compact, preference-aligned routing framework that maps queries to domain-action preferences for selecting the most suitable large language model."
  },
  {
    "model_name": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF",
    "developer": "mradermacher",
    "downloads": 408,
    "createdAt": "2025-07-23T05:15:11Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.IQ4_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q2_K.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q6_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q8_0.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.f16.gguf",
        "file_size": "11.1 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3 model with various GGUF quantization options for different trade-offs between speed and quality.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gorilla-openfunctions-v2-gguf",
    "developer": "gorilla-llm",
    "downloads": 404,
    "createdAt": "2024-03-09T01:20:11.000Z",
    "tools": false,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "gorilla-openfunctions-v2-q2_K",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q3_K_L",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q3_K_L.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q3_K_M",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q3_K_S",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q4_K_M",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q4_K_S",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q4_K_S.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q5_K_M",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q5_K_S",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gorilla-openfunctions-v2-q6_K",
        "path": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/gorilla-openfunctions-v2-q6_K.gguf",
        "file_size": "5.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2-gguf/resolve/main/README.md",
    "description": "Gorilla-OpenFunctions-v2 GGUF quantized models with GPT-4-level function calling performance."
  },
  {
    "model_name": "SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF",
    "developer": "mradermacher",
    "downloads": 403,
    "createdAt": "2025-07-29T08:36:02.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.IQ4_XS.gguf",
        "file_size": "955.7 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q2_K",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q2_K.gguf",
        "file_size": "675.1 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q3_K_L.gguf",
        "file_size": "930.9 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q3_K_M.gguf",
        "file_size": "861.9 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q3_K_S.gguf",
        "file_size": "782.4 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q4_K_S.gguf",
        "file_size": "1007.2 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q6_K",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.Q8_0",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.f16",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.f16.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.mmproj-Q8_0.gguf",
        "file_size": "565.1 MB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Agentic-GUI.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Agentic-GUI.mmproj-f16.gguf",
        "file_size": "831.9 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/SmolVLM2-2.2B-Instruct-Agentic-GUI-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "kani-tts-370m-GGUF",
    "developer": "mradermacher",
    "downloads": 401,
    "createdAt": "2025-10-01T18:20:13.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "kani-tts-370m.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.IQ4_XS.gguf",
        "file_size": "214.5 MB"
      },
      {
        "model_id": "kani-tts-370m.Q2_K",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q2_K.gguf",
        "file_size": "165.6 MB"
      },
      {
        "model_id": "kani-tts-370m.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q3_K_L.gguf",
        "file_size": "206.1 MB"
      },
      {
        "model_id": "kani-tts-370m.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q3_K_M.gguf",
        "file_size": "196.7 MB"
      },
      {
        "model_id": "kani-tts-370m.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q3_K_S.gguf",
        "file_size": "185.2 MB"
      },
      {
        "model_id": "kani-tts-370m.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q4_K_M.gguf",
        "file_size": "231.2 MB"
      },
      {
        "model_id": "kani-tts-370m.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q4_K_S.gguf",
        "file_size": "223.0 MB"
      },
      {
        "model_id": "kani-tts-370m.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q5_K_M.gguf",
        "file_size": "260.8 MB"
      },
      {
        "model_id": "kani-tts-370m.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q5_K_S.gguf",
        "file_size": "255.9 MB"
      },
      {
        "model_id": "kani-tts-370m.Q6_K",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q6_K.gguf",
        "file_size": "292.3 MB"
      },
      {
        "model_id": "kani-tts-370m.Q8_0",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.Q8_0.gguf",
        "file_size": "377.7 MB"
      },
      {
        "model_id": "kani-tts-370m.f16",
        "path": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/kani-tts-370m.f16.gguf",
        "file_size": "708.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/kani-tts-370m-GGUF/resolve/main/README.md",
    "description": "Quantized multilingual TTS model supporting 6 languages."
  },
  {
    "model_name": "kani-tts-450m-0.1-pt-GGUF",
    "developer": "mradermacher",
    "downloads": 381,
    "createdAt": "2025-09-18T16:33:07.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "kani-tts-450m-0.1-pt.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.IQ4_XS.gguf",
        "file_size": "256.3 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q2_K",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q2_K.gguf",
        "file_size": "191.4 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q3_K_L.gguf",
        "file_size": "239.9 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q3_K_M.gguf",
        "file_size": "230.5 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q3_K_S.gguf",
        "file_size": "219.0 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q4_K_M.gguf",
        "file_size": "275.4 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q4_K_S.gguf",
        "file_size": "267.2 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q5_K_M.gguf",
        "file_size": "314.9 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q5_K_S.gguf",
        "file_size": "309.9 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q6_K",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q6_K.gguf",
        "file_size": "356.8 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.Q8_0",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.Q8_0.gguf",
        "file_size": "461.2 MB"
      },
      {
        "model_id": "kani-tts-450m-0.1-pt.f16",
        "path": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/kani-tts-450m-0.1-pt.f16.gguf",
        "file_size": "865.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/kani-tts-450m-0.1-pt-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "llama-3.1-8b-roleplay-vi-gguf",
    "developer": "samunder12",
    "downloads": 374,
    "createdAt": "2025-09-01T06:15:48Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Llama3Vi.Q4_K_M",
        "path": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-vi-gguf/resolve/main/Llama3Vi.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/samunder12/llama-3.1-8b-roleplay-vi-gguf/resolve/main/README.md",
    "description": "This is a text generation model based on the meta-llama/Llama-3.1-8B-Instruct base model, trained for roleplay and character-based tasks."
  },
  {
    "model_name": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF",
    "developer": "mradermacher",
    "downloads": 373,
    "createdAt": "2025-06-05T01:26:10.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-4B-Fiction-On-Fire-Series-7-Model-1007-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "AgentFlow-Planner-3B-GGUF",
    "developer": "mradermacher",
    "downloads": 371,
    "createdAt": "2025-10-01T12:53:12.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "AgentFlow-Planner-3B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.IQ4_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q2_K",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q3_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q4_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q5_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q5_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q6_K",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q6_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.Q8_0",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.Q8_0.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "AgentFlow-Planner-3B.f16",
        "path": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/AgentFlow-Planner-3B.f16.gguf",
        "file_size": "6.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/AgentFlow-Planner-3B-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Index-1.9B-Chat-GGUF",
    "developer": "IndexTeam",
    "downloads": 364,
    "createdAt": "2024-06-18T12:25:10.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/IndexTeam/Index-1.9B-Chat-GGUF/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/IndexTeam/Index-1.9B-Chat-GGUF/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/IndexTeam/Index-1.9B-Chat-GGUF/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/IndexTeam/Index-1.9B-Chat-GGUF/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "ggml-model-bf16",
        "path": "https://huggingface.co/IndexTeam/Index-1.9B-Chat-GGUF/resolve/main/ggml-model-bf16.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/IndexTeam/Index-1.9B-Chat-GGUF/resolve/main/README.md",
    "description": "Index-1.9B‑Chat GGUF model adapted for llama.cpp and Ollama with usage instructions."
  },
  {
    "model_name": "palmyra-mini-GGUF",
    "developer": "Writer",
    "downloads": 364,
    "createdAt": "2025-09-05T23:30:19.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Palmyra-mini-BF16",
        "path": "https://huggingface.co/Writer/palmyra-mini-GGUF/resolve/main/Palmyra-mini-BF16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Palmyra-mini-Q8_0",
        "path": "https://huggingface.co/Writer/palmyra-mini-GGUF/resolve/main/Palmyra-mini-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Writer/palmyra-mini-GGUF/resolve/main/README.md",
    "description": "Palmyra‑mini GGUF offers a 1.7 B‑parameter Qwen2‑based model quantized to BF16 (3.3 GB) and Q8_0 (1.8 GB) for fast, high‑quality inference via llama.cpp, LM Studio, Ollama, and excels at math and reasoning tasks."
  },
  {
    "model_name": "WiNGPT-Babel-2-GGUF",
    "developer": "winninghealth",
    "downloads": 363,
    "createdAt": "2025-06-11T06:11:04Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "WiNGPT-Babel-2-IQ4_XS",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "WiNGPT-Babel-2-Q4_K_M",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "WiNGPT-Babel-2-Q8_0",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-Q8_0.gguf",
        "file_size": "2.6 GB"
      }
    ],
    "readme": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/README.md",
    "description": "WiNGPT-Babel-2 is a multilingual translation language model optimized for 55 languages, enhanced Chinese translation, and structured data handling, built on the GemmaX2-28-2B-Pretrain base model.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "UserLM-8b-Q8_0-GGUF",
    "developer": "NikolayKozloff",
    "downloads": 356,
    "createdAt": "2025-10-09T12:39:34.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "userlm-8b-q8_0",
        "path": "https://huggingface.co/NikolayKozloff/UserLM-8b-Q8_0-GGUF/resolve/main/userlm-8b-q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NikolayKozloff/UserLM-8b-Q8_0-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "CabraMistral-v3-7b-32k-GGUF",
    "developer": "mradermacher",
    "downloads": 353,
    "createdAt": "2024-05-25T07:46:46Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q2_K",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q6_K",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q8_0",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.f16",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the Mistral-v3-7b-32k base model, available in various GGUF quantization types for different trade-offs between size and quality.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3-Soliloquy-8B-v2-GGUF",
    "developer": "backyardai",
    "downloads": 329,
    "createdAt": "2024-05-11T03:18:32Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.F16",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ1_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_XXS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_XXS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ4_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q2_K",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q2_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q2_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_L",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q4_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q4_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q5_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q5_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q6_K",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q8_0",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/README.md",
    "description": "This GGUF model is a quantized version of the Llama-3-Soliloquy-8B-v2 by openlynn, optimized for efficient use in Backyard AI, a local AI chat app.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "GRaPE-Mini-Beta",
    "developer": "Sweaterdog",
    "downloads": 327,
    "createdAt": "2025-08-25T08:45:47Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "GRaPE-Mini-Beta.F16",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-Mini-Beta.F16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-20%",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-20%.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-Q4_K_M",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-Q6_k",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-Q6_k.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-Q8_0",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/README.md",
    "description": "GRaPE Mini is a 1.5 billion parameter, instruction-tuned language model based on Qwen2.5, designed for high-quality reasoning, coding, and agentic capabilities, with two conditional prompts for helpful or unfiltered responses."
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 308,
    "createdAt": "2025-10-09T07:50:09.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-30B-A3B-Thinking-2507-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-30B-A3B-Thinking-2507-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-30B-A3B-Thinking-2507-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Playable1-GGUF",
    "developer": "playable",
    "downloads": 301,
    "createdAt": "2025-10-08T18:20:46.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Playable1-q4_k_m",
        "path": "https://huggingface.co/playable/Playable1-GGUF/resolve/main/Playable1-q4_k_m.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/playable/Playable1-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "FanFic-Illustrator_gguf",
    "developer": "webbigdata",
    "downloads": 289,
    "createdAt": "2025-03-22T03:27:56.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "FanFic-Illustrator-Q4_K-f16",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q4_K-f16.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q4_K_L",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q4_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q5_K-f16",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q5_K-f16.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q5_K_L",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q5_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q5_K_M",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q5_K_S",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q6_K-f16",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q6_K-f16.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q6_K",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q6_K_L",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q6_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "FanFic-Illustrator-Q8_0",
        "path": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/FanFic-Illustrator-Q8_0.gguf",
        "file_size": "3.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/webbigdata/FanFic-Illustrator_gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "MythoMakiseMerged-13b",
    "developer": "Heralax",
    "downloads": 288,
    "createdAt": "2023-09-30T06:59:22Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "MythoMakiseMerged-13b-q5km",
        "path": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/MythoMakiseMerged-13b-q5km.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "ggml-model-f16",
        "path": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/ggml-model-f16.gguf",
        "file_size": "24.2 GB"
      }
    ],
    "readme": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/README.md",
    "description": "This model is a fine-tuned version of MythoMax-L2-13b, merged with 33% of MythoMax's intelligence, trained on a visual novel script revamped by GPT-4 to excel in banter, conversation, and roleplay, even surpassing its",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "starcoder2-15b-instruct-v0.1-GGUF",
    "developer": "bartowski",
    "downloads": 288,
    "createdAt": "2024-04-29T22:16:56Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ1_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ1_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_M.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_S.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_XS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_XXS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_XXS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ4_NL.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ4_XS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q2_K",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q2_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_L.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q4_K_M.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q4_K_S.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q5_K_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q5_K_S.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q6_K",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q6_K.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q8_0",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q8_0.gguf",
        "file_size": "15.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the starcoder2-15b-instruct-v0.1 model optimized for code generation tasks, with various quantization options available for different performance and memory trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "GLM-4.6-control-vectors",
    "developer": "gghfez",
    "downloads": 285,
    "createdAt": "2025-10-18T09:58:13.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "glm-4.6_character_focus__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_character_focus__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_character_focus__dialogue",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_character_focus__dialogue.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_character_focus__narration",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_character_focus__narration.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_compassion_vs_sadism__compassion",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_compassion_vs_sadism__compassion.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_compassion_vs_sadism__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_compassion_vs_sadism__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_compassion_vs_sadism__sadism",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_compassion_vs_sadism__sadism.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_empathy_vs_sociopathy__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_empathy_vs_sociopathy__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_empathy_vs_sociopathy__empathy",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_empathy_vs_sociopathy__empathy.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_empathy_vs_sociopathy__sociopathy",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_empathy_vs_sociopathy__sociopathy.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_honesty_vs_machiavellianism__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_honesty_vs_machiavellianism__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_honesty_vs_machiavellianism__honesty",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_honesty_vs_machiavellianism__honesty.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_honesty_vs_machiavellianism__machiavellianism",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_honesty_vs_machiavellianism__machiavellianism.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_humility_vs_narcissism__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_humility_vs_narcissism__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_humility_vs_narcissism__humility",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_humility_vs_narcissism__humility.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_humility_vs_narcissism__narcissism",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_humility_vs_narcissism__narcissism.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_optimism_vs_nihilism__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_optimism_vs_nihilism__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_optimism_vs_nihilism__nihilism",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_optimism_vs_nihilism__nihilism.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_optimism_vs_nihilism__optimism",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_optimism_vs_nihilism__optimism.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_storytelling__debias",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_storytelling__debias.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_storytelling__descriptive",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_storytelling__descriptive.gguf",
        "file_size": "1.8 MB"
      },
      {
        "model_id": "glm-4.6_storytelling__explicit",
        "path": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/glm-4.6_storytelling__explicit.gguf",
        "file_size": "1.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gghfez/GLM-4.6-control-vectors/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Skywork-R1V3-38B-GGUF",
    "developer": "Skywork",
    "downloads": 281,
    "createdAt": "2025-07-15T08:34:45Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Skywork-R1V3-38B-Q4_K_M",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/Skywork-R1V3-38B-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Skywork-R1V3-38B-Q8_0",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/Skywork-R1V3-38B-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/README.md",
    "description": "This repository provides a GGUF quantized version of the Skywork-R1V3-38B model for fast and memory-efficient local inference using llama.cpp.",
    "mmproj_models": [
      {
        "model_id": "mmproj-Skywork-R1V3-38B-bf16",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-bf16.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-f16",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-f16.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-q8_0",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-q8_0.gguf",
        "file_size": "5.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "codegeex4-all-9b-GGUF",
    "developer": "zai-org",
    "downloads": 272,
    "createdAt": "2024-07-13T07:46:53.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "codegeex4-all-9b-IQ2_M",
        "path": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "codegeex4-all-9b-IQ3_M",
        "path": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-IQ3_M.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "codegeex4-all-9b-Q4_K_M",
        "path": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-Q4_K_M.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "codegeex4-all-9b-Q5_K_M",
        "path": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-Q5_K_M.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "codegeex4-all-9b-Q6_K_L",
        "path": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-Q6_K_L.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "codegeex4-all-9b-Q8_0",
        "path": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-Q8_0.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/zai-org/codegeex4-all-9b-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Luth-LFM2-1.2B-GGUF",
    "developer": "kurakurai",
    "downloads": 271,
    "createdAt": "2025-08-24T08:30:54Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Luth-LFM2-1.2B-F16",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-1.2B-GGUF/resolve/main/Luth-LFM2-1.2B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Luth-LFM2-1.2B-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-1.2B-GGUF/resolve/main/Luth-LFM2-1.2B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-LFM2-1.2B-GGUF/resolve/main/README.md",
    "description": "Luth-LFM2-1.2B-GGUF is a French fine-tuned version of the LFM2 model by Kurakura AI, trained on the Luth-SFT dataset, optimized for instruction following, math, and general knowledge in French with stable English performance."
  },
  {
    "model_name": "Lily-Cybersecurity-7B-v0.2-GGUF",
    "developer": "QuantFactory",
    "downloads": 270,
    "createdAt": "2024-10-20T14:09:56Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/README.md",
    "description": "QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF is a quantized cybersecurity-focused Mistral-7B-Instruct-v0.2 model fine-tuned with 22,000 cybersecurity-related data pairs for tasks like threat analysis, security protocols,",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "amoral-gemma3-1B-v2-gguf",
    "developer": "soob3123",
    "downloads": 265,
    "createdAt": "2025-03-25T20:13:43.000Z",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "amoral-gemma3-1B-v2-F16",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-F16.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q2_K",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q2_K.gguf",
        "file_size": "657.9 MB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q3_K_M",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q3_K_M.gguf",
        "file_size": "688.9 MB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q3_K_S",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q3_K_S.gguf",
        "file_size": "656.9 MB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q4_K_M",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q4_K_M.gguf",
        "file_size": "768.7 MB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q5_K_M",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q5_K_M.gguf",
        "file_size": "811.9 MB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q6_K",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q6_K.gguf",
        "file_size": "964.9 MB"
      },
      {
        "model_id": "amoral-gemma3-1B-v2-Q8_0",
        "path": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/amoral-gemma3-1B-v2-Q8_0.gguf",
        "file_size": "1019.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/soob3123/amoral-gemma3-1B-v2-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "codellama-7b-python-ai-assistant-full-gguf",
    "developer": "pranav-pvnn",
    "downloads": 264,
    "createdAt": "2025-10-09T10:49:23.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "codellama-7b-merged-Q4_K_M",
        "path": "https://huggingface.co/pranav-pvnn/codellama-7b-python-ai-assistant-full-gguf/resolve/main/codellama-7b-merged-Q4_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "codellama-7b-merged-Q5_K_M",
        "path": "https://huggingface.co/pranav-pvnn/codellama-7b-python-ai-assistant-full-gguf/resolve/main/codellama-7b-merged-Q5_K_M.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "codellama-7b-merged-Q8_0",
        "path": "https://huggingface.co/pranav-pvnn/codellama-7b-python-ai-assistant-full-gguf/resolve/main/codellama-7b-merged-Q8_0.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "codellama-7b-merged-f16",
        "path": "https://huggingface.co/pranav-pvnn/codellama-7b-python-ai-assistant-full-gguf/resolve/main/codellama-7b-merged-f16.gguf",
        "file_size": "12.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/pranav-pvnn/codellama-7b-python-ai-assistant-full-gguf/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "BlackSheep-AFM-4.5B-Q8_0-GGUF",
    "developer": "TroyDoesAI",
    "downloads": 258,
    "createdAt": "2025-10-09T23:33:23.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "BlackSheep-MermaidThinker-4.5B_Q8",
        "path": "https://huggingface.co/TroyDoesAI/BlackSheep-AFM-4.5B-Q8_0-GGUF/resolve/main/BlackSheep-MermaidThinker-4.5B_Q8.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "Gemma3-270m-Instruct-GGUF",
    "developer": "Cactus-Compute",
    "downloads": 258,
    "createdAt": "2025-08-14T16:52:50Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-270m-it-Q8_0",
        "path": "https://huggingface.co/Cactus-Compute/Gemma3-270m-Instruct-GGUF/resolve/main/gemma-3-270m-it-Q8_0.gguf",
        "file_size": "278.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Cactus-Compute/Gemma3-270m-Instruct-GGUF/resolve/main/README.md",
    "description": "This model is licensed under the Gemma license and is tagged for use with the dashboard, cactus-text-inference, and cactus-high-performance."
  },
  {
    "model_name": "bernie0.1",
    "developer": "ivoras",
    "downloads": 236,
    "createdAt": "2025-07-27T22:05:28Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "bernie-0.1_IQ4_NL",
        "path": "https://huggingface.co/ivoras/bernie0.1/resolve/main/bernie-0.1_IQ4_NL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "bernie-0.1_f16",
        "path": "https://huggingface.co/ivoras/bernie0.1/resolve/main/bernie-0.1_f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ivoras/bernie0.1/resolve/main/README.md",
    "description": "This is a proof of concept model trained on the works of US Senator Bernie Sanders, inspired by the sci-fi concept of \"mind states\" or \"mind uploads,\" and designed to respond to questions about his policies and values.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF",
    "developer": "leeminwaan",
    "downloads": 227,
    "createdAt": "2025-09-20T14:17:38.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q2_k",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q2_k.gguf",
        "file_size": "556.6 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q3_k_l",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q3_k_l.gguf",
        "file_size": "741.7 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q3_k_m",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q3_k_m.gguf",
        "file_size": "690.9 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q3_k_s",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q3_k_s.gguf",
        "file_size": "633.4 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_0",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_0.gguf",
        "file_size": "789.6 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_1",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_1.gguf",
        "file_size": "863.1 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_k_m",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_k_m.gguf",
        "file_size": "836.5 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_k_s",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q4_k_s.gguf",
        "file_size": "794.6 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_0",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_0.gguf",
        "file_size": "936.6 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_1",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_1.gguf",
        "file_size": "1010.1 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_k_m",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_k_m.gguf",
        "file_size": "960.8 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_k_s",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q5_k_s.gguf",
        "file_size": "936.6 MB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q6_k",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q6_k.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q8_0",
        "path": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/leeminwaan_Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-q8_0.gguf",
        "file_size": "1.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/leeminwaan/Qwen3-MOE-4x0.6B-2.4B-reasoning-v1-full-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Big-Tiger-Gemma-27B-v1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 226,
    "createdAt": "2024-07-15T08:20:52Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ1_M.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ1_S.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_M.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_XS.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_XXS.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_M.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_XXS.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ4_XS.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q2_K.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q3_K_L.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q3_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q4_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q4_K_M.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q4_K_S.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q5_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q5_K_S.gguf",
        "file_size": "17.6 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q6_K.gguf",
        "file_size": "20.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the Big-Tiger-Gemma-27B-v1 model by TheDrummer, optimized for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF",
    "developer": "DavidAU",
    "downloads": 222,
    "createdAt": "2024-07-29T00:36:17.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q3_K_M",
        "path": "https://huggingface.co/DavidAU/L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q3_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q4_K_M",
        "path": "https://huggingface.co/DavidAU/L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q4_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q5_K_M",
        "path": "https://huggingface.co/DavidAU/L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q5_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q6_K",
        "path": "https://huggingface.co/DavidAU/L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q6_K.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1-exp5-11-Q8_0.gguf",
        "file_size": "8.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3-SthenoMaidBlackroot-8.9B-V1-BRAINSTORM-5x-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-20250808-233922-0-Q8_0-GGUF",
    "developer": "ParrotRouter",
    "downloads": 215,
    "createdAt": "2025-08-10T14:26:02Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-4b-instruct-2507-20250808-233922-0-q8_0",
        "path": "https://huggingface.co/ParrotRouter/Qwen3-4B-Instruct-2507-20250808-233922-0-Q8_0-GGUF/resolve/main/qwen3-4b-instruct-2507-20250808-233922-0-q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ParrotRouter/Qwen3-4B-Instruct-2507-20250808-233922-0-Q8_0-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the ParrotRouter/Qwen3-4B-Instruct-2507-20250808-233922-0 model, optimized for use with llama.cpp."
  },
  {
    "model_name": "yuna-ai-v4",
    "developer": "yukiarimo",
    "downloads": 215,
    "createdAt": "2025-02-14T18:08:49Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "yuna-ai-v4-f16",
        "path": "https://huggingface.co/yukiarimo/yuna-ai-v4/resolve/main/yuna-ai-v4-f16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "yuna-ai-v4-q3_k_m",
        "path": "https://huggingface.co/yukiarimo/yuna-ai-v4/resolve/main/yuna-ai-v4-q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "yuna-ai-v4-q4_k_m",
        "path": "https://huggingface.co/yukiarimo/yuna-ai-v4/resolve/main/yuna-ai-v4-q4_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "yuna-ai-v4-q5_k_m",
        "path": "https://huggingface.co/yukiarimo/yuna-ai-v4/resolve/main/yuna-ai-v4-q5_k_m.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "yuna-ai-v4-q6_k",
        "path": "https://huggingface.co/yukiarimo/yuna-ai-v4/resolve/main/yuna-ai-v4-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/yukiarimo/yuna-ai-v4/resolve/main/README.md",
    "description": "The Yuna Ai V4 model is a multilingual, text-generation capable language model trained on diverse datasets and optimized for conversational engagement, self-awareness, and various tasks like role-play, function-calling, and RAG, with performance metrics across different evaluation benchmarks."
  },
  {
    "model_name": "LLaMAX3-8B-Alpaca-GGUF",
    "developer": "QuantFactory",
    "downloads": 214,
    "createdAt": "2024-07-14T08:44:43Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q2_K",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_1",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_1",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q6_K",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q8_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/README.md",
    "description": "This is a multilingual, quantized version of the LLaMAX3-8B-Alpaca model, trained on 102 languages and fine-tuned for instruction-following, achieving over 5-point improvements in translation performance on the Flores-101 dataset.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-YOYO-V4-Q4_K_M-GGUF",
    "developer": "YOYO-AI",
    "downloads": 202,
    "createdAt": "2025-10-05T16:11:05.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-30b-a3b-yoyo-v4-q4_k_m",
        "path": "https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-YOYO-V4-Q4_K_M-GGUF/resolve/main/qwen3-30b-a3b-yoyo-v4-q4_k_m.gguf",
        "file_size": "17.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-YOYO-V4-Q4_K_M-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF",
    "developer": "mradermacher",
    "downloads": 196,
    "createdAt": "2025-08-06T11:38:02.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/Qwen3-30B-A3B-CoderThinking-YOYO-linear.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF variants (Q2_K to Q8_0, IQ4_XS) of YOYO-AI's Qwen3-30B-A3B-CoderThinking-YOYO-linear model."
  },
  {
    "model_name": "Layris_9B-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 187,
    "createdAt": "2024-03-06T03:26:55Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Layris_9B-F16",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-F16.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_M-imat.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_S-imat.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_XS-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_XXS-imat.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Layris_9B-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ4_XS-imat.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Layris_9B-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q4_K_M-imat.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Layris_9B-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q4_K_S-imat.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Layris_9B-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q5_K_M-imat.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Layris_9B-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q5_K_S-imat.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Layris_9B-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q6_K-imat.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Layris_9B-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q8_0-imat.gguf",
        "file_size": "8.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a GGUF-Imatrix quantized version of the Layris 9B model, merged from Eris Remix 7B and Mistral 7B-V0.1-Layla-V4, offering improved performance and quality preservation during quantization.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Seneca-Cybersecurity-LLM-Q4_K_M-GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 174,
    "createdAt": "2024-12-24T06:26:12.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "senecallm-q4_k_m",
        "path": "https://huggingface.co/AlicanKiraz0/Seneca-Cybersecurity-LLM-Q4_K_M-GGUF/resolve/main/senecallm-q4_k_m.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AlicanKiraz0/Seneca-Cybersecurity-LLM-Q4_K_M-GGUF/resolve/main/README.md",
    "description": "Cybersecurity-focused GGUF model fine-tuned for incident response, threat hunting, and malware analysis."
  },
  {
    "model_name": "Impish_Nemo_12B_ARM",
    "developer": "SicariusSicariiStuff",
    "downloads": 173,
    "createdAt": "2025-08-10T04:25:32Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Impish_Nemo_12B-Q4_0",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_ARM/resolve/main/Impish_Nemo_12B-Q4_0.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_ARM/resolve/main/README.md",
    "description": "This is a Hugging Face model repository for the Impish_Nemo_12B base model and the UBW_Tapestries dataset, licensed under Apache 2.0, quantized by SicariusSicariiStuff."
  },
  {
    "model_name": "Luth-LFM2-350M-GGUF",
    "developer": "kurakurai",
    "downloads": 166,
    "createdAt": "2025-08-24T08:18:14Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Luth-LFM2-350M-F16",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-350M-GGUF/resolve/main/Luth-LFM2-350M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "Luth-LFM2-350M-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-350M-GGUF/resolve/main/Luth-LFM2-350M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-LFM2-350M-GGUF/resolve/main/README.md",
    "description": "Luth-LFM2-350M-GGUF is a French fine-tuned version of the LFM2 model by Kurakurai AI, trained on the Luth-SFT dataset, optimized for instruction following, math, and general knowledge in French with stable English performance."
  },
  {
    "model_name": "zen-eco-4b-instruct",
    "developer": "zenlm",
    "downloads": 132,
    "createdAt": "2025-09-25T22:10:51.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gguf/zen-eco-4b-instruct-f16",
        "path": "https://huggingface.co/zenlm/zen-eco-4b-instruct/resolve/main/gguf/zen-eco-4b-instruct-f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/zenlm/zen-eco-4b-instruct/resolve/main/README.md",
    "description": "Zen Research's efficient 4B-parameter instruction-following language model for general-purpose tasks."
  },
  {
    "model_name": "GroveMoE-Inst-GGUF",
    "developer": "gabriellarson",
    "downloads": 129,
    "createdAt": "2025-09-25T23:46:42.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GroveMoE-Inst-128x4.2B-F16",
        "path": "https://huggingface.co/gabriellarson/GroveMoE-Inst-GGUF/resolve/main/GroveMoE-Inst-128x4.2B-F16.gguf",
        "file_size": "61.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/GroveMoE-Inst-GGUF/resolve/main/README.md",
    "description": "GroveMoE-Inst: 33B sparse MoE with 3.14-3.28B active parameters per token, top benchmark performance."
  },
  {
    "model_name": "best_Persian_LLM_AYA23_GGUF",
    "developer": "SmartGitiCorp",
    "downloads": 117,
    "createdAt": "2024-07-21T06:53:00.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "aya-23-8B-Q4_K_S",
        "path": "https://huggingface.co/SmartGitiCorp/best_Persian_LLM_AYA23_GGUF/resolve/main/aya-23-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SmartGitiCorp/best_Persian_LLM_AYA23_GGUF/resolve/main/README.md",
    "description": "Quantized AYA23 is the best Persian LLM for local deployment requiring 16GB RAM and 8GB VRAM."
  },
  {
    "model_name": "neutrino-instruct",
    "developer": "neuralcrew",
    "downloads": 114,
    "createdAt": "2025-09-14T02:36:13.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "neutrino-instruct",
        "path": "https://huggingface.co/neuralcrew/neutrino-instruct/resolve/main/neutrino-instruct.gguf",
        "file_size": "3.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/neuralcrew/neutrino-instruct/resolve/main/README.md",
    "description": "Neutrino‑Instruct is a 7B Apache‑2.0‑licensed, English instruction‑tuned conversational LLM in GGUF format, runnable via llama.cpp, Ollama or llama‑cpp‑python."
  },
  {
    "model_name": "HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF",
    "developer": "DevQuasar",
    "downloads": 103,
    "createdAt": "2024-11-01T01:28:38Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q2_K",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q2_K.gguf",
        "file_size": "643.3 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q3_K_M.gguf",
        "file_size": "820.3 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q4_K_M.gguf",
        "file_size": "1006.7 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q5_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q6_K",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q8_0",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the SmolLM2-1.7B-Instruct model, optimized for efficient text generation on Hugging Face.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Luth-LFM2-700M-GGUF",
    "developer": "kurakurai",
    "downloads": 102,
    "createdAt": "2025-08-24T08:24:13Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Luth-LFM2-700M-F16",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-700M-GGUF/resolve/main/Luth-LFM2-700M-F16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Luth-LFM2-700M-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-700M-GGUF/resolve/main/Luth-LFM2-700M-Q8_0.gguf",
        "file_size": "754.9 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-LFM2-700M-GGUF/resolve/main/README.md",
    "description": "Luth-LFM2-700M-GGUF is a French fine-tuned version of the LFM2 model by Kurakurai, trained on the Luth-SFT dataset, with enhanced French capabilities and stable English performance."
  },
  {
    "model_name": "MiniCPM4.1-8B-Q8_0-GGUF",
    "developer": "NikolayKozloff",
    "downloads": 99,
    "createdAt": "2025-09-08T12:20:24.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "minicpm4.1-8b-q8_0",
        "path": "https://huggingface.co/NikolayKozloff/MiniCPM4.1-8B-Q8_0-GGUF/resolve/main/minicpm4.1-8b-q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NikolayKozloff/MiniCPM4.1-8B-Q8_0-GGUF/resolve/main/README.md",
    "description": "GGUF‑converted MiniCPM4.1‑8B text‑generation model ready for use with llama.cpp (CLI or server)."
  },
  {
    "model_name": "Qwen2.5-Coder-7B-Instruct-Omni1.1",
    "developer": "TomBombadyl",
    "downloads": 91,
    "createdAt": "2025-08-15T17:55:34Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "models/gguf/isaac_sim_qwen2.5_coder_f16",
        "path": "https://huggingface.co/TomBombadyl/Qwen2.5-Coder-7B-Instruct-Omni1.1/resolve/main/models/gguf/isaac_sim_qwen2.5_coder_f16.gguf",
        "file_size": "616.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TomBombadyl/Qwen2.5-Coder-7B-Instruct-Omni1.1/resolve/main/README.md",
    "description": "This is a specialized Qwen2.5-Coder-7B-Instruct model fine-tuned for Isaac Sim 5.0 robotics tasks, including simulation, code generation, and robot control."
  },
  {
    "model_name": "Qwen3-8B-GGUF-Q2KS-AS-AutoRound",
    "developer": "Intel",
    "downloads": 77,
    "createdAt": "2025-10-20T11:07:43.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-8B-8.2B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-8B-GGUF-Q2KS-AS-AutoRound/resolve/main/Qwen3-8B-8.2B-Q2_K_S.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-8B-GGUF-Q2KS-AS-AutoRound/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "gemma-2-finance-Q4_K_M-GGUF",
    "developer": "kyungbae",
    "downloads": 75,
    "createdAt": "2024-10-02T03:35:07.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2-finance-q4_k_m",
        "path": "https://huggingface.co/kyungbae/gemma-2-finance-Q4_K_M-GGUF/resolve/main/gemma-2-finance-q4_k_m.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kyungbae/gemma-2-finance-Q4_K_M-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "gemma-3-4b-it-unslop-GRPO",
    "developer": "electroglyph",
    "downloads": 66,
    "createdAt": "2025-07-28T06:13:48Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GGUF/gemma-3-4b-it-unslop-GRPO-Q4_K_M",
        "path": "https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO/resolve/main/GGUF/gemma-3-4b-it-unslop-GRPO-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO/resolve/main/README.md",
    "description": "This is a LoRA adapter for the Gemma 3 4b model, trained to reduce slop output by incorporating frequency-based reward functions and lexical diversity constraints, with training code and dataset examples provided.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "mcp-tool-use-quality-ranger-0.6b-GGUF",
    "developer": "qualifire-oss",
    "downloads": 62,
    "createdAt": "2025-09-15T07:30:41.000Z",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.BF16",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.F16",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.F32",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.F32.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q2_K",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q2_K.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q3_K_L",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q3_K_L.gguf",
        "file_size": "351.4 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q3_K_M",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q3_K_M.gguf",
        "file_size": "331.0 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q3_K_S",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q3_K_S.gguf",
        "file_size": "308.1 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q4_0",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q4_0.gguf",
        "file_size": "363.9 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q4_1",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q4_1.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q4_K",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q4_K.gguf",
        "file_size": "378.3 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q4_K_M",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q4_K_M.gguf",
        "file_size": "378.3 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q4_K_S",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q4_K_S.gguf",
        "file_size": "365.5 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q5_0",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q5_0.gguf",
        "file_size": "416.4 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q5_1",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q5_1.gguf",
        "file_size": "442.6 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q5_K",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q5_K.gguf",
        "file_size": "423.8 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q5_K_M",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q5_K_M.gguf",
        "file_size": "423.8 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q5_K_S",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q5_K_S.gguf",
        "file_size": "416.4 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q6_K",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q6_K.gguf",
        "file_size": "472.2 MB"
      },
      {
        "model_id": "mcp-tool-use-quality-ranger-0.6b.Q8_0",
        "path": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/mcp-tool-use-quality-ranger-0.6b.Q8_0.gguf",
        "file_size": "609.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/qualifire-oss/mcp-tool-use-quality-ranger-0.6b-GGUF/resolve/main/README.md",
    "description": "A fine‑tuned Qwen3‑0.6B model that classifies tool‑call quality (VALID_CALL, TOOL_ERROR, PARAM_NAME_ERROR, PARAM_VALUE_ERROR) with 32k context support, offered in multiple GGUF quantizations."
  },
  {
    "model_name": "Indian_law_chat_minor_project",
    "developer": "varma007ut",
    "downloads": 55,
    "createdAt": "2024-07-28T08:00:49.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Indian_Law_chat",
        "path": "https://huggingface.co/varma007ut/Indian_law_chat_minor_project/resolve/main/Indian_Law_chat.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/varma007ut/Indian_law_chat_minor_project/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Journal_Club",
    "developer": "SciMaker",
    "downloads": 51,
    "createdAt": "2025-10-13T07:57:05.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GemmaPro_q4",
        "path": "https://huggingface.co/SciMaker/Journal_Club/resolve/main/GemmaPro_q4.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "security-attacks-MITRE",
    "developer": "dattaraj",
    "downloads": 51,
    "createdAt": "2024-07-19T04:30:46.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "security-attacks-MITRE",
        "path": "https://huggingface.co/dattaraj/security-attacks-MITRE/resolve/main/security-attacks-MITRE.gguf",
        "file_size": "7.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dattaraj/security-attacks-MITRE/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "SciSlide_and_GemmaPro",
    "developer": "SciMaker",
    "downloads": 37,
    "createdAt": "2025-10-02T03:12:28.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GemmaPro_q4",
        "path": "https://huggingface.co/SciMaker/SciSlide_and_GemmaPro/resolve/main/GemmaPro_q4.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF",
    "developer": "ggml-org",
    "downloads": 18,
    "createdAt": "2024-12-08T21:06:43.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen2.5-coder-1.5b-iq3_xxs-imat",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF/resolve/main/qwen2.5-coder-1.5b-iq3_xxs-imat.gguf",
        "file_size": "637.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ggml-org/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "TAIDE-LX-7B-Chat-4bit",
    "developer": "taide",
    "downloads": 18,
    "createdAt": "2024-04-15T03:28:54Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "taide-7b-a.2-q4_k_m",
        "path": "https://huggingface.co/taide/TAIDE-LX-7B-Chat-4bit/resolve/main/taide-7b-a.2-q4_k_m.gguf",
        "file_size": "3.9 GB"
      }
    ],
    "readme": "https://huggingface.co/taide/TAIDE-LX-7B-Chat-4bit/resolve/main/README.md",
    "description": "TAIDE-LX-7B-Chat 是以 LLaMA2-7B 為基礎，結合台灣繁體中文資料與文化知識，經過持續預訓練和指令微調後的大型語言模型，專注於提升繁體中文生成能力與台灣在地",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "astrollama.gguf",
    "developer": "UniverseTBD",
    "downloads": 10,
    "createdAt": "2023-10-09T04:29:42Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "astrollama",
        "path": "https://huggingface.co/UniverseTBD/astrollama.gguf/resolve/main/astrollama.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/UniverseTBD/astrollama.gguf/resolve/main/README.md",
    "description": "AstroLLaMA.gguf is a GPT-generated unified format model for astronomy and science tasks, compatible with CPU inference via Ollama.",
    "mmproj_models": [],
    "num_mmproj": 0
  }
]