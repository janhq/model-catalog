[
  {
    "model_name": "Jan-v1-4B-GGUF",
    "developer": "janhq",
    "downloads": 49737,
    "createdAt": "2025-08-11T06:21:13.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Jan-v1-4B-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-v1-4B-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-v1-4B-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-v1-4B-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/Jan-v1-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-v1-4B-GGUF/resolve/main/README.md",
    "description": "Jan-v1 is an advanced agentic language model built on the Qwen3-4B-thinking model, optimized for complex problem-solving and integration with the Jan App."
  },
  {
    "model_name": "gpt-oss-20b-GGUF",
    "developer": "ggml-org",
    "downloads": 116538,
    "createdAt": "2025-08-02T10:45:18.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gpt-oss-20b-mxfp4",
        "path": "https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-mxfp4.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "The GPT OSS 120B GGUF model can be run using `llama-server` with the command `llama-server -hf ggml-org/gpt-oss-20b-GGUF -c 0 -fa --jinja --reasoning-format none` and"
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 12383,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling comprehensive analysis of long documents and complex conversations without performance loss."
  },
  {
    "model_name": "Lucy-128k-gguf",
    "developer": "Menlo",
    "downloads": 9449,
    "createdAt": "2025-07-18T08:52:46.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "lucy_128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "lucy_128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "lucy_128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "lucy_128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "lucy_128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "lucy_128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "lucy_128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "lucy_128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "lucy_128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "lucy_128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/README.md",
    "description": "Lucy is a 1.7B mobile-optimized model for agentic web search and lightweight browsing, built on Qwen3-1.7B with machine-generated task vectors for efficient performance on CPU-only devices."
  },
  {
    "model_name": "Jan-nano-gguf",
    "developer": "Menlo",
    "downloads": 8645,
    "createdAt": "2025-06-11T07:14:33.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-4b-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-4b-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/README.md",
    "description": "Jan Nano is a compact, quantized version of the Qwen3 architecture, optimized for efficient text generation in local or embedded environments with enhanced tool use and research capabilities."
  },
  {
    "model_name": "Lucy-gguf",
    "developer": "Menlo",
    "downloads": 3299,
    "createdAt": "2025-07-18T07:04:35.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Lucy-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Lucy-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Lucy-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Lucy-Q4_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Lucy-Q4_1",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Lucy-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Lucy-Q5_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q5_1",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Lucy-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Lucy-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q6_K",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Lucy-Q8_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/README.md",
    "description": "Lucy is a 1.7B mobile-optimized model for agentic web search and lightweight browsing, built on Qwen3-1.7B with machine-generated task vectors for efficient performance on CPU-only devices.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 472833,
    "createdAt": "2024-09-25T18:35:33.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct model by Bartowski, offering various quantization types (e.g., Q4_K_M, Q5_K_S) for different performance and memory trade-offs, with a focus on compatibility with ARM and CPU"
  },
  {
    "model_name": "gemma-2b-it",
    "developer": "google",
    "downloads": 293382,
    "createdAt": "2024-02-08T13:23:59.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b-it",
        "path": "https://huggingface.co/google/gemma-2b-it/resolve/main/gemma-2b-it.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-2b-it/resolve/main/README.md",
    "description": "The Gemma 2B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, offering high performance for tasks like question answering, summarization, and reasoning, with support for CPU/GPU inference and fine"
  },
  {
    "model_name": "ComfyUI-Auto_installer",
    "developer": "UmeAiRT",
    "downloads": 245202,
    "createdAt": "2024-09-26T13:03:26.000Z",
    "tools": false,
    "num_quants": 106,
    "quants": [
      {
        "model_id": "models/clip/Qwen2.5-VL-7B-Instruct-UD-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/Qwen2.5-VL-7B-Instruct-UD-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "models/clip/t5-v1_1-xxl-encoder-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/t5-v1_1-xxl-encoder-Q3_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "models/clip/t5-v1_1-xxl-encoder-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/t5-v1_1-xxl-encoder-Q4_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "models/clip/t5-v1_1-xxl-encoder-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/t5-v1_1-xxl-encoder-Q5_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "models/clip/t5-v1_1-xxl-encoder-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/t5-v1_1-xxl-encoder-Q6_K.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "models/clip/t5-v1_1-xxl-encoder-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/t5-v1_1-xxl-encoder-Q8_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "models/clip/umt5-xxl-encoder-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/umt5-xxl-encoder-Q3_K_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "models/clip/umt5-xxl-encoder-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/umt5-xxl-encoder-Q5_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "models/clip/umt5-xxl-encoder-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/clip/umt5-xxl-encoder-Q8_0.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-canny-dev-fp16-Q4_0-GGUF",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-canny-dev-fp16-Q4_0-GGUF.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-canny-dev-fp16-Q5_0-GGUF",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-canny-dev-fp16-Q5_0-GGUF.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-canny-dev-fp16-Q8_0-GGUF",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-canny-dev-fp16-Q8_0-GGUF.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-depth-dev-fp16-Q4_0-GGUF",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-depth-dev-fp16-Q4_0-GGUF.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-depth-dev-fp16-Q5_0-GGUF",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-depth-dev-fp16-Q5_0-GGUF.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-depth-dev-fp16-Q8_0-GGUF",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-depth-dev-fp16-Q8_0-GGUF.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-dev-Q2_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-dev-Q2_K.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-dev-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-dev-Q3_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-dev-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-dev-Q4_K_S.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-dev-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-dev-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-dev-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-dev-Q6_K.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-dev-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-dev-Q8_0.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-fill-dev-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-fill-dev-Q3_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-fill-dev-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-fill-dev-Q4_K_S.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-fill-dev-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-fill-dev-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-fill-dev-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-fill-dev-Q6_K.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-fill-dev-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-fill-dev-Q8_0.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-kontext-dev-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-kontext-dev-Q4_K_S.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-kontext-dev-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-kontext-dev-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "models/unet/FLUX/flux1-kontext-dev-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/FLUX/flux1-kontext-dev-Q8_0.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "models/unet/HiDream/hidream-i1-dev-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/HiDream/hidream-i1-dev-Q4_K_S.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "models/unet/HiDream/hidream-i1-dev-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/HiDream/hidream-i1-dev-Q5_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "models/unet/HiDream/hidream-i1-dev-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/HiDream/hidream-i1-dev-Q8_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "models/unet/LTXV/ltxv-13b-0.9.7-dev-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/LTXV/ltxv-13b-0.9.7-dev-Q3_K_S.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "models/unet/LTXV/ltxv-13b-0.9.7-dev-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/LTXV/ltxv-13b-0.9.7-dev-Q5_K_M.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "models/unet/LTXV/ltxv-13b-0.9.7-dev-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/LTXV/ltxv-13b-0.9.7-dev-Q8_0.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "models/unet/QWEN/Qwen_Image_Distill-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/QWEN/Qwen_Image_Distill-Q4_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "models/unet/QWEN/Qwen_Image_Distill-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/QWEN/Qwen_Image_Distill-Q5_K_S.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "models/unet/QWEN/Qwen_Image_Distill-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/QWEN/Qwen_Image_Distill-Q8_0.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "models/unet/QWEN/Qwen_Image_Edit-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/QWEN/Qwen_Image_Edit-Q4_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "models/unet/QWEN/Qwen_Image_Edit-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/QWEN/Qwen_Image_Edit-Q5_K_S.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "models/unet/QWEN/Qwen_Image_Edit-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/QWEN/Qwen_Image_Edit-Q8_0.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.1-VACE-14B-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.1-VACE-14B-Q4_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.1-VACE-14B-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.1-VACE-14B-Q5_K_S.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.1-VACE-14B-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.1-VACE-14B-Q8_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q3_K_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q4_K_S.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q5_K_S.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q6_K.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-HighNoise-Q8_0.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q3_K_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q4_K_S.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q5_K_S.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q6_K.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control-Camera-LowNoise-Q8_0.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q4_K_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_HighNoise-Q8_0.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q4_K_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-Control_LowNoise-Q8_0.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q4_K_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_HighNoise-Q8_0.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q4_K_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-Fun-A14B-InP_LowNoise-Q8_0.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q4_K_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q5_K_S.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-HighNoise-Q8_0.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q4_K_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q5_K_S.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/Wan2.2-T2V-A14B-LowNoise-Q8_0.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-fun-14b-control-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-fun-14b-control-Q3_K_S.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-fun-14b-control-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-fun-14b-control-Q5_K_M.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-fun-14b-control-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-fun-14b-control-Q8_0.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-i2v-14b-480p-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-i2v-14b-480p-Q3_K_S.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-i2v-14b-480p-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-i2v-14b-480p-Q5_K_M.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-i2v-14b-480p-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-i2v-14b-480p-Q8_0.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-i2v-14b-720p-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-i2v-14b-720p-Q3_K_S.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-i2v-14b-720p-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-i2v-14b-720p-Q5_K_M.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-i2v-14b-720p-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-i2v-14b-720p-Q8_0.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-t2v-14b-Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-t2v-14b-Q3_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-t2v-14b-Q5_K_M",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-t2v-14b-Q5_K_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.1-t2v-14b-Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.1-t2v-14b-Q8_0.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_high_noise_14B_Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_high_noise_14B_Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_high_noise_14B_Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_high_noise_14B_Q4_K_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_high_noise_14B_Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_high_noise_14B_Q5_K_S.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_high_noise_14B_Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_high_noise_14B_Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_high_noise_14B_Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_high_noise_14B_Q8_0.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_low_noise_14B_Q3_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_low_noise_14B_Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_low_noise_14B_Q4_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_low_noise_14B_Q4_K_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_low_noise_14B_Q5_K_S",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_low_noise_14B_Q5_K_S.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_low_noise_14B_Q6_K",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_low_noise_14B_Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "models/unet/WAN/wan2.2_i2v_low_noise_14B_Q8_0",
        "path": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/models/unet/WAN/wan2.2_i2v_low_noise_14B_Q8_0.gguf",
        "file_size": "14.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/UmeAiRT/ComfyUI-Auto_installer/resolve/main/README.md",
    "description": "UmeAiRT is a ComfyUI auto installer that downloads ComfyUI, workflows, models, and custom nodes with minimal user interaction."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 238616,
    "createdAt": "2024-05-22T17:27:45.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ2_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Mistral-7B-Instruct-v0.3, quantized for efficient inference on various platforms.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "developer": "unsloth",
    "downloads": 235018,
    "createdAt": "2025-05-29T14:17:25.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "This is a large language model based on the DeepSeek-R1-0528 architecture, optimized for reasoning tasks with improved performance on benchmarks like AIME, and available in various quantized formats for efficient local inference."
  },
  {
    "model_name": "Mistral-Nemo-Instruct-2407-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 220122,
    "createdAt": "2024-07-18T14:49:08.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.fp16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/README.md",
    "description": "This GGUF format model is a quantized version of the Mistral-Nemo-Instruct-2407 model by Mistral AI, compatible with various frameworks and tools like llama.cpp, llama-cpp-python, LM Studio, and others.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 219609,
    "createdAt": "2024-04-18T16:43:25.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.fp16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized and GGUF version of the Meta Llama 3 8B Instruct model, optimized for efficient inference and compatible with llama.cpp, based on the original Meta Llama 3 model.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Phi-3.5-mini-instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 210824,
    "createdAt": "2024-08-20T20:07:57.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Phi-3.5-mini-instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ1_M.gguf",
        "file_size": "874.6 MB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ1_S.gguf",
        "file_size": "802.6 MB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ2_XS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q4_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Microsoft Phi-3.5-mini-instruct model, quantized in 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit, and available for text generation tasks."
  },
  {
    "model_name": "gemma-2b",
    "developer": "google",
    "downloads": 209610,
    "createdAt": "2024-02-08T08:11:26.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b",
        "path": "https://huggingface.co/google/gemma-2b/resolve/main/gemma-2b.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-2b/resolve/main/README.md",
    "description": "The Gemma 2B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with support for fine-tuning, multi-GPU training, and various precision optimizations, and evaluated on multiple benchmarks for performance and"
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 205482,
    "createdAt": "2025-08-07T22:20:03.000Z",
    "tools": true,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-Q5_1.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "This model is a specialized uncensored/abliterated GPT-oss 20B MOE variant with 128k context support, offering enhanced performance for creative, code generation, and problem-solving tasks, but requires specific prompts and settings to generate graphic, explicit, or unconventional content effectively"
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 202377,
    "createdAt": "2024-07-23T16:17:10.000Z",
    "tools": false,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Meta-Llama-3.1-8B-Instruct-GGUF model is a GGUF-formatted, 8B parameter, multilingual, instruction-tuned text generation model from Meta, optimized for efficiency and performance across various languages and tasks.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Phi-4-mini-instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 178112,
    "createdAt": "2025-03-01T10:43:15.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Phi-4-mini-instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.fp16.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF-quantized version of the Microsoft Phi-4-mini-instruct model, optimized for efficient text generation on various platforms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 152980,
    "createdAt": "2024-09-25T19:26:01.000Z",
    "tools": false,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ1_M.gguf",
        "file_size": "394.4 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ1_S.gguf",
        "file_size": "375.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ2_XS.gguf",
        "file_size": "453.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ3_XS.gguf",
        "file_size": "592.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ4_XS.gguf",
        "file_size": "708.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q2_K.gguf",
        "file_size": "554.0 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_L.gguf",
        "file_size": "698.6 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_M.gguf",
        "file_size": "658.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_S.gguf",
        "file_size": "612.0 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q4_K_M.gguf",
        "file_size": "770.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q4_K_S.gguf",
        "file_size": "739.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q5_K_M.gguf",
        "file_size": "869.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q5_K_S.gguf",
        "file_size": "851.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q6_K.gguf",
        "file_size": "974.5 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q8_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.fp16.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the meta-llama/Llama-3.2-1B-Instruct model, quantized in 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit, and others, suitable for text",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "openai_gpt-oss-20b-GGUF",
    "developer": "bartowski",
    "downloads": 136082,
    "createdAt": "2025-08-05T21:29:47.000Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "openai_gpt-oss-20b-IQ2_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_XXS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_XXS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_NL.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-MXFP4.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q2_K.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_L.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_0",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_1",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q6_K_L.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q8_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-bf16",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-bf16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-imatrix",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenAI GPT-oss-20b model, optimized for various inference speeds and resource constraints using llama.cpp with imatrix calibration."
  },
  {
    "model_name": "gemma-7b-it",
    "developer": "google",
    "downloads": 127038,
    "createdAt": "2024-02-13T01:07:30.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b-it",
        "path": "https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-7b-it/resolve/main/README.md",
    "description": "The Gemma 7B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, and suitable for various text generation tasks with support for fine-tuning, GPU acceleration, and ethical safety measures."
  },
  {
    "model_name": "gemma-2-2b-it-GGUF",
    "developer": "bartowski",
    "downloads": 119886,
    "createdAt": "2024-07-31T16:45:13.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gemma-2-2b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-2-2b-it model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp imatrix quantization.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Violet_Twilight-v0.2-GGUF",
    "developer": "Epiculous",
    "downloads": 98966,
    "createdAt": "2024-09-13T13:19:21.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Violet_Twilight-v0.2.IQ1_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_NL",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_L",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_1",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q6_K",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q8_0",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.bf16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.bf16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f32",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f32.gguf",
        "file_size": "45.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/README.md",
    "description": "The Violet_Twilight-v0.2 model is a SLERP merge of Azure_Dusk-v0.2 and Crimson_Dawn-v0.2, trained on ChatML with support for gguf quantization.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
    "developer": "lmstudio-community",
    "downloads": 93264,
    "createdAt": "2025-07-31T12:09:12.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model by Qwen, optimized for efficient text generation using GGUF quantization from llama.cpp."
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 91450,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B model with mixture of experts for creative writing, fiction, and roleplay, capable of generating vivid, uncensored, and genre-"
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 88130,
    "createdAt": "2025-03-12T12:32:41.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned, multimodal model with 128K context window and multilingual support, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks for reasoning, STEM, code, and"
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 76601,
    "createdAt": "2025-05-11T13:03:32.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Qwen2.5-VL-7B-Instruct model is a versatile vision-language model that supports image-text understanding, video analysis, structured output generation, and agent-like reasoning, with enhanced performance through dynamic resolution and frame rate training, efficient vision encoders, and compatibility with various visual input"
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "unsloth",
    "downloads": 71539,
    "createdAt": "2025-06-26T12:24:35.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q2_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q3_K_XL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q6_K_XL.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q8_K_XL.gguf",
        "file_size": "9.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with high performance and accuracy, available on Hugging"
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-GGUF",
    "developer": "unsloth",
    "downloads": 71144,
    "createdAt": "2025-08-06T19:21:40.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-F16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "The Qwen3-4B-Instruct-2507 model is a 4.0B parameter causal language model with significant improvements in instruction following, reasoning, multilingualism, and long-context understanding, offering superior performance and alignment compared to previous versions."
  },
  {
    "model_name": "sqlcoder-7b-2",
    "developer": "defog",
    "downloads": 70220,
    "createdAt": "2024-02-05T14:36:51.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "sqlcoder-7b-q5_k_m",
        "path": "https://huggingface.co/defog/sqlcoder-7b-2/resolve/main/sqlcoder-7b-q5_k_m.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/defog/sqlcoder-7b-2/resolve/main/README.md",
    "description": "The SQLCoder-7B-2 model is a large language model capable of generating SQL queries from natural language questions, developed by Defog, Inc., based on CodeLlama-7B, and licensed under CC-by-SA-4.0."
  },
  {
    "model_name": "gemma-3-270m-it-GGUF",
    "developer": "unsloth",
    "downloads": 63741,
    "createdAt": "2025-08-13T00:40:53.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3-270m-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-F16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-IQ4_NL.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-IQ4_XS.gguf",
        "file_size": "229.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q2_K.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q2_K_L.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q3_K_M.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q3_K_S.gguf",
        "file_size": "225.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_0.gguf",
        "file_size": "230.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_1.gguf",
        "file_size": "236.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_K_M.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_K_S.gguf",
        "file_size": "238.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q5_K_M.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q5_K_S.gguf",
        "file_size": "246.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q6_K.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q8_0.gguf",
        "file_size": "278.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-IQ2_M.gguf",
        "file_size": "174.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-IQ2_XXS.gguf",
        "file_size": "171.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-IQ3_XXS.gguf",
        "file_size": "176.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q2_K_XL.gguf",
        "file_size": "226.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q3_K_XL.gguf",
        "file_size": "231.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q4_K_XL.gguf",
        "file_size": "242.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q5_K_XL.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q6_K_XL.gguf",
        "file_size": "272.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q8_K_XL.gguf",
        "file_size": "449.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 270M model for text generation, offering improved performance and efficiency with support for multimodal tasks, multilingual capabilities, and various training formats like GGUF, 4-bit, and 16-bit."
  },
  {
    "model_name": "Devstral-Small-2507-GGUF",
    "developer": "unsloth",
    "downloads": 57519,
    "createdAt": "2025-07-10T13:20:40.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Devstral-Small-2507-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/README.md",
    "description": "This model is a lightweight, open-source agentic LLM for software engineering tasks, fine-tuned from Mistral-Small-3.1 with a 128k context window and Apache 2.0 license, supporting tool calling and optional vision, and optimized for local deployment with various"
  },
  {
    "model_name": "Phi-3-mini-4k-instruct-gguf",
    "developer": "microsoft",
    "downloads": 54483,
    "createdAt": "2024-04-22T17:02:08.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Phi-3-mini-4k-instruct-fp16",
        "path": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Phi-3-mini-4k-instruct-q4",
        "path": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/README.md",
    "description": "This repo provides the GGUF format for the Phi-3-Mini-4K-Instruct model, a 3.8B parameter, lightweight, state-of-the-art open model trained on high-quality and reasoning dense data, suitable for commercial and research use in English with chat format prompts."
  },
  {
    "model_name": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 53217,
    "createdAt": "2025-08-07T03:36:06.000Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ2_XXS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ3_XXS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_NL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_NL.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_L.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_1",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q6_K_L.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q8_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-bf16",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-bf16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-imatrix",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gpt-oss-20b-BF16-abliterated model by huihui-ai, optimized for text generation using llama.cpp with various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated",
    "developer": "huihui-ai",
    "downloads": 51787,
    "createdAt": "2025-08-06T15:29:56.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GGUF/ggml-model-Q3_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GGUF/ggml-model-Q4_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GGUF/ggml-model-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-f16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "GGUF/ggml-model-q8_0",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/README.md",
    "description": "This is an uncensored version of the unsloth/gpt-oss-20b-BF16 model, optimized for GGUF format and suitable for research or experimental use with potential risks of generating sensitive or inappropriate content."
  },
  {
    "model_name": "Magistral-Small-2506-GGUF",
    "developer": "unsloth",
    "downloads": 51146,
    "createdAt": "2025-06-10T07:31:42.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Magistral-Small-2506-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "The Magistral-Small-2506 model is a 24B parameter, multilingual reasoning model built on Mistral Small 3.1, offering efficient performance with Apache 2.0 license and supporting deployment via frameworks like llama.cpp, vLLM, and Oll",
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "gemma-7b",
    "developer": "google",
    "downloads": 50264,
    "createdAt": "2024-02-08T22:36:43.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b",
        "path": "https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-7b/resolve/main/README.md",
    "description": "The Gemma 7B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with strong performance on benchmark tasks and ethical safety considerations."
  },
  {
    "model_name": "Qwen3-4B-GGUF",
    "developer": "unsloth",
    "downloads": 47975,
    "createdAt": "2025-04-28T07:55:09.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B is a 4.0B parameter large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes, and is available for fine-tuning and deployment via Hugging Face and"
  },
  {
    "model_name": "gemma-3-12b-it-GGUF",
    "developer": "unsloth",
    "downloads": 47517,
    "createdAt": "2025-03-12T10:34:12.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 12B model, optimized for performance and memory efficiency with support for GGUF, 4-bit, and 16-bit formats, and available on Hugging Face."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-GGUF",
    "developer": "lmstudio-community",
    "downloads": 47343,
    "createdAt": "2025-08-06T15:20:18.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Qwen3-4B-Thinking-2507 model by Qwen, optimized for efficient text generation on LM Studio."
  },
  {
    "model_name": "gemma-3n-E4B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 47100,
    "createdAt": "2025-06-26T15:10:43.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E4B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 44741,
    "createdAt": "2025-03-12T09:04:23.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 model using Unsloth, offering efficient training and export options to GGUF, Ollama, llama.cpp, or Hugging Face."
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 41712,
    "createdAt": "2024-09-17T13:57:52.000Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-1.5b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-fp16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q2_k.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q3_k_m.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf",
        "file_size": "1016.8 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q6_k.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the instruction-tuned 1.5B Qwen2.5 model in GGUF format, offering improved knowledge, coding/math capabilities, long-context support, multilingual support, and various quantization options.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "unsloth",
    "downloads": 41441,
    "createdAt": "2025-06-20T22:27:21.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized Mistral-3.2 Small 24B model optimized for instruction following, function calling, and vision reasoning, available via vLLM or Transformers with a system prompt for enhanced performance."
  },
  {
    "model_name": "SmallThinker-21BA3B-Instruct-GGUF",
    "developer": "PowerInfer",
    "downloads": 38261,
    "createdAt": "2025-07-27T16:10:02.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_K_S.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.F16",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.F16.gguf",
        "file_size": "40.1 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_NL",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_NL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K.imatrix.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K_S.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K_S.imatrix.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.imatrix.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0.powerinfer",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.powerinfer.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_1",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_1.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_1.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_1.imatrix.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K.imatrix.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K_S.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K_S.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K_S.imatrix.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q5_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q5_K.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q5_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q5_K.imatrix.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q6_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q6_K.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q6_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q6_K.imatrix.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q8_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q8_0.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q8_0.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q8_0.imatrix.gguf",
        "file_size": "21.3 GB"
      }
    ],
    "readme": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/README.md",
    "description": "SmallThinker-21BA3B-Instruct-GGUF is a low-latency, on-device MoE language model with 21B parameters, 3B activated parameters, and 52 layers, designed for efficient local deployment using either llama.cpp or PowerInfer frameworks.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-GGUF",
    "developer": "lmstudio-community",
    "downloads": 38137,
    "createdAt": "2025-07-28T09:05:17.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Qwen3-30B-A3B-Instruct-2507 model by Qwen, suitable for text generation tasks.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "SmolVLM-500M-Instruct-GGUF",
    "developer": "ggml-org",
    "downloads": 37763,
    "createdAt": "2025-04-21T19:02:08.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "SmolVLM-500M-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-Q8_0.gguf",
        "file_size": "416.6 MB"
      },
      {
        "model_id": "SmolVLM-500M-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf",
        "file_size": "782.4 MB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a 500 million parameter instruction-tuned version of SmolVLM, based on the HuggingFaceTB/SmolVLM-500M-Instruct model.",
    "mmproj_models": [
      {
        "model_id": "mmproj-SmolVLM-500M-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf",
        "file_size": "103.7 MB"
      },
      {
        "model_id": "mmproj-SmolVLM-500M-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf",
        "file_size": "190.2 MB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "Gemmasutra-Mini-2B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 36558,
    "createdAt": "2024-08-03T09:55:36.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "Gemmasutra-Mini-2B-v1-BF16",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-BF16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_L",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_4_4",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_4_4.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_4_8",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_4_8.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_8_8",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_8_8.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-f16",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-f16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-f32",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/README.md",
    "description": "The BeaverAI team presents Gemmasutra Mini 2B v1, a compact 2B parameter RP model designed for efficient use across various devices, offering a satisfying conversational experience with 4K context support and multiple quantized versions for different platforms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-0.6B-GGUF",
    "developer": "unsloth",
    "downloads": 33936,
    "createdAt": "2025-04-28T10:24:13.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-0.6B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-0.6B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-IQ4_NL.gguf",
        "file_size": "363.9 MB"
      },
      {
        "model_id": "Qwen3-0.6B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-IQ4_XS.gguf",
        "file_size": "350.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K_L.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q3_K_M.gguf",
        "file_size": "331.0 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q3_K_S.gguf",
        "file_size": "308.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_0.gguf",
        "file_size": "364.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_1.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_M.gguf",
        "file_size": "378.3 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_S.gguf",
        "file_size": "365.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_M.gguf",
        "file_size": "423.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_S.gguf",
        "file_size": "416.4 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q6_K.gguf",
        "file_size": "472.2 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf",
        "file_size": "609.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ1_M.gguf",
        "file_size": "210.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ1_S.gguf",
        "file_size": "204.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ2_M.gguf",
        "file_size": "256.3 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ2_XXS.gguf",
        "file_size": "223.2 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ3_XXS.gguf",
        "file_size": "269.0 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q2_K_XL.gguf",
        "file_size": "287.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q3_K_XL.gguf",
        "file_size": "340.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q4_K_XL.gguf",
        "file_size": "386.6 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q5_K_XL.gguf",
        "file_size": "425.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q6_K_XL.gguf",
        "file_size": "549.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q8_K_XL.gguf",
        "file_size": "805.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/README.md",
    "description": "Qwen3-0.6B is a 0.6B parameter large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes."
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "unsloth",
    "downloads": 33473,
    "createdAt": "2025-06-26T12:24:52.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-F16.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ3_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q2_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q4_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q5_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q6_K_XL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q8_K_XL.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with instruction-tuned variants available for fine-tuning",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-1.7B-GGUF",
    "developer": "unsloth",
    "downloads": 33020,
    "createdAt": "2025-04-28T12:22:37.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-1.7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K_L.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_M.gguf",
        "file_size": "535.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_S.gguf",
        "file_size": "512.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_M.gguf",
        "file_size": "675.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_XXS.gguf",
        "file_size": "577.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ3_XXS.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q2_K_XL.gguf",
        "file_size": "760.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q3_K_XL.gguf",
        "file_size": "924.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q5_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q6_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q8_K_XL.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/README.md",
    "description": "Qwen3-1.7B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes for optimal performance."
  },
  {
    "model_name": "Devstral-Small-2505-GGUF",
    "developer": "unsloth",
    "downloads": 32437,
    "createdAt": "2025-05-21T14:20:05.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Devstral-Small-2505-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "GLM-4.1V-9B-Thinking-GGUF",
    "developer": "unsloth",
    "downloads": 31900,
    "createdAt": "2025-07-25T11:47:05.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "GLM-4.1V-9B-Thinking-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-BF16.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q2_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q3_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q3_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q5_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q6_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q8_0.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ1_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ3_XXS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q2_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q3_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q4_K_XL.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q5_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q6_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q8_K_XL.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/README.md",
    "description": "This is a 9B-parameter vision-language model (GLM-4.1V-9B-Thinking) that enhances reasoning capabilities through a thinking paradigm and reinforcement learning, achieving state-of-the-art performance on 18 benchmark tasks."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Llama-8B-GGUF",
    "developer": "unsloth",
    "downloads": 30111,
    "createdAt": "2025-01-20T13:04:25.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-F16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q2_K_L.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q3_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q6_K_XL.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q8_K_XL.gguf",
        "file_size": "9.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/README.md",
    "description": "The DeepSeek-R1 model, developed by DeepSeek-AI, is a reasoning model based on Llama3.1 and Qwen2.5, offering improved performance on various benchmarks and available for local use with tools like llama.cpp and vLLM, with detailed instructions for training and deployment"
  },
  {
    "model_name": "MN-12B-Mag-Mell-R1-GGUF",
    "developer": "mradermacher",
    "downloads": 29530,
    "createdAt": "2024-09-16T19:10:02.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q8_0",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-8B-GGUF",
    "developer": "Qwen",
    "downloads": 29183,
    "createdAt": "2025-05-03T06:33:59.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "Qwen3-8B-GGUF is a large language model with 8.2B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, multilingual support, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
    "developer": "gabriellarson",
    "downloads": 27557,
    "createdAt": "2025-08-07T06:30:31.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Huihui-gpt-oss-20B-abliterated-F16",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20B-abliterated-F16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the unsloth/gpt-oss-20b-BF16 model, created with abliteration, designed for research and experimental use with potential risks of generating sensitive or inappropriate content."
  },
  {
    "model_name": "gemma-3-1b-it-GGUF",
    "developer": "unsloth",
    "downloads": 27320,
    "createdAt": "2025-03-12T10:57:04.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-BF16.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3-1b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-IQ4_NL.gguf",
        "file_size": "688.4 MB"
      },
      {
        "model_id": "gemma-3-1b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-IQ4_XS.gguf",
        "file_size": "681.3 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q2_K.gguf",
        "file_size": "657.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q2_K_L.gguf",
        "file_size": "657.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q3_K_M.gguf",
        "file_size": "688.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q3_K_S.gguf",
        "file_size": "656.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_0.gguf",
        "file_size": "688.5 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_1.gguf",
        "file_size": "728.6 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf",
        "file_size": "768.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_S.gguf",
        "file_size": "744.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q5_K_M.gguf",
        "file_size": "811.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q5_K_S.gguf",
        "file_size": "797.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q6_K.gguf",
        "file_size": "964.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q8_0.gguf",
        "file_size": "1019.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ1_M.gguf",
        "file_size": "533.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ1_S.gguf",
        "file_size": "531.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ2_M.gguf",
        "file_size": "551.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ2_XXS.gguf",
        "file_size": "538.3 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ3_XXS.gguf",
        "file_size": "564.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q2_K_XL.gguf",
        "file_size": "661.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q3_K_XL.gguf",
        "file_size": "693.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q4_K_XL.gguf",
        "file_size": "769.6 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q5_K_XL.gguf",
        "file_size": "833.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q6_K_XL.gguf",
        "file_size": "983.3 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q8_K_XL.gguf",
        "file_size": "1.4 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "bartowski",
    "downloads": 27315,
    "createdAt": "2025-06-20T19:03:22.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistral-Small-3.2-24B-Instruct-2506 model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "tools": true,
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 27132,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored Gemma 3 27B IT model created using a layerwise abliteration technique to achieve high acceptance rates while preserving model capabilities."
  },
  {
    "model_name": "Qwen3-14B-GGUF",
    "developer": "unsloth",
    "downloads": 25215,
    "createdAt": "2025-04-28T10:01:12.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-14B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-BF16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Qwen3-14B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q2_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Qwen3-14B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen3-14B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ1_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ1_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ2_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ2_XXS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ3_XXS.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q2_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q3_K_XL.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q4_K_XL.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q5_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q6_K_XL.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q8_K_XL.gguf",
        "file_size": "17.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/README.md",
    "description": "Qwen3-14B is a large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes, and it can be fine-tuned or deployed using Hugging Face Transformers and various inference frameworks."
  },
  {
    "model_name": "NemoMix-Unleashed-12B-GGUF",
    "developer": "bartowski",
    "downloads": 25144,
    "createdAt": "2024-08-22T04:03:49.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "NemoMix-Unleashed-12B-IQ2_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ3_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q2_K",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q2_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_XL.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q6_K",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q8_0",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-f16",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-f16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the NemoMix-Unleashed-12B model for efficient text generation on various platforms, with recommendations based on system resources and performance needs."
  },
  {
    "model_name": "gemma-3-12b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 24978,
    "createdAt": "2025-05-28T17:31:37.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q2_k.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q6_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/gemma-3-12b-it-abliterated-v2.q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored, enhanced version of the Gemma 3 12B IT model with improved refusal handling and quantized GGUF weights."
  },
  {
    "model_name": "LFM2-1.2B-GGUF",
    "developer": "LiquidAI",
    "downloads": 24738,
    "createdAt": "2025-07-12T12:01:44.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-1.2B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-1.2B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/README.md",
    "description": "LFM2-1.2B-GGUF is a hybrid model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-GGUF",
    "developer": "dphn",
    "downloads": 24720,
    "createdAt": "2025-01-02T22:11:05.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-F16",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q2_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_L",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q6_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q8_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/README.md",
    "description": "Dolphin 3.0 Llama 3.1 8B is a general-purpose instruct-tuned model designed for coding, math, agentic, and function calling tasks, offering full control to the user through customizable system prompts and data ownership."
  },
  {
    "model_name": "Ministral-8B-Instruct-2410-GGUF",
    "developer": "bartowski",
    "downloads": 24364,
    "createdAt": "2024-10-21T16:27:21.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ2_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K_L.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q8_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-f16",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-f16.gguf",
        "file_size": "14.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Mistral-8B-Instruct-2410 model for various inference platforms, with a focus on research use under the Mistral AI Research License, requiring users to agree to specific terms and conditions for non-commercial purposes.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "medgemma-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 23406,
    "createdAt": "2025-05-20T19:18:08.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "medgemma-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/README.md",
    "description": "MedGemma is a medical multimodal model trained on diverse medical data, optimized for healthcare applications involving text and images, with variants available for radiology, dermatology, pathology, and more.",
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF",
    "developer": "DavidAU",
    "downloads": 23047,
    "createdAt": "2025-02-12T00:29:33.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-IQ4_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q2_k.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_l.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_m.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_s.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_m.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_s.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q5_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q6_k.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q8_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-q5_k_m.gguf",
        "file_size": "13.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF model, a powerful mixture of 8 top L3.2 4B models combined into a 21"
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
    "developer": "unsloth",
    "downloads": 21837,
    "createdAt": "2025-01-20T13:47:45.000Z",
    "tools": true,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-BF16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf",
        "file_size": "770.2 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_M.gguf",
        "file_size": "683.1 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_S.gguf",
        "file_size": "667.7 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_M.gguf",
        "file_size": "755.5 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_XXS.gguf",
        "file_size": "739.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ3_XXS.gguf",
        "file_size": "841.2 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ4_XS.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q2_K_XL.gguf",
        "file_size": "844.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q3_K_XL.gguf",
        "file_size": "923.4 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/README.md",
    "description": "This model is a 1.5B parameter version of DeepSeek-R1, fine-tuned for faster inference with 70% less memory using Unsloth, and can be run with llama.cpp for efficient performance.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 21814,
    "createdAt": "2025-08-10T01:10:17.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEO-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEO-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEO-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEO-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEO-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEO-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOCODE-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOCODE-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOCODE-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOCODE-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOCODE-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOCODE-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRR-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRR-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRR-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRR-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRRCODE-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRRCODE-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-NEOHRRCODE-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-NEOHRRCODE-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRR-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRR-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRR-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRR-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-MAO2-uncensored-OT-NEOHRRCODE-Q5_1.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-MODERATE-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "This model is a specialized, moderate uncensored version of the OpenAI 20B MOE (Mixture of Experts) GPT-oss model, optimized for creative tasks with support for 128k context, multiple experts, and various quantization types like IQ4_NL,"
  },
  {
    "model_name": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF",
    "developer": "QuantFactory",
    "downloads": 21683,
    "createdAt": "2024-07-28T07:02:48.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/README.md",
    "description": "The QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF is a quantized, uncensored, and multilingual version of the Meta Llama 3.1 8B model, optimized for roleplay, instruction"
  },
  {
    "model_name": "TheDrummer_Cydonia-24B-v4.1-GGUF",
    "developer": "bartowski",
    "downloads": 21663,
    "createdAt": "2025-08-18T16:31:24.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4.1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4.1-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Cydonia-24B-v4.1 model using llama.cpp's imatrix method, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 21571,
    "createdAt": "2025-01-21T20:59:59.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Uncensored.i1-Q6_K.gguf",
        "file_size": "5.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the DeepSeek-R1-Distill-Qwen-7B-Uncensored model, offering various GGUF quantization options for different quality and performance trade-offs."
  },
  {
    "model_name": "L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 20680,
    "createdAt": "2024-06-05T18:21:00.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "test",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/test.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a quantized version of the Sao10K/L3-8B-Stheno-v3.2 model for roleplay and sillytavern tasks, optimized for performance with specific quantization settings and recommended samplers."
  },
  {
    "model_name": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF",
    "developer": "bartowski",
    "downloads": 19048,
    "createdAt": "2025-05-09T15:21:21.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dolphin-Mistral-24B-Venice-Edition model by cognitivecomputations, optimized for various inference speeds and quality levels using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "MiniCPM-V-4-gguf",
    "developer": "openbmb",
    "downloads": 18949,
    "createdAt": "2025-07-12T09:45:29.000Z",
    "tools": false,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_1.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-iOS",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16-iOS.gguf",
        "file_size": "128.8 MB"
      },
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16.gguf",
        "file_size": "914.4 MB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/README.md",
    "description": "The MiniCPM-V 4.0 is a high-performance, efficient, and versatile multimodal large language model capable of understanding and generating text from single images, multiple images, and videos on mobile devices, with 4.1B parameters and superior performance compared to other models in various benchmarks."
  },
  {
    "model_name": "oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF",
    "developer": "mradermacher",
    "downloads": 18269,
    "createdAt": "2025-01-15T09:26:16.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q2_K",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q6_K",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q8_0",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.f16",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Nous-Hermes-2-Mistral-7B-DPO-GGUF",
    "developer": "NousResearch",
    "downloads": 18070,
    "createdAt": "2024-02-20T06:25:05.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q2_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q6_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q8_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/README.md",
    "description": "Nous-Hermes 2 Mistral 7B DPO is a 7B parameter model fine-tuned with DPO from Teknium/OpenHermes-2.5-Mistral-7B, showing improved performance on AGIEval, BigBench, GPT4"
  },
  {
    "model_name": "Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF",
    "developer": "DavidAU",
    "downloads": 17434,
    "createdAt": "2024-10-25T00:19:23.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a highly advanced, uncensored Gemma2 model designed for creative writing, particularly fiction and storytelling, combining four top storytelling models with additional modifications for vivid, detailed, and emotionally engaging prose across various genres, including science fiction, horror, and romance, with options for different quantization levels and"
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 16937,
    "createdAt": "2025-08-02T05:35:18.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Instruct-2507 model, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "Qwen",
    "downloads": 16301,
    "createdAt": "2025-05-05T08:38:52.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "Qwen3-30B-A3B-GGUF is a large language model with 30.5B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning, multilingual capabilities, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "Jan-nano-128k-GGUF",
    "developer": "unsloth",
    "downloads": 14760,
    "createdAt": "2025-06-25T07:31:40.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-128k-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling efficient processing of long documents and complex multi-turn conversations without performance degradation.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF",
    "developer": "mradermacher",
    "downloads": 14618,
    "createdAt": "2025-07-23T06:06:13.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/README.md",
    "description": "This is a multi-modal visual language model (VLm) for image captioning and text generation, quantized in various formats including GGUF, available in English, Chinese, and Thai."
  },
  {
    "model_name": "MN-Violet-Lotus-12B-GGUF",
    "developer": "mradermacher",
    "downloads": 14466,
    "createdAt": "2024-11-17T05:53:19.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "MN-Violet-Lotus-12B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_0_4_4",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_0_4_4.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q8_0",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the MN-Violet-Lotus-12B model, optimized for storywriting, text adventure, and creative writing tasks, available in various quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "Qwen2.5-Omni-7B-GGUF",
    "developer": "unsloth",
    "downloads": 14112,
    "createdAt": "2025-05-28T19:15:45.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen2.5-Omni-7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "4.9 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/README.md",
    "description": "Qwen2.5-Omni is a state-of-the-art end-to-end multimodal model that excels in text, image, audio, and video understanding and generation, outperforming other models in various benchmarks and tasks, with support for audio output and customizable voice types."
  },
  {
    "model_name": "Sakura-GalTransl-7B-v3.7",
    "developer": "SakuraLLM",
    "downloads": 13915,
    "createdAt": "2024-05-22T03:04:31.000Z",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "GalTransl-7B-v1-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v1-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "GalTransl-7B-v1.5-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v1.5-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "GalTransl-7B-v2-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v2-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "GalTransl-7B-v2.6-Q6_K",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/GalTransl-7B-v2.6-Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Sakura-GalTransl-7B-v3-Q5_K_S",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-GalTransl-7B-v3-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Sakura-GalTransl-7B-v3.5-Q5_K_S",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-GalTransl-7B-v3.5-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Sakura-Galtransl-7B-v3.7-IQ4_XS",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-Galtransl-7B-v3.7-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Sakura-Galtransl-7B-v3.7",
        "path": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/Sakura-Galtransl-7B-v3.7.gguf",
        "file_size": "5.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SakuraLLM/Sakura-GalTransl-7B-v3.7/resolve/main/README.md",
    "description": "Sakura-GalTransl模型由sakuraumi和xd2333共同构建，专为视觉小说翻译优化，支持日文到简体中文的翻译，并适配多种翻译工具和部署方式。"
  },
  {
    "model_name": "SmolLM3-3B-GGUF",
    "developer": "ggml-org",
    "downloads": 13891,
    "createdAt": "2025-07-08T12:36:39.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "SmolLM3-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SmolLM3-f16",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-f16.gguf",
        "file_size": "5.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/README.md",
    "description": "SmolLM3-GGUF is a 3B parameter multilingual language model with advanced reasoning, long context support, and open-source licensing, offering strong performance across various benchmarks and supported by frameworks like transformers and llama.cpp.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 13583,
    "createdAt": "2024-10-26T00:23:07.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ3_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q2_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q2_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_4_4.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_4_8.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_8_8.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q8_0.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-f16.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct-uncensored model using llama.cpp's imatrix method, optimized for various hardware platforms with different quality and performance trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-270m-it-qat-GGUF",
    "developer": "unsloth",
    "downloads": 13581,
    "createdAt": "2025-08-14T11:08:31.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3-270m-it-qat-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-F16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-IQ4_NL.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-IQ4_XS.gguf",
        "file_size": "229.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q2_K.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q2_K_L.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q3_K_M.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q3_K_S.gguf",
        "file_size": "225.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_0.gguf",
        "file_size": "380.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_1.gguf",
        "file_size": "236.2 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_K_M.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q4_K_S.gguf",
        "file_size": "238.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q5_K_M.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q5_K_S.gguf",
        "file_size": "246.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q6_K.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-Q8_0.gguf",
        "file_size": "428.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-IQ2_M.gguf",
        "file_size": "174.3 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-IQ2_XXS.gguf",
        "file_size": "171.8 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-IQ3_XXS.gguf",
        "file_size": "176.0 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q2_K_XL.gguf",
        "file_size": "226.7 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q3_K_XL.gguf",
        "file_size": "231.4 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q4_K_XL.gguf",
        "file_size": "242.5 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q5_K_XL.gguf",
        "file_size": "248.1 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q6_K_XL.gguf",
        "file_size": "272.9 MB"
      },
      {
        "model_id": "gemma-3-270m-it-qat-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/gemma-3-270m-it-qat-UD-Q8_K_XL.gguf",
        "file_size": "449.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Gemma 3 (270M) model by Unsloth, optimized for text generation with specific settings and available in a collection of various Gemma 3 versions including GGUF, 4-bit, and 16-bit formats."
  },
  {
    "model_name": "osmosis-mcp-4b",
    "developer": "osmosis-ai",
    "downloads": 13296,
    "createdAt": "2025-05-08T18:46:48.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "osmosis-mcp-4B-BF16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-BF16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-F16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/README.md",
    "description": "Osmosis-MCP-4B is a Qwen3-4B model fine-tuned with reinforcement learning to excel at multi-step tool usage in a curriculum-based training approach, offering a practical, open-source solution for MCP-style agents.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "SmallThinker-4BA0.6B-Instruct-GGUF",
    "developer": "PowerInfer",
    "downloads": 13010,
    "createdAt": "2025-07-27T16:09:24.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.F16",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.F16.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.IQ4_NL",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.IQ4_NL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.IQ4_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q3_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q3_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_0.powerinfer",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_0.powerinfer.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_1",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q5_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q5_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q6_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q6_K.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q8_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q8_0.gguf",
        "file_size": "4.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/README.md",
    "description": "SmallThinker-4BA0.6B-Instruct-GGUF is a low-latency, on-device Mixture-of-Experts language model designed for efficient local deployment with support for both llama.cpp and PowerInfer frameworks."
  },
  {
    "model_name": "Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF",
    "developer": "bartowski",
    "downloads": 12567,
    "createdAt": "2024-09-26T14:36:19.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_4.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_8.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_8_8.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K_L.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q8_0",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q8_0.gguf",
        "file_size": "22.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-f16",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-f16.gguf",
        "file_size": "41.4 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the Mistral-Small-22B-ArliAI-RPMax-v1.1 model for text generation, with options ranging from high-quality Q8_0 to low-quality IQ2_S, optimized for different hardware and performance needs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "TheDrummer_Cydonia-24B-v4-GGUF",
    "developer": "bartowski",
    "downloads": 12454,
    "createdAt": "2025-07-18T17:30:25.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Cydonia-24B-v4 model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-24B-v4.1-GGUF",
    "developer": "TheDrummer",
    "downloads": 12395,
    "createdAt": "2025-08-17T10:23:58.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4j-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4j-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/Cydonia-24B-v4j-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF/resolve/main/README.md",
    "description": "Cydonia 24B v4.1 is a high-quality, creative, and well-written large language model that has surpassed previous versions in performance and prose capabilities."
  },
  {
    "model_name": "Qwen2.5-Coder-32B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 12273,
    "createdAt": "2024-11-06T19:20:14.000Z",
    "tools": true,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_4.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_8_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen2.5-Coder-32B-Instruct model using llama.cpp's imatrix method, optimized for various hardware platforms and inference speeds, with recommended options marked for best performance and quality.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "claude-3.7-sonnet-reasoning-gemma3-12B",
    "developer": "reedmayhew",
    "downloads": 12190,
    "createdAt": "2025-03-22T19:40:09.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0",
        "path": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/README.md",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL."
  },
  {
    "model_name": "google_gemma-3n-E4B-it-GGUF",
    "developer": "bartowski",
    "downloads": 12158,
    "createdAt": "2025-06-26T19:50:49.000Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 12134,
    "createdAt": "2025-08-08T13:20:46.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ1_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_M.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-IQ4_XS.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q2_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_L.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_1.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.i1-Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated.imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-gpt-oss-20b-BF16-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gpt-oss-20b-BF16 model, offering various quantization types for efficient deployment on different hardware platforms."
  },
  {
    "model_name": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF",
    "developer": "bartowski",
    "downloads": 11895,
    "createdAt": "2025-05-23T18:09:44.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the PocketDoc/Dans-PersonalityEngine-V1.3.0-24b model for efficient text generation on various hardware, including CPUs and GPUs, with options for different quantization levels and formats like Q6_K_L, Q4_K_M,",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 11862,
    "createdAt": "2025-07-10T15:26:23.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_NL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_1",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-bf16",
        "path": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-bf16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/huihui-ai_Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Huihui-gemma-3n-E4B-it-abliterated model using llama.cpp, with options ranging from high-quality Q8_0 to low-quality Q2_K, suitable for various hardware and performance requirements."
  },
  {
    "model_name": "stable-code-3b",
    "developer": "stabilityai",
    "downloads": 11768,
    "createdAt": "2024-01-09T02:03:58.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "stable-code-3b-Q5_K_M",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q5_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "stable-code-3b-Q6_K",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q6_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "stable-code-3b",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b.gguf",
        "file_size": "5.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/README.md",
    "description": "Stable Code 3B is a 3B parameter decoder-only language model pre-trained on diverse code and text datasets, excelling in code generation across multiple programming languages with state-of-the-art performance on MultiPL-E metrics."
  },
  {
    "model_name": "Magistral-Small-2507-GGUF",
    "developer": "unsloth",
    "downloads": 11751,
    "createdAt": "2025-07-24T21:29:04.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Magistral-Small-2507-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/README.md",
    "description": "Magistral Small 1.1 is a multilingual, efficient reasoning model with 24B parameters, supporting dozens of languages and offering SOTA performance in model quantization, optimized for deployment on a single RTX 4090 or 32GB RAM MacBook.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-24B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 11660,
    "createdAt": "2025-07-01T19:33:48.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4h-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/README.md",
    "description": "Cydonia 24B v4 is a wordy and thick model with a novel style, excelling at long-form storytelling and descriptive tasks, and is dedicated to the memory of SleepDeprived, a beloved community member.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3-8B-Lexi-Uncensored-GGUF",
    "developer": "Orenguteng",
    "downloads": 10876,
    "createdAt": "2024-04-23T21:57:52.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_F16",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q4_K_M",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q5_K_M",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q8_0",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/README.md",
    "description": "This model is based on Llama-3-8b-Instruct and is uncensored, governed by Meta's Llama-3 Community License Agreement, requiring responsible use and alignment layer implementation for compliance."
  },
  {
    "model_name": "gemma-3-12b-it-qat-GGUF",
    "developer": "unsloth",
    "downloads": 10763,
    "createdAt": "2025-04-21T03:55:27.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-qat-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/README.md",
    "description": "This repository contains the unquantized 12B instruction-tuned Gemma 3 model from Google, suitable for text and image generation tasks with a 128K context window and open weights, but requires acknowledgment of Google's usage license for access.",
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF",
    "developer": "DavidAU",
    "downloads": 10721,
    "createdAt": "2024-10-28T10:09:51.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m.gguf",
        "file_size": "5.0 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/README.md",
    "description": "This is a modified Llama 3.2 model designed for creative writing, with enhanced prose generation, uncensored content, and support for multiple genres including science fiction, horror, and romance, offering vivid, detailed, and emotionally charged outputs through advanced parameters and templates.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Nanonets-OCR-s-GGUF",
    "developer": "unsloth",
    "downloads": 10568,
    "createdAt": "2025-06-16T10:35:11.000Z",
    "tools": false,
    "num_quants": 0,
    "quants": [],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/README.md",
    "description": "Nanonets-OCR-s is a state-of-the-art image-to-markdown OCR model that extracts text, equations, tables, and other structured elements from documents, converting them into semantic markdown for efficient processing by LLMs."
  },
  {
    "model_name": "Jinx-gpt-oss-20b-GGUF",
    "developer": "Jinx-org",
    "downloads": 10535,
    "createdAt": "2025-08-14T06:57:05.000Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "jinx-gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "jinx-gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/jinx-gpt-oss-20b-Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "Jinx is a \"helpful-only\" language model variant that responds to all queries without safety refusals, designed for AI safety research to study alignment failures and evaluate safety boundaries."
  },
  {
    "model_name": "phi-4-gguf",
    "developer": "microsoft",
    "downloads": 10412,
    "createdAt": "2025-01-08T19:55:31.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "phi-4-IQ2_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ2_M.gguf",
        "file_size": "561.6 MB"
      },
      {
        "model_id": "phi-4-IQ3_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "phi-4-IQ3_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-IQ3_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "phi-4-IQ3_XXS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XXS.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "phi-4-IQ4_NL",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_NL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-IQ4_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_XS.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "phi-4-Q2_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "phi-4-Q3_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "phi-4-Q3_K_L",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "phi-4-Q3_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-Q4_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "phi-4-Q4_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "phi-4-Q4_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "phi-4-Q4_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-Q5_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_0.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q5_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_1.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "phi-4-Q5_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "phi-4-Q5_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q6_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "phi-4-Q8_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "phi-4-TQ1_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ1_0.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "phi-4-TQ2_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ2_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "phi-4-bf16",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-bf16.gguf",
        "file_size": "27.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/README.md",
    "description": "The Phi-4 model is a 14B parameter, English-focused, chat-oriented, decoder-only Transformer model trained on high-quality synthetic and academic data, designed for reasoning, code generation, and factual responses in memory-constrained environments."
  },
  {
    "model_name": "Phi-4-reasoning-plus-GGUF",
    "developer": "unsloth",
    "downloads": 10352,
    "createdAt": "2025-05-01T02:04:54.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Phi-4-reasoning-plus-BF16",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-BF16.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-IQ4_NL.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-IQ4_XS.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q2_K",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q2_K_L.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_0",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_1",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q6_K",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q8_0",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ2_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q2_K_XL.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q3_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q4_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q5_K_XL.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q6_K_XL.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q8_K_XL.gguf",
        "file_size": "16.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/README.md",
    "description": "Phi-4-reasoning-plus is a 14B parameter, dense decoder-only Transformer model trained for reasoning tasks with enhanced accuracy and token generation, suitable for math, science, and coding, and can be fine-tuned or run via Colab, Ollama, llama.cpp, or"
  },
  {
    "model_name": "Devstral-Small-2507_gguf",
    "developer": "mistralai",
    "downloads": 10104,
    "createdAt": "2025-07-07T12:22:38.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Devstral-Small-2507-BF16",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/README.md",
    "description": "The Devstral Small 1.1 model is a lightweight, agentic LLM with a 128k context window, Apache 2.0 license, and supports GGUF quantized versions for efficient local deployment and use in software engineering tasks."
  },
  {
    "model_name": "MS3.2-24B-Magnum-Diamond-GGUF",
    "developer": "Doctor-Shotgun",
    "downloads": 9829,
    "createdAt": "2025-06-22T18:07:51.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_NL",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_L",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q6_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q8_0",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-bf16",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Doctor-Shotgun/MS3.2-24B-Magnum-Diamond model, optimized for creative writing and roleplay with specific SillyTavern presets provided.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2",
    "developer": "BasedBase",
    "downloads": 9722,
    "createdAt": "2025-08-11T07:56:50.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-Coder-480B-Distill-v2-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-30B-A3B-Instruct-Coder-480B-Distill-v2-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q2_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q3_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_0.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q6_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2-Q6_K.gguf",
        "file_size": "23.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2/resolve/main/README.md",
    "description": "This model is a SVD-based distilled version of Qwen3-Coder-480B for enhanced code generation capabilities with a specialized focus on programming tasks."
  },
  {
    "model_name": "Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf",
    "developer": "DavidAU",
    "downloads": 9663,
    "createdAt": "2024-12-26T11:22:27.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-IQ4_XS.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q2_k.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_l.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_s.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_s.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q5_k_s.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q6_k.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q8_0.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-q5_k_m.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q2_k.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q6_k.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q8_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "18.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "24.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/README.md",
    "description": "This is an uncensored, vividly detailed, and highly creative Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-GGUF model that combines four Mistral 7B models into a 24B parameter model"
  },
  {
    "model_name": "Phi-4-mini-reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 9304,
    "createdAt": "2025-05-01T01:39:08.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Phi-4-mini-reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q4_0",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q3_K_XL.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q4_K_XL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Phi-4-mini-reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/Phi-4-mini-reasoning-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/resolve/main/README.md",
    "description": "Phi-4-mini-reasoning is a lightweight, math-focused language model fine-tuned for reasoning tasks with 3.8B parameters and 128K token context length, optimized for efficiency and accuracy in mathematical problem-solving."
  },
  {
    "model_name": "LFM2-VL-1.6B-GGUF",
    "developer": "LiquidAI",
    "downloads": 9283,
    "createdAt": "2025-08-17T08:50:50.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "LFM2-VL-1.6B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/LFM2-VL-1.6B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-VL-1.6B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/LFM2-VL-1.6B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-VL-1.6B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/LFM2-VL-1.6B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2-VL-1.6B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/mmproj-LFM2-VL-1.6B-F16.gguf",
        "file_size": "791.9 MB"
      },
      {
        "model_id": "mmproj-LFM2-VL-1.6B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/mmproj-LFM2-VL-1.6B-Q8_0.gguf",
        "file_size": "538.0 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-VL-1.6B-GGUF/resolve/main/README.md",
    "description": "LFM2-VL-1.6B-GGUF is a vision model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 9065,
    "createdAt": "2025-03-20T22:44:27.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, multimodal AI model from Google, capable of handling text and image inputs to generate text outputs, trained on diverse data including text, code, math, and images, with performance metrics across various benchmarks and languages, and available in multiple sizes"
  },
  {
    "model_name": "ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 8867,
    "createdAt": "2025-06-30T05:25:35.000Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "ERNIE-4.5-0.3B-PT-F16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-F16.gguf",
        "file_size": "690.4 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.0 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "462.6 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-0.3B is a text dense, Apache-2.0 licensed large language model optimized for text generation and fine-tuning with ERNIEKit and FastDeploy, and supported by the transformers library.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Seed-OSS-36B-Instruct-GGUF",
    "developer": "yarikdevcom",
    "downloads": 8783,
    "createdAt": "2025-08-22T19:46:34.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Seed_OSS_36B_Instruct_Q2_K",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q2_K.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q3_K_M",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q3_K_M.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q4_K_M",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q4_K_M.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q6_K",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q6_K.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "Seed_OSS_36B_Instruct_Q8_0",
        "path": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/Seed_OSS_36B_Instruct_Q8_0.gguf",
        "file_size": "35.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an experimental implementation of the Seed-OSS-36B-Instruct model using llama.cpp with GGML and CUDA optimizations, built from the GGML repository's PR 15490."
  },
  {
    "model_name": "Llama-OuteTTS-1.0-1B-GGUF",
    "developer": "OuteAI",
    "downloads": 8609,
    "createdAt": "2025-04-06T19:18:26.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Llama-OuteTTS-1.0-1B-FP16",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-FP16.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q2_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q2_K.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_L",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_L.gguf",
        "file_size": "708.6 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_M.gguf",
        "file_size": "668.8 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_S.gguf",
        "file_size": "622.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_0.gguf",
        "file_size": "745.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_1.gguf",
        "file_size": "803.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_M.gguf",
        "file_size": "780.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_S.gguf",
        "file_size": "749.7 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_0.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_1.gguf",
        "file_size": "919.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_M.gguf",
        "file_size": "879.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_S.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q6_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q6_K.gguf",
        "file_size": "984.5 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q8_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF",
    "developer": "DavidAU",
    "downloads": 8154,
    "createdAt": "2025-07-17T07:28:04.000Z",
    "tools": true,
    "num_quants": 81,
    "quants": [
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "325.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "310.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "292.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "274.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "417.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "383.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "345.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "482.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "462.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "360.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "338.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "463.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "432.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "483.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "521.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "503.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "484.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "561.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "600.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "572.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "560.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "644.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q8_0.gguf",
        "file_size": "833.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "325.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "310.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "292.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "274.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "417.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "383.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "345.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "482.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "462.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "360.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "338.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "463.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "432.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "483.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "521.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "503.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "484.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "561.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "600.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "572.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "560.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "644.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q8_0.gguf",
        "file_size": "797.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "302.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "286.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "269.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "251.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "374.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "355.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "341.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "322.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "439.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "419.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "317.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "295.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "420.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "355.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "440.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "478.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "460.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "441.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "518.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "557.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "529.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "518.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "601.7 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q8_0.gguf",
        "file_size": "754.3 MB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/README.md",
    "description": "This is a high-performance coder model based on Qwen3, capable of generating code and reasoning blocks efficiently, with optimized variants for different quantization levels and use cases.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "GLM-4-9B-0414-GGUF",
    "developer": "unsloth",
    "downloads": 7877,
    "createdAt": "2025-04-30T18:36:06.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "GLM-4-9B-0414-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-BF16.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q2_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q3_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q3_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q5_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q6_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q8_0.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ1_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ3_XXS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q2_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q3_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q4_K_XL.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q5_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q6_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q8_K_XL.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gpt-oss-20b-uncensored-bf16-GGUF",
    "developer": "mradermacher",
    "downloads": 7649,
    "createdAt": "2025-08-09T10:06:03.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.IQ4_XS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q2_K",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q3_K_L.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q6_K",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-bf16.Q8_0",
        "path": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/gpt-oss-20b-uncensored-bf16.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the GPT-oss-20B uncensored model, available in various quantization formats for efficient deployment."
  },
  {
    "model_name": "Openai_gpt-oss-20b-NEO-GGUF",
    "developer": "DavidAU",
    "downloads": 7614,
    "createdAt": "2025-08-06T01:05:36.000Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE2",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE2.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE3",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE3.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE4",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE4.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO2-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO2-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO2-Q5_1.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO3-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO3-Q5_1.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/README.md",
    "description": "This model is a specialized quantized version of the OpenAI GPT-oss-20B MOE model, optimized for code generation, reasoning, and creative writing with support for up to 24 experts and 128k context, offering high performance in various tasks including coding,"
  },
  {
    "model_name": "Qwen_Qwen3-4B-Thinking-2507-GGUF",
    "developer": "bartowski",
    "downloads": 7582,
    "createdAt": "2025-08-06T15:35:47.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-bf16",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-imatrix",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-4B-Thinking-2507 model for efficient inference on various hardware, with options ranging from high-quality Q8_0 to low-quality Q2_K quantizations."
  },
  {
    "model_name": "Qwen_Qwen3-4B-Instruct-2507-GGUF",
    "developer": "bartowski",
    "downloads": 7570,
    "createdAt": "2025-08-06T15:35:31.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-bf16",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-imatrix",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-4B-Instruct-2507 model using llama.cpp's imatrix method, optimized for different hardware and performance requirements."
  },
  {
    "model_name": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF",
    "developer": "DavidAU",
    "downloads": 7538,
    "createdAt": "2025-06-12T08:58:57.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-IQ4_XS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q2_k.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_m.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q3_k_s.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q4_k_s.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q5_k_s.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-Q8_0.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-D_AU-q5_k_m.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Llama-3.2-9B-Uncensored-Brainstorm-Alpha-Q6_K",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-Q6_K.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-9B-Uncensored-Brainstorm-Alpha-GGUF/resolve/main/README.md",
    "description": "This is a 9B parameter, uncensored, and enhanced Llama-3.2 model with Brainstorm 40x, designed for creative writing across all genres, offering vivid, intense, and emotionally engaging prose with a focus on realism, horror, and storytelling, suitable for both role"
  },
  {
    "model_name": "MiniCPM-V-4_5-gguf",
    "developer": "openbmb",
    "downloads": 7523,
    "createdAt": "2025-08-24T10:41:57.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Model-8.2B-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/Model-8.2B-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "ggml-model-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "ggml-model-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ggml-model-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/mmproj-model-f16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf/resolve/main/README.md",
    "description": "MiniCPM-V 4.5 is a high-performance, efficient, and multilingual vision-language model capable of understanding single images, multiple images, and videos on phones, with state-of-the-art capabilities in OCR, document parsing, and video comprehension."
  },
  {
    "model_name": "Kunoichi-DPO-v2-7B-GGUF-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 7237,
    "createdAt": "2024-02-27T10:16:40.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q4_K_M-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q4_K_M-imatrix.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q4_K_S-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q4_K_S-imatrix.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q5_K_M-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q5_K_M-imatrix.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q5_K_S-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q5_K_S-imatrix.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q6_K-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q6_K-imatrix.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Kunoichi-DPO-v2-7B-Q8_0-imatrix",
        "path": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q8_0-imatrix.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/README.md",
    "description": "The SanjiWatsuki/Kunoichi-DPO-v2-7B model is a GGUF-Imatrix quantized version of the original model, designed to improve performance while maintaining efficiency through the use of an Importance Matrix for activation quantization."
  },
  {
    "model_name": "Impish_Nemo_12B_GGUF",
    "developer": "SicariusSicariiStuff",
    "downloads": 7125,
    "createdAt": "2025-08-10T04:25:16.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Impish_Nemo_12B-Q4_K_M",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q4_K_S",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q5_K_M",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q5_K_S",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q6_K",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Impish_Nemo_12B-Q8_0",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/Impish_Nemo_12B-Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/resolve/main/README.md",
    "description": "This is a Hugging Face model repository for the Impish_Nemo_12B base model and the UBW_Tapestries dataset, licensed under Apache 2.0, quantized by SicariusSicariiStuff."
  },
  {
    "model_name": "XortronCriminalComputingConfig-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 7004,
    "createdAt": "2025-05-05T07:31:49.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides various GGUF quantized versions of the darkc0de/XortronCriminalComputingConfig model, ranging from low-quality to high-quality, with notes on their performance and suitability for different use cases.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "openai_gpt-oss-20b-GGUF-MXFP4-Experimental",
    "developer": "bartowski",
    "downloads": 6639,
    "createdAt": "2025-08-05T17:43:11.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "openai_gpt-oss-20b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental/resolve/main/openai_gpt-oss-20b-MXFP4.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental/resolve/main/README.md",
    "description": "This is an experimental MXFP4 quantized version of the gpt-oss-20b model by OpenAI, requiring the llama.cpp branch `gpt-oss-mxfp4` for use."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 6613,
    "createdAt": "2025-05-28T22:03:00.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the Google Gemma-3-27B IT model, created using an advanced abliteration technique to enhance refusal accuracy while maintaining coherent outputs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "SpaceThinker-Qwen2.5VL-3B",
    "developer": "remyxai",
    "downloads": 6457,
    "createdAt": "2025-04-17T17:34:23.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gguf/spacethinker-qwen2.5VL-3B-F16",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5VL-3B-F16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gguf/spacethinker-qwen2.5vl-3b-vision",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5vl-3b-vision.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/README.md",
    "description": "SpaceThinker-Qwen2.5VL-3B is a multimodal vision-language model trained to enhance spatial reasoning through test-time compute, achieving strong performance on various spatial reasoning benchmarks like SpatialScore and QSpatial-Bench with a focus on quantitative spatial tasks such as distance estimation and object relations.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-Coder-3B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 6433,
    "createdAt": "2024-11-09T12:46:15.000Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-coder-3b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-fp16.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q2_k.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q3_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q4_k_m.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q5_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q5_k_m.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q6_k.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the instruction-tuned 3B Qwen2.5-Coder model in GGUF format, optimized for code generation, reasoning, and fixing with 3.09B parameters and full 32,768 token context length.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-4b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 6254,
    "createdAt": "2025-03-12T20:43:28.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 4B model is a lightweight, open-source, instruction-tuned multimodal model from Google, capable of handling text and image inputs, generating text outputs, and trained on diverse data including text, code, math, and images, with strong performance across various benchmarks and tasks",
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-4B",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-4B.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "num_mmproj": 1
  },
  {
    "model_name": "Qwen3-4B-2507-abliterated-GGUF",
    "developer": "prithivMLmods",
    "downloads": 6117,
    "createdAt": "2025-08-08T16:18:22.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-abliterated-GGUF/Qwen3-4B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "The Qwen3-4B-2507-abliterated-GGUF model is an uncensored, proof-of-concept version of the Qwen3-4B-Instruct-2507 large language model, created using an abliteration method to bypass standard refusal behaviors,"
  },
  {
    "model_name": "GLM-4.5-Air-GGUF",
    "developer": "ddh0",
    "downloads": 5975,
    "createdAt": "2025-08-04T20:25:03.000Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-IQ3_S-IQ3_S-Q5_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-IQ3_S-IQ3_S-Q5_0.gguf",
        "file_size": "57.4 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q5_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q5_0.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q5_1",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q5_1.gguf",
        "file_size": "67.8 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q8_0.gguf",
        "file_size": "77.7 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q5_K-Q5_K-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q5_K-Q5_K-Q8_0.gguf",
        "file_size": "85.6 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0-FFN-Q6_K-Q6_K-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0-FFN-Q6_K-Q6_K-Q8_0.gguf",
        "file_size": "94.1 GB"
      },
      {
        "model_id": "GLM-4.5-Air-Q8_0",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-Q8_0.gguf",
        "file_size": "109.4 GB"
      },
      {
        "model_id": "GLM-4.5-Air-bf16",
        "path": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air-bf16.gguf",
        "file_size": "205.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ddh0/GLM-4.5-Air-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the GLM-4.5-Air model for use with llama.cpp, with Q8_0 as the default quantization for most tensors, except for the dense FFN block and conditional experts which are downgraded."
  },
  {
    "model_name": "Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF",
    "developer": "DavidAU",
    "downloads": 5956,
    "createdAt": "2024-12-12T01:30:01.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/README.md",
    "description": "This model is a powerful Llama 3.2 MOE with 10B parameters, combining four top Llama 3.2 3B models for exceptional creative writing, vivid prose, and uncensored output across all genres, including horror, science fiction, romance, and roleplay",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gpt-oss-20b-uncensored",
    "developer": "Combatti",
    "downloads": 5756,
    "createdAt": "2025-08-08T21:43:45.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gpt-oss-20b-uncensored-F16",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-F16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-Q4_0",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-Q4_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-Q5_0",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-Q5_0.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "gpt-oss-20b-uncensored-Q8_0",
        "path": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/gpt-oss-20b-uncensored-Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Combatti/gpt-oss-20b-uncensored/resolve/main/README.md",
    "description": "This is an uncensored, highly modified version of the unsloth/gpt-oss-20b-BF16 model designed for research and controlled use, with minimal safety filtering and potential for generating sensitive or controversial content."
  },
  {
    "model_name": "SmolLM3-3B-128K-GGUF",
    "developer": "unsloth",
    "downloads": 5540,
    "createdAt": "2025-07-08T23:13:59.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "SmolLM3-3B-128K-BF16",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-BF16.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-IQ4_NL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-IQ4_XS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q2_K",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q2_K_L",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q2_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q3_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q3_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_0",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_1",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_1.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q5_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q5_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q6_K",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q8_0",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ2_XXS.gguf",
        "file_size": "910.9 MB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q2_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q3_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q4_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q5_K_XL.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q6_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q8_K_XL.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "M3.2-24B-Loki-V1.3-GGUF",
    "developer": "CrucibleLab-TG",
    "downloads": 5528,
    "createdAt": "2025-08-04T10:13:33.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "M3.2-24B-Loki-V1.3-BF16",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ1_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_XS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ2_XXS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_XS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ3_XXS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ4_NL",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-IQ4_XS",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q2_K",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q2_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q3_K_L",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q3_K_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q3_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q4_0",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q4_K_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q4_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q5_0",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q5_0.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q5_K_M",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q5_K_S",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q6_K",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "M3.2-24B-Loki-V1.3-Q8_0",
        "path": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/M3.2-24B-Loki-V1.3-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/CrucibleLab-TG/M3.2-24B-Loki-V1.3-GGUF/resolve/main/README.md",
    "description": "This is the BF16 quantized GGUF version of the M3.2-24B-Loki-V1.3 model from CrucibleLab."
  },
  {
    "model_name": "mistralai_Magistral-Small-2506-GGUF",
    "developer": "bartowski",
    "downloads": 5525,
    "createdAt": "2025-06-10T16:09:49.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2506 model by bartowski, optimized for various inference speeds and quality levels using llama.cpp.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Falcon-H1-7B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 5488,
    "createdAt": "2025-05-13T15:59:29.000Z",
    "tools": true,
    "num_quants": 34,
    "quants": [
      {
        "model_id": "Falcon-H1-7B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-BF16.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-F16.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-F32.gguf",
        "file_size": "28.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ1_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_XXS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-TQ1_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-TQ2_0.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Falcon-H1 series is a hybrid Transformers + Mamba architecture, multilingual, causal decoder-only language model developed by TII, licensed under the Falcon-LLM License, and available for inference via Hugging Face Transformers, vLLM, or llama.cpp.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 5414,
    "createdAt": "2025-01-26T02:35:09.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "EXAONE-4.0-1.2B-GGUF",
    "developer": "LGAI-EXAONE",
    "downloads": 5325,
    "createdAt": "2025-07-11T07:03:24.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "EXAONE-4.0-1.2B-BF16",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-BF16.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-IQ4_XS",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-IQ4_XS.gguf",
        "file_size": "718.9 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q4_K_M",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q4_K_M.gguf",
        "file_size": "774.8 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q5_K_M",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q5_K_M.gguf",
        "file_size": "886.6 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q6_K",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q6_K.gguf",
        "file_size": "1005.3 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q8_0",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q8_0.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/README.md",
    "description": "The EXAONE-4.0-1.2B-GGUF model is a multilingual, hybrid reasoning model that combines non-reasoning and reasoning modes, offering high performance with support for Korean, English, and Spanish, and is available in GGUF format with various quantization options"
  },
  {
    "model_name": "Qwen2.5-VL-Abliterated-Caption-GGUF",
    "developer": "prithivMLmods",
    "downloads": 5294,
    "createdAt": "2025-08-19T03:51:50.000Z",
    "tools": false,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.f16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "805.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-3B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/Qwen2.5-VL-Abliterated-Caption-GGUF/resolve/main/README.md",
    "description": "Qwen2.5-VL-Abliterated-Caption-it-GGUF models are fine-tuned for uncensored image captioning, offering high-fidelity descriptions across diverse visual inputs with options for 3B and 7B variants in various quantization formats."
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-GGUF",
    "developer": "lmstudio-community",
    "downloads": 5275,
    "createdAt": "2024-06-17T18:01:28.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-IQ3_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-IQ3_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-IQ4_XS",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-IQ4_XS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q3_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q6_K.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q8_0.gguf",
        "file_size": "15.6 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a brand new MoE model from DeepSeek, specializing in coding instructions and offering excellent inference speed with 16B total weights and 2.4B activated, suitable for both instruction following and code completion.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "POLARIS-Project_Polaris-4B-Preview-GGUF",
    "developer": "bartowski",
    "downloads": 5255,
    "createdAt": "2025-06-24T04:38:50.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ2_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_NL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_1",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q8_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-bf16",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-bf16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Polaris-4B-Preview model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "HuggingFaceTB_SmolLM3-3B-GGUF",
    "developer": "bartowski",
    "downloads": 5125,
    "createdAt": "2025-07-08T16:09:26.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ2_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ2_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ3_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q2_K",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q2_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_0",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_1",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_1.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q5_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q6_K",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q6_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q8_0",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-bf16",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-bf16.gguf",
        "file_size": "5.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-Omni-3B-GGUF",
    "developer": "ggml-org",
    "downloads": 5125,
    "createdAt": "2025-05-26T08:52:28.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen2.5-Omni-3B-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-3B-Q8_0",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q8_0.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-3B-f16",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-f16.gguf",
        "file_size": "6.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/README.md",
    "description": "This is a Qwen2.5-Omni-3B model converted to GGUF format for efficient deployment, supporting text, audio, and image input but not video or audio generation.",
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen2.5-Omni-3B-Q8_0",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-Q8_0.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "mmproj-Qwen2.5-Omni-3B-f16",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-f16.gguf",
        "file_size": "2.4 GB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "Baichuan-M2-32B-GGUF",
    "developer": "mradermacher",
    "downloads": 4986,
    "createdAt": "2025-08-11T17:24:17.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Baichuan-M2-32B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Baichuan-M2-32B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/Baichuan-M2-32B.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Baichuan-M2-32B-GGUF/resolve/main/README.md",
    "description": "The Baichuan-M2-32B model from Baichuan Inc. is available in various GGUF quantized versions for efficient deployment in both English and Chinese, with the highest quality version being Q8_0 at 34.9GB."
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 4962,
    "createdAt": "2025-08-03T08:14:34.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Thinking-2507 model, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "gpt-oss-120b-GGUF",
    "developer": "gabriellarson",
    "downloads": 4899,
    "createdAt": "2025-08-05T17:10:46.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gpt-oss-120B-F16",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120B-F16.gguf",
        "file_size": "60.9 GB"
      },
      {
        "model_id": "gpt-oss-120b-Q4_0",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-Q4_0.gguf",
        "file_size": "58.3 GB"
      },
      {
        "model_id": "gpt-oss-120b-Q8_0",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-Q8_0.gguf",
        "file_size": "59.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/README.md",
    "description": "This is a large, open-source GPT model (gpt-oss-120b) with Apache 2.0 license, designed for high reasoning tasks, agentic capabilities, and fine-tuning on H100 GPU or consumer hardware."
  },
  {
    "model_name": "Gemma-3-R1-27B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 4819,
    "createdAt": "2025-08-04T14:46:49.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/README.md",
    "description": "Hugging Face provides a platform for sharing and accessing machine learning models, datasets, and tools, with a focus on ease of use and community collaboration."
  },
  {
    "model_name": "tencent_Hunyuan-7B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4760,
    "createdAt": "2025-08-04T12:45:17.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q6_K_L.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q8_0.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-bf16",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-bf16.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-imatrix",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-imatrix.gguf",
        "file_size": "4.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Tencent Hunyuan-7B-Instruct model using llama.cpp's imatrix quantization method, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 4756,
    "createdAt": "2025-08-03T04:39:41.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Instruct-2507 model with various quantization types available for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "QVikhr-3-8B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 4731,
    "createdAt": "2025-08-06T14:36:58.000Z",
    "tools": true,
    "num_quants": 30,
    "quants": [
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_XS.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q2_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_K.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "QVikhr-3-8B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/QVikhr-3-8B-Instruction-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-8B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-8B-Instruction — это инструктивная модель на основе Qwen/Qwen3-8B, обученная на русскоязычном датасете GrandMaster2, предназначенная для эффективной обработки текстов на рус"
  },
  {
    "model_name": "orpheus-3b-0.1-ft-Q4_K_M-GGUF",
    "developer": "isaiahbjork",
    "downloads": 4724,
    "createdAt": "2025-03-20T01:43:53.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "orpheus-3b-0.1-ft-q4_k_m",
        "path": "https://huggingface.co/isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF/resolve/main/orpheus-3b-0.1-ft-q4_k_m.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF/resolve/main/README.md",
    "description": "Orpheus-TTS-Local is a lightweight client for running the Orpheus TTS model locally using LM Studio API for text-to-speech synthesis with multiple voice options and emotion tags."
  },
  {
    "model_name": "Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 4717,
    "createdAt": "2025-08-08T00:03:56.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-4B-Thinking-2507-abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-4B-Thinking-2507-abliterated.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-4B-Thinking-2507 model, offering various quantization options for efficient deployment."
  },
  {
    "model_name": "dolphin-2.9.3-mistral-nemo-12b-gguf",
    "developer": "dphn",
    "downloads": 4695,
    "createdAt": "2024-07-24T16:00:27.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.F16",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.F16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q2_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_L",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_1",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_1",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q6_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q8_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/README.md",
    "description": "This is the llama.cpp gguf conversion of the original model located here: https://huggingface.co/cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b, curated and trained by Eric Hartford and Cognitive Computations, based on mistralai",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Big-Tiger-Gemma-27B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 4646,
    "createdAt": "2025-07-04T09:28:17.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q2_K.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q3_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q4_K_M.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q5_K_M.gguf",
        "file_size": "18.9 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q6_K.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q8_0.gguf",
        "file_size": "28.1 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/README.md",
    "description": "Big Tiger Gemma 27B v3 is a more neutral and less positive version of the Gemma 3 27B model, optimized for vision tasks and harder themes.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "google_gemma-3n-E2B-it-GGUF",
    "developer": "bartowski",
    "downloads": 4644,
    "createdAt": "2025-06-26T19:50:06.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3n-E2B-it model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-R1-24B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 4637,
    "createdAt": "2025-08-01T14:22:10.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-R1-24B-v4c-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/README.md",
    "description": "Cydonia R1 24B v4 is a large language model developed by TheDrummer, known for its creativity, solid reasoning, and ability to maintain coherent narratives with a large number of characters.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Hermes-3-Llama-3.1-8B-GGUF",
    "developer": "NousResearch",
    "downloads": 4506,
    "createdAt": "2024-08-05T04:56:41.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q6_K",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Hermes-3-Llama-3.1-8B.Q8_0",
        "path": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/README.md",
    "description": "Hermes 3 is a GGUF quantized version of the Llama-3.1 8B model by Nous Research, designed for advanced agentic capabilities, roleplaying, multi-turn conversations, and function calling, with a chat template compatible with Hugging Face Transformers and llama.cpp."
  },
  {
    "model_name": "gemma-3n-E2B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 4496,
    "createdAt": "2025-06-26T15:16:13.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E2B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-14B_Uncensored_Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4445,
    "createdAt": "2024-09-22T22:26:11.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen2.5-14B_Uncensored_Instruct model using llama.cpp, with various quantization types and optimizations for different hardware, including ARM and Apple Metal, suitable for different performance and quality trade-offs.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 4354,
    "createdAt": "2025-08-03T07:56:40.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the Huihui-Qwen3-Coder-30B-A3B-Instruct, offering various quantization types for efficient deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "bitnet-b1.58-2B-4T-gguf",
    "developer": "microsoft",
    "downloads": 4327,
    "createdAt": "2025-04-15T04:25:42.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ggml-model-i2_s",
        "path": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/README.md",
    "description": "This repository provides the weights for BitNet b1.58 2B4T, a native 1-bit large language model with 2 billion parameters, offering significant computational efficiency gains over full-precision models through specialized C++ inference code."
  },
  {
    "model_name": "Devstral-Small-2505_gguf",
    "developer": "mistralai",
    "downloads": 4280,
    "createdAt": "2025-05-19T16:34:03.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "devstral",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstral.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "devstralQ4_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "devstralQ4_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "devstralQ5_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "devstralQ8_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/README.md",
    "description": "Devstral-Small-2505 is a lightweight, open-source agentic LLM for software engineering tasks with a 128k context window and Apache 2.0 license, available in GGUF quantized formats for efficient local deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Tifa-DeepsexV3-14b-GGUF-Q6",
    "developer": "ValueFX9507",
    "downloads": 4236,
    "createdAt": "2025-06-26T10:15:21.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/README.md",
    "description": "Tifa-DeepSexV3-14b 是基于 Qwen14b 的深度优化模型，支持长文生成、超长关联、控制器调节输出风格和字数，并能避免负面词汇，适用于角色扮演和多种文本生成任务。"
  },
  {
    "model_name": "CausalLM-14B-DPO-alpha-GGUF",
    "developer": "tastypear",
    "downloads": 4233,
    "createdAt": "2023-11-25T15:58:11.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "causallm_14b-dpo-alpha.Q3_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q4_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q4_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q5_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q5_K_S",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q5_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q6_K",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q6_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q8_0",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q8_0.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.f16",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.f16.gguf",
        "file_size": "26.4 GB"
      }
    ],
    "readme": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/README.md",
    "description": "This is a GGUF format model for CausalLM's 14B-DPO-alpha, optimized for text generation with a prompt template and licensing under the WTFPL license with Meta Llama 2 License Terms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-v1.3-Magnum-v4-22B-GGUF",
    "developer": "knifeayumu",
    "downloads": 4219,
    "createdAt": "2024-11-21T09:27:30.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-F16",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-F16.gguf",
        "file_size": "41.4 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q2_K",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q3_K_L",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q3_K_M",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q3_K_S",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q4_K_M",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q4_K_S",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q5_K_M",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q5_K_S",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q6_K",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q8_0",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q8_0.gguf",
        "file_size": "22.0 GB"
      }
    ],
    "readme": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Irix-12B-Model_Stock-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 4211,
    "createdAt": "2025-03-28T16:03:25.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q2_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q6_K.gguf",
        "file_size": "9.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 4149,
    "createdAt": "2025-01-25T01:35:17.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the DeepSeek-R1-Distill-Qwen-14B-Uncensored base model, available in various GGUF quantization formats for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-2506-GGUF",
    "developer": "ggml-org",
    "downloads": 4136,
    "createdAt": "2025-08-20T22:12:29.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-Q4_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-Q8_0",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-Q8_0.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-bf16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-bf16.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "Kimi-VL-A3B-Thinking-2506-f16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/Kimi-VL-A3B-Thinking-2506-f16.gguf",
        "file_size": "29.7 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-Kimi-VL-A3B-Thinking-2506-Q8_0",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/mmproj-Kimi-VL-A3B-Thinking-2506-Q8_0.gguf",
        "file_size": "589.5 MB"
      },
      {
        "model_id": "mmproj-Kimi-VL-A3B-Thinking-2506-bf16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/mmproj-Kimi-VL-A3B-Thinking-2506-bf16.gguf",
        "file_size": "864.7 MB"
      },
      {
        "model_id": "mmproj-Kimi-VL-A3B-Thinking-2506-f16",
        "path": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/mmproj-Kimi-VL-A3B-Thinking-2506-f16.gguf",
        "file_size": "863.4 MB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF/resolve/main/README.md",
    "description": "The model Kimi-VL-A3B-Thinking-2506 from moonshotai is now supported in llama.cpp via the PR #15458."
  },
  {
    "model_name": "SmolVLM2-2.2B-Instruct-GGUF",
    "developer": "ggml-org",
    "downloads": 4136,
    "createdAt": "2025-04-21T19:03:24.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-f16.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [
      {
        "model_id": "mmproj-SmolVLM2-2.2B-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/mmproj-SmolVLM2-2.2B-Instruct-Q8_0.gguf",
        "file_size": "565.1 MB"
      },
      {
        "model_id": "mmproj-SmolVLM2-2.2B-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/mmproj-SmolVLM2-2.2B-Instruct-f16.gguf",
        "file_size": "831.9 MB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "Qwen2-VL-2B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4036,
    "createdAt": "2024-12-14T19:43:17.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ2_M.gguf",
        "file_size": "573.2 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ3_M.gguf",
        "file_size": "740.7 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ3_XS.gguf",
        "file_size": "697.8 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ4_NL.gguf",
        "file_size": "893.0 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ4_XS.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q2_K_L.gguf",
        "file_size": "698.9 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_XL.gguf",
        "file_size": "893.3 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_0.gguf",
        "file_size": "894.1 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_K_L.gguf",
        "file_size": "994.3 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q5_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q6_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-f16.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen2-VL-2B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen2-VL-2B-Instruct-f16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-Qwen2-VL-2B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen2-VL-2B-Instruct-f32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "Gemma-3-R1-4B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 4028,
    "createdAt": "2025-08-07T12:31:17.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF/resolve/main/README.md",
    "description": "Gemma 3 R1 4B v1 is a reasoning-tuned version of the Google Gemma-3-4B model that offers enhanced capabilities and a more unique, less positive personality, with vision support available in GGUF format."
  },
  {
    "model_name": "XBai-o4-GGUF",
    "developer": "mradermacher",
    "downloads": 4024,
    "createdAt": "2025-08-01T15:41:31.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "XBai-o4.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "XBai-o4.Q2_K",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "XBai-o4.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "XBai-o4.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "XBai-o4.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "XBai-o4.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "XBai-o4.Q6_K",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "XBai-o4.Q8_0",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/README.md",
    "description": "The MetaStoneTec/XBai-o4 model is a quantized version of the original model, available in various quantization formats including Q2_K, Q3_K_S, Q4_K_S, Q6_K, and Q8_0, optimized for efficient inference on GGUF files",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NuMarkdown-8B-Thinking-GGUF",
    "developer": "mradermacher",
    "downloads": 3988,
    "createdAt": "2025-08-07T10:00:41.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "NuMarkdown-8B-Thinking.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q2_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q6_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.Q8_0",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.f16",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/NuMarkdown-8B-Thinking.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-GGUF/resolve/main/README.md",
    "description": "The NuMarkdown-8B-Thinking model is a large vision-language model capable of document-to-markdown conversion, OCR, and reasoning, available in various quantized formats for efficient deployment."
  },
  {
    "model_name": "Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF",
    "developer": "DavidAU",
    "downloads": 3965,
    "createdAt": "2025-08-06T05:23:38.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-2-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE2",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE2.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE3",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE3.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE4",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/OpenAI-20B-NEO-CODE-DIMAT-MXFP4_MOE4.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-CODER-NEO-CODE-DI-MATRIX-GGUF/resolve/main/README.md",
    "description": "This model is a specialized GPT-oss-20B MOE (Mixture of Experts) variant with 128k context, 24 experts, and supports code generation, reasoning, and chat, offering high performance with quants like IQ4_NL and MXFP4"
  },
  {
    "model_name": "mistralai_Magistral-Small-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3949,
    "createdAt": "2025-07-24T16:40:51.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2507 model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp's imatrix quantization method.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-8B-128K-GGUF",
    "developer": "unsloth",
    "downloads": 3900,
    "createdAt": "2025-04-28T22:37:42.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-8B-128K-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Qwen3-8B-128K-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/Qwen3-8B-128K-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF/resolve/main/README.md",
    "description": "The Qwen3-8B model is a large language model with 8.2B parameters, supporting seamless switching between thinking and non-thinking modes for enhanced reasoning and dialogue capabilities, and is available on Hugging Face via the transformers library."
  },
  {
    "model_name": "RuadaptQwen3-4B-Instruct-GGUF",
    "developer": "RefalMachine",
    "downloads": 3883,
    "createdAt": "2025-06-30T06:13:47.000Z",
    "tools": true,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "BF16",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "IQ3_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "IQ3_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "IQ4_NL",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "IQ4_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Q2_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q3_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Q3_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Q4_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q4_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Q4_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q5_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q5_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Q5_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q6_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Q8_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "RuadaptQwen3-4B-Instruct is a Russian-adapted version of Qwen/Qwen3-4B, featuring a new tokenizer, continued pre-training on Russian data, and LEP (Learned Embedding Propagation) applied to enhance Russian text generation speed by up to",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Lexi-Llama-3-8B-Uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 3872,
    "createdAt": "2024-04-24T03:51:21.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ1_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ1_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "baidu_ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 3865,
    "createdAt": "2025-06-30T03:51:10.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ2_M.gguf",
        "file_size": "155.4 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_M.gguf",
        "file_size": "195.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XS.gguf",
        "file_size": "184.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS.gguf",
        "file_size": "164.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_NL.gguf",
        "file_size": "222.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_XS.gguf",
        "file_size": "215.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "199.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_L.gguf",
        "file_size": "214.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL.gguf",
        "file_size": "238.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_L.gguf",
        "file_size": "254.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_L.gguf",
        "file_size": "280.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K_L.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-bf16.gguf",
        "file_size": "690.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-0.3B-PT model using llama.cpp's imatrix method, offering various quantization options for different performance and memory trade-offs.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "openai_gpt-oss-120b-GGUF-MXFP4-Experimental",
    "developer": "bartowski",
    "downloads": 3809,
    "createdAt": "2025-08-05T18:09:24.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "openai_gpt-oss-120b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-120b-GGUF-MXFP4-Experimental/resolve/main/openai_gpt-oss-120b-MXFP4.gguf",
        "file_size": "59.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-120b-GGUF-MXFP4-Experimental/resolve/main/README.md",
    "description": "This is an experimental MXFP4 quantized version of the gpt-oss-120b model by OpenAI, requiring the llama.cpp branch `gpt-oss-mxfp4` to run."
  },
  {
    "model_name": "TheDrummer_Gemma-3-R1-12B-v1-GGUF",
    "developer": "bartowski",
    "downloads": 3805,
    "createdAt": "2025-08-12T16:43:38.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ2_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ2_S.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ3_XS.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q2_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q3_K_XL.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_K_L.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q5_K_L.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q6_K_L.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-bf16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "TheDrummer_Gemma-3-R1-12B-v1-imatrix",
        "path": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/TheDrummer_Gemma-3-R1-12B-v1-imatrix.gguf",
        "file_size": "7.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Gemma-3-R1-12B-v1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-3-R1-12B-v1 model by TheDrummer, optimized for various inference speeds and quality levels using llama.cpp's imatrix calibration method."
  },
  {
    "model_name": "llama-joycaption-beta-one-hf-llava-mmproj-gguf",
    "developer": "concedo",
    "downloads": 3752,
    "createdAt": "2025-05-15T13:36:04.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-F16",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-Q4_K",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-Q8_0",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-llava-mmproj-model-f16",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/llama-joycaption-beta-one-llava-mmproj-model-f16.gguf",
        "file_size": "837.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/README.md",
    "description": "These GGUF quantized models for Joycaption Beta One, along with the mmproj file, are compatible with KoboldCpp 1.91 and above."
  },
  {
    "model_name": "Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8",
    "developer": "ValueFX9507",
    "downloads": 3734,
    "createdAt": "2025-02-15T13:30:54.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV2-7b-0218-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-0218-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0301-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0301-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0228-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0228-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0325-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0325-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Q8.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/README.md",
    "description": "Tifa-DeepSexV2-7b-MGRPO 是基于 Qwen2.5-7B 的深度优化模型，具备 100 万字上下文能力，通过 MGRPO 算法和 Tifa_220B 数据集训练，",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Hunyuan-7B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 3660,
    "createdAt": "2025-08-04T06:40:11.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-7B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-F16.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q8_0.gguf",
        "file_size": "7.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This Hugging Face repository provides the Tencent Hunyuan-7B-Instruct large language model, offering efficient inference with support for hybrid reasoning, ultra-long context understanding, and various quantization formats like FP8 and INT4, suitable for deployment across different computational environments.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Menlo_Lucy-GGUF",
    "developer": "bartowski",
    "downloads": 3626,
    "createdAt": "2025-07-18T05:28:56.000Z",
    "tools": true,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Menlo_Lucy-IQ3_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_M.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_XS.gguf",
        "file_size": "795.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_XXS.gguf",
        "file_size": "719.4 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q2_K",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q2_K_L.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_XL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_0",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_1",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q6_K",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q6_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q8_0",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Menlo_Lucy-bf16",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-bf16.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Lucy model by Menlo, optimized for various quantization types and suitable for use with llama.cpp or LM Studio.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "ERNIE-4.5-21B-A3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 3604,
    "createdAt": "2025-07-18T01:00:17.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-BF16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-BF16.gguf",
        "file_size": "40.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-IQ4_NL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-IQ4_XS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q2_K.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q2_K_L.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q3_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_0.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_1.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_K_M.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q5_K_M.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q5_K_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q6_K.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q8_0.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ1_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ1_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ2_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ2_XXS.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ3_XXS.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-TQ1_0.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-21B-A3B-PT is a large-scale text generation model with 21B parameters and 3B activated parameters per token, trained using advanced multimodal MoE techniques and optimized for high-performance inference on various hardware platforms under the Apache 2",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "mistralai_Devstral-Small-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3525,
    "createdAt": "2025-07-10T14:40:40.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Devstral-Small-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mmproj-mistralai_Devstral-Small-2507-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "num_mmproj": 1
  },
  {
    "model_name": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF",
    "developer": "DavidAU",
    "downloads": 3506,
    "createdAt": "2025-01-03T00:04:03.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k.gguf",
        "file_size": "19.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0.gguf",
        "file_size": "24.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "21.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "26.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/README.md",
    "description": "This is a high-precision, 24.9B parameter Llama3 MOE model combining four 8B models, designed for vivid, uncensored creative writing across all genres, with specialized quants and settings for enhanced instruction following and output quality, particularly in horror, fiction, and",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "LFM2-VL-450M-GGUF",
    "developer": "LiquidAI",
    "downloads": 3464,
    "createdAt": "2025-08-17T09:11:52.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "LFM2-VL-450M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/LFM2-VL-450M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-VL-450M-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/LFM2-VL-450M-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-VL-450M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/LFM2-VL-450M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2-VL-450M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/mmproj-LFM2-VL-450M-F16.gguf",
        "file_size": "184.4 MB"
      },
      {
        "model_id": "mmproj-LFM2-VL-450M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/mmproj-LFM2-VL-450M-Q8_0.gguf",
        "file_size": "99.1 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-VL-450M-GGUF/resolve/main/README.md",
    "description": "LFM2-VL-450M-GGUF is a vision model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "tencent_Hunyuan-1.8B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 3418,
    "createdAt": "2025-08-04T12:44:52.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ2_M.gguf",
        "file_size": "666.1 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ3_M.gguf",
        "file_size": "859.1 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ3_XS.gguf",
        "file_size": "801.2 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ3_XXS.gguf",
        "file_size": "732.9 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ4_NL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-IQ4_XS.gguf",
        "file_size": "986.0 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q2_K.gguf",
        "file_size": "741.5 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q2_K_L.gguf",
        "file_size": "798.6 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_L.gguf",
        "file_size": "971.7 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_M.gguf",
        "file_size": "907.0 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_S.gguf",
        "file_size": "831.5 MB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_1",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q4_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q5_K_L.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q6_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-bf16",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-bf16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-1.8B-Instruct-imatrix",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/tencent_Hunyuan-1.8B-Instruct-imatrix.gguf",
        "file_size": "2.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/tencent_Hunyuan-1.8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Tencent Hunyuan-1.8B-Instruct model using llama.cpp, with various quantization types and quality-performance trade-offs for different hardware."
  },
  {
    "model_name": "CrucibleLab_M3.2-24B-Loki-V1.3-GGUF",
    "developer": "bartowski",
    "downloads": 3232,
    "createdAt": "2025-08-23T20:08:40.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_XS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_NL",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_XS",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_0",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_1",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_M",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_S",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K_L",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-Q8_0",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-bf16",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "CrucibleLab_M3.2-24B-Loki-V1.3-imatrix",
        "path": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/CrucibleLab_M3.2-24B-Loki-V1.3-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/CrucibleLab_M3.2-24B-Loki-V1.3-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the M3.2-24B-Loki-V1.3 model by CrucibleLab, optimized for various inference speeds and resource constraints using llama.cpp's imatrix calibration method."
  },
  {
    "model_name": "ruGPT-3.5-13B-GGUF",
    "developer": "oblivious",
    "downloads": 3099,
    "createdAt": "2024-01-27T06:12:52.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "ruGPT-3.5-13B-Q2_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q2_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_L",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_L.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_XS",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_XS.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_1",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_1.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_K_M",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_K_M.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_K_S.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_1",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_1.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_K_M",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_K_M.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q6_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q6_K.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q8_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q8_0.gguf",
        "file_size": "13.0 GB"
      }
    ],
    "readme": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/README.md",
    "description": "ruGPT-3.5-13B-GGUF is a quantized GGUF format model for Russian and English text generation based on the ruGPT-3.5-13B base model.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cydonia-24B-v4.1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 3096,
    "createdAt": "2025-08-18T20:32:34.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.1.imatrix",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/Cydonia-24B-v4.1.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Cydonia-24B-v4.1-i1-GGUF/resolve/main/README.md",
    "description": "TheDrummer/Cydonia-24B-v4.1 is a large language model quantized by mradermacher into various GGUF formats, including imatrix and different quantization types, offering options for size, speed, and quality."
  },
  {
    "model_name": "Llama-3_3-Nemotron-Super-49B-v1_5-GGUF",
    "developer": "gabriellarson",
    "downloads": 3083,
    "createdAt": "2025-07-25T23:29:14.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_S.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_M.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XS.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XXS.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_M.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_S.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XS.gguf",
        "file_size": "19.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XXS.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_NL.gguf",
        "file_size": "26.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_XS.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_L.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_M.gguf",
        "file_size": "22.6 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_S.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_0.gguf",
        "file_size": "26.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_M.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_0.gguf",
        "file_size": "32.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_M.gguf",
        "file_size": "33.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K.gguf",
        "file_size": "38.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q8_0.gguf",
        "file_size": "49.4 GB"
      },
      {
        "model_id": "Llama-49B-3_3-Nemotron-Super-v1_5-F16",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-49B-3_3-Nemotron-Super-v1_5-F16.gguf",
        "file_size": "92.9 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/README.md",
    "description": "The Llama-3.3-Nemotron-Super-49B-v1.5 is a large language model derived from Meta's Llama-3.3-70B-Instruct, optimized for reasoning and chat tasks with a 128K token context length, enhanced",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 3068,
    "createdAt": "2025-08-13T14:29:33.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.i1-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.imatrix",
        "path": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2.imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2 model, optimized for various GGUF quantization types with different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "Magistral-Small-2506_gguf",
    "developer": "mistralai",
    "downloads": 3039,
    "createdAt": "2025-06-09T09:25:46.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Magistral-Small-2506",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506_Q8_0",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506_Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/README.md",
    "description": "Magistral-Small-2506_gguf is a lightweight, efficient reasoning model with 24B parameters, trained on Mistral Small 3.1 with SFT and RL, available under Apache 2.0 license for local deployment and inference using llama.ccp.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "nvidia_OpenReasoning-Nemotron-1.5B-GGUF",
    "developer": "bartowski",
    "downloads": 3034,
    "createdAt": "2025-07-18T20:42:31.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_M.gguf",
        "file_size": "740.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XS.gguf",
        "file_size": "697.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XXS.gguf",
        "file_size": "637.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ4_NL.gguf",
        "file_size": "893.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ4_XS.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q2_K_L.gguf",
        "file_size": "698.9 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_XL.gguf",
        "file_size": "893.3 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_0.gguf",
        "file_size": "894.1 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_1.gguf",
        "file_size": "969.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_L.gguf",
        "file_size": "994.3 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q6_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-bf16.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenReasoning-Nemotron-1.5B model by nvidia using llama.cpp, offering various quantization options for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "QVikhr-3-4B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 2850,
    "createdAt": "2025-06-28T21:03:17.000Z",
    "num_quants": 31,
    "quants": [
      {
        "model_id": "QVikhr-3-4B-Instruction-F16",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-4B-Instruction is an instructive model based on Qwen/Qwen3-4B, trained on the Russian-language dataset GrandMaster2 for high-efficiency text processing in Russian and English.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Huihui-gemma-3n-E4B-it-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 2833,
    "createdAt": "2025-07-11T00:48:21.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.f16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gemma-3n-E4B-it-abliterated model, available in various GGUF quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "Cydonia-24B-v4-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2798,
    "createdAt": "2025-07-09T12:21:43.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Gemma-R1-4B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 2787,
    "createdAt": "2025-08-07T12:31:17.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-4B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/Gemma-3-R1-4B-v1b-Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-R1-4B-v3-GGUF/resolve/main/README.md",
    "description": "Gemma R1 4B v3 is a reasoning-tuned version of the Google Gemma-3-4B model that offers enhanced capabilities and a more unique, less positive personality, with vision support available through GGUF format."
  },
  {
    "model_name": "TheDrummer_Tiger-Gemma-12B-v3-GGUF",
    "developer": "bartowski",
    "downloads": 2762,
    "createdAt": "2025-07-09T16:53:33.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ2_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ2_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ3_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ3_XS.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ3_XXS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ4_NL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ4_XS.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q2_K.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q2_K_L.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_M.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_S.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_XL.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_0.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_1.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_K_L.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_K_M.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_K_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q5_K_L.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q5_K_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q5_K_S.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q6_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q6_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q8_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-bf16.gguf",
        "file_size": "23.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [
      {
        "model_id": "mmproj-TheDrummer_Tiger-Gemma-12B-v3-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/mmproj-TheDrummer_Tiger-Gemma-12B-v3-bf16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-TheDrummer_Tiger-Gemma-12B-v3-f16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/mmproj-TheDrummer_Tiger-Gemma-12B-v3-f16.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "InternVL3_5-30B-A3B-GGUF",
    "developer": "lmstudio-community",
    "downloads": 2725,
    "createdAt": "2025-08-26T04:43:58.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "InternVL3_5-30B-A3B-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "InternVL3_5-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "InternVL3_5-30B-A3B-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "InternVL3_5-30B-A3B-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/InternVL3_5-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/mmproj-model-f16.gguf",
        "file_size": "606.6 MB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/InternVL3_5-30B-A3B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenGVLab/InternVL3_5-30B-A3B model by bartowski, optimized for use with LM Studio's image-text-to-text pipeline."
  },
  {
    "model_name": "LFM2-700M-GGUF",
    "developer": "unsloth",
    "downloads": 2720,
    "createdAt": "2025-07-11T20:08:04.000Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "LFM2-700M-F16",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-F16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "LFM2-700M-Q2_K",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q2_K.gguf",
        "file_size": "300.5 MB"
      },
      {
        "model_id": "LFM2-700M-Q2_K_L",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q2_K_L.gguf",
        "file_size": "300.5 MB"
      },
      {
        "model_id": "LFM2-700M-Q3_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q3_K_M.gguf",
        "file_size": "369.7 MB"
      },
      {
        "model_id": "LFM2-700M-Q3_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q3_K_S.gguf",
        "file_size": "344.4 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_0",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_0.gguf",
        "file_size": "425.6 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_1",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_1.gguf",
        "file_size": "463.9 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_K_M.gguf",
        "file_size": "446.9 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_K_S.gguf",
        "file_size": "428.6 MB"
      },
      {
        "model_id": "LFM2-700M-Q5_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q5_K_M.gguf",
        "file_size": "513.1 MB"
      },
      {
        "model_id": "LFM2-700M-Q5_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q5_K_S.gguf",
        "file_size": "502.1 MB"
      },
      {
        "model_id": "LFM2-700M-Q6_K",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q6_K.gguf",
        "file_size": "583.4 MB"
      },
      {
        "model_id": "LFM2-700M-Q8_0",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q8_0.gguf",
        "file_size": "754.9 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q2_K_XL.gguf",
        "file_size": "300.5 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q3_K_XL.gguf",
        "file_size": "369.7 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q4_K_XL.gguf",
        "file_size": "446.9 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q5_K_XL.gguf",
        "file_size": "513.1 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q6_K_XL.gguf",
        "file_size": "606.7 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q8_K_XL.gguf",
        "file_size": "844.9 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "nvidia_OpenReasoning-Nemotron-7B-GGUF",
    "developer": "bartowski",
    "downloads": 2706,
    "createdAt": "2025-07-18T20:42:50.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ2_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenReasoning-Nemotron-7B model by nvidia, optimized for various inference speeds and memory usage using llama.cpp's imatrix quantization method.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "LFM2-350M-GGUF",
    "developer": "LiquidAI",
    "downloads": 2696,
    "createdAt": "2025-07-12T12:02:17.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-350M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-350M-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-350M-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_K_M.gguf",
        "file_size": "218.7 MB"
      },
      {
        "model_id": "LFM2-350M-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q5_K_M.gguf",
        "file_size": "248.3 MB"
      },
      {
        "model_id": "LFM2-350M-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q6_K.gguf",
        "file_size": "279.8 MB"
      },
      {
        "model_id": "LFM2-350M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/README.md",
    "description": "LFM2-350M-GGUF is a hybrid model developed by Liquid AI for edge AI and on-device deployment, optimized for quality, speed, and memory efficiency."
  },
  {
    "model_name": "Seed-Coder-8B-Reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 2689,
    "createdAt": "2025-05-18T12:29:03.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Seed-Coder-8B-Reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-BF16.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q8_0.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/README.md",
    "description": "The Seed-Coder-8B-Reasoning model is a high-performance, parameter-efficient, and transparent open-source code model trained for enhanced reasoning capabilities using RL.",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-32B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 2678,
    "createdAt": "2025-05-02T23:05:43.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-32B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the Qwen3-32B-Uncensored base model, offering various GGUF quantization options for efficient deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-1b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 2628,
    "createdAt": "2025-03-10T22:42:41.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf",
        "file_size": "957.1 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and available in GGUF format with QAT quantization for efficient deployment.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Voxtral-3B-But-4B-Text-Only-GGUF",
    "developer": "SaisExperiments",
    "downloads": 2574,
    "createdAt": "2025-07-16T08:55:59.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-F16",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-IQ4_NL",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-IQ4_XS",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q4_0",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q5_K_M",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q6_K",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q8_0",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the Mistral AI Voxtral-Mini-3B-2507 base model, with Whisper layers removed and mixed with MiniStral configurations, resulting in a 4B text-only model.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "mistralai_Voxtral-Mini-3B-2507-GGUF",
    "developer": "bartowski",
    "downloads": 2478,
    "createdAt": "2025-07-28T17:11:40.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q2_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q6_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-imatrix.gguf",
        "file_size": "3.2 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistralai Voxtral-Mini-3B-2507 model using llama.cpp imatrix quantization, offering various quantization types for different performance and memory trade-offs.",
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Voxtral-Mini-3B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Mini-3B-2507-bf16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Mini-3B-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Mini-3B-2507-f16.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "MN-12B-Lyra-v4-GGUF",
    "developer": "bartowski",
    "downloads": 2473,
    "createdAt": "2024-09-09T10:04:37.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MN-12B-Lyra-v4-IQ2_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ3_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ3_XS",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ4_XS",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q2_K",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q2_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q2_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_XL.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_4_4.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_4_8.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_8_8.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q6_K",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q6_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q8_0",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-f16",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-f16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the MN-12B-Lyra-v4 model using llama.cpp, optimized for different hardware and performance needs, with recommended options for quality and speed.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound",
    "developer": "Intel",
    "downloads": 2404,
    "createdAt": "2025-08-04T08:15:17.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q4_K_M",
        "path": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound/resolve/main/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q4_K_M.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model, optimized for efficiency using Intel's auto-round algorithm, suitable for deployment with Llamacpp."
  },
  {
    "model_name": "Gemma-3-R1-12B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 2311,
    "createdAt": "2025-08-11T09:13:40.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Gemma-3-R1-12B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/Gemma-3-R1-12B-v1b-Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF/resolve/main/README.md",
    "description": "This is a reasoning-tuned version of the Gemma 3 12B model, designed to enhance reasoning capabilities while reducing positivity, and is vision capable."
  },
  {
    "model_name": "survival-uncensored-gemma-270m",
    "developer": "q1776",
    "downloads": 2272,
    "createdAt": "2025-08-15T13:48:57.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "uncensored-q-270m-f16",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m-f16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "uncensored-q-270m-f32",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m-f32.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "uncensored-q-270m-q8",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m-q8.gguf",
        "file_size": "278.0 MB"
      },
      {
        "model_id": "uncensored-q-270m",
        "path": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/uncensored-q-270m.gguf",
        "file_size": "278.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/q1776/survival-uncensored-gemma-270m/resolve/main/README.md",
    "description": "Uncensored-Q-270M is a fine-tuned, uncensored version of Google's Gemma-3-270M model, optimized for survival, resistance, and psychological resilience tasks with direct, unfiltered responses."
  },
  {
    "model_name": "ChatGPT-5-Q8_0-GGUF",
    "developer": "Hack337",
    "downloads": 2262,
    "createdAt": "2025-04-17T18:33:42.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "chatgpt-5-q8_0",
        "path": "https://huggingface.co/Hack337/ChatGPT-5-Q8_0-GGUF/resolve/main/chatgpt-5-q8_0.gguf",
        "file_size": "506.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Hack337/ChatGPT-5-Q8_0-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF version of Hack337/ChatGPT-5, converted using llama.cpp and GGUF-my-repo for use with llama.cpp CLI or server."
  },
  {
    "model_name": "qwen3-4B-rpg-roleplay",
    "developer": "Chun121",
    "downloads": 2260,
    "createdAt": "2025-04-30T23:55:22.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gguf_f16/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_f16/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.Q4_K_M",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gguf_q8_0/unsloth.Q8_0",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q8_0/unsloth.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/README.md",
    "description": "This is a LoRA fine-tuned version of Qwen3-4B, optimized for character-based conversations and roleplay scenarios using the Gryphe-Aesir-RPG-Charcards-Opus-Mixed-split dataset.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Nidum-Llama-3.2-3B-Uncensored-GGUF",
    "developer": "nidum",
    "downloads": 2258,
    "createdAt": "2024-12-05T04:55:35.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Nidum-Llama-3.2-3B-Uncensored-F16",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/Nidum-Llama-3.2-3B-Uncensored-F16.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "model-Q2_K",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "model-Q3_K_L",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "model-Q3_K_M",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "model-Q3_K_S",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "model-Q4_0_4_4",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q4_0_4_8",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q4_0_8_8",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "model-Q4_K_S",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q5_K_M",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "model-Q5_K_S",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "model-Q6_K",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "model-TQ1_0",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-TQ1_0.gguf",
        "file_size": "883.4 MB"
      },
      {
        "model_id": "model-TQ2_0",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-TQ2_0.gguf",
        "file_size": "1009.4 MB"
      }
    ],
    "readme": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF",
    "developer": "ArliAI",
    "downloads": 2177,
    "createdAt": "2024-08-31T17:20:54.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q2_K",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_L",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q4_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q4_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q5_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q5_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q6_K",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-fp16",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-fp16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-q8_0",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/README.md",
    "description": "ArliAI-RPMax-12B-v1.1 is a creative and non-repetitive RP model based on Mistral Nemo 12B Instruct 2407, trained on diverse and deduplicated datasets to avoid repetition sickness.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-14B-GGUF",
    "developer": "bartowski",
    "downloads": 2146,
    "createdAt": "2025-08-26T03:54:46.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ2_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ2_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-bf16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-14B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/OpenGVLab_InternVL3_5-14B-imatrix.gguf",
        "file_size": "7.4 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-14B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-14B-bf16.gguf",
        "file_size": "673.8 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-14B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-14B-f16.gguf",
        "file_size": "672.7 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-14B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-14B model using llama.cpp's imatrix calibration method, suitable for deployment on various hardware platforms with different performance and memory trade-offs."
  },
  {
    "model_name": "Falcon-H1-1.5B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 2115,
    "createdAt": "2025-05-13T14:03:46.000Z",
    "num_quants": 35,
    "quants": [
      {
        "model_id": "Falcon-H1-1.5B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-BF16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-F32.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ1_M.gguf",
        "file_size": "412.1 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ1_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ1_S.gguf",
        "file_size": "385.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_M.gguf",
        "file_size": "551.8 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_S.gguf",
        "file_size": "516.2 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_XS.gguf",
        "file_size": "493.7 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_XXS.gguf",
        "file_size": "456.6 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_M.gguf",
        "file_size": "703.3 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_S.gguf",
        "file_size": "693.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_XS.gguf",
        "file_size": "675.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_XXS.gguf",
        "file_size": "617.9 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ4_NL.gguf",
        "file_size": "873.1 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ4_XS.gguf",
        "file_size": "831.2 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q2_K.gguf",
        "file_size": "583.9 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q2_K_S.gguf",
        "file_size": "563.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K_L.gguf",
        "file_size": "762.9 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K_M.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K_S.gguf",
        "file_size": "691.8 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_0.gguf",
        "file_size": "871.6 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_1.gguf",
        "file_size": "956.3 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_K.gguf",
        "file_size": "901.0 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_K_M.gguf",
        "file_size": "901.0 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_K_S.gguf",
        "file_size": "875.3 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_K.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-TQ1_0.gguf",
        "file_size": "440.5 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-TQ2_0.gguf",
        "file_size": "498.0 MB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "baidu_ERNIE-4.5-21B-A3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 2082,
    "createdAt": "2025-06-30T03:51:30.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ2_M.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ2_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ2_S.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ2_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ2_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ3_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q2_K.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q2_K_L.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_L.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_S.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_XL.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_0.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_1.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_S.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_L.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_S.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q6_K.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q6_K_L.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q8_0.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-bf16.gguf",
        "file_size": "40.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-21B-A3B-PT model by baidu, optimized for text generation using llama.cpp with various quantization types including Q6_K_L, Q5_K_M, IQ4_XS, etc., recommended",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Falcon-H1-3B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 2037,
    "createdAt": "2025-05-13T14:36:15.000Z",
    "num_quants": 34,
    "quants": [
      {
        "model_id": "Falcon-H1-3B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-BF16.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-F16.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-F32.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ1_M.gguf",
        "file_size": "773.2 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_S.gguf",
        "file_size": "988.6 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_XS.gguf",
        "file_size": "952.2 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_XXS.gguf",
        "file_size": "870.9 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q2_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_1.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-TQ1_0.gguf",
        "file_size": "793.6 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-TQ2_0.gguf",
        "file_size": "919.3 MB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Phi-3.5-mini-instruct_Uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 2034,
    "createdAt": "2024-08-22T02:48:33.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ2_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ2_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ3_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q2_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q4_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q4_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q5_K_L.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q6_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct_Uncensored-f16",
        "path": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/Phi-3.5-mini-instruct_Uncensored-f16.gguf",
        "file_size": "7.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Phi-3.5-mini-instruct_Uncensored model using llama.cpp's imatrix method, optimized for various inference speeds and resource constraints on different platforms."
  },
  {
    "model_name": "Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2022,
    "createdAt": "2025-07-30T10:08:40.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.imatrix",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/README.md",
    "description": "The model Cydonia-v4-MS3.2-Magnum-Diamond-24B is quantized into various formats by mradermacher, including imatrix and IQ quantizations, with the GGUF file format available for download.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NuMarkdown-8B-Thinking-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2020,
    "createdAt": "2025-08-07T10:05:31.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.i1-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "NuMarkdown-8B-Thinking.imatrix",
        "path": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/NuMarkdown-8B-Thinking.imatrix.gguf",
        "file_size": "4.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/NuMarkdown-8B-Thinking-i1-GGUF/resolve/main/README.md",
    "description": "The NuMarkdown-8B-Thinking model is a vision-language model capable of document-to-markdown conversion, reasoning, and RAG, available in various quantized versions for efficient deployment."
  },
  {
    "model_name": "Triplex",
    "developer": "SciPhi",
    "downloads": 2018,
    "createdAt": "2024-07-10T21:58:18.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "quantized_model-Q4_K_M",
        "path": "https://huggingface.co/SciPhi/Triplex/resolve/main/quantized_model-Q4_K_M.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SciPhi/Triplex/resolve/main/README.md",
    "description": "Triplex is a state-of-the-art LLM fine-tuned for efficient knowledge graph construction from unstructured data, offering a 98% cost reduction compared to traditional methods."
  },
  {
    "model_name": "A.X-4.0-Light-gguf",
    "developer": "mykor",
    "downloads": 1974,
    "createdAt": "2025-07-04T06:38:36.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "A.X-4.0-Light-BF16",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-BF16.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "A.X-4.0-Light-F32",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-F32.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "A.X-4.0-Light-IQ4_NL",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "A.X-4.0-Light-IQ4_XS",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q2_K",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q3_K_L",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q3_K_M",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q3_K_S",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q4_0",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q4_K_M",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q4_K_S",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q5_0",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q5_K_M",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q5_K_S",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q6_K",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q6_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q8_0",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-8B-GGUF",
    "developer": "bartowski",
    "downloads": 1970,
    "createdAt": "2025-08-26T03:23:48.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ3_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q2_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q3_K_XL.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q5_K_L.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q6_K_L.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-bf16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-8B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/OpenGVLab_InternVL3_5-8B-imatrix.gguf",
        "file_size": "5.1 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-8B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-8B-bf16.gguf",
        "file_size": "647.8 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-8B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-8B-f16.gguf",
        "file_size": "646.7 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-8B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-8B model using llama.cpp's imatrix method, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "MS3.2-PaintedFantasy-Visage-33B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1962,
    "createdAt": "2025-07-03T14:52:05.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_S.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XXS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_M.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_S.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XXS.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ4_XS.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_L.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_M.gguf",
        "file_size": "15.1 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_S.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_0.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_1.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_S.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_M.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_S.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q6_K.gguf",
        "file_size": "25.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-Coder-7B-Instruct-128K-GGUF",
    "developer": "unsloth",
    "downloads": 1961,
    "createdAt": "2024-11-12T11:37:59.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/README.md",
    "description": "This repository provides the 0.5B Qwen2.5-Coder model, a code-specific large language model that offers improved code generation, reasoning, and fixing capabilities, and is optimized for faster performance and lower memory usage with Unsloth.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Gemma-R1-27B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 1957,
    "createdAt": "2025-08-04T14:46:49.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Gemma-R1-27B-v3-GGUF/resolve/main/README.md",
    "description": "This is a reasoning-tuned version of the Gemma 3 27B model, offering enhanced reasoning capabilities and a more structured, less positive output style, with vision support available."
  },
  {
    "model_name": "Snowpiercer-15B-v2-GGUF",
    "developer": "TheDrummer",
    "downloads": 1941,
    "createdAt": "2025-07-10T01:07:59.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Snowpiercer-15B-v1g-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q4_K_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q6_K.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q8_0.gguf",
        "file_size": "14.8 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "UIGEN-X-4B-0729-GGUF",
    "developer": "gabriellarson",
    "downloads": 1938,
    "createdAt": "2025-07-29T22:26:22.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "UIGEN-X-4B-0729-F16",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_XS.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_XXS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ4_NL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q2_K",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q2_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q6_K",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q8_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/README.md",
    "description": "UIGEN-X-4B-0729 is a reasoning-only UI generation model based on Qwen3-32B, capable of systematically planning, architecting, and implementing complete user interfaces across 26 languages, 7 platforms, and 26 major categories of web",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-Distill",
    "developer": "BasedBase",
    "downloads": 1927,
    "createdAt": "2025-08-05T21:00:28.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill/resolve/main/README.md",
    "description": "The model is licensed under the Apache 2.0 license."
  },
  {
    "model_name": "KAT-V1-40B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1892,
    "createdAt": "2025-07-22T07:40:11.000Z",
    "tools": true,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "KAT-V1-40B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ1_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ1_S.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_M.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_S.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_XS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_XXS.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_M.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_S.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_XS.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_XXS.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ4_XS.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q2_K.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q2_K_S.gguf",
        "file_size": "13.2 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_L.gguf",
        "file_size": "19.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_S.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_0.gguf",
        "file_size": "21.5 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_1.gguf",
        "file_size": "23.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_K_M.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_K_S.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q5_K_M.gguf",
        "file_size": "26.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q5_K_S.gguf",
        "file_size": "26.1 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q6_K.gguf",
        "file_size": "31.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides various GGUF quantized versions of the Kwaipilot/KAT-V1-40B model, ranging from low-quality i1-IQ1_S to high-quality i1-Q6_K, with notes on their performance and suitability.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1856,
    "createdAt": "2025-08-03T11:10:53.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Thinking-2507 model with various GGUF quantization types available for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "Cydonia-R1-24B-v4b-GGUF",
    "developer": "BeaverAI",
    "downloads": 1839,
    "createdAt": "2025-07-25T08:16:40.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-R1-24B-v4b-Q2_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q3_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q4_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q5_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q6_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q8_0",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": null,
    "description": "",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Intern-S1-mini-GGUF",
    "developer": "internlm",
    "downloads": 1828,
    "createdAt": "2025-08-18T08:40:07.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Q8_0/Intern-S1-mini-Q8_0",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/Q8_0/Intern-S1-mini-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Q8_0/mmproj-Intern-S1-mini-Q8_0",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/Q8_0/mmproj-Intern-S1-mini-Q8_0.gguf",
        "file_size": "346.7 MB"
      },
      {
        "model_id": "f16/Intern-S1-mini-f16",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/f16/Intern-S1-mini-f16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "f16/mmproj-Intern-S1-mini-f16",
        "path": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/f16/mmproj-Intern-S1-mini-f16.gguf",
        "file_size": "646.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/internlm/Intern-S1-mini-GGUF/resolve/main/README.md",
    "description": "The Intern-S1-mini-GGUF model is a quantized version of the Intern-S1-mini base model, compatible with llama.cpp for efficient inference on various hardware platforms, including deployment via OpenAI API or Ollama."
  },
  {
    "model_name": "Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1826,
    "createdAt": "2025-08-16T21:07:14.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_M.gguf",
        "file_size": "810.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ1_S.gguf",
        "file_size": "754.4 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_S.gguf",
        "file_size": "1012.7 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XS.gguf",
        "file_size": "983.8 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ2_XXS.gguf",
        "file_size": "904.3 MB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q2_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.i1-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-3B-Abliterated-Caption-it.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/Qwen2.5-VL-3B-Abliterated-Caption-it.imatrix.gguf",
        "file_size": "3.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-VL-3B-Abliterated-Caption-it-i1-GGUF/resolve/main/README.md",
    "description": "This is a GGUF-quantized vision model for image captioning and text generation, offering various quantization options for different trade-offs between speed, size, and quality."
  },
  {
    "model_name": "T-pro-it-2.0-GGUF",
    "developer": "t-tech",
    "downloads": 1750,
    "createdAt": "2025-07-17T21:36:22.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "T-pro-it-2.0-Q4_K_M",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_0",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_K_M",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_K_S",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q6_K",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q8_0",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/README.md",
    "description": "This repository provides the T-pro-it-2.0 model in GGUF format for use with llama.cpp or Ollama, with various quantization options for different hardware requirements."
  },
  {
    "model_name": "google_gemma-3-270m-it-GGUF",
    "developer": "bartowski",
    "downloads": 1738,
    "createdAt": "2025-08-14T15:31:10.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "google_gemma-3-270m-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ3_M.gguf",
        "file_size": "227.9 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ3_XS.gguf",
        "file_size": "226.1 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ3_XXS.gguf",
        "file_size": "225.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ4_NL.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-IQ4_XS.gguf",
        "file_size": "229.7 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_L.gguf",
        "file_size": "235.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_M.gguf",
        "file_size": "230.8 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_S.gguf",
        "file_size": "225.7 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q3_K_XL.gguf",
        "file_size": "235.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_0.gguf",
        "file_size": "230.4 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_1.gguf",
        "file_size": "236.2 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_K_L.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_K_M.gguf",
        "file_size": "241.4 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q4_K_S.gguf",
        "file_size": "238.3 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q5_K_L.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q5_K_M.gguf",
        "file_size": "248.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q5_K_S.gguf",
        "file_size": "246.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q6_K.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q6_K_L.gguf",
        "file_size": "269.9 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-Q8_0.gguf",
        "file_size": "278.0 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-bf16",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-bf16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "google_gemma-3-270m-it-imatrix",
        "path": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/google_gemma-3-270m-it-imatrix.gguf",
        "file_size": "460.0 KB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/google_gemma-3-270m-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3-270M model using llama.cpp's imatrix calibration method, optimized for various hardware platforms and quantization types like Q4_K_M, Q6_K_L, IQ4_XS, etc., with recommended options for performance"
  },
  {
    "model_name": "mistralai_Voxtral-Small-24B-2507-GGUF",
    "developer": "bartowski",
    "downloads": 1698,
    "createdAt": "2025-07-28T17:12:11.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistralai Voxtral-Small-24B-2507 model using llama.cpp's imatrix quantization method, optimized for various hardware platforms with different performance and quality trade-offs.",
    "mmproj_models": [
      {
        "model_id": "mmproj-mistralai_Voxtral-Small-24B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Small-24B-2507-bf16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Small-24B-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Small-24B-2507-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "LongWriter-llama3.1-8b-GGUF",
    "developer": "bartowski",
    "downloads": 1695,
    "createdAt": "2024-08-13T18:59:54.000Z",
    "num_quants": 20,
    "quants": [
      {
        "model_id": "LongWriter-llama3.1-8b-IQ2_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-IQ3_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q2_K",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q6_K",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q8_0",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-f32",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "ubergarm",
    "downloads": 1649,
    "createdAt": "2025-07-02T00:39:08.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-IQ3_KS.gguf",
        "file_size": "34.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an imatrix quantized version of Hunyuan-A13B-Instruct using ik_llama.cpp, optimized for high perplexity and performance with specific layer quantization settings and hardware requirements.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "llama4-dolphin-8B-GGUF",
    "developer": "mradermacher",
    "downloads": 1649,
    "createdAt": "2024-04-27T08:17:50.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "llama4-dolphin-8B.IQ3_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q2_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q6_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q8_0",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.f16",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/README.md",
    "description": "The Hugging Face repository provides GGUF quantized versions of the Manavshah/llama4-dolphin-8B model, including options like Q2_K, IQ3_XS, Q8_0, and others, with notes on quality and performance.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "XortronCriminalComputingConfig-GGUF",
    "developer": "mradermacher",
    "downloads": 1618,
    "createdAt": "2025-05-05T05:05:00.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "XortronCriminalComputingConfig.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.IQ4_XS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q2_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q6_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q8_0",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Midm-2.0-Mini-Instruct-gguf",
    "developer": "mykor",
    "downloads": 1565,
    "createdAt": "2025-07-04T12:38:48.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Midm-2.0-Mini-Instruct-BF16",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-BF16.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-F32",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-F32.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-IQ4_NL",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-IQ4_NL.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-IQ4_XS",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-IQ4_XS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q2_K",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q2_K.gguf",
        "file_size": "930.2 MB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q3_K_L",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q3_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q3_K_M",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q3_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q3_K_S",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q3_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q4_0",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q4_0.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q4_K_M",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q4_K_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q4_K_S",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q4_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q5_0",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q5_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q5_K_M",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q5_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q5_K_S",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q5_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q6_K",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q6_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q8_0",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q8_0.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "YandexGPT-5-Lite-8B-instruct-GGUF",
    "developer": "yandex",
    "downloads": 1516,
    "createdAt": "2025-03-28T16:24:33.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "YandexGPT-5-Lite-8B-instruct-Q4_K_M",
        "path": "https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct-GGUF/resolve/main/YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "readme": "https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 1483,
    "createdAt": "2025-07-30T03:03:56.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-30B-A3B-Instruct-2507-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-30B-A3B-Instruct-2507 model using the Intel AutoRound algorithm, with the embedding layer and lm-head layer quantized to 8 bits and non-expert layers to 4 bits, optimized for efficient"
  },
  {
    "model_name": "Hunyuan-1.8B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 1470,
    "createdAt": "2025-08-04T07:00:44.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-1.8B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-F16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_M.gguf",
        "file_size": "666.1 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_S.gguf",
        "file_size": "626.6 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_XS.gguf",
        "file_size": "603.8 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_XXS.gguf",
        "file_size": "560.3 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_M.gguf",
        "file_size": "859.1 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_S.gguf",
        "file_size": "835.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_XS.gguf",
        "file_size": "801.2 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_XXS.gguf",
        "file_size": "732.9 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ4_NL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ4_XS.gguf",
        "file_size": "986.0 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q2_K.gguf",
        "file_size": "741.5 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q2_K_S.gguf",
        "file_size": "700.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_L.gguf",
        "file_size": "971.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_M.gguf",
        "file_size": "907.0 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_S.gguf",
        "file_size": "831.5 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the Hugging Face implementation of the Hunyuan series of efficient large language models, including pre-trained and instruction-tuned variants with support for FP8, INT4, and other quantization formats, optimized for diverse deployment scenarios.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Sugoi-14B-Ultra-GGUF",
    "developer": "sugoitoolkit",
    "downloads": 1463,
    "createdAt": "2025-08-19T22:54:35.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Sugoi-14B-Ultra-F16",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-F16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Sugoi-14B-Ultra-Q2_K",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Sugoi-14B-Ultra-Q4_K_M",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Sugoi-14B-Ultra-Q8_0",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/Sugoi-14B-Ultra-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF/resolve/main/README.md",
    "description": "Sugoi LLM 14B Ultra (GGUF version) is a Japanese-to-English translation model with near-double BLEU score and strong prompt-following skills, ideal for RPG Maker text localization."
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-GGUF",
    "developer": "ubergarm",
    "downloads": 1458,
    "createdAt": "2025-07-30T15:32:04.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ1_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ1_KT.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ2_KL",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ2_KL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ2_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ2_KT.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ3_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ3_K.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ3_KS.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_K.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_KSS",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_KSS.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_KT.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ5_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ5_K.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Q4_0.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-30B-A3B-Thinking-2507 model using ik_llama.cpp, optimized for different memory footprints and perplexity performance.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Dream-org_Dream-v0-Instruct-7B-GGUF",
    "developer": "bartowski",
    "downloads": 1431,
    "createdAt": "2025-07-16T19:48:13.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q2_K",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_0",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_1",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q6_K",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q8_0",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-bf16",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dream-v0-Instruct-7B model using llama.cpp's imatrix method, offering various quantization types for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Kunoichi-DPO-v2-7B-GGUF",
    "developer": "brittlewis12",
    "downloads": 1380,
    "createdAt": "2024-01-16T16:33:41.000Z",
    "num_quants": 28,
    "quants": [
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ1_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ1_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_XXS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_XXS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ4_NL",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ4_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q2_K",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q2_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_L",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_1",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_1",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q6_K",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q8_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.fp16",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "SmolLM2-135M-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 1358,
    "createdAt": "2024-10-31T19:25:41.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "SmolLM2-135M-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-IQ3_M.gguf",
        "file_size": "86.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-IQ3_XS.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-IQ4_XS.gguf",
        "file_size": "86.7 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q2_K.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q2_K_L.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_L.gguf",
        "file_size": "93.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_M.gguf",
        "file_size": "89.2 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_S.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_XL.gguf",
        "file_size": "93.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0.gguf",
        "file_size": "87.6 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0_4_4.gguf",
        "file_size": "87.5 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0_4_8.gguf",
        "file_size": "87.5 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0_8_8.gguf",
        "file_size": "87.5 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_K_L.gguf",
        "file_size": "100.6 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_K_M.gguf",
        "file_size": "100.6 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_K_S.gguf",
        "file_size": "97.3 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q5_K_L.gguf",
        "file_size": "106.9 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q5_K_M.gguf",
        "file_size": "106.9 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q5_K_S.gguf",
        "file_size": "104.9 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q6_K.gguf",
        "file_size": "132.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q6_K_L.gguf",
        "file_size": "132.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q8_0.gguf",
        "file_size": "138.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-f16",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-f16.gguf",
        "file_size": "258.3 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "jina-reranker-m0-GGUF",
    "developer": "jinaai",
    "downloads": 1344,
    "createdAt": "2025-07-20T21:30:25.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "jina-reranker-m0-F16",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "jina-reranker-m0-Q3_K_M",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "jina-reranker-m0-Q4_K_M",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "jina-reranker-m0-Q5_K_M",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "jina-reranker-m0-Q6_K",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "jina-reranker-m0-Q8_0",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q8_0.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF",
    "developer": "DavidAU",
    "downloads": 1319,
    "createdAt": "2025-05-15T23:44:27.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_M-imat.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_S-imat.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XS-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_M-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_S-imat.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XS-imat.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ4_XS-imat.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K_S-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_L-imat.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_M-imat.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_S-imat.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_0-imat.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_M-imat.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_S-imat.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_M-imat.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_S-imat.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q6_K-imat.gguf",
        "file_size": "14.1 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/README.md",
    "description": "This model is a highly advanced Llama 3.2 MOE variant with 18.4B parameters, featuring reasoning capabilities, vivid creative writing, and uncensored output across all genres, with the ability to control individual expert models for tailored generation, and requires specific system prompts to activate reasoning",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-4B-GGUF",
    "developer": "bartowski",
    "downloads": 1313,
    "createdAt": "2025-08-26T03:23:23.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ3_XXS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ4_NL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q2_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q3_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_1.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q5_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q6_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-Q8_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-bf16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-4B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/OpenGVLab_InternVL3_5-4B-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-4B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-4B-bf16.gguf",
        "file_size": "616.3 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-4B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-4B-f16.gguf",
        "file_size": "615.1 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-4B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-4B model using llama.cpp's imatrix calibration method, optimized for various hardware platforms and quantization types."
  },
  {
    "model_name": "Hunyuan-4B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 1309,
    "createdAt": "2025-08-04T06:44:18.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-4B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-F16.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ4_NL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ4_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q6_K.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q8_0.gguf",
        "file_size": "4.2 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "This Hugging Face repository provides the Hunyuan series of large language models (0.5B, 1.8B, 4B, 7B) with instruction tuning, quantization support (FP8, INT4), and deployment options using TensorRT-LLM, v",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 1304,
    "createdAt": "2025-08-24T08:05:24.000Z",
    "tools": false,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "ARM-CaptainErisNebula-12B-Chimera-v1.1-Q4_0-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/ARM-CaptainErisNebula-12B-Chimera-v1.1-Q4_0-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-BF16",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-BF16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-F16",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-F16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_XS-imat.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ3_XXS-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-IQ4_XS-imat.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q3_K_L-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q3_K_L-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q3_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q3_K_M-imat.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q4_K_M-imat.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q4_K_S-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q5_K_M-imat.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q5_K_S-imat.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q6_K-imat.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "CaptainErisNebula-12B-Chimera-v1.1-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/CaptainErisNebula-12B-Chimera-v1.1-Q8_0-imat.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "imatrix-fp16",
        "path": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/imatrix-fp16.gguf",
        "file_size": "6.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/CaptainErisNebula-12B-Chimera-v1.1-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This model is a GGUF quantized version of Nitral-AI/CaptainErisNebula-12B-Chimera-v1.1 for text generation inference and testing."
  },
  {
    "model_name": "GPT-OSS-Code-Reasoning-20B-GGUF",
    "developer": "GetSoloTech",
    "downloads": 1302,
    "createdAt": "2025-08-22T22:23:07.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q3_K_M",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q4_K_M",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q5_K_M",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "GPT-OSS-Code-Reasoning-20B.Q8_0",
        "path": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/GPT-OSS-Code-Reasoning-20B.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/GetSoloTech/GPT-OSS-Code-Reasoning-20B-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized model is designed for efficient inference of Python/C++ code generation and algorithmic reasoning in competitive programming tasks."
  },
  {
    "model_name": "google_medgemma-4b-it-GGUF",
    "developer": "bartowski",
    "downloads": 1283,
    "createdAt": "2025-07-12T15:54:02.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "google_medgemma-4b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ3_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q2_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q6_K_L.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-bf16",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-bf16.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [
      {
        "model_id": "mmproj-google_medgemma-4b-it-bf16",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/mmproj-google_medgemma-4b-it-bf16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-google_medgemma-4b-it-f16",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/mmproj-google_medgemma-4b-it-f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "num_mmproj": 2
  },
  {
    "model_name": "L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF",
    "developer": "DavidAU",
    "downloads": 1242,
    "createdAt": "2024-06-25T03:18:51.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ1_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ1_M-imat13.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_M-imat13.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_XS-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_XS-imat13.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ3_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ3_M-imat13.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ4_XS-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ4_XS-imat13.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_M-imat13.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_S-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_S-imat13.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_M-imat13.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_S-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_S-imat13.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q6_K-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q6_K-imat13.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/README.md",
    "description": "This model is a high-quality, ultra-high precision Llama3-based quantized version with improved performance and creativity, optimized for use in chat and roleplay scenarios with specific settings for smoother operation.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "reka-flash-3.1-rekaquant-q3_k_s",
    "developer": "RekaAI",
    "downloads": 1237,
    "createdAt": "2025-05-30T14:20:48.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "reka-flash-3.1-rekaquant-q3_k_s",
        "path": "https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s/resolve/main/reka-flash-3.1-rekaquant-q3_k_s.gguf",
        "file_size": "8.6 GB"
      }
    ],
    "readme": "https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Home-Cook-Mistral-Small-Omni-24B-2507-GGUF",
    "developer": "ngxson",
    "downloads": 1191,
    "createdAt": "2025-07-28T19:50:10.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-Q4_K_M",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-bf16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-f16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-f16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/mmproj-model-f16.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/README.md",
    "description": "This model merges Mistral Small 2506 (text) and Voxtral 2507 (audio) into a multimodal model using a modified mergekit tool."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 1185,
    "createdAt": "2025-08-04T04:33:48.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model using the Intel AutoRound algorithm, with 8-bit embeddings and 4-bit non-expert layers, optimized for efficient inference on the codeparrot/github-code-clean dataset."
  },
  {
    "model_name": "llama-3-Korean-Bllossom-8B-gguf-Q4_K_M",
    "developer": "MLP-KTLim",
    "downloads": 1168,
    "createdAt": "2024-05-08T23:06:02.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "llama-3-Korean-Bllossom-8B-Q4_K_M",
        "path": "https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/resolve/main/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/resolve/main/README.md",
    "description": "The Bllossom language model is a Korean-English bilingual model based on Llama3, enhanced with vocabulary expansion, knowledge linking, instruction tuning, and human feedback, and available in GGUF format for efficient CPU/GPU inference."
  },
  {
    "model_name": "reka-flash-3.1-GGUF",
    "developer": "mradermacher",
    "downloads": 1134,
    "createdAt": "2025-07-10T18:09:29.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "reka-flash-3.1.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q2_K",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q2_K.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q3_K_L.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q3_K_M.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q3_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q4_K_M.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q5_K_S.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q6_K",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q6_K.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q8_0",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "MindLink-32B-0801-GGUF",
    "developer": "gabriellarson",
    "downloads": 1099,
    "createdAt": "2025-08-02T05:10:42.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MindLink-32B-0801-F16",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-F16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_M.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_XXS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q2_K",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q2_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q6_K",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q8_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/README.md",
    "description": "MindLink is a new family of large language models developed by Kunlun Inc, built on Qwen with advanced post-training techniques, offering strong performance across various benchmarks and diverse AI applications.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "II-Search-4B-GGUF",
    "developer": "prithivMLmods",
    "downloads": 1082,
    "createdAt": "2025-08-05T21:17:53.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": " II-Search-4B-GGUF.BF16",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.F16",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.F32",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/README.md",
    "description": "II-Search-4B-GGUF is a 4-billion-parameter language model fine-tuned for advanced information seeking and web-integrated reasoning tasks, offering various quantized versions for efficient inference."
  },
  {
    "model_name": "AFM-4.5B-GGUF",
    "developer": "arcee-ai",
    "downloads": 1072,
    "createdAt": "2025-07-29T12:43:40.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "AFM-4.5B-IQ2_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_XS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_XXS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_XXS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ4_NL",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ4_XS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "AFM-4.5B-Q2_K",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AFM-4.5B-Q2_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_XL",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_0",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_1",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "AFM-4.5B-Q6_K",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q6_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q6_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q8_0",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q8_0.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "AFM-4.5B-bf16",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-bf16.gguf",
        "file_size": "8.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/README.md",
    "description": "AFM-4.5B-GGUF is a 4.5 billion parameter instruction-tuned model developed by Arcee.ai for enterprise-grade performance, licensed under the Arcee Model License (AML) with specific commercial usage restrictions."
  },
  {
    "model_name": "OpenGVLab_InternVL3_5-1B-GGUF",
    "developer": "bartowski",
    "downloads": 1063,
    "createdAt": "2025-08-26T03:22:54.000Z",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ2_M.gguf",
        "file_size": "316.4 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ3_M.gguf",
        "file_size": "384.2 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ3_XS.gguf",
        "file_size": "362.0 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ3_XXS.gguf",
        "file_size": "329.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ4_NL.gguf",
        "file_size": "447.3 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-IQ4_XS.gguf",
        "file_size": "429.6 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q2_K.gguf",
        "file_size": "331.2 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q2_K_L.gguf",
        "file_size": "476.1 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_L.gguf",
        "file_size": "415.2 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_M.gguf",
        "file_size": "394.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_S.gguf",
        "file_size": "371.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q3_K_XL.gguf",
        "file_size": "545.0 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_0.gguf",
        "file_size": "447.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_1.gguf",
        "file_size": "482.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_K_L.gguf",
        "file_size": "571.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_K_M.gguf",
        "file_size": "461.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q4_K_S.gguf",
        "file_size": "449.0 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q5_K_L.gguf",
        "file_size": "617.4 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q5_K_M.gguf",
        "file_size": "525.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q5_K_S.gguf",
        "file_size": "518.4 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q6_K.gguf",
        "file_size": "593.9 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q6_K_L.gguf",
        "file_size": "665.8 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-Q8_0.gguf",
        "file_size": "767.5 MB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-bf16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "OpenGVLab_InternVL3_5-1B-imatrix",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/OpenGVLab_InternVL3_5-1B-imatrix.gguf",
        "file_size": "1.1 MB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-1B-bf16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-1B-bf16.gguf",
        "file_size": "593.8 MB"
      },
      {
        "model_id": "mmproj-OpenGVLab_InternVL3_5-1B-f16",
        "path": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/mmproj-OpenGVLab_InternVL3_5-1B-f16.gguf",
        "file_size": "592.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/OpenGVLab_InternVL3_5-1B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the OpenGVLab/InternVL3_5-1B model using llama.cpp, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "DeepSWE-Preview-GGUF",
    "developer": "lmstudio-community",
    "downloads": 1025,
    "createdAt": "2025-07-03T13:18:41.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "DeepSWE-Preview-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "DeepSWE-Preview-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "DeepSWE-Preview-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "DeepSWE-Preview-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "UI-TARS-1.5-7B-GGUF",
    "developer": "mradermacher",
    "downloads": 995,
    "createdAt": "2025-04-17T09:40:55.000Z",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "UI-TARS-1.5-7B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q2_K",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q6_K",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.Q8_0",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.f16",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.mmproj-Q8_0.gguf",
        "file_size": "816.5 MB"
      },
      {
        "model_id": "UI-TARS-1.5-7B.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/UI-TARS-1.5-7B.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/UI-TARS-1.5-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ByteDance-Seed/UI-TARS-1.5-7B model, offering various quantization options for efficient inference."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-abliterated-GGUF",
    "developer": "Misaka27260",
    "downloads": 993,
    "createdAt": "2025-04-13T00:39:29.000Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-F16Out-Q4_K_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-F16Out-Q4_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ2_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ2_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_S",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_XXS",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_XXS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ4_XS",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ4_XS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q3_K_L",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q3_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q4_K_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q4_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q5_K_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q5_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q6_K",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q6_K.gguf",
        "file_size": "6.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized gguf file of the Qwen2.5-VL-7B-Instruct model, optimized for deployment with LM Studio, using FP16 output precision and requiring a runtime environment >= 1.29.0(beta).",
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen2.5-VL-7B-Instruct",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/mmproj-Qwen2.5-VL-7B-Instruct.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "num_mmproj": 1
  },
  {
    "model_name": "Luth-1.7B-Instruct-GGUF",
    "developer": "kurakurai",
    "downloads": 944,
    "createdAt": "2025-08-11T12:50:34.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Luth-1.7B-Instruct-BF16",
        "path": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/Luth-1.7B-Instruct-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Luth-1.7B-Instruct-F16",
        "path": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/Luth-1.7B-Instruct-F16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Luth-1.7B-Instruct-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/Luth-1.7B-Instruct-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-1.7B-Instruct-GGUF/resolve/main/README.md",
    "description": "Luth-1.7B-Instruct is a French fine-tuned version of Qwen3-1.7B, trained on the Luth-SFT dataset, showing significant improvements in French instruction following, math, and general knowledge tasks while maintaining strong English performance."
  },
  {
    "model_name": "L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 920,
    "createdAt": "2024-06-23T20:13:10.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-BF16",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-F16",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a GGUF-IQ-Imatrix quantized version of the Sao10K/L3-8B-Stheno-v3.3-32K model, trained for roleplay and SillyTavern tasks with 32K context support, optimized for use with Kob",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "gemma-3-270m_mitsuki_gguf",
    "developer": "dahara1",
    "downloads": 849,
    "createdAt": "2025-08-18T08:14:59.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-270m_mitsuki-BF16",
        "path": "https://huggingface.co/dahara1/gemma-3-270m_mitsuki_gguf/resolve/main/gemma-3-270m_mitsuki-BF16.gguf",
        "file_size": "517.7 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dahara1/gemma-3-270m_mitsuki_gguf/resolve/main/README.md",
    "description": "This is a lightweight SLM based on gemma-3-270m, fine-tuned as a chat and streaming support character for a fantasy café, and converted to GGUF format for compatibility across various platforms."
  },
  {
    "model_name": "Luth-0.6B-Instruct-GGUF",
    "developer": "kurakurai",
    "downloads": 844,
    "createdAt": "2025-08-11T12:31:50.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Luth-0.6B-Instruct-BF16",
        "path": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/Luth-0.6B-Instruct-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Luth-0.6B-Instruct-F16",
        "path": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/Luth-0.6B-Instruct-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Luth-0.6B-Instruct-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/Luth-0.6B-Instruct-Q8_0.gguf",
        "file_size": "609.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-0.6B-Instruct-GGUF/resolve/main/README.md",
    "description": "Luth-0.6B-Instruct is a French fine-tuned version of Qwen3-0.6B, trained on the Luth-SFT dataset, showing significant improvements in French instruction following, math, and general knowledge while maintaining strong English performance."
  },
  {
    "model_name": "Medical-Diagnosis-COT-Gemma3-270M",
    "developer": "alpha-ai",
    "downloads": 843,
    "createdAt": "2025-08-17T05:24:15.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Medical-Diagnosis-COT-Gemma3-270M.BF16",
        "path": "https://huggingface.co/alpha-ai/Medical-Diagnosis-COT-Gemma3-270M/resolve/main/Medical-Diagnosis-COT-Gemma3-270M.BF16.gguf",
        "file_size": "517.7 MB"
      },
      {
        "model_id": "Medical-Diagnosis-COT-Gemma3-270M.Q8_0",
        "path": "https://huggingface.co/alpha-ai/Medical-Diagnosis-COT-Gemma3-270M/resolve/main/Medical-Diagnosis-COT-Gemma3-270M.Q8_0.gguf",
        "file_size": "278.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/alpha-ai/Medical-Diagnosis-COT-Gemma3-270M/resolve/main/README.md",
    "description": "This model is a fine-tuned Gemma-3 270M for medical question answering with explicit chain-of-thought (CoT) reasoning, producing `</think>...</think>` blocks followed by final answers for research and internal use, but not for clinical decisions."
  },
  {
    "model_name": "OuteTTS-1.0-0.6B-GGUF",
    "developer": "OuteAI",
    "downloads": 833,
    "createdAt": "2025-05-18T17:35:16.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "OuteTTS-1.0-0.6B-FP16",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-FP16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q2_K",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q2_K.gguf",
        "file_size": "287.3 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q3_K_L",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q3_K_L.gguf",
        "file_size": "356.2 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q3_K_M",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q3_K_M.gguf",
        "file_size": "335.8 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q3_K_S",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q3_K_S.gguf",
        "file_size": "312.9 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_0",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_0.gguf",
        "file_size": "368.7 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_1",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_1.gguf",
        "file_size": "394.9 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_K_M",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_K_M.gguf",
        "file_size": "383.1 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q4_K_S",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q4_K_S.gguf",
        "file_size": "370.3 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_0",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_0.gguf",
        "file_size": "421.2 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_1",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_1.gguf",
        "file_size": "447.4 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_K_M",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_K_M.gguf",
        "file_size": "428.6 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q5_K_S",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q5_K_S.gguf",
        "file_size": "421.2 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q6_K",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q6_K.gguf",
        "file_size": "477.0 MB"
      },
      {
        "model_id": "OuteTTS-1.0-0.6B-Q8_0",
        "path": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/OuteTTS-1.0-0.6B-Q8_0.gguf",
        "file_size": "616.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B-GGUF/resolve/main/README.md",
    "description": "OuteTTS Version 1.0 is a multilingual text-to-speech model offering improved speech synthesis and voice cloning capabilities with batched inference support for efficient generation."
  },
  {
    "model_name": "gpt-oss-20b-gguf-q4ks-AutoRound",
    "developer": "Intel",
    "downloads": 829,
    "createdAt": "2025-08-08T07:40:21.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gpt-oss-20b-32x2.4B-Q4_K_S",
        "path": "https://huggingface.co/Intel/gpt-oss-20b-gguf-q4ks-AutoRound/resolve/main/gpt-oss-20b-32x2.4B-Q4_K_S.gguf",
        "file_size": "10.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Intel/gpt-oss-20b-gguf-q4ks-AutoRound/resolve/main/README.md",
    "description": "This model is a gguf q4ks quantized version of the openai/gpt-oss-20b model, optimized for efficient inference using the Intel auto-round algorithm."
  },
  {
    "model_name": "codegemma-2b-GGUF",
    "developer": "bartowski",
    "downloads": 809,
    "createdAt": "2024-04-09T14:56:27.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "codegemma-2b-IQ1_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ1_M.gguf",
        "file_size": "776.1 MB"
      },
      {
        "model_id": "codegemma-2b-IQ1_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ1_S.gguf",
        "file_size": "735.2 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_M.gguf",
        "file_size": "972.2 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_S.gguf",
        "file_size": "917.7 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_XS.gguf",
        "file_size": "901.1 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_XXS.gguf",
        "file_size": "844.3 MB"
      },
      {
        "model_id": "codegemma-2b-IQ3_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-IQ3_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_XS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_XXS.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "codegemma-2b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ4_NL.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "codegemma-2b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ4_XS.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "codegemma-2b-Q2_K",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "codegemma-2b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "codegemma-2b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q3_K_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "codegemma-2b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q3_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q4_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "codegemma-2b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "codegemma-2b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q5_K_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "codegemma-2b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q5_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "codegemma-2b-Q6_K",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q6_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "codegemma-2b-Q8_0",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q8_0.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "deepseek-r1-0528-distilled-qwen3-gguf",
    "developer": "ertghiu256",
    "downloads": 800,
    "createdAt": "2025-06-16T12:31:00.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "converted-model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/deepseek-r1-0528-distilled-qwen3-gguf/resolve/main/converted-model-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "converted-model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/deepseek-r1-0528-distilled-qwen3-gguf/resolve/main/converted-model-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/deepseek-r1-0528-distilled-qwen3-gguf/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "dolphin-2.9.3-mistral-7B-32k-GGUF",
    "developer": "mradermacher",
    "downloads": 793,
    "createdAt": "2024-06-25T03:14:26.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ3_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ3_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q2_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q6_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q8_0",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.f16",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Mixtral-4x3B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 757,
    "createdAt": "2025-07-27T10:59:37.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Mixtral-4x3B-v1a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q3_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q4_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q5_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q6_K.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q8_0.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/README.md",
    "description": "This is a MoE-ified version of Voxtral trained on a specific dataset, designed to generate dialogue with a strong socialist perspective, though it may be unstable and requires careful prompting for coherent outputs.",
    "mmproj_models": [
      {
        "model_id": "mmproj-Voxtral-Mini-3B-2507-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/mmproj-Voxtral-Mini-3B-2507-Q8_0.gguf",
        "file_size": "682.6 MB"
      }
    ],
    "num_mmproj": 1
  },
  {
    "model_name": "Open-Insurance-LLM-Llama3-8B-GGUF",
    "developer": "bartowski",
    "downloads": 756,
    "createdAt": "2024-11-26T18:52:43.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q2_K",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q6_K",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q8_0",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-f16",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Sugoi-32B-Ultra-GGUF",
    "developer": "sugoitoolkit",
    "downloads": 747,
    "createdAt": "2025-08-23T13:16:07.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Sugoi-32B-Ultra-F16",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-F16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "Sugoi-32B-Ultra-Q2_K",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Sugoi-32B-Ultra-Q4_K_M",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Sugoi-32B-Ultra-Q8_0",
        "path": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/Sugoi-32B-Ultra-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/sugoitoolkit/Sugoi-32B-Ultra-GGUF/resolve/main/README.md",
    "description": "Sugoi LLM 32B Ultra is an ultra-large language model based on the Qwen/Qwen2.5-32B-Instruct model, optimized for Japanese and English translation tasks using the GGUF format."
  },
  {
    "model_name": "Skyfall-31B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 746,
    "createdAt": "2025-08-26T16:14:26.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Skyfall-31B-v4j-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q2_K.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q3_K_M.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q4_K_M.gguf",
        "file_size": "17.7 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q5_K_M.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q6_K.gguf",
        "file_size": "24.0 GB"
      },
      {
        "model_id": "Skyfall-31B-v4j-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/Skyfall-31B-v4j-Q8_0.gguf",
        "file_size": "31.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/resolve/main/README.md",
    "description": "Skyfall 31B v4 is an efficient and creative upscaled version of Mistral Small 3.2 2507, offering enhanced versatility and performance."
  },
  {
    "model_name": "DrakIdol-Roleplayer-1.0-GGUF",
    "developer": "mradermacher",
    "downloads": 702,
    "createdAt": "2025-07-27T21:16:36.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DrakIdol-Roleplayer-1.0.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q2_K",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q6_K",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q8_0",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.f16",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.f16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.mmproj-Q8_0.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.mmproj-f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the DrakIdol-Roleplayer-1.0 model by aifeifei798, optimized for roleplay tasks with various quantization options available.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "RL-MemoryAgent-14B-GGUF",
    "developer": "mradermacher",
    "downloads": 656,
    "createdAt": "2025-06-22T17:33:53.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "RL-MemoryAgent-14B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q2_K",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q6_K",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q8_0",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "llama-3-sqlcoder-8b-GGUF",
    "developer": "QuantFactory",
    "downloads": 619,
    "createdAt": "2024-05-29T04:25:33.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "llama-3-sqlcoder-8b.Q2_K",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_0",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_1",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_0",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_1",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q6_K",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q8_0",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF",
    "developer": "DavidAU",
    "downloads": 578,
    "createdAt": "2024-11-07T06:58:54.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-IQ4_XS.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_l.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_m.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_4.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_8.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_8_8.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q6_k.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q8_0.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "CausalLM-7B-DPO-alpha-GGUF",
    "developer": "tastypear",
    "downloads": 545,
    "createdAt": "2023-11-19T15:36:16.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "causallm_7b-dpo-alpha.Q3_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q3_K_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q4_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q5_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q5_K_S",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q6_K",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q8_0",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q8_0.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.f16",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.f16.gguf",
        "file_size": "14.4 GB"
      }
    ],
    "readme": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/README.md",
    "description": "This is a GGUF format quantized version of the CausalLM 7B-DPO-alpha model, compatible with llama.cpp and various UIs, licensed under WTFPL and Meta Llama 2 terms.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-4B-Valiant-Polaris-f32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 524,
    "createdAt": "2025-08-03T04:40:54.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.BF16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.F16",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.F32",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": " Qwen3-4B-Valiant-Polaris.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/ Qwen3-4B-Valiant-Polaris.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/Qwen3-4B-Valiant-Polaris-f32-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B-Valiant-Polaris-f32-GGUF is a 4B-parameter language model combining the reasoning capabilities of Polaris, creativity of Dot-Goat and RP-V3, and scientific depth of ShiningValiant3, optimized for advanced reasoning,",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-4b-tcomanr-merge",
    "developer": "ertghiu256",
    "downloads": 523,
    "createdAt": "2025-07-17T14:34:16.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "model-F16",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/README.md",
    "description": "This is a merged Qwen3-4B model combining code, math, and reasoning capabilities from multiple fine-tuned versions using the TIES method.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Spiral-Qwen3-4B-F32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 516,
    "createdAt": "2025-07-05T12:11:00.000Z",
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Spiral-Qwen3-4B.BF16",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.F16",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.F32",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Llama-3-8B-LexiFun-Uncensored-V1-GGUF",
    "developer": "bartowski",
    "downloads": 504,
    "createdAt": "2024-05-01T16:13:51.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ1_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ1_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q2_K",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "dolphin-2.9.1-yi-1.5-34b-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 486,
    "createdAt": "2024-05-19T14:02:07.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ1_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ1_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_M.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_S.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XS.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XXS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_M.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XS.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XXS.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ4_XS.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_L.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_M.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_S.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_M.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_S.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_M.gguf",
        "file_size": "22.7 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_S.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q6_K.gguf",
        "file_size": "26.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/README.md",
    "description": "The model is a 34B parameter version of Dolphin-2.9.1-Yi-1.5, quantized into various GGUF formats for different performance and quality trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "DeepHat-V1-7B-GGUF",
    "developer": "mradermacher",
    "downloads": 474,
    "createdAt": "2025-07-22T05:01:10.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "DeepHat-V1-7B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.f16",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/README.md",
    "description": "The DeepHat-V1-7B model by mradermacher is a 7B parameter language model quantized in various formats (Q2_K, Q3_K_S, Q4_K_S, etc.) for efficient deployment, with the highest quality version being Q8_0 at",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "qwen3-1.7b-mixture-of-thought",
    "developer": "ertghiu256",
    "downloads": 458,
    "createdAt": "2025-07-19T10:25:11.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/model-Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/model-Q8_0.gguf",
        "file_size": "2.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/README.md",
    "description": "This is a Qwen3-1.7B model trained on 20k conversations from `open-r1/Mixture-of-Thoughts` and 3k from `mlabonne/FineTome-100k` to enhance reasoning capabilities, optimized for weaker devices.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "MedScholar-1.5B-f32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 446,
    "createdAt": "2025-08-01T12:56:11.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "MedScholar-1.5B.BF16",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.BF16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "MedScholar-1.5B.F16",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "MedScholar-1.5B.F32",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.F32.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q4_K_S.gguf",
        "file_size": "896.7 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q8_0.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/README.md",
    "description": "MedScholar-1.5B-f32-GGUF is a compact, instruction-aligned medical question-answering model fine-tuned on the MIRIAD-4.4M dataset for research and educational use only, not for diagnosis or medical decision-making.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF",
    "developer": "jukofyork",
    "downloads": 445,
    "createdAt": "2025-07-18T11:08:53.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-128k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-128k-Q4_0.gguf",
        "file_size": "426.3 MB"
      },
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-32k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-32k-Q4_0.gguf",
        "file_size": "426.3 MB"
      },
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-64k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-64k-Q4_0.gguf",
        "file_size": "426.3 MB"
      }
    ],
    "readme": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/README.md",
    "description": "This is a 0.6B parameter draft model for speculative decoding with Kimi-K2-Instruct, quantized in Q4_0 format for context lengths of 32k, 64k, and 128k.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-Q6_K-GGUF",
    "developer": "DavidAU",
    "downloads": 426,
    "createdAt": "2024-04-21T10:43:00.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "nsfw_dpo_noromaid-7b-mistral-7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/DavidAU/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-Q6_K-GGUF/resolve/main/nsfw_dpo_noromaid-7b-mistral-7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "5.5 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-Q6_K-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 425,
    "createdAt": "2025-01-21T03:41:56.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/README.md",
    "description": "BaronLLM is a large-language model fine-tuned for offensive cybersecurity research and adversarial simulation, providing exploit reasoning, red-team scenario generation, and safety-constrained content."
  },
  {
    "model_name": "Arch-Router-1.5B.gguf",
    "developer": "katanemo",
    "downloads": 422,
    "createdAt": "2025-05-30T18:18:40.000Z",
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Arch-Router-1.5B-Q2_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_L",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Arch-Router-Q6_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-Q6_K.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/README.md",
    "description": "The katanemo/Arch-Router-1.5B model is a compact, preference-aligned routing framework that maps queries to domain-action preferences for selecting the most suitable large language model.",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill",
    "developer": "BasedBase",
    "downloads": 422,
    "createdAt": "2025-08-26T08:50:43.000Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q3_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_0.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_S",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q5_K_M",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q6_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-model-Q2_K",
        "path": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-model-Q2_K.gguf",
        "file_size": "10.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BasedBase/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill/resolve/main/README.md",
    "description": "This model is a distilled version of Qwen3-30B-A3B-Thinking-2507, inheriting reasoning and behavioral traits from the larger DeepSeek-V3.1 teacher model through a high-rank LoRA distillation process with MoE layer synthesis."
  },
  {
    "model_name": "Osmosis-Apply-1.7B",
    "developer": "osmosis-ai",
    "downloads": 415,
    "createdAt": "2025-06-19T07:02:07.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "osmosis-apply-1.7b-bf16",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-apply-1.7b-bf16.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.IQ4_XS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q2_K.gguf",
        "file_size": "839.1 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_M.gguf",
        "file_size": "1023.5 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_S.gguf",
        "file_size": "954.6 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q4_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q5_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q5_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q6_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q8_0.gguf",
        "file_size": "2.0 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/README.md",
    "description": "Osmosis-Apply-1.7B is a language model finetuned on Qwen3-1.7B to apply edit snippets to original code for code merges, with a reward function prioritizing exactness in the output.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF",
    "developer": "mradermacher",
    "downloads": 408,
    "createdAt": "2025-07-23T05:15:11.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.IQ4_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q2_K.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q6_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q8_0.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.f16.gguf",
        "file_size": "11.1 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3 model with various GGUF quantization options for different trade-offs between speed and quality.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Foundation-Sec-8B-Instruct-Q8_0-GGUF",
    "developer": "fdtn-ai",
    "downloads": 375,
    "createdAt": "2025-08-22T01:00:07.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "foundation-sec-8b-instruct-q8_0",
        "path": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct-Q8_0-GGUF/resolve/main/foundation-sec-8b-instruct-q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct-Q8_0-GGUF/resolve/main/README.md",
    "description": "The Foundation-Sec-8B-Q8_0-GGUF model is an 8-bit quantized version of the Foundation-Sec-8B-Instruct cybersecurity-specialized LLaMA 3.1 model, reducing memory usage from ~16GB to ~8.54GB"
  },
  {
    "model_name": "WiNGPT-Babel-2-GGUF",
    "developer": "winninghealth",
    "downloads": 363,
    "createdAt": "2025-06-11T06:11:04.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "WiNGPT-Babel-2-IQ4_XS",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "WiNGPT-Babel-2-Q4_K_M",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "WiNGPT-Babel-2-Q8_0",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-Q8_0.gguf",
        "file_size": "2.6 GB"
      }
    ],
    "readme": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/README.md",
    "description": "WiNGPT-Babel-2 is a multilingual translation language model optimized for 55 languages, enhanced Chinese translation, and structured data handling, built on the GemmaX2-28-2B-Pretrain base model.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "CabraMistral-v3-7b-32k-GGUF",
    "developer": "mradermacher",
    "downloads": 353,
    "createdAt": "2024-05-25T07:46:46.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q2_K",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q6_K",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q8_0",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.f16",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the Mistral-v3-7b-32k base model, available in various GGUF quantization types for different trade-offs between size and quality.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Jinx-gpt-OSS-20B-MXFP4-GGUF",
    "developer": "Joseph717171",
    "downloads": 348,
    "createdAt": "2025-08-24T22:44:20.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Jinx-gpt-OSS-20B",
        "path": "https://huggingface.co/Joseph717171/Jinx-gpt-OSS-20B-MXFP4-GGUF/resolve/main/Jinx-gpt-OSS-20B.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Joseph717171/Jinx-gpt-OSS-20B-MXFP4-GGUF/resolve/main/README.md",
    "description": "This is a GGUF MXFP4 quantized version of the Jinx-gpt-oss-20b model, optimized for accuracy and precision while reducing file size to ~12.11 GB."
  },
  {
    "model_name": "Llama-3-Soliloquy-8B-v2-GGUF",
    "developer": "backyardai",
    "downloads": 329,
    "createdAt": "2024-05-11T03:18:32.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.F16",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ1_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_XXS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_XXS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ4_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q2_K",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q2_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q2_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_L",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q4_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q4_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q5_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q5_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q6_K",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q8_0",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/README.md",
    "description": "This GGUF model is a quantized version of the Llama-3-Soliloquy-8B-v2 by openlynn, optimized for efficient use in Backyard AI, a local AI chat app.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "MythoMakiseMerged-13b",
    "developer": "Heralax",
    "downloads": 288,
    "createdAt": "2023-09-30T06:59:22.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "MythoMakiseMerged-13b-q5km",
        "path": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/MythoMakiseMerged-13b-q5km.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "ggml-model-f16",
        "path": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/ggml-model-f16.gguf",
        "file_size": "24.2 GB"
      }
    ],
    "readme": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/README.md",
    "description": "This model is a fine-tuned version of MythoMax-L2-13b, merged with 33% of MythoMax's intelligence, trained on a visual novel script revamped by GPT-4 to excel in banter, conversation, and roleplay, even surpassing its",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "starcoder2-15b-instruct-v0.1-GGUF",
    "developer": "bartowski",
    "downloads": 288,
    "createdAt": "2024-04-29T22:16:56.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ1_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ1_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_M.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_S.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_XS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_XXS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_XXS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ4_NL.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ4_XS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q2_K",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q2_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_L.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q4_K_M.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q4_K_S.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q5_K_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q5_K_S.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q6_K",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q6_K.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q8_0",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q8_0.gguf",
        "file_size": "15.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the starcoder2-15b-instruct-v0.1 model optimized for code generation tasks, with various quantization options available for different performance and memory trade-offs.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "BitCPM4-1B-GGUF",
    "developer": "openbmb",
    "downloads": 286,
    "createdAt": "2025-06-13T11:41:44.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "BitCPM4-1B-q2_k_s",
        "path": "https://huggingface.co/openbmb/BitCPM4-1B-GGUF/resolve/main/BitCPM4-1B-q2_k_s.gguf",
        "file_size": "488.7 MB"
      },
      {
        "model_id": "BitCPM4-1B-q4_0",
        "path": "https://huggingface.co/openbmb/BitCPM4-1B-GGUF/resolve/main/BitCPM4-1B-q4_0.gguf",
        "file_size": "759.6 MB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/BitCPM4-1B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Skywork-R1V3-38B-GGUF",
    "developer": "Skywork",
    "downloads": 281,
    "createdAt": "2025-07-15T08:34:45.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Skywork-R1V3-38B-Q4_K_M",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/Skywork-R1V3-38B-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Skywork-R1V3-38B-Q8_0",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/Skywork-R1V3-38B-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/README.md",
    "description": "This repository provides a GGUF quantized version of the Skywork-R1V3-38B model for fast and memory-efficient local inference using llama.cpp.",
    "mmproj_models": [
      {
        "model_id": "mmproj-Skywork-R1V3-38B-bf16",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-bf16.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-f16",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-f16.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-q8_0",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-q8_0.gguf",
        "file_size": "5.6 GB"
      }
    ],
    "num_mmproj": 3
  },
  {
    "model_name": "Sorachio-1B-Chat",
    "developer": "IzzulGod",
    "downloads": 277,
    "createdAt": "2025-07-05T05:04:13.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "sorachio-1b-chat-f16",
        "path": "https://huggingface.co/IzzulGod/Sorachio-1B-Chat/resolve/main/sorachio-1b-chat-f16.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "sorachio-1b-chat-q8_0",
        "path": "https://huggingface.co/IzzulGod/Sorachio-1B-Chat/resolve/main/sorachio-1b-chat-q8_0.gguf",
        "file_size": "1019.8 MB"
      }
    ],
    "readme": "https://huggingface.co/IzzulGod/Sorachio-1B-Chat/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Lily-Cybersecurity-7B-v0.2-GGUF",
    "developer": "QuantFactory",
    "downloads": 270,
    "createdAt": "2024-10-20T14:09:56.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/README.md",
    "description": "QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF is a quantized cybersecurity-focused Mistral-7B-Instruct-v0.2 model fine-tuned with 22,000 cybersecurity-related data pairs for tasks like threat analysis, security protocols,",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "GRaPE-Mini-Beta",
    "developer": "Sweaterdog",
    "downloads": 259,
    "createdAt": "2025-08-25T08:45:47.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "GRaPE-Mini-Beta.F16",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-Mini-Beta.F16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-20%",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-20%.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-Q4_K_M",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-Q6_k",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-Q6_k.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "GRaPE-mini-beta-Q8_0",
        "path": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/GRaPE-mini-beta-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Sweaterdog/GRaPE-Mini-Beta/resolve/main/README.md",
    "description": "GRaPE Mini is a 1.5 billion parameter, instruction-tuned language model based on Qwen2.5, designed for high-quality reasoning, coding, and agentic capabilities, with two conditional prompts for helpful or unfiltered responses."
  },
  {
    "model_name": "Gemma3-270m-Instruct-GGUF",
    "developer": "Cactus-Compute",
    "downloads": 258,
    "createdAt": "2025-08-14T16:52:50.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-270m-it-Q8_0",
        "path": "https://huggingface.co/Cactus-Compute/Gemma3-270m-Instruct-GGUF/resolve/main/gemma-3-270m-it-Q8_0.gguf",
        "file_size": "278.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Cactus-Compute/Gemma3-270m-Instruct-GGUF/resolve/main/README.md",
    "description": "This model is licensed under the Gemma license and is tagged for use with the dashboard, cactus-text-inference, and cactus-high-performance."
  },
  {
    "model_name": "bernie0.1",
    "developer": "ivoras",
    "downloads": 236,
    "createdAt": "2025-07-27T22:05:28.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "bernie-0.1_IQ4_NL",
        "path": "https://huggingface.co/ivoras/bernie0.1/resolve/main/bernie-0.1_IQ4_NL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "bernie-0.1_f16",
        "path": "https://huggingface.co/ivoras/bernie0.1/resolve/main/bernie-0.1_f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ivoras/bernie0.1/resolve/main/README.md",
    "description": "This is a proof of concept model trained on the works of US Senator Bernie Sanders, inspired by the sci-fi concept of \"mind states\" or \"mind uploads,\" and designed to respond to questions about his policies and values.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Big-Tiger-Gemma-27B-v1-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 226,
    "createdAt": "2024-07-15T08:20:52.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ1_M.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ1_S.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_M.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_XS.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ2_XXS.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_M.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ3_XXS.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-IQ4_XS.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q2_K.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q3_K_L.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q3_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q4_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q4_K_M.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q4_K_S.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q5_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q5_K_S.gguf",
        "file_size": "17.6 GB"
      },
      {
        "model_id": "Big-Tiger-Gemma-27B-v1.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/Big-Tiger-Gemma-27B-v1.i1-Q6_K.gguf",
        "file_size": "20.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Big-Tiger-Gemma-27B-v1-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the Big-Tiger-Gemma-27B-v1 model by TheDrummer, optimized for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-20250808-233922-0-Q8_0-GGUF",
    "developer": "ParrotRouter",
    "downloads": 215,
    "createdAt": "2025-08-10T14:26:02.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-4b-instruct-2507-20250808-233922-0-q8_0",
        "path": "https://huggingface.co/ParrotRouter/Qwen3-4B-Instruct-2507-20250808-233922-0-Q8_0-GGUF/resolve/main/qwen3-4b-instruct-2507-20250808-233922-0-q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ParrotRouter/Qwen3-4B-Instruct-2507-20250808-233922-0-Q8_0-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the ParrotRouter/Qwen3-4B-Instruct-2507-20250808-233922-0 model, optimized for use with llama.cpp."
  },
  {
    "model_name": "LLaMAX3-8B-Alpaca-GGUF",
    "developer": "QuantFactory",
    "downloads": 214,
    "createdAt": "2024-07-14T08:44:43.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q2_K",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_1",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_1",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q6_K",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q8_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/README.md",
    "description": "This is a multilingual, quantized version of the LLaMAX3-8B-Alpaca model, trained on 102 languages and fine-tuned for instruction-following, achieving over 5-point improvements in translation performance on the Flores-101 dataset.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "prompt-generator-GGUF",
    "developer": "mav23",
    "downloads": 206,
    "createdAt": "2024-11-30T14:30:00.000Z",
    "num_quants": 17,
    "quants": [
      {
        "model_id": "prompt-generator.Q2_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K_L",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K_M",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K_S",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "prompt-generator.Q4_0",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "prompt-generator.Q4_1",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_1.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "prompt-generator.Q4_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q4_K_M",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q4_K_S",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "prompt-generator.Q5_0",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "prompt-generator.Q5_1",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q5_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "prompt-generator.Q5_K_M",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "prompt-generator.Q5_K_S",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q6_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "prompt-generator.Q8_0",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Research-Reasoner-7B-v0.3",
    "developer": "Raymond-dev-546730",
    "downloads": 190,
    "createdAt": "2025-04-03T05:26:56.000Z",
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ3_XS",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_NL",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_XS",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q2_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_M",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_S",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_0",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_1",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_M",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_S",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_0",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_1",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_M",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_S",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q6_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q8_0",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-f16",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/README.md",
    "description": "",
    "tools": true,
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Layris_9B-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 187,
    "createdAt": "2024-03-06T03:26:55.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Layris_9B-F16",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-F16.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_M-imat.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_S-imat.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_XS-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_XXS-imat.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Layris_9B-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ4_XS-imat.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Layris_9B-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q4_K_M-imat.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Layris_9B-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q4_K_S-imat.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Layris_9B-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q5_K_M-imat.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Layris_9B-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q5_K_S-imat.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Layris_9B-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q6_K-imat.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Layris_9B-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q8_0-imat.gguf",
        "file_size": "8.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a GGUF-Imatrix quantized version of the Layris 9B model, merged from Eris Remix 7B and Mistral 7B-V0.1-Layla-V4, offering improved performance and quality preservation during quantization.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Impish_Nemo_12B_ARM",
    "developer": "SicariusSicariiStuff",
    "downloads": 173,
    "createdAt": "2025-08-10T04:25:32.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Impish_Nemo_12B-Q4_0",
        "path": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_ARM/resolve/main/Impish_Nemo_12B-Q4_0.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_ARM/resolve/main/README.md",
    "description": "This is a Hugging Face model repository for the Impish_Nemo_12B base model and the UBW_Tapestries dataset, licensed under Apache 2.0, quantized by SicariusSicariiStuff."
  },
  {
    "model_name": "Luth-LFM2-350M-GGUF",
    "developer": "kurakurai",
    "downloads": 121,
    "createdAt": "2025-08-24T08:18:14.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Luth-LFM2-350M-F16",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-350M-GGUF/resolve/main/Luth-LFM2-350M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "Luth-LFM2-350M-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-350M-GGUF/resolve/main/Luth-LFM2-350M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-LFM2-350M-GGUF/resolve/main/README.md",
    "description": "Luth-LFM2-350M-GGUF is a French fine-tuned version of the LFM2 model by Kurakurai AI, trained on the Luth-SFT dataset, optimized for instruction following, math, and general knowledge in French with stable English performance."
  },
  {
    "model_name": "Luth-LFM2-1.2B-GGUF",
    "developer": "kurakurai",
    "downloads": 114,
    "createdAt": "2025-08-24T08:30:54.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Luth-LFM2-1.2B-F16",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-1.2B-GGUF/resolve/main/Luth-LFM2-1.2B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Luth-LFM2-1.2B-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-1.2B-GGUF/resolve/main/Luth-LFM2-1.2B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-LFM2-1.2B-GGUF/resolve/main/README.md",
    "description": "Luth-LFM2-1.2B-GGUF is a French fine-tuned version of the LFM2 model by Kurakura AI, trained on the Luth-SFT dataset, optimized for instruction following, math, and general knowledge in French with stable English performance."
  },
  {
    "model_name": "HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF",
    "developer": "DevQuasar",
    "downloads": 103,
    "createdAt": "2024-11-01T01:28:38.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q2_K",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q2_K.gguf",
        "file_size": "643.3 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q3_K_M.gguf",
        "file_size": "820.3 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q4_K_M.gguf",
        "file_size": "1006.7 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q5_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q6_K",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q8_0",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the SmolLM2-1.7B-Instruct model, optimized for efficient text generation on Hugging Face.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Qwen2.5-Coder-7B-Instruct-Omni1.1",
    "developer": "TomBombadyl",
    "downloads": 91,
    "createdAt": "2025-08-15T17:55:34.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "models/gguf/isaac_sim_qwen2.5_coder_f16",
        "path": "https://huggingface.co/TomBombadyl/Qwen2.5-Coder-7B-Instruct-Omni1.1/resolve/main/models/gguf/isaac_sim_qwen2.5_coder_f16.gguf",
        "file_size": "616.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TomBombadyl/Qwen2.5-Coder-7B-Instruct-Omni1.1/resolve/main/README.md",
    "description": "This is a specialized Qwen2.5-Coder-7B-Instruct model fine-tuned for Isaac Sim 5.0 robotics tasks, including simulation, code generation, and robot control."
  },
  {
    "model_name": "gemma-3-4b-it-unslop-GRPO",
    "developer": "electroglyph",
    "downloads": 66,
    "createdAt": "2025-07-28T06:13:48.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GGUF/gemma-3-4b-it-unslop-GRPO-Q4_K_M",
        "path": "https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO/resolve/main/GGUF/gemma-3-4b-it-unslop-GRPO-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO/resolve/main/README.md",
    "description": "This is a LoRA adapter for the Gemma 3 4b model, trained to reduce slop output by incorporating frequency-based reward functions and lexical diversity constraints, with training code and dataset examples provided.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "Luth-LFM2-700M-GGUF",
    "developer": "kurakurai",
    "downloads": 40,
    "createdAt": "2025-08-24T08:24:13.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Luth-LFM2-700M-F16",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-700M-GGUF/resolve/main/Luth-LFM2-700M-F16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Luth-LFM2-700M-Q8_0",
        "path": "https://huggingface.co/kurakurai/Luth-LFM2-700M-GGUF/resolve/main/Luth-LFM2-700M-Q8_0.gguf",
        "file_size": "754.9 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/kurakurai/Luth-LFM2-700M-GGUF/resolve/main/README.md",
    "description": "Luth-LFM2-700M-GGUF is a French fine-tuned version of the LFM2 model by Kurakurai, trained on the Luth-SFT dataset, with enhanced French capabilities and stable English performance."
  },
  {
    "model_name": "TAIDE-LX-7B-Chat-4bit",
    "developer": "taide",
    "downloads": 18,
    "createdAt": "2024-04-15T03:28:54.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "taide-7b-a.2-q4_k_m",
        "path": "https://huggingface.co/taide/TAIDE-LX-7B-Chat-4bit/resolve/main/taide-7b-a.2-q4_k_m.gguf",
        "file_size": "3.9 GB"
      }
    ],
    "readme": "https://huggingface.co/taide/TAIDE-LX-7B-Chat-4bit/resolve/main/README.md",
    "description": "TAIDE-LX-7B-Chat 是以 LLaMA2-7B 為基礎，結合台灣繁體中文資料與文化知識，經過持續預訓練和指令微調後的大型語言模型，專注於提升繁體中文生成能力與台灣在地",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "RimTalk-Mini-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 13,
    "createdAt": "2025-07-27T04:40:01.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "RimDialogue-3B-v1a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "RimDialogue-3B-v1a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/RimDialogue-3B-v1a-Q8_0.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF/resolve/main/README.md",
    "description": "RimTalk Mini v1 is a compact, efficient version of the RimTalk model optimized for performance and resource usage."
  },
  {
    "model_name": "astrollama.gguf",
    "developer": "UniverseTBD",
    "downloads": 10,
    "createdAt": "2023-10-09T04:29:42.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "astrollama",
        "path": "https://huggingface.co/UniverseTBD/astrollama.gguf/resolve/main/astrollama.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/UniverseTBD/astrollama.gguf/resolve/main/README.md",
    "description": "AstroLLaMA.gguf is a GPT-generated unified format model for astronomy and science tasks, compatible with CPU inference via Ollama.",
    "mmproj_models": [],
    "num_mmproj": 0
  },
  {
    "model_name": "ArmenianGPT-0.1-12B",
    "developer": "ArmGPT",
    "downloads": 7,
    "createdAt": "2025-07-11T21:50:18.000Z",
    "num_quants": 5,
    "quants": [
      {
        "model_id": "ArmenianGPT-0.1-12B-Q5_0",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q5_0.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q5_K_M",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q5_K_S",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q6_K",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q8_0",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/README.md",
    "description": "",
    "tools": false,
    "mmproj_models": [],
    "num_mmproj": 0
  }
]