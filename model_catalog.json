[
  {
    "model_name": "gpt-oss-20b-GGUF",
    "developer": "ggml-org",
    "downloads": 31688,
    "createdAt": "2025-08-02T10:45:18.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gpt-oss-20b-mxfp4",
        "path": "https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-mxfp4.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "The GPT OSS 120B GGUF model can be run using `llama-server` with the command `llama-server -hf ggml-org/gpt-oss-20b-GGUF -c 0 -fa --jinja --reasoning-format none` and"
  },
  {
    "model_name": "Jan-nano-gguf",
    "developer": "Menlo",
    "downloads": 64530,
    "createdAt": "2025-06-11T07:14:33.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-4b-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-4b-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/README.md",
    "description": "Jan Nano is a compact, quantized version of the Qwen3 architecture, optimized for efficient text generation in local or embedded environments with enhanced tool use and research capabilities.",
    "tools": true
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 50317,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling comprehensive analysis of long documents and complex conversations without performance loss.",
    "tools": true
  },
  {
    "model_name": "Lucy-128k-gguf",
    "developer": "Menlo",
    "downloads": 7590,
    "createdAt": "2025-07-18T08:52:46.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "lucy_128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "lucy_128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "lucy_128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "lucy_128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "lucy_128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "lucy_128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "lucy_128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "lucy_128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "lucy_128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "lucy_128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "lucy_128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/README.md",
    "description": "Lucy is a 1.7B mobile-optimized model for agentic web search and lightweight browsing, built on Qwen3-1.7B with machine-generated task vectors for efficient performance on CPU-only devices."
  },
  {
    "model_name": "Lucy-gguf",
    "developer": "Menlo",
    "downloads": 3299,
    "createdAt": "2025-07-18T07:04:35.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Lucy-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Lucy-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Lucy-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Lucy-Q4_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_0.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Lucy-Q4_1",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Lucy-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Lucy-Q5_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q5_1",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Lucy-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Lucy-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Lucy-Q6_K",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Lucy-Q8_0",
        "path": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/Lucy-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Lucy-gguf/resolve/main/README.md",
    "description": "Lucy is a 1.7B mobile-optimized model for agentic web search and lightweight browsing, built on Qwen3-1.7B with machine-generated task vectors for efficient performance on CPU-only devices."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 484644,
    "createdAt": "2024-09-25T18:35:33.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct model by Bartowski, offering various quantization types (e.g., Q4_K_M, Q5_K_S) for different performance and memory trade-offs, with a focus on compatibility with ARM and CPU"
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "developer": "unsloth",
    "downloads": 424168,
    "createdAt": "2025-05-29T14:17:25.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "This is a large language model based on the DeepSeek-R1-0528 architecture, optimized for reasoning tasks with improved performance on benchmarks like AIME, and available in various quantized formats for efficient local inference."
  },
  {
    "model_name": "Phi-3.5-mini-instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 254745,
    "createdAt": "2024-08-20T20:07:57.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Phi-3.5-mini-instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ1_M.gguf",
        "file_size": "874.6 MB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ1_S.gguf",
        "file_size": "802.6 MB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ2_XS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q4_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-3.5-mini-instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Microsoft Phi-3.5-mini-instruct model, quantized in 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit, and available for text generation tasks."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 238616,
    "createdAt": "2024-05-22T17:27:45.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ2_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Mistral-7B-Instruct-v0.3, quantized for efficient inference on various platforms.",
    "tools": false
  },
  {
    "model_name": "Mistral-Nemo-Instruct-2407-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 220122,
    "createdAt": "2024-07-18T14:49:08.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-Instruct-2407.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407.fp16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/README.md",
    "description": "This GGUF format model is a quantized version of the Mistral-Nemo-Instruct-2407 model by Mistral AI, compatible with various frameworks and tools like llama.cpp, llama-cpp-python, LM Studio, and others."
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 219609,
    "createdAt": "2024-04-18T16:43:25.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.fp16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized and GGUF version of the Meta Llama 3 8B Instruct model, optimized for efficient inference and compatible with llama.cpp, based on the original Meta Llama 3 model.",
    "tools": false
  },
  {
    "model_name": "gemma-2b",
    "developer": "google",
    "downloads": 215535,
    "createdAt": "2024-02-08T08:11:26.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b",
        "path": "https://huggingface.co/google/gemma-2b/resolve/main/gemma-2b.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-2b/resolve/main/README.md",
    "description": "The Gemma 2B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with support for fine-tuning, multi-GPU training, and various precision optimizations, and evaluated on multiple benchmarks for performance and"
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 202377,
    "createdAt": "2024-07-23T16:17:10.000Z",
    "tools": false,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Meta-Llama-3.1-8B-Instruct-GGUF model is a GGUF-formatted, 8B parameter, multilingual, instruction-tuned text generation model from Meta, optimized for efficiency and performance across various languages and tasks."
  },
  {
    "model_name": "Phi-4-mini-instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 163160,
    "createdAt": "2025-03-01T10:43:15.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Phi-4-mini-instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q5_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.fp16.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Phi-4-mini-instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF-quantized version of the Microsoft Phi-4-mini-instruct model, optimized for efficient text generation on various platforms."
  },
  {
    "model_name": "gemma-7b",
    "developer": "google",
    "downloads": 156016,
    "createdAt": "2024-02-08T22:36:43.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b",
        "path": "https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-7b/resolve/main/README.md",
    "description": "The Gemma 7B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with strong performance on benchmark tasks and ethical safety considerations."
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 152980,
    "createdAt": "2024-09-25T19:26:01.000Z",
    "tools": false,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ1_M.gguf",
        "file_size": "394.4 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ1_S.gguf",
        "file_size": "375.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ2_XS.gguf",
        "file_size": "453.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ3_XS.gguf",
        "file_size": "592.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.IQ4_XS.gguf",
        "file_size": "708.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q2_K.gguf",
        "file_size": "554.0 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_L.gguf",
        "file_size": "698.6 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_M.gguf",
        "file_size": "658.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q3_K_S.gguf",
        "file_size": "612.0 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q4_K_M.gguf",
        "file_size": "770.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q4_K_S.gguf",
        "file_size": "739.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q5_K_M.gguf",
        "file_size": "869.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q5_K_S.gguf",
        "file_size": "851.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q6_K.gguf",
        "file_size": "974.5 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.Q8_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct.fp16.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Llama-3.2-1B-Instruct-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the meta-llama/Llama-3.2-1B-Instruct model, quantized in 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit, and others, suitable for text"
  },
  {
    "model_name": "gemma-2b-it",
    "developer": "google",
    "downloads": 152820,
    "createdAt": "2024-02-08T13:23:59.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b-it",
        "path": "https://huggingface.co/google/gemma-2b-it/resolve/main/gemma-2b-it.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-2b-it/resolve/main/README.md",
    "description": "The Gemma 2B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, offering high performance for tasks like question answering, summarization, and reasoning, with support for CPU/GPU inference and fine"
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "unsloth",
    "downloads": 132485,
    "createdAt": "2025-06-26T12:24:35.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q2_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q3_K_XL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q6_K_XL.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q8_K_XL.gguf",
        "file_size": "9.9 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with high performance and accuracy, available on Hugging"
  },
  {
    "model_name": "gemma-2-2b-it-GGUF",
    "developer": "bartowski",
    "downloads": 119886,
    "createdAt": "2024-07-31T16:45:13.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gemma-2-2b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-2-2b-it model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp imatrix quantization.",
    "tools": false
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 105322,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B model with mixture of experts for creative writing, fiction, and roleplay, capable of generating vivid, uncensored, and genre-"
  },
  {
    "model_name": "gemma-7b-it",
    "developer": "google",
    "downloads": 101921,
    "createdAt": "2024-02-13T01:07:30.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b-it",
        "path": "https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-7b-it/resolve/main/README.md",
    "description": "The Gemma 7B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, and suitable for various text generation tasks with support for fine-tuning, GPU acceleration, and ethical safety measures."
  },
  {
    "model_name": "Violet_Twilight-v0.2-GGUF",
    "developer": "Epiculous",
    "downloads": 98966,
    "createdAt": "2024-09-13T13:19:21.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Violet_Twilight-v0.2.IQ1_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_NL",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_L",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_1",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q6_K",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q8_0",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.bf16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.bf16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f32",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f32.gguf",
        "file_size": "45.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/README.md",
    "description": "The Violet_Twilight-v0.2 model is a SLERP merge of Azure_Dusk-v0.2 and Crimson_Dawn-v0.2, trained on ChatML with support for gguf quantization."
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 88892,
    "createdAt": "2025-03-12T12:32:41.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned, multimodal model with 128K context window and multilingual support, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks for reasoning, STEM, code, and",
    "tools": false
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "unsloth",
    "downloads": 67075,
    "createdAt": "2025-06-20T22:27:21.000Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized Mistral-3.2 Small 24B model optimized for instruction following, function calling, and vision reasoning, available via vLLM or Transformers with a system prompt for enhanced performance."
  },
  {
    "model_name": "Phi-3-mini-4k-instruct-gguf",
    "developer": "microsoft",
    "downloads": 63580,
    "createdAt": "2024-04-22T17:02:08.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Phi-3-mini-4k-instruct-fp16",
        "path": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Phi-3-mini-4k-instruct-q4",
        "path": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/README.md",
    "description": "This repo provides the GGUF format for the Phi-3-Mini-4K-Instruct model, a 3.8B parameter, lightweight, state-of-the-art open model trained on high-quality and reasoning dense data, suitable for commercial and research use in English with chat format prompts."
  },
  {
    "model_name": "Devstral-Small-2507-GGUF",
    "developer": "unsloth",
    "downloads": 62492,
    "createdAt": "2025-07-10T13:20:40.000Z",
    "tools": true,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Devstral-Small-2507-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/Devstral-Small-2507-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2507-GGUF/resolve/main/README.md",
    "description": "This model is a lightweight, open-source agentic LLM for software engineering tasks, fine-tuned from Mistral-Small-3.1 with a 128k context window and Apache 2.0 license, supporting tool calling and optional vision, and optimized for local deployment with various"
  },
  {
    "model_name": "Qwen3-8B-GGUF",
    "developer": "Qwen",
    "downloads": 56611,
    "createdAt": "2025-05-03T06:33:59.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "Qwen3-8B-GGUF is a large language model with 8.2B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, multilingual support, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "gemma-3-12b-it-GGUF",
    "developer": "unsloth",
    "downloads": 52596,
    "createdAt": "2025-03-12T10:34:12.000Z",
    "tools": false,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 12B model, optimized for performance and memory efficiency with support for GGUF, 4-bit, and 16-bit formats, and available on Hugging Face."
  },
  {
    "model_name": "Magistral-Small-2506-GGUF",
    "developer": "unsloth",
    "downloads": 51146,
    "createdAt": "2025-06-10T07:31:42.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "Magistral-Small-2506-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "The Magistral-Small-2506 model is a 24B parameter, multilingual reasoning model built on Mistral Small 3.1, offering efficient performance with Apache 2.0 license and supporting deployment via frameworks like llama.cpp, vLLM, and Oll"
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
    "developer": "lmstudio-community",
    "downloads": 48133,
    "createdAt": "2025-07-31T12:09:12.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model by Qwen, optimized for efficient text generation using GGUF quantization from llama.cpp."
  },
  {
    "model_name": "gemma-3n-E4B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 47100,
    "createdAt": "2025-06-26T15:10:43.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E4B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment.",
    "tools": false
  },
  {
    "model_name": "openai_gpt-oss-20b-GGUF",
    "developer": "bartowski",
    "downloads": 44430,
    "createdAt": "2025-08-05T21:29:47.000Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "openai_gpt-oss-20b-IQ2_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ2_XXS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ3_XXS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_NL.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-MXFP4.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q2_K.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_L.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_0",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_1",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_L.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q6_K_L.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q8_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-bf16",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-bf16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "openai_gpt-oss-20b-imatrix",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenAI GPT-oss-20b model, optimized for various inference speeds and resource constraints using llama.cpp with imatrix calibration."
  },
  {
    "model_name": "Qwen3-4B-GGUF",
    "developer": "unsloth",
    "downloads": 44104,
    "createdAt": "2025-04-28T07:55:09.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B is a 4.0B parameter large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes, and is available for fine-tuning and deployment via Hugging Face and"
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 41712,
    "createdAt": "2024-09-17T13:57:52.000Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-1.5b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-fp16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q2_k.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q3_k_m.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf",
        "file_size": "1016.8 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q6_k.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the instruction-tuned 1.5B Qwen2.5 model in GGUF format, offering improved knowledge, coding/math capabilities, long-context support, multilingual support, and various quantization options."
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-GGUF",
    "developer": "lmstudio-community",
    "downloads": 38137,
    "createdAt": "2025-07-28T09:05:17.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/Qwen3-30B-A3B-Instruct-2507-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Qwen3-30B-A3B-Instruct-2507 model by Qwen, suitable for text generation tasks."
  },
  {
    "model_name": "SmallThinker-21BA3B-Instruct-GGUF",
    "developer": "PowerInfer",
    "downloads": 37867,
    "createdAt": "2025-07-27T16:10:02.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct-QAT.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_K_S.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.F16",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.F16.gguf",
        "file_size": "40.1 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_NL",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_NL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K.imatrix.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q3_K_S.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q3_K_S.imatrix.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.imatrix.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_0.powerinfer",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_0.powerinfer.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_1",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_1.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_1.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_1.imatrix.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K.imatrix.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K_S.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q4_K_S.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q4_K_S.imatrix.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q5_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q5_K.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q5_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q5_K.imatrix.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q6_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q6_K.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q6_K.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q6_K.imatrix.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q8_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q8_0.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "SmallThinker-21B-A3B-Instruct.Q8_0.imatrix",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/SmallThinker-21B-A3B-Instruct.Q8_0.imatrix.gguf",
        "file_size": "21.3 GB"
      }
    ],
    "readme": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/resolve/main/README.md",
    "description": "SmallThinker-21BA3B-Instruct-GGUF is a low-latency, on-device MoE language model with 21B parameters, 3B activated parameters, and 52 layers, designed for efficient local deployment using either llama.cpp or PowerInfer frameworks."
  },
  {
    "model_name": "SmolVLM-500M-Instruct-GGUF",
    "developer": "ggml-org",
    "downloads": 37763,
    "createdAt": "2025-04-21T19:02:08.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "SmolVLM-500M-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-Q8_0.gguf",
        "file_size": "416.6 MB"
      },
      {
        "model_id": "SmolVLM-500M-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf",
        "file_size": "782.4 MB"
      },
      {
        "model_id": "mmproj-SmolVLM-500M-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf",
        "file_size": "103.7 MB"
      },
      {
        "model_id": "mmproj-SmolVLM-500M-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf",
        "file_size": "190.2 MB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a 500 million parameter instruction-tuned version of SmolVLM, based on the HuggingFaceTB/SmolVLM-500M-Instruct model."
  },
  {
    "model_name": "sqlcoder-7b-2",
    "developer": "defog",
    "downloads": 36791,
    "createdAt": "2024-02-05T14:36:51.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "sqlcoder-7b-q5_k_m",
        "path": "https://huggingface.co/defog/sqlcoder-7b-2/resolve/main/sqlcoder-7b-q5_k_m.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/defog/sqlcoder-7b-2/resolve/main/README.md",
    "description": "The SQLCoder-7B-2 model is a large language model capable of generating SQL queries from natural language questions, developed by Defog, Inc., based on CodeLlama-7B, and licensed under CC-by-SA-4.0."
  },
  {
    "model_name": "Gemmasutra-Mini-2B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 36558,
    "createdAt": "2024-08-03T09:55:36.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "Gemmasutra-Mini-2B-v1-BF16",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-BF16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_L",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q3_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_4_4",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_4_4.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_4_8",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_4_8.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_0_8_8",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_0_8_8.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q4_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q5_K_S",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-f16",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-f16.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemmasutra-Mini-2B-v1-f32",
        "path": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/Gemmasutra-Mini-2B-v1-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF/resolve/main/README.md",
    "description": "The BeaverAI team presents Gemmasutra Mini 2B v1, a compact 2B parameter RP model designed for efficient use across various devices, offering a satisfying conversational experience with 4K context support and multiple quantized versions for different platforms."
  },
  {
    "model_name": "T-pro-it-2.0-GGUF",
    "developer": "t-tech",
    "downloads": 33551,
    "createdAt": "2025-07-17T21:36:22.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "T-pro-it-2.0-Q4_K_M",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_0",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_K_M",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q5_K_S",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q6_K",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "T-pro-it-2.0-Q8_0",
        "path": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/T-pro-it-2.0-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/t-tech/T-pro-it-2.0-GGUF/resolve/main/README.md",
    "description": "This repository provides the T-pro-it-2.0 model in GGUF format for use with llama.cpp or Ollama, with various quantization options for different hardware requirements."
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "unsloth",
    "downloads": 33473,
    "createdAt": "2025-06-26T12:24:52.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-F16.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ3_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q2_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q4_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q5_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q6_K_XL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q8_K_XL.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with instruction-tuned variants available for fine-tuning"
  },
  {
    "model_name": "Devstral-Small-2505-GGUF",
    "developer": "unsloth",
    "downloads": 32437,
    "createdAt": "2025-05-21T14:20:05.000Z",
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Devstral-Small-2505-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "LFM2-1.2B-GGUF",
    "developer": "LiquidAI",
    "downloads": 32360,
    "createdAt": "2025-07-12T12:01:44.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-1.2B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2-1.2B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2-1.2B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/README.md",
    "description": "LFM2-1.2B-GGUF is a hybrid model by Liquid AI optimized for edge AI and on-device deployment, offering high quality, speed, and memory efficiency."
  },
  {
    "model_name": "gemma-3-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 32176,
    "createdAt": "2025-03-12T09:04:23.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "DeepSeek-R1-Distill-Llama-8B-GGUF",
    "developer": "unsloth",
    "downloads": 31542,
    "createdAt": "2025-01-20T13:04:25.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-F16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q2_K_L.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ1_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ1_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q3_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q6_K_XL.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Llama-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-UD-Q8_K_XL.gguf",
        "file_size": "9.8 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/README.md",
    "description": "The DeepSeek-R1 model, developed by DeepSeek-AI, is a reasoning model based on Llama3.1 and Qwen2.5, offering improved performance on various benchmarks and available for local use with tools like llama.cpp and vLLM, with detailed instructions for training and deployment"
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-GGUF",
    "developer": "dphn",
    "downloads": 30969,
    "createdAt": "2025-01-02T22:11:05.000Z",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-F16",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q2_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_L",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q6_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q8_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/README.md",
    "description": "Dolphin 3.0 Llama 3.1 8B is a general-purpose instruct-tuned model designed for coding, math, agentic, and function calling tasks, offering full control to the user through customizable system prompts and data ownership."
  },
  {
    "model_name": "MN-12B-Mag-Mell-R1-GGUF",
    "developer": "mradermacher",
    "downloads": 29530,
    "createdAt": "2024-09-16T19:10:02.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-12B-Mag-Mell-R1.Q8_0",
        "path": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Qwen3-0.6B-GGUF",
    "developer": "unsloth",
    "downloads": 28594,
    "createdAt": "2025-04-28T10:24:13.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-0.6B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-0.6B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-IQ4_NL.gguf",
        "file_size": "363.9 MB"
      },
      {
        "model_id": "Qwen3-0.6B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-IQ4_XS.gguf",
        "file_size": "350.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q2_K_L.gguf",
        "file_size": "282.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q3_K_M.gguf",
        "file_size": "331.0 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q3_K_S.gguf",
        "file_size": "308.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_0.gguf",
        "file_size": "364.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_1.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_M.gguf",
        "file_size": "378.3 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_S.gguf",
        "file_size": "365.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_M.gguf",
        "file_size": "423.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q5_K_S.gguf",
        "file_size": "416.4 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q6_K.gguf",
        "file_size": "472.2 MB"
      },
      {
        "model_id": "Qwen3-0.6B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf",
        "file_size": "609.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ1_M.gguf",
        "file_size": "210.5 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ1_S.gguf",
        "file_size": "204.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ2_M.gguf",
        "file_size": "256.3 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ2_XXS.gguf",
        "file_size": "223.2 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-IQ3_XXS.gguf",
        "file_size": "269.0 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q2_K_XL.gguf",
        "file_size": "287.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q3_K_XL.gguf",
        "file_size": "340.1 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q4_K_XL.gguf",
        "file_size": "386.6 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q5_K_XL.gguf",
        "file_size": "425.7 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q6_K_XL.gguf",
        "file_size": "549.8 MB"
      },
      {
        "model_id": "Qwen3-0.6B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-UD-Q8_K_XL.gguf",
        "file_size": "805.2 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/README.md",
    "description": "Qwen3-0.6B is a 0.6B parameter large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with support for seamless switching between thinking and non-thinking modes."
  },
  {
    "model_name": "GLM-4.1V-9B-Thinking-GGUF",
    "developer": "unsloth",
    "downloads": 28543,
    "createdAt": "2025-07-25T11:47:05.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "GLM-4.1V-9B-Thinking-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-BF16.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q2_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q3_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q3_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q4_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q5_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q6_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-Q8_0.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ1_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-IQ3_XXS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q2_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q3_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q4_K_XL.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q5_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q6_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4.1V-9B-Thinking-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/GLM-4.1V-9B-Thinking-UD-Q8_K_XL.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF/resolve/main/README.md",
    "description": "This is a 9B-parameter vision-language model (GLM-4.1V-9B-Thinking) that enhances reasoning capabilities through a thinking paradigm and reinforcement learning, achieving state-of-the-art performance on 18 benchmark tasks."
  },
  {
    "model_name": "gemma-3-1b-it-GGUF",
    "developer": "unsloth",
    "downloads": 27320,
    "createdAt": "2025-03-12T10:57:04.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-BF16.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3-1b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-IQ4_NL.gguf",
        "file_size": "688.4 MB"
      },
      {
        "model_id": "gemma-3-1b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-IQ4_XS.gguf",
        "file_size": "681.3 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q2_K.gguf",
        "file_size": "657.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q2_K_L.gguf",
        "file_size": "657.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q3_K_M.gguf",
        "file_size": "688.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q3_K_S.gguf",
        "file_size": "656.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_0.gguf",
        "file_size": "688.5 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_1.gguf",
        "file_size": "728.6 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf",
        "file_size": "768.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_S.gguf",
        "file_size": "744.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q5_K_M.gguf",
        "file_size": "811.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q5_K_S.gguf",
        "file_size": "797.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q6_K.gguf",
        "file_size": "964.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q8_0.gguf",
        "file_size": "1019.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ1_M.gguf",
        "file_size": "533.9 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ1_S.gguf",
        "file_size": "531.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ2_M.gguf",
        "file_size": "551.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ2_XXS.gguf",
        "file_size": "538.3 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-IQ3_XXS.gguf",
        "file_size": "564.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q2_K_XL.gguf",
        "file_size": "661.7 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q3_K_XL.gguf",
        "file_size": "693.2 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q4_K_XL.gguf",
        "file_size": "769.6 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q5_K_XL.gguf",
        "file_size": "833.8 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q6_K_XL.gguf",
        "file_size": "983.3 MB"
      },
      {
        "model_id": "gemma-3-1b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-UD-Q8_K_XL.gguf",
        "file_size": "1.4 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "bartowski",
    "downloads": 27315,
    "createdAt": "2025-06-20T19:03:22.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistral-Small-3.2-24B-Instruct-2506 model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "tools": true
  },
  {
    "model_name": "Nous-Hermes-2-Mistral-7B-DPO-GGUF",
    "developer": "NousResearch",
    "downloads": 27040,
    "createdAt": "2024-02-20T06:25:05.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q2_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q6_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q8_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/README.md",
    "description": "Nous-Hermes 2 Mistral 7B DPO is a 7B parameter model fine-tuned with DPO from Teknium/OpenHermes-2.5-Mistral-7B, showing improved performance on AGIEval, BigBench, GPT4",
    "tools": false
  },
  {
    "model_name": "Ministral-8B-Instruct-2410-GGUF",
    "developer": "bartowski",
    "downloads": 24364,
    "createdAt": "2024-10-21T16:27:21.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ2_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K_L.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q8_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-f16",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-f16.gguf",
        "file_size": "14.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Mistral-8B-Instruct-2410 model for various inference platforms, with a focus on research use under the Mistral AI Research License, requiring users to agree to specific terms and conditions for non-commercial purposes."
  },
  {
    "model_name": "Qwen3-14B-GGUF",
    "developer": "Qwen",
    "downloads": 24180,
    "createdAt": "2025-05-01T10:25:24.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-14B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_0.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen3-14B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/README.md",
    "description": "Qwen3-14B-GGUF is a large language model with 14.8B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, multilingual support, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "NemoMix-Unleashed-12B-GGUF",
    "developer": "bartowski",
    "downloads": 23568,
    "createdAt": "2024-08-22T04:03:49.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "NemoMix-Unleashed-12B-IQ2_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ3_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q2_K",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q2_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q3_K_XL.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q6_K",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-Q8_0",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "NemoMix-Unleashed-12B-f16",
        "path": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-f16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the NemoMix-Unleashed-12B model for efficient text generation on various platforms, with recommendations based on system resources and performance needs."
  },
  {
    "model_name": "medgemma-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 23406,
    "createdAt": "2025-05-20T19:18:08.000Z",
    "tools": false,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "medgemma-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "medgemma-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/medgemma-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/medgemma-4b-it-GGUF/resolve/main/README.md",
    "description": "MedGemma is a medical multimodal model trained on diverse medical data, optimized for healthcare applications involving text and images, with variants available for radiology, dermatology, pathology, and more."
  },
  {
    "model_name": "Nanonets-OCR-s-GGUF",
    "developer": "unsloth",
    "downloads": 23229,
    "createdAt": "2025-06-16T10:35:11.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/README.md",
    "description": "Nanonets-OCR-s is a state-of-the-art image-to-markdown OCR model that extracts text, equations, tables, and other structured elements from documents, converting them into semantic markdown for efficient processing by LLMs.",
    "tools": false
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
    "developer": "unsloth",
    "downloads": 21837,
    "createdAt": "2025-01-20T13:47:45.000Z",
    "tools": true,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-BF16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf",
        "file_size": "770.2 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_M.gguf",
        "file_size": "683.1 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ1_S.gguf",
        "file_size": "667.7 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_M.gguf",
        "file_size": "755.5 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ2_XXS.gguf",
        "file_size": "739.0 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ3_XXS.gguf",
        "file_size": "841.2 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-IQ4_XS.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q2_K_XL.gguf",
        "file_size": "844.6 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q3_K_XL.gguf",
        "file_size": "923.4 MB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/README.md",
    "description": "This model is a 1.5B parameter version of DeepSeek-R1, fine-tuned for faster inference with 70% less memory using Unsloth, and can be run with llama.cpp for efficient performance."
  },
  {
    "model_name": "Qwen3-1.7B-GGUF",
    "developer": "unsloth",
    "downloads": 21135,
    "createdAt": "2025-04-28T12:22:37.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-1.7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K_L.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_M.gguf",
        "file_size": "535.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_S.gguf",
        "file_size": "512.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_M.gguf",
        "file_size": "675.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_XXS.gguf",
        "file_size": "577.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ3_XXS.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q2_K_XL.gguf",
        "file_size": "760.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q3_K_XL.gguf",
        "file_size": "924.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q5_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q6_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q8_K_XL.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/README.md",
    "description": "Qwen3-1.7B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes for optimal performance."
  },
  {
    "model_name": "Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF",
    "developer": "DavidAU",
    "downloads": 20804,
    "createdAt": "2025-02-12T00:29:33.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-IQ4_XS.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q2_k.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_l.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_m.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q3_k_s.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_m.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q4_k_s.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q5_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q6_k.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-Q8_0.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/L3.2-8X4B-MOE-V2-Dark-Champion-Inst-21B-uncen-ablit-D_AU-q5_k_m.gguf",
        "file_size": "13.9 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF model, a powerful mixture of 8 top L3.2 4B models combined into a 21"
  },
  {
    "model_name": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF",
    "developer": "bartowski",
    "downloads": 20255,
    "createdAt": "2025-05-09T15:21:21.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dolphin-Mistral-24B-Venice-Edition model by cognitivecomputations, optimized for various inference speeds and quality levels using llama.cpp's imatrix quantization method.",
    "tools": false
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 19922,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored Gemma 3 27B IT model created using a layerwise abliteration technique to achieve high acceptance rates while preserving model capabilities."
  },
  {
    "model_name": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF",
    "developer": "QuantFactory",
    "downloads": 19736,
    "createdAt": "2024-07-28T07:02:48.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/README.md",
    "description": "The QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF is a quantized, uncensored, and multilingual version of the Meta Llama 3.1 8B model, optimized for roleplay, instruction"
  },
  {
    "model_name": "oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF",
    "developer": "mradermacher",
    "downloads": 18269,
    "createdAt": "2025-01-15T09:26:16.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q2_K",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q6_K",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q8_0",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "oh-dcft-v3.1-claude-3-5-sonnet-20241022.f16",
        "path": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/oh-dcft-v3.1-claude-3-5-sonnet-20241022.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/oh-dcft-v3.1-claude-3-5-sonnet-20241022-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF",
    "developer": "DavidAU",
    "downloads": 17853,
    "createdAt": "2024-10-25T00:19:23.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a highly advanced, uncensored Gemma2 model designed for creative writing, particularly fiction and storytelling, combining four top storytelling models with additional modifications for vivid, detailed, and emotionally engaging prose across various genres, including science fiction, horror, and romance, with options for different quantization levels and"
  },
  {
    "model_name": "L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 16940,
    "createdAt": "2024-06-05T18:21:00.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "test",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/test.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a quantized version of the Sao10K/L3-8B-Stheno-v3.2 model for roleplay and sillytavern tasks, optimized for performance with specific quantization settings and recommended samplers."
  },
  {
    "model_name": "llama-joycaption-beta-one-hf-llava-GGUF",
    "developer": "Mungert",
    "downloads": 15206,
    "createdAt": "2025-06-08T03:11:55.000Z",
    "tools": false,
    "num_quants": 45,
    "quants": [
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-bf16",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-bf16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-bf16_q4_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-bf16_q6_k.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-bf16_q8_0.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-f16",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-f16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-f16_q4_k",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-f16_q4_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-f16_q6_k",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-f16_q6_k.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-f16_q8_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-f16_q8_0.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq1_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq1_m.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq1_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq1_s.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq2_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq2_m.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq2_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq2_s.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq2_xs",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq2_xs.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq2_xxs",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq2_xxs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq3_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq3_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq3_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq3_xs",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq3_xs.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq3_xxs",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq3_xxs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq4_nl",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq4_nl.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-iq4_xs",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-iq4_xs.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q2_k_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q2_k_l.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q2_k_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q2_k_m.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q2_k_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q3_k_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q3_k_l.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q3_k_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q3_k_m.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q3_k_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q3_k_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_0_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_0_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_1",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_1.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_1_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_1_l.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_k_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_k_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_k_m.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q4_k_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q4_k_s.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_0_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_0_l.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_1",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_1_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_1_l.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_k_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_k_l.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_k_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_k_m.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q5_k_s",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q5_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q6_k_l",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q6_k_l.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q6_k_m",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q6_k_m.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-q8_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-tq1_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-tq1_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-hf-llava-tq2_0",
        "path": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/llama-joycaption-beta-one-hf-llava-tq2_0.gguf",
        "file_size": "2.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/llama-joycaption-beta-one-hf-llava-GGUF/resolve/main/README.md",
    "description": "This model is a free, open, and uncensored image captioning Visual Language Model (VLM) built for the community to use in training Diffusion models, offering near-GPT4o performance with various quantization options for different hardware capabilities."
  },
  {
    "model_name": "Devstral-Small-2507_gguf",
    "developer": "mistralai",
    "downloads": 15088,
    "createdAt": "2025-07-07T12:22:38.000Z",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Devstral-Small-2507-BF16",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Devstral-Small-2507_gguf/resolve/main/README.md",
    "description": "The Devstral Small 1.1 model is a lightweight, agentic LLM with a 128k context window, Apache 2.0 license, and supports GGUF quantized versions for efficient local deployment and use in software engineering tasks."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 15072,
    "createdAt": "2025-05-11T13:03:32.000Z",
    "tools": false,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Qwen2.5-VL-7B-Instruct model is a versatile vision-language model that supports image-text understanding, video analysis, structured output generation, and agent-like reasoning, with enhanced performance through dynamic resolution and frame rate training, efficient vision encoders, and compatibility with various visual input"
  },
  {
    "model_name": "Jan-nano-128k-GGUF",
    "developer": "unsloth",
    "downloads": 14760,
    "createdAt": "2025-06-25T07:31:40.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-128k-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling efficient processing of long documents and complex multi-turn conversations without performance degradation.",
    "tools": true
  },
  {
    "model_name": "SmolLM3-3B-GGUF",
    "developer": "ggml-org",
    "downloads": 13891,
    "createdAt": "2025-07-08T12:36:39.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "SmolLM3-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SmolLM3-f16",
        "path": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/SmolLM3-f16.gguf",
        "file_size": "5.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/resolve/main/README.md",
    "description": "SmolLM3-GGUF is a 3B parameter multilingual language model with advanced reasoning, long context support, and open-source licensing, offering strong performance across various benchmarks and supported by frameworks like transformers and llama.cpp."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 13583,
    "createdAt": "2024-10-26T00:23:07.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ3_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ3_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-IQ4_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q2_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q2_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_4_4.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_4_8.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_0_8_8.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q5_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q8_0.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-uncensored-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-f16.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct-uncensored model using llama.cpp's imatrix method, optimized for various hardware platforms with different quality and performance trade-offs."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "Qwen",
    "downloads": 13556,
    "createdAt": "2025-05-05T08:38:52.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "Qwen3-30B-A3B-GGUF is a large language model with 30.5B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning, multilingual capabilities, and efficient inference via quantization methods like Q8_0."
  },
  {
    "model_name": "osmosis-mcp-4b",
    "developer": "osmosis-ai",
    "downloads": 13296,
    "createdAt": "2025-05-08T18:46:48.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "osmosis-mcp-4B-BF16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-BF16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-F16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/README.md",
    "description": "Osmosis-MCP-4B is a Qwen3-4B model fine-tuned with reinforcement learning to excel at multi-step tool usage in a curriculum-based training approach, offering a practical, open-source solution for MCP-style agents.",
    "tools": true
  },
  {
    "model_name": "Phi-4-reasoning-plus-GGUF",
    "developer": "unsloth",
    "downloads": 12844,
    "createdAt": "2025-05-01T02:04:54.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Phi-4-reasoning-plus-BF16",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-BF16.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-IQ4_NL.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-IQ4_XS.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q2_K",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q2_K_L.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_0",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_1",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q6_K",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-Q8_0",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ2_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ2_XXS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-IQ3_XXS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q2_K_XL.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q3_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q4_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q5_K_XL.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q6_K_XL.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Phi-4-reasoning-plus-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/Phi-4-reasoning-plus-UD-Q8_K_XL.gguf",
        "file_size": "16.8 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/resolve/main/README.md",
    "description": "Phi-4-reasoning-plus is a 14B parameter, dense decoder-only Transformer model trained for reasoning tasks with enhanced accuracy and token generation, suitable for math, science, and coding, and can be fine-tuned or run via Colab, Ollama, llama.cpp, or"
  },
  {
    "model_name": "MN-Violet-Lotus-12B-GGUF",
    "developer": "mradermacher",
    "downloads": 12717,
    "createdAt": "2024-11-17T05:53:19.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "MN-Violet-Lotus-12B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q2_K",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_0_4_4",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_0_4_4.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q6_K",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-Violet-Lotus-12B.Q8_0",
        "path": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF",
    "developer": "bartowski",
    "downloads": 12567,
    "createdAt": "2024-09-26T14:36:19.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ2_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_M.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ3_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q2_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q3_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_4.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_4_8.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_0_8_8.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q6_K_L.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-Q8_0",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-Q8_0.gguf",
        "file_size": "22.0 GB"
      },
      {
        "model_id": "Mistral-Small-22B-ArliAI-RPMax-v1.1-f16",
        "path": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/Mistral-Small-22B-ArliAI-RPMax-v1.1-f16.gguf",
        "file_size": "41.4 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Mistral-Small-22B-ArliAI-RPMax-v1.1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the Mistral-Small-22B-ArliAI-RPMax-v1.1 model for text generation, with options ranging from high-quality Q8_0 to low-quality IQ2_S, optimized for different hardware and performance needs."
  },
  {
    "model_name": "TheDrummer_Cydonia-24B-v4-GGUF",
    "developer": "bartowski",
    "downloads": 12454,
    "createdAt": "2025-07-18T17:30:25.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ2_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "TheDrummer_Cydonia-24B-v4-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/TheDrummer_Cydonia-24B-v4-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Cydonia-24B-v4 model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs."
  },
  {
    "model_name": "Qwen2.5-Coder-32B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 12273,
    "createdAt": "2024-11-06T19:20:14.000Z",
    "tools": true,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_4.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_8_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen2.5-Coder-32B-Instruct model using llama.cpp's imatrix method, optimized for various hardware platforms and inference speeds, with recommended options marked for best performance and quality."
  },
  {
    "model_name": "SmallThinker-4BA0.6B-Instruct-GGUF",
    "developer": "PowerInfer",
    "downloads": 12199,
    "createdAt": "2025-07-27T16:09:24.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.F16",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.F16.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.IQ4_NL",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.IQ4_NL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.IQ4_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q3_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q3_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_0.powerinfer",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_0.powerinfer.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_1",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q4_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q5_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q5_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q6_K",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q6_K.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "SmallThinker-4B-A0.6B-Instruct.Q8_0",
        "path": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/SmallThinker-4B-A0.6B-Instruct.Q8_0.gguf",
        "file_size": "4.2 GB"
      }
    ],
    "readme": "https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF/resolve/main/README.md",
    "description": "SmallThinker-4BA0.6B-Instruct-GGUF is a low-latency, on-device Mixture-of-Experts language model designed for efficient local deployment with support for both llama.cpp and PowerInfer frameworks."
  },
  {
    "model_name": "google_gemma-3n-E4B-it-GGUF",
    "developer": "bartowski",
    "downloads": 12158,
    "createdAt": "2025-06-26T19:50:49.000Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs.",
    "tools": false
  },
  {
    "model_name": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF",
    "developer": "bartowski",
    "downloads": 11895,
    "createdAt": "2025-05-23T18:09:44.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the PocketDoc/Dans-PersonalityEngine-V1.3.0-24b model for efficient text generation on various hardware, including CPUs and GPUs, with options for different quantization levels and formats like Q6_K_L, Q4_K_M,",
    "tools": false
  },
  {
    "model_name": "Magistral-Small-2507-GGUF",
    "developer": "unsloth",
    "downloads": 11751,
    "createdAt": "2025-07-24T21:29:04.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Magistral-Small-2507-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/Magistral-Small-2507-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2507-GGUF/resolve/main/README.md",
    "description": "Magistral Small 1.1 is a multilingual, efficient reasoning model with 24B parameters, supporting dozens of languages and offering SOTA performance in model quantization, optimized for deployment on a single RTX 4090 or 32GB RAM MacBook."
  },
  {
    "model_name": "Cydonia-24B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 11660,
    "createdAt": "2025-07-01T19:33:48.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4h-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4h-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/Cydonia-24B-v4h-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-24B-v4-GGUF/resolve/main/README.md",
    "description": "Cydonia 24B v4 is a wordy and thick model with a novel style, excelling at long-form storytelling and descriptive tasks, and is dedicated to the memory of SleepDeprived, a beloved community member."
  },
  {
    "model_name": "Qwen2.5-Omni-7B-GGUF",
    "developer": "unsloth",
    "downloads": 11321,
    "createdAt": "2025-05-28T19:15:45.000Z",
    "tools": false,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-Omni-7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/Qwen2.5-Omni-7B-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "4.9 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF/resolve/main/README.md",
    "description": "Qwen2.5-Omni is a state-of-the-art end-to-end multimodal model that excels in text, image, audio, and video understanding and generation, outperforming other models in various benchmarks and tasks, with support for audio output and customizable voice types."
  },
  {
    "model_name": "gemma-3-12b-it-qat-GGUF",
    "developer": "unsloth",
    "downloads": 10763,
    "createdAt": "2025-04-21T03:55:27.000Z",
    "tools": false,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-qat-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-qat-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/gemma-3-12b-it-qat-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF/resolve/main/README.md",
    "description": "This repository contains the unquantized 12B instruction-tuned Gemma 3 model from Google, suitable for text and image generation tasks with a 128K context window and open weights, but requires acknowledgment of Google's usage license for access."
  },
  {
    "model_name": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF",
    "developer": "DavidAU",
    "downloads": 10721,
    "createdAt": "2024-10-28T10:09:51.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m.gguf",
        "file_size": "5.0 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/README.md",
    "description": "This is a modified Llama 3.2 model designed for creative writing, with enhanced prose generation, uncensored content, and support for multiple genres including science fiction, horror, and romance, offering vivid, detailed, and emotionally charged outputs through advanced parameters and templates."
  },
  {
    "model_name": "phi-4-gguf",
    "developer": "microsoft",
    "downloads": 10297,
    "createdAt": "2025-01-08T19:55:31.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "phi-4-IQ2_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ2_M.gguf",
        "file_size": "561.6 MB"
      },
      {
        "model_id": "phi-4-IQ3_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "phi-4-IQ3_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-IQ3_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "phi-4-IQ3_XXS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XXS.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "phi-4-IQ4_NL",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_NL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-IQ4_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_XS.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "phi-4-Q2_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "phi-4-Q3_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "phi-4-Q3_K_L",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "phi-4-Q3_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-Q4_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "phi-4-Q4_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "phi-4-Q4_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "phi-4-Q4_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-Q5_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_0.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q5_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_1.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "phi-4-Q5_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "phi-4-Q5_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q6_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "phi-4-Q8_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "phi-4-TQ1_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ1_0.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "phi-4-TQ2_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ2_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "phi-4-bf16",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-bf16.gguf",
        "file_size": "27.3 GB"
      }
    ],
    "readme": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/README.md",
    "description": "The Phi-4 model is a 14B parameter, English-focused, chat-oriented, decoder-only Transformer model trained on high-quality synthetic and academic data, designed for reasoning, code generation, and factual responses in memory-constrained environments."
  },
  {
    "model_name": "claude-3.7-sonnet-reasoning-gemma3-12B",
    "developer": "reedmayhew",
    "downloads": 9967,
    "createdAt": "2025-03-22T19:40:09.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0",
        "path": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "readme": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/README.md",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL."
  },
  {
    "model_name": "MS3.2-24B-Magnum-Diamond-GGUF",
    "developer": "Doctor-Shotgun",
    "downloads": 9829,
    "createdAt": "2025-06-22T18:07:51.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_NL",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_L",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q6_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q8_0",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-bf16",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Doctor-Shotgun/MS3.2-24B-Magnum-Diamond model, optimized for creative writing and roleplay with specific SillyTavern presets provided.",
    "tools": false
  },
  {
    "model_name": "Llama-3-8B-Lexi-Uncensored-GGUF",
    "developer": "Orenguteng",
    "downloads": 9496,
    "createdAt": "2024-04-23T21:57:52.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_F16",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q4_K_M",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q5_K_M",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored_Q8_0",
        "path": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 8867,
    "createdAt": "2025-06-30T05:25:35.000Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "ERNIE-4.5-0.3B-PT-F16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-F16.gguf",
        "file_size": "690.4 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.0 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "462.6 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-0.3B is a text dense, Apache-2.0 licensed large language model optimized for text generation and fine-tuning with ERNIEKit and FastDeploy, and supported by the transformers library.",
    "tools": false
  },
  {
    "model_name": "stable-code-3b",
    "developer": "stabilityai",
    "downloads": 8834,
    "createdAt": "2024-01-09T02:03:58.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "stable-code-3b-Q5_K_M",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q5_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "stable-code-3b-Q6_K",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q6_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "stable-code-3b",
        "path": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b.gguf",
        "file_size": "5.2 GB"
      }
    ],
    "readme": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/README.md",
    "description": "Stable Code 3B is a 3B parameter decoder-only language model pre-trained on diverse code and text datasets, excelling in code generation across multiple programming languages with state-of-the-art performance on MultiPL-E metrics."
  },
  {
    "model_name": "Llama-OuteTTS-1.0-1B-GGUF",
    "developer": "OuteAI",
    "downloads": 8609,
    "createdAt": "2025-04-06T19:18:26.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Llama-OuteTTS-1.0-1B-FP16",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-FP16.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q2_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q2_K.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_L",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_L.gguf",
        "file_size": "708.6 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_M.gguf",
        "file_size": "668.8 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_S.gguf",
        "file_size": "622.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_0.gguf",
        "file_size": "745.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_1.gguf",
        "file_size": "803.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_M.gguf",
        "file_size": "780.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_S.gguf",
        "file_size": "749.7 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_0.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_1.gguf",
        "file_size": "919.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_M.gguf",
        "file_size": "879.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_S.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q6_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q6_K.gguf",
        "file_size": "984.5 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q8_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 8215,
    "createdAt": "2025-03-20T22:44:27.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, multimodal AI model from Google, capable of handling text and image inputs to generate text outputs, trained on diverse data including text, code, math, and images, with performance metrics across various benchmarks and languages, and available in multiple sizes"
  },
  {
    "model_name": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF",
    "developer": "DavidAU",
    "downloads": 8154,
    "createdAt": "2025-07-17T07:28:04.000Z",
    "tools": true,
    "num_quants": 81,
    "quants": [
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "325.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "310.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "292.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "274.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "417.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "383.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "345.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "482.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "462.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "360.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "338.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "463.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "432.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "483.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "521.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "503.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "484.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "561.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "600.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "572.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "560.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "644.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-D_AU-Q8_0.gguf",
        "file_size": "833.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "325.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "310.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "292.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "274.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "417.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "383.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "345.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "482.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "462.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "360.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "338.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "463.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "432.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "398.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "483.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "521.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "503.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "484.5 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "561.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "600.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "572.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "560.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "644.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO2-EX-D_AU-Q8_0.gguf",
        "file_size": "797.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-BF16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-BF16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-F16",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-F16.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_M-imat.gguf",
        "file_size": "302.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_S-imat.gguf",
        "file_size": "286.8 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XS-imat.gguf",
        "file_size": "269.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "251.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_M-imat.gguf",
        "file_size": "374.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_S-imat.gguf",
        "file_size": "355.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XS-imat.gguf",
        "file_size": "341.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "322.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_NL-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_NL-imat.gguf",
        "file_size": "439.3 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-IQ4_XS-imat.gguf",
        "file_size": "419.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K-imat.gguf",
        "file_size": "317.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q2_K_S-imat.gguf",
        "file_size": "295.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_L-imat.gguf",
        "file_size": "420.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_M-imat.gguf",
        "file_size": "390.1 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q3_K_S-imat.gguf",
        "file_size": "355.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_0-imat.gguf",
        "file_size": "440.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_1-imat.gguf",
        "file_size": "478.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_M-imat.gguf",
        "file_size": "460.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q4_K_S-imat.gguf",
        "file_size": "441.6 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_0-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_0-imat.gguf",
        "file_size": "518.9 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_1-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_1-imat.gguf",
        "file_size": "557.4 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_M-imat.gguf",
        "file_size": "529.2 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q5_K_S-imat.gguf",
        "file_size": "518.0 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q6_K-imat.gguf",
        "file_size": "601.7 MB"
      },
      {
        "model_id": "Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/Qwen3-Zero-Coder-Reasoning-0.8B-NEO3-EX-D_AU-Q8_0.gguf",
        "file_size": "754.3 MB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B-NEO-EX-GGUF/resolve/main/README.md",
    "description": "This is a high-performance coder model based on Qwen3, capable of generating code and reasoning blocks efficiently, with optimized variants for different quantization levels and use cases."
  },
  {
    "model_name": "Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf",
    "developer": "DavidAU",
    "downloads": 8033,
    "createdAt": "2024-12-26T11:22:27.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-IQ4_XS.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q2_k.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_l.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_s.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_s.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q5_k_s.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q6_k.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q8_0.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-q5_k_m.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q2_k.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q6_k.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-D_AU-Q8_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "18.7 GB"
      },
      {
        "model_id": "M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "24.1 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-gguf/resolve/main/README.md",
    "description": "This is an uncensored, vividly detailed, and highly creative Mistral-MOE-4X7B-Dark-MultiVerse-Uncensored-Enhanced32-24B-GGUF model that combines four Mistral 7B models into a 24B parameter model"
  },
  {
    "model_name": "GLM-4-9B-0414-GGUF",
    "developer": "unsloth",
    "downloads": 7877,
    "createdAt": "2025-04-30T18:36:06.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "GLM-4-9B-0414-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-BF16.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q2_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q3_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q3_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q4_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q5_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q6_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-Q8_0.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ1_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-IQ3_XXS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q2_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q3_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q4_K_XL.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q5_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q6_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4-9B-0414-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/GLM-4-9B-0414-UD-Q8_K_XL.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/GLM-4-9B-0414-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "EXAONE-4.0-1.2B-GGUF",
    "developer": "LGAI-EXAONE",
    "downloads": 7853,
    "createdAt": "2025-07-11T07:03:24.000Z",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "EXAONE-4.0-1.2B-BF16",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-BF16.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-IQ4_XS",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-IQ4_XS.gguf",
        "file_size": "718.9 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q4_K_M",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q4_K_M.gguf",
        "file_size": "774.8 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q5_K_M",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q5_K_M.gguf",
        "file_size": "886.6 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q6_K",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q6_K.gguf",
        "file_size": "1005.3 MB"
      },
      {
        "model_id": "EXAONE-4.0-1.2B-Q8_0",
        "path": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/EXAONE-4.0-1.2B-Q8_0.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "readme": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF/resolve/main/README.md",
    "description": "The EXAONE-4.0-1.2B-GGUF model is a multilingual, hybrid reasoning model that combines non-reasoning and reasoning modes, offering high performance with support for Korean, English, and Spanish, and is available in GGUF format with various quantization options"
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 7796,
    "createdAt": "2025-08-02T05:35:18.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Instruct-2507 model, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "XortronCriminalComputingConfig-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 7004,
    "createdAt": "2025-05-05T07:31:49.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/XortronCriminalComputingConfig.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides various GGUF quantized versions of the darkc0de/XortronCriminalComputingConfig model, ranging from low-quality to high-quality, with notes on their performance and suitability for different use cases."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 6613,
    "createdAt": "2025-05-28T22:03:00.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the Google Gemma-3-27B IT model, created using an advanced abliteration technique to enhance refusal accuracy while maintaining coherent outputs.",
    "tools": false
  },
  {
    "model_name": "SpaceThinker-Qwen2.5VL-3B",
    "developer": "remyxai",
    "downloads": 6457,
    "createdAt": "2025-04-17T17:34:23.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gguf/spacethinker-qwen2.5VL-3B-F16",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5VL-3B-F16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gguf/spacethinker-qwen2.5vl-3b-vision",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5vl-3b-vision.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/README.md",
    "description": "SpaceThinker-Qwen2.5VL-3B is a multimodal vision-language model trained to enhance spatial reasoning through test-time compute, achieving strong performance on various spatial reasoning benchmarks like SpatialScore and QSpatial-Bench with a focus on quantitative spatial tasks such as distance estimation and object relations.",
    "tools": true
  },
  {
    "model_name": "Qwen2.5-Coder-3B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 6433,
    "createdAt": "2024-11-09T12:46:15.000Z",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-coder-3b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-fp16.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q2_k.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q3_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q4_k_m.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q5_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q5_k_m.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q6_k.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "qwen2.5-coder-3b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/qwen2.5-coder-3b-instruct-q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the instruction-tuned 3B Qwen2.5-Coder model in GGUF format, optimized for code generation, reasoning, and fixing with 3.09B parameters and full 32,768 token context length."
  },
  {
    "model_name": "llama-joycaption-beta-one-hf-llava-mmproj-gguf",
    "developer": "concedo",
    "downloads": 6412,
    "createdAt": "2025-05-15T13:36:04.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-F16",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-Q4_K",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-Joycaption-Beta-One-Hf-Llava-Q8_0",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/Llama-Joycaption-Beta-One-Hf-Llava-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama-joycaption-beta-one-llava-mmproj-model-f16",
        "path": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/llama-joycaption-beta-one-llava-mmproj-model-f16.gguf",
        "file_size": "837.1 MB"
      }
    ],
    "readme": "https://huggingface.co/concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "gemma-3-4b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 6254,
    "createdAt": "2025-03-12T20:43:28.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mmproj-model-f16-4B",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-4B.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 4B model is a lightweight, open-source, instruction-tuned multimodal model from Google, capable of handling text and image inputs, generating text outputs, and trained on diverse data including text, code, math, and images, with strong performance across various benchmarks and tasks"
  },
  {
    "model_name": "Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF",
    "developer": "DavidAU",
    "downloads": 5956,
    "createdAt": "2024-12-12T01:30:01.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/README.md",
    "description": "This model is a powerful Llama 3.2 MOE with 10B parameters, combining four top Llama 3.2 3B models for exceptional creative writing, vivid prose, and uncensored output across all genres, including horror, science fiction, romance, and roleplay"
  },
  {
    "model_name": "bitnet-b1.58-2B-4T-gguf",
    "developer": "microsoft",
    "downloads": 5610,
    "createdAt": "2025-04-15T04:25:42.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ggml-model-i2_s",
        "path": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "openai_gpt-oss-20b-GGUF-MXFP4-Experimental",
    "developer": "bartowski",
    "downloads": 5561,
    "createdAt": "2025-08-05T17:43:11.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "openai_gpt-oss-20b-MXFP4",
        "path": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental/resolve/main/openai_gpt-oss-20b-MXFP4.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental/resolve/main/README.md",
    "description": "This is an experimental MXFP4 quantized version of the gpt-oss-20b model by OpenAI, requiring the llama.cpp branch `gpt-oss-mxfp4` for use."
  },
  {
    "model_name": "SmolLM3-3B-128K-GGUF",
    "developer": "unsloth",
    "downloads": 5540,
    "createdAt": "2025-07-08T23:13:59.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "SmolLM3-3B-128K-BF16",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-BF16.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-IQ4_NL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-IQ4_XS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q2_K",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q2_K_L",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q2_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q3_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q3_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_0",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_1",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_1.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q4_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q5_K_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q5_K_S",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q6_K",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-Q8_0",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ2_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ2_XXS.gguf",
        "file_size": "910.9 MB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q2_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q3_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q4_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q5_K_XL.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q6_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "SmolLM3-3B-128K-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/SmolLM3-3B-128K-UD-Q8_K_XL.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/SmolLM3-3B-128K-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "mistralai_Magistral-Small-2506-GGUF",
    "developer": "bartowski",
    "downloads": 5525,
    "createdAt": "2025-06-10T16:09:49.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2506 model by bartowski, optimized for various inference speeds and quality levels using llama.cpp.",
    "tools": false
  },
  {
    "model_name": "Huihui-gemma-3n-E4B-it-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 5503,
    "createdAt": "2025-07-11T00:48:21.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Huihui-gemma-3n-E4B-it-abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/Huihui-gemma-3n-E4B-it-abliterated.f16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-gemma-3n-E4B-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-gemma-3n-E4B-it-abliterated model, available in various GGUF quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "Falcon-H1-7B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 5488,
    "createdAt": "2025-05-13T15:59:29.000Z",
    "tools": true,
    "num_quants": 34,
    "quants": [
      {
        "model_id": "Falcon-H1-7B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-BF16.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-F16.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-F32.gguf",
        "file_size": "28.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ1_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ2_XXS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-TQ1_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Falcon-H1-7B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/Falcon-H1-7B-Instruct-TQ2_0.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Falcon-H1 series is a hybrid Transformers + Mamba architecture, multilingual, causal decoder-only language model developed by TII, licensed under the Falcon-LLM License, and available for inference via Hugging Face Transformers, vLLM, or llama.cpp."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 5414,
    "createdAt": "2025-01-26T02:35:09.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-GGUF",
    "developer": "lmstudio-community",
    "downloads": 5275,
    "createdAt": "2024-06-17T18:01:28.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-IQ3_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-IQ3_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-IQ4_XS",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-IQ4_XS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q3_K_L.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q6_K.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "DeepSeek-Coder-V2-Lite-Instruct-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/DeepSeek-Coder-V2-Lite-Instruct-Q8_0.gguf",
        "file_size": "15.6 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a brand new MoE model from DeepSeek, specializing in coding instructions and offering excellent inference speed with 16B total weights and 2.4B activated, suitable for both instruction following and code completion."
  },
  {
    "model_name": "POLARIS-Project_Polaris-4B-Preview-GGUF",
    "developer": "bartowski",
    "downloads": 5255,
    "createdAt": "2025-06-24T04:38:50.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ2_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_NL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_1",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q8_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-bf16",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-bf16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Polaris-4B-Preview model using llama.cpp, offering various quantization types for different performance and quality trade-offs.",
    "tools": false
  },
  {
    "model_name": "HuggingFaceTB_SmolLM3-3B-GGUF",
    "developer": "bartowski",
    "downloads": 5125,
    "createdAt": "2025-07-08T16:09:26.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ2_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ2_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ3_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q2_K",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q2_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q2_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q3_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_0",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_1",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_1.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q5_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q6_K",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q6_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-Q8_0",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "HuggingFaceTB_SmolLM3-3B-bf16",
        "path": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/HuggingFaceTB_SmolLM3-3B-bf16.gguf",
        "file_size": "5.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/HuggingFaceTB_SmolLM3-3B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Tifa-DeepsexV3-14b-GGUF-Q6",
    "developer": "ValueFX9507",
    "downloads": 4963,
    "createdAt": "2025-06-26T10:15:21.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/README.md",
    "description": "Tifa-DeepSexV3-14b 是基于 Qwen14b 的深度优化模型，支持长文生成、超长关联、控制器调节输出风格和字数，并能避免负面词汇，适用于角色扮演和多种文本生成任务。"
  },
  {
    "model_name": "MiniCPM-V-4-gguf",
    "developer": "openbmb",
    "downloads": 4852,
    "createdAt": "2025-07-12T09:45:29.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_1.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "mmproj-model-f16-iOS",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16-iOS.gguf",
        "file_size": "128.8 MB"
      },
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16.gguf",
        "file_size": "914.4 MB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/README.md",
    "description": "The MiniCPM-V 4.0 is a high-performance, efficient, and versatile multimodal large language model capable of understanding and generating text from single images, multiple images, and videos on mobile devices, with 4.1B parameters and superior performance compared to other models in various benchmarks."
  },
  {
    "model_name": "Qwen2.5-Omni-3B-GGUF",
    "developer": "ggml-org",
    "downloads": 4844,
    "createdAt": "2025-05-26T08:52:28.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen2.5-Omni-3B-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-3B-Q8_0",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q8_0.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen2.5-Omni-3B-f16",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-f16.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "mmproj-Qwen2.5-Omni-3B-Q8_0",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-Q8_0.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "mmproj-Qwen2.5-Omni-3B-f16",
        "path": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-f16.gguf",
        "file_size": "2.4 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/README.md",
    "description": "This is a Qwen2.5-Omni-3B model converted to GGUF format for efficient deployment, supporting text, audio, and image input but not video or audio generation."
  },
  {
    "model_name": "dolphin-2.9.3-mistral-nemo-12b-gguf",
    "developer": "dphn",
    "downloads": 4695,
    "createdAt": "2024-07-24T16:00:27.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.F16",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.F16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q2_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_L",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q3_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_1",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q4_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_1",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_K_M",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q5_K_S",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q6_K",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-nemo-12b.Q8_0",
        "path": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/dolphin-2.9.3-mistral-nemo-12b.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/dphn/dolphin-2.9.3-mistral-nemo-12b-gguf/resolve/main/README.md",
    "description": "This is the llama.cpp gguf conversion of the original model located here: https://huggingface.co/cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b, curated and trained by Eric Hartford and Cognitive Computations, based on mistralai"
  },
  {
    "model_name": "Big-Tiger-Gemma-27B-v3-GGUF",
    "developer": "TheDrummer",
    "downloads": 4646,
    "createdAt": "2025-07-04T09:28:17.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q2_K.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q3_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q4_K_M.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q5_K_M.gguf",
        "file_size": "18.9 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q6_K.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Tiger-Gemma-27B-v3a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/Tiger-Gemma-27B-v3a-Q8_0.gguf",
        "file_size": "28.1 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3-GGUF/resolve/main/README.md",
    "description": "Big Tiger Gemma 27B v3 is a more neutral and less positive version of the Gemma 3 27B model, optimized for vision tasks and harder themes."
  },
  {
    "model_name": "google_gemma-3n-E2B-it-GGUF",
    "developer": "bartowski",
    "downloads": 4644,
    "createdAt": "2025-06-26T19:50:06.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3n-E2B-it model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "gemma-3n-E2B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 4496,
    "createdAt": "2025-06-26T15:16:13.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E2B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment.",
    "tools": false
  },
  {
    "model_name": "Qwen2.5-14B_Uncensored_Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4445,
    "createdAt": "2024-09-22T22:26:11.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen2.5-14B_Uncensored_Instruct model using llama.cpp, with various quantization types and optimizations for different hardware, including ARM and Apple Metal, suitable for different performance and quality trade-offs.",
    "tools": true
  },
  {
    "model_name": "Devstral-Small-2505_gguf",
    "developer": "mistralai",
    "downloads": 4280,
    "createdAt": "2025-05-19T16:34:03.000Z",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "devstral",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstral.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "devstralQ4_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "devstralQ4_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "devstralQ5_K_M",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "devstralQ8_0",
        "path": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/devstralQ8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Devstral-Small-2505_gguf/resolve/main/README.md",
    "description": "Devstral-Small-2505 is a lightweight, open-source agentic LLM for software engineering tasks with a 128k context window and Apache 2.0 license, available in GGUF quantized formats for efficient local deployment."
  },
  {
    "model_name": "CausalLM-14B-DPO-alpha-GGUF",
    "developer": "tastypear",
    "downloads": 4233,
    "createdAt": "2023-11-25T15:58:11.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "causallm_14b-dpo-alpha.Q3_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q4_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q4_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q5_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q5_K_S",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q5_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q6_K",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q6_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.Q8_0",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.Q8_0.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "causallm_14b-dpo-alpha.f16",
        "path": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/causallm_14b-dpo-alpha.f16.gguf",
        "file_size": "26.4 GB"
      }
    ],
    "readme": "https://huggingface.co/tastypear/CausalLM-14B-DPO-alpha-GGUF/resolve/main/README.md",
    "description": "This is a GGUF format model for CausalLM's 14B-DPO-alpha, optimized for text generation with a prompt template and licensing under the WTFPL license with Meta Llama 2 License Terms."
  },
  {
    "model_name": "Cydonia-v1.3-Magnum-v4-22B-GGUF",
    "developer": "knifeayumu",
    "downloads": 4219,
    "createdAt": "2024-11-21T09:27:30.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-F16",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-F16.gguf",
        "file_size": "41.4 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q2_K",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q2_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q3_K_L",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q3_K_L.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q3_K_M",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q3_K_M.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q3_K_S",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q3_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q4_K_M",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q4_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q4_K_S",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q5_K_M",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q5_K_S",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q5_K_S.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q6_K",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q6_K.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Cydonia-v1.3-Magnum-v4-22B-Q8_0",
        "path": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/Cydonia-v1.3-Magnum-v4-22B-Q8_0.gguf",
        "file_size": "22.0 GB"
      }
    ],
    "readme": "https://huggingface.co/knifeayumu/Cydonia-v1.3-Magnum-v4-22B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Irix-12B-Model_Stock-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 4211,
    "createdAt": "2025-03-28T16:03:25.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q2_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_1.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Irix-12B-Model_Stock.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/Irix-12B-Model_Stock.i1-Q6_K.gguf",
        "file_size": "9.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Irix-12B-Model_Stock-i1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 4149,
    "createdAt": "2025-01-25T01:35:17.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the DeepSeek-R1-Distill-Qwen-14B-Uncensored base model, available in various GGUF quantization formats for different performance and quality trade-offs."
  },
  {
    "model_name": "SmolVLM2-2.2B-Instruct-GGUF",
    "developer": "ggml-org",
    "downloads": 4136,
    "createdAt": "2025-04-21T19:03:24.000Z",
    "num_quants": 5,
    "quants": [
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "SmolVLM2-2.2B-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/SmolVLM2-2.2B-Instruct-f16.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "mmproj-SmolVLM2-2.2B-Instruct-Q8_0",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/mmproj-SmolVLM2-2.2B-Instruct-Q8_0.gguf",
        "file_size": "565.1 MB"
      },
      {
        "model_id": "mmproj-SmolVLM2-2.2B-Instruct-f16",
        "path": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/mmproj-SmolVLM2-2.2B-Instruct-f16.gguf",
        "file_size": "831.9 MB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/SmolVLM2-2.2B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Qwen2-VL-2B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 4036,
    "createdAt": "2024-12-14T19:43:17.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ2_M.gguf",
        "file_size": "573.2 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ3_M.gguf",
        "file_size": "740.7 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ3_XS.gguf",
        "file_size": "697.8 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ4_NL.gguf",
        "file_size": "893.0 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-IQ4_XS.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q2_K_L.gguf",
        "file_size": "698.9 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q3_K_XL.gguf",
        "file_size": "893.3 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_0.gguf",
        "file_size": "894.1 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_K_L.gguf",
        "file_size": "994.3 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q5_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q6_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen2-VL-2B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/Qwen2-VL-2B-Instruct-f16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mmproj-Qwen2-VL-2B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen2-VL-2B-Instruct-f16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-Qwen2-VL-2B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen2-VL-2B-Instruct-f32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 4026,
    "createdAt": "2025-08-03T04:39:41.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Instruct-2507 model with various quantization types available for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "mistralai_Magistral-Small-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3949,
    "createdAt": "2025-07-24T16:40:51.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/mistralai_Magistral-Small-2507-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2507 model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "RuadaptQwen3-4B-Instruct-GGUF",
    "developer": "RefalMachine",
    "downloads": 3883,
    "createdAt": "2025-06-30T06:13:47.000Z",
    "tools": true,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "BF16",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "IQ3_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "IQ3_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "IQ4_NL",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "IQ4_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Q2_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q3_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Q3_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Q4_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q4_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Q4_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q5_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q5_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Q5_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q6_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Q8_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "RuadaptQwen3-4B-Instruct is a Russian-adapted version of Qwen/Qwen3-4B, featuring a new tokenizer, continued pre-training on Russian data, and LEP (Learned Embedding Propagation) applied to enhance Russian text generation speed by up to"
  },
  {
    "model_name": "Lexi-Llama-3-8B-Uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 3872,
    "createdAt": "2024-04-24T03:51:21.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ1_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ1_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "baidu_ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 3865,
    "createdAt": "2025-06-30T03:51:10.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ2_M.gguf",
        "file_size": "155.4 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_M.gguf",
        "file_size": "195.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XS.gguf",
        "file_size": "184.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS.gguf",
        "file_size": "164.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_NL.gguf",
        "file_size": "222.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_XS.gguf",
        "file_size": "215.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "199.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_L.gguf",
        "file_size": "214.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL.gguf",
        "file_size": "238.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_L.gguf",
        "file_size": "254.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_L.gguf",
        "file_size": "280.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K_L.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-bf16.gguf",
        "file_size": "690.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-0.3B-PT model using llama.cpp's imatrix method, offering various quantization options for different performance and memory trade-offs.",
    "tools": false
  },
  {
    "model_name": "Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8",
    "developer": "ValueFX9507",
    "downloads": 3734,
    "createdAt": "2025-02-15T13:30:54.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV2-7b-0218-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-0218-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0301-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0301-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0228-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0228-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0325-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0325-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Q8.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/README.md",
    "description": "Tifa-DeepSexV2-7b-MGRPO 是基于 Qwen2.5-7B 的深度优化模型，具备 100 万字上下文能力，通过 MGRPO 算法和 Tifa_220B 数据集训练，"
  },
  {
    "model_name": "Menlo_Lucy-GGUF",
    "developer": "bartowski",
    "downloads": 3626,
    "createdAt": "2025-07-18T05:28:56.000Z",
    "tools": true,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Menlo_Lucy-IQ3_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_M.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_XS.gguf",
        "file_size": "795.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ3_XXS.gguf",
        "file_size": "719.4 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q2_K",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q2_K_L.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_L.gguf",
        "file_size": "957.0 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q3_K_XL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_0",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_1",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q6_K",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q6_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Menlo_Lucy-Q8_0",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Menlo_Lucy-bf16",
        "path": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/Menlo_Lucy-bf16.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Menlo_Lucy-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Lucy model by Menlo, optimized for various quantization types and suitable for use with llama.cpp or LM Studio."
  },
  {
    "model_name": "ERNIE-4.5-21B-A3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 3604,
    "createdAt": "2025-07-18T01:00:17.000Z",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-BF16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-BF16.gguf",
        "file_size": "40.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-IQ4_NL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-IQ4_XS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q2_K.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q2_K_L.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q3_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_0.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_1.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_K_M.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q4_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q5_K_M.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q5_K_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q6_K.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-Q8_0.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ1_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ1_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ2_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ2_XXS.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-IQ3_XXS.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "ERNIE-4.5-21B-A3B-PT-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/ERNIE-4.5-21B-A3B-PT-UD-TQ1_0.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-21B-A3B-PT is a large-scale text generation model with 21B parameters and 3B activated parameters per token, trained using advanced multimodal MoE techniques and optimized for high-performance inference on various hardware platforms under the Apache 2"
  },
  {
    "model_name": "Qwen_Qwen3-4B-Thinking-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3538,
    "createdAt": "2025-08-06T15:35:47.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-bf16",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Thinking-2507-imatrix",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Thinking-2507-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-4B-Thinking-2507 model for efficient inference on various hardware, with options ranging from high-quality Q8_0 to low-quality Q2_K quantizations."
  },
  {
    "model_name": "mistralai_Devstral-Small-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3525,
    "createdAt": "2025-07-10T14:40:40.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Devstral-Small-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mistralai_Devstral-Small-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mmproj-mistralai_Devstral-Small-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/mmproj-mistralai_Devstral-Small-2507-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Devstral-Small-2507-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF",
    "developer": "DavidAU",
    "downloads": 3506,
    "createdAt": "2025-01-03T00:04:03.000Z",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k.gguf",
        "file_size": "19.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0.gguf",
        "file_size": "24.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "21.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "26.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/README.md",
    "description": "This is a high-precision, 24.9B parameter Llama3 MOE model combining four 8B models, designed for vivid, uncensored creative writing across all genres, with specialized quants and settings for enhanced instruction following and output quality, particularly in horror, fiction, and"
  },
  {
    "model_name": "Qwen_Qwen3-4B-Instruct-2507-GGUF",
    "developer": "bartowski",
    "downloads": 3503,
    "createdAt": "2025-08-06T15:35:31.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-bf16",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-4B-Instruct-2507-imatrix",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen_Qwen3-4B-Instruct-2507-imatrix.gguf",
        "file_size": "3.7 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-4B-Instruct-2507 model using llama.cpp's imatrix method, optimized for different hardware and performance requirements."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-GGUF",
    "developer": "lmstudio-community",
    "downloads": 3483,
    "createdAt": "2025-08-06T15:20:18.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Qwen3-4B-Thinking-2507 model by Qwen, optimized for efficient text generation on LM Studio."
  },
  {
    "model_name": "Hunyuan-7B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 3437,
    "createdAt": "2025-08-04T06:40:11.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-7B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-F16.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q2_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Hunyuan-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/Hunyuan-7B-Instruct-Q8_0.gguf",
        "file_size": "7.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This Hugging Face repository provides the Tencent Hunyuan-7B-Instruct large language model, offering efficient inference with support for hybrid reasoning, ultra-long context understanding, and various quantization formats like FP8 and INT4, suitable for deployment across different computational environments."
  },
  {
    "model_name": "Cydonia-R1-24B-v4-GGUF",
    "developer": "TheDrummer",
    "downloads": 3388,
    "createdAt": "2025-08-01T14:22:10.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-R1-24B-v4c-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4c-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/Cydonia-R1-24B-v4c-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF/resolve/main/README.md",
    "description": "Cydonia R1 24B v4 is a large language model developed by TheDrummer, known for its creativity, solid reasoning, and ability to maintain coherent narratives with a large number of characters."
  },
  {
    "model_name": "tencent_Hunyuan-7B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 3277,
    "createdAt": "2025-08-04T12:45:17.000Z",
    "tools": true,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ3_XXS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q3_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q6_K_L.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-Q8_0.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-bf16",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-bf16.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "tencent_Hunyuan-7B-Instruct-imatrix",
        "path": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/tencent_Hunyuan-7B-Instruct-imatrix.gguf",
        "file_size": "4.8 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/tencent_Hunyuan-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Tencent Hunyuan-7B-Instruct model using llama.cpp's imatrix quantization method, optimized for various hardware platforms and performance trade-offs."
  },
  {
    "model_name": "ruGPT-3.5-13B-GGUF",
    "developer": "oblivious",
    "downloads": 3099,
    "createdAt": "2024-01-27T06:12:52.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "ruGPT-3.5-13B-Q2_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q2_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_L",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_L.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q3_K_XS",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q3_K_XS.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_1",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_1.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_K_M",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_K_M.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q4_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q4_K_S.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_1",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_1.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_K_M",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_K_M.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q5_K_S",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q6_K",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q6_K.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "ruGPT-3.5-13B-Q8_0",
        "path": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q8_0.gguf",
        "file_size": "13.0 GB"
      }
    ],
    "readme": "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/README.md",
    "description": "ruGPT-3.5-13B-GGUF is a quantized GGUF format model for Russian and English text generation based on the ruGPT-3.5-13B base model."
  },
  {
    "model_name": "XBai-o4-GGUF",
    "developer": "mradermacher",
    "downloads": 3098,
    "createdAt": "2025-08-01T15:41:31.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "XBai-o4.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "XBai-o4.Q2_K",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "XBai-o4.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "XBai-o4.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "XBai-o4.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "XBai-o4.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "XBai-o4.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "XBai-o4.Q6_K",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "XBai-o4.Q8_0",
        "path": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/XBai-o4.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XBai-o4-GGUF/resolve/main/README.md",
    "description": "The MetaStoneTec/XBai-o4 model is a quantized version of the original model, available in various quantization formats including Q2_K, Q3_K_S, Q4_K_S, Q6_K, and Q8_0, optimized for efficient inference on GGUF files"
  },
  {
    "model_name": "Llama-3_3-Nemotron-Super-49B-v1_5-GGUF",
    "developer": "gabriellarson",
    "downloads": 3083,
    "createdAt": "2025-07-25T23:29:14.000Z",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ1_S.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_M.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XS.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_XXS.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_M.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_S.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XS.gguf",
        "file_size": "19.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ3_XXS.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_NL.gguf",
        "file_size": "26.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_XS.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q2_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_L.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_M.gguf",
        "file_size": "22.6 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q3_K_S.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_0.gguf",
        "file_size": "26.5 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_M.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_0.gguf",
        "file_size": "32.2 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_M.gguf",
        "file_size": "33.0 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q5_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K.gguf",
        "file_size": "38.1 GB"
      },
      {
        "model_id": "Llama-3_3-Nemotron-Super-49B-v1_5-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-3_3-Nemotron-Super-49B-v1_5-Q8_0.gguf",
        "file_size": "49.4 GB"
      },
      {
        "model_id": "Llama-49B-3_3-Nemotron-Super-v1_5-F16",
        "path": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/Llama-49B-3_3-Nemotron-Super-v1_5-F16.gguf",
        "file_size": "92.9 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF/resolve/main/README.md",
    "description": "The Llama-3.3-Nemotron-Super-49B-v1.5 is a large language model derived from Meta's Llama-3.3-70B-Instruct, optimized for reasoning and chat tasks with a 128K token context length, enhanced"
  },
  {
    "model_name": "Magistral-Small-2506_gguf",
    "developer": "mistralai",
    "downloads": 3039,
    "createdAt": "2025-06-09T09:25:46.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Magistral-Small-2506",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506_Q8_0",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506_Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/README.md",
    "description": "Magistral-Small-2506_gguf is a lightweight, efficient reasoning model with 24B parameters, trained on Mistral Small 3.1 with SFT and RL, available under Apache 2.0 license for local deployment and inference using llama.ccp."
  },
  {
    "model_name": "nvidia_OpenReasoning-Nemotron-1.5B-GGUF",
    "developer": "bartowski",
    "downloads": 3034,
    "createdAt": "2025-07-18T20:42:31.000Z",
    "tools": false,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_M.gguf",
        "file_size": "740.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XS.gguf",
        "file_size": "697.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ3_XXS.gguf",
        "file_size": "637.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ4_NL.gguf",
        "file_size": "893.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-IQ4_XS.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q2_K_L.gguf",
        "file_size": "698.9 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q3_K_XL.gguf",
        "file_size": "893.3 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_0.gguf",
        "file_size": "894.1 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_1.gguf",
        "file_size": "969.7 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_L.gguf",
        "file_size": "994.3 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q6_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-1.5B-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-1.5B-bf16.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenReasoning-Nemotron-1.5B model by nvidia using llama.cpp, offering various quantization options for different performance and quality trade-offs."
  },
  {
    "model_name": "gpt-oss-120b-GGUF",
    "developer": "gabriellarson",
    "downloads": 2926,
    "createdAt": "2025-08-05T17:10:46.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gpt-oss-120B-F16",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120B-F16.gguf",
        "file_size": "60.9 GB"
      },
      {
        "model_id": "gpt-oss-120b-Q4_0",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-Q4_0.gguf",
        "file_size": "58.3 GB"
      },
      {
        "model_id": "gpt-oss-120b-Q8_0",
        "path": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-Q8_0.gguf",
        "file_size": "59.0 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/gpt-oss-120b-GGUF/resolve/main/README.md",
    "description": "This is a large, open-source GPT model (gpt-oss-120b) with Apache 2.0 license, designed for high reasoning tasks, agentic capabilities, and fine-tuning on H100 GPU or consumer hardware."
  },
  {
    "model_name": "QVikhr-3-4B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 2850,
    "createdAt": "2025-06-28T21:03:17.000Z",
    "num_quants": 31,
    "quants": [
      {
        "model_id": "QVikhr-3-4B-Instruction-F16",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-4B-Instruction is an instructive model based on Qwen/Qwen3-4B, trained on the Russian-language dataset GrandMaster2 for high-efficiency text processing in Russian and English.",
    "tools": true
  },
  {
    "model_name": "Cydonia-24B-v4-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2798,
    "createdAt": "2025-07-09T12:21:43.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "Cydonia-24B-v4.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-24B-v4.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/Cydonia-24B-v4.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Cydonia-24B-v4-i1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "TheDrummer_Tiger-Gemma-12B-v3-GGUF",
    "developer": "bartowski",
    "downloads": 2762,
    "createdAt": "2025-07-09T16:53:33.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ2_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ2_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ2_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ2_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ3_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ3_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ3_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ3_XS.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ3_XXS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ4_NL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ4_NL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-IQ4_XS",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-IQ4_XS.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q2_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q2_K.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q2_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q2_K_L.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_M.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_S.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q3_K_XL.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_0.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_1",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_1.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_K_L.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_K_M.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q4_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q4_K_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q5_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q5_K_L.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q5_K_M",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q5_K_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q5_K_S",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q5_K_S.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q6_K",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q6_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q6_K_L",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q6_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-Q8_0",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-Q8_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "TheDrummer_Tiger-Gemma-12B-v3-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/TheDrummer_Tiger-Gemma-12B-v3-bf16.gguf",
        "file_size": "23.8 GB"
      },
      {
        "model_id": "mmproj-TheDrummer_Tiger-Gemma-12B-v3-bf16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/mmproj-TheDrummer_Tiger-Gemma-12B-v3-bf16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-TheDrummer_Tiger-Gemma-12B-v3-f16",
        "path": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/mmproj-TheDrummer_Tiger-Gemma-12B-v3-f16.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/TheDrummer_Tiger-Gemma-12B-v3-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "LFM2-700M-GGUF",
    "developer": "unsloth",
    "downloads": 2720,
    "createdAt": "2025-07-11T20:08:04.000Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "LFM2-700M-F16",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-F16.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "LFM2-700M-Q2_K",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q2_K.gguf",
        "file_size": "300.5 MB"
      },
      {
        "model_id": "LFM2-700M-Q2_K_L",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q2_K_L.gguf",
        "file_size": "300.5 MB"
      },
      {
        "model_id": "LFM2-700M-Q3_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q3_K_M.gguf",
        "file_size": "369.7 MB"
      },
      {
        "model_id": "LFM2-700M-Q3_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q3_K_S.gguf",
        "file_size": "344.4 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_0",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_0.gguf",
        "file_size": "425.6 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_1",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_1.gguf",
        "file_size": "463.9 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_K_M.gguf",
        "file_size": "446.9 MB"
      },
      {
        "model_id": "LFM2-700M-Q4_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q4_K_S.gguf",
        "file_size": "428.6 MB"
      },
      {
        "model_id": "LFM2-700M-Q5_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q5_K_M.gguf",
        "file_size": "513.1 MB"
      },
      {
        "model_id": "LFM2-700M-Q5_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q5_K_S.gguf",
        "file_size": "502.1 MB"
      },
      {
        "model_id": "LFM2-700M-Q6_K",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q6_K.gguf",
        "file_size": "583.4 MB"
      },
      {
        "model_id": "LFM2-700M-Q8_0",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q8_0.gguf",
        "file_size": "754.9 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q2_K_XL.gguf",
        "file_size": "300.5 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q3_K_XL.gguf",
        "file_size": "369.7 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q4_K_XL.gguf",
        "file_size": "446.9 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q5_K_XL.gguf",
        "file_size": "513.1 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q6_K_XL.gguf",
        "file_size": "606.7 MB"
      },
      {
        "model_id": "LFM2-700M-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-UD-Q8_K_XL.gguf",
        "file_size": "844.9 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "nvidia_OpenReasoning-Nemotron-7B-GGUF",
    "developer": "bartowski",
    "downloads": 2706,
    "createdAt": "2025-07-18T20:42:50.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ2_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q2_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_1",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q6_K",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-Q8_0",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "nvidia_OpenReasoning-Nemotron-7B-bf16",
        "path": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/nvidia_OpenReasoning-Nemotron-7B-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the OpenReasoning-Nemotron-7B model by nvidia, optimized for various inference speeds and memory usage using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "Seed-Coder-8B-Reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 2689,
    "createdAt": "2025-05-18T12:29:03.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Seed-Coder-8B-Reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-BF16.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q8_0.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/README.md",
    "description": "The Seed-Coder-8B-Reasoning model is a high-performance, parameter-efficient, and transparent open-source code model trained for enhanced reasoning capabilities using RL.",
    "tools": false
  },
  {
    "model_name": "gemma-3-1b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 2628,
    "createdAt": "2025-03-10T22:42:41.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf",
        "file_size": "957.1 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and available in GGUF format with QAT quantization for efficient deployment."
  },
  {
    "model_name": "Voxtral-3B-But-4B-Text-Only-GGUF",
    "developer": "SaisExperiments",
    "downloads": 2574,
    "createdAt": "2025-07-16T08:55:59.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-F16",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-IQ4_NL",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-IQ4_XS",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q4_0",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q5_K_M",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q6_K",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Voxtral-3B-But-4B-Text-Only-Q8_0",
        "path": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/Voxtral-3B-But-4B-Text-Only-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/SaisExperiments/Voxtral-3B-But-4B-Text-Only-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the Mistral AI Voxtral-Mini-3B-2507 base model, with Whisper layers removed and mixed with MiniStral configurations, resulting in a 4B text-only model."
  },
  {
    "model_name": "mistralai_Voxtral-Mini-3B-2507-GGUF",
    "developer": "bartowski",
    "downloads": 2478,
    "createdAt": "2025-07-28T17:11:40.000Z",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q2_K_L.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q3_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q6_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Mini-3B-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mistralai_Voxtral-Mini-3B-2507-imatrix.gguf",
        "file_size": "3.2 MB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Mini-3B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Mini-3B-2507-bf16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Mini-3B-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Mini-3B-2507-f16.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistralai Voxtral-Mini-3B-2507 model using llama.cpp imatrix quantization, offering various quantization types for different performance and memory trade-offs."
  },
  {
    "model_name": "MN-12B-Lyra-v4-GGUF",
    "developer": "bartowski",
    "downloads": 2473,
    "createdAt": "2024-09-09T10:04:37.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MN-12B-Lyra-v4-IQ2_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ3_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ3_XS",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-IQ4_XS",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q2_K",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q2_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q2_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q3_K_XL.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_4_4.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_4_8.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_0_8_8.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q4_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_L.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_M",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q5_K_S",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q6_K",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q6_K_L",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q6_K_L.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-Q8_0",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "MN-12B-Lyra-v4-f16",
        "path": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-f16.gguf",
        "file_size": "22.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the MN-12B-Lyra-v4 model using llama.cpp, optimized for different hardware and performance needs, with recommended options for quality and speed."
  },
  {
    "model_name": "Openai_gpt-oss-20b-NEO-GGUF",
    "developer": "DavidAU",
    "downloads": 2416,
    "createdAt": "2025-08-06T01:05:36.000Z",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-IQ4_NL.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE2",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE2.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE3",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE3.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-MXFP4_MOE4",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-MXFP4_MOE4.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO-Q5_1.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO2-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO2-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO2-Q5_1.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO3-Q5_1",
        "path": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/OpenAI-20B-NEO3-Q5_1.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Openai_gpt-oss-20b-NEO-GGUF/resolve/main/README.md",
    "description": "This model is a specialized quantized version of the OpenAI GPT-oss-20B MOE model, optimized for code generation, reasoning, and creative writing with support for up to 24 experts and 128k context, offering high performance in various tasks including coding,"
  },
  {
    "model_name": "Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF",
    "developer": "mradermacher",
    "downloads": 2318,
    "createdAt": "2025-07-23T06:06:13.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-Q8_0.gguf",
        "file_size": "813.6 MB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/Qwen2.5-VL-7B-Abliterated-Caption-it.mmproj-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen2.5-VL-7B-Abliterated-Caption-it-GGUF/resolve/main/README.md",
    "description": "This is a multi-modal visual language model (VLm) for image captioning and text generation, quantized in various formats including GGUF, available in English, Chinese, and Thai."
  },
  {
    "model_name": "LFM2-350M-GGUF",
    "developer": "LiquidAI",
    "downloads": 2317,
    "createdAt": "2025-07-12T12:02:17.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2-350M-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-F16.gguf",
        "file_size": "678.5 MB"
      },
      {
        "model_id": "LFM2-350M-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_0.gguf",
        "file_size": "209.1 MB"
      },
      {
        "model_id": "LFM2-350M-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_K_M.gguf",
        "file_size": "218.7 MB"
      },
      {
        "model_id": "LFM2-350M-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q5_K_M.gguf",
        "file_size": "248.3 MB"
      },
      {
        "model_id": "LFM2-350M-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q6_K.gguf",
        "file_size": "279.8 MB"
      },
      {
        "model_id": "LFM2-350M-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q8_0.gguf",
        "file_size": "361.6 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/README.md",
    "description": "LFM2-350M-GGUF is a hybrid model developed by Liquid AI for edge AI and on-device deployment, optimized for quality, speed, and memory efficiency."
  },
  {
    "model_name": "Triplex",
    "developer": "SciPhi",
    "downloads": 2299,
    "createdAt": "2024-07-10T21:58:18.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "quantized_model-Q4_K_M",
        "path": "https://huggingface.co/SciPhi/Triplex/resolve/main/quantized_model-Q4_K_M.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/SciPhi/Triplex/resolve/main/README.md",
    "description": "Triplex is a state-of-the-art LLM fine-tuned for efficient knowledge graph construction from unstructured data, offering a 98% cost reduction compared to traditional methods."
  },
  {
    "model_name": "qwen3-4B-rpg-roleplay",
    "developer": "Chun121",
    "downloads": 2260,
    "createdAt": "2025-04-30T23:55:22.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gguf_f16/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_f16/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.Q4_K_M",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gguf_q8_0/unsloth.Q8_0",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q8_0/unsloth.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/README.md",
    "description": "This is a LoRA fine-tuned version of Qwen3-4B, optimized for character-based conversations and roleplay scenarios using the Gryphe-Aesir-RPG-Charcards-Opus-Mixed-split dataset.",
    "tools": true
  },
  {
    "model_name": "Nidum-Llama-3.2-3B-Uncensored-GGUF",
    "developer": "nidum",
    "downloads": 2258,
    "createdAt": "2024-12-05T04:55:35.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Nidum-Llama-3.2-3B-Uncensored-F16",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/Nidum-Llama-3.2-3B-Uncensored-F16.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "model-Q2_K",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q2_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "model-Q3_K_L",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "model-Q3_K_M",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q3_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "model-Q3_K_S",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q3_K_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "model-Q4_0_4_4",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q4_0_4_8",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q4_0_8_8",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "model-Q4_K_S",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "model-Q5_K_M",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "model-Q5_K_S",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "model-Q6_K",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "model-TQ1_0",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-TQ1_0.gguf",
        "file_size": "883.4 MB"
      },
      {
        "model_id": "model-TQ2_0",
        "path": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/model-TQ2_0.gguf",
        "file_size": "1009.4 MB"
      }
    ],
    "readme": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF",
    "developer": "ArliAI",
    "downloads": 2177,
    "createdAt": "2024-08-31T17:20:54.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q2_K",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_L",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q3_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q4_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q4_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q5_K_M",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q5_K_S",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-Q6_K",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-fp16",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-fp16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "ArliAI-RPMax-12B-v1.1-q8_0",
        "path": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/ArliAI-RPMax-12B-v1.1-q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF/resolve/main/README.md",
    "description": "ArliAI-RPMax-12B-v1.1 is a creative and non-repetitive RP model based on Mistral Nemo 12B Instruct 2407, trained on diverse and deduplicated datasets to avoid repetition sickness."
  },
  {
    "model_name": "Falcon-H1-1.5B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 2115,
    "createdAt": "2025-05-13T14:03:46.000Z",
    "num_quants": 35,
    "quants": [
      {
        "model_id": "Falcon-H1-1.5B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-BF16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-F32.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ1_M.gguf",
        "file_size": "412.1 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ1_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ1_S.gguf",
        "file_size": "385.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_M.gguf",
        "file_size": "551.8 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_S.gguf",
        "file_size": "516.2 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_XS.gguf",
        "file_size": "493.7 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ2_XXS.gguf",
        "file_size": "456.6 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_M.gguf",
        "file_size": "703.3 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_S.gguf",
        "file_size": "693.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_XS.gguf",
        "file_size": "675.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ3_XXS.gguf",
        "file_size": "617.9 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ4_NL.gguf",
        "file_size": "873.1 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-IQ4_XS.gguf",
        "file_size": "831.2 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q2_K.gguf",
        "file_size": "583.9 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q2_K_S.gguf",
        "file_size": "563.4 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K_L.gguf",
        "file_size": "762.9 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K_M.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q3_K_S.gguf",
        "file_size": "691.8 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_0.gguf",
        "file_size": "871.6 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_1.gguf",
        "file_size": "956.3 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_K.gguf",
        "file_size": "901.0 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_K_M.gguf",
        "file_size": "901.0 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q4_K_S.gguf",
        "file_size": "875.3 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_K.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-Q8_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-TQ1_0.gguf",
        "file_size": "440.5 MB"
      },
      {
        "model_id": "Falcon-H1-1.5B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/Falcon-H1-1.5B-Instruct-TQ2_0.gguf",
        "file_size": "498.0 MB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-GGUF",
    "developer": "lmstudio-community",
    "downloads": 2115,
    "createdAt": "2025-08-06T14:35:28.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-4B-Instruct-2507 model by Qwen, optimized for use with the LM Studio platform."
  },
  {
    "model_name": "baidu_ERNIE-4.5-21B-A3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 2082,
    "createdAt": "2025-06-30T03:51:30.000Z",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ2_M.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ2_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ2_S.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ2_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ2_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ3_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ3_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-IQ4_XS.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q2_K.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q2_K_L.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_L.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_S.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q3_K_XL.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_0.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_1.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_L.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q4_K_S.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_L.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q5_K_S.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q6_K.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q6_K_L.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-Q8_0.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-21B-A3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-21B-A3B-PT-bf16.gguf",
        "file_size": "40.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-21B-A3B-PT model by baidu, optimized for text generation using llama.cpp with various quantization types including Q6_K_L, Q5_K_M, IQ4_XS, etc., recommended"
  },
  {
    "model_name": "Dolphin3.0-Llama3.2-3B-GGUF",
    "developer": "Mungert",
    "downloads": 2082,
    "createdAt": "2025-07-25T05:14:52.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-bf16",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-bf16.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-bf16_q4_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-bf16_q6_k.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-bf16_q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-f16",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-f16.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-f16_q4_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-f16_q6_k.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-f16_q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq2_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq2_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq2_s",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq2_s.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq2_xs",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq2_xs.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq2_xxs",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq2_xxs.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq3_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq3_m.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq3_s",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq3_s.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq3_xs",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq3_xs.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq3_xxs.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq4_nl",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq4_nl.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-iq4_xs",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-iq4_xs.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q2_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q2_k_l.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q2_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q2_k_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q2_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q2_k_s.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q3_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q3_k_l.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q3_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q3_k_m.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q3_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q3_k_s.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_0",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_0_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_0_l.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_1",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_1_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_1_l.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_k_l.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_k_m.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q4_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q4_k_s.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_0",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_0_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_0_l.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_1",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_1.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_1_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_1_l.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_k_l.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_k_m.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q5_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q5_k_s.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q6_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q6_k_l.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q6_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q6_k_m.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.2-3B-q8_0",
        "path": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/Dolphin3.0-Llama3.2-3B-q8_0.gguf",
        "file_size": "3.2 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/Dolphin3.0-Llama3.2-3B-GGUF/resolve/main/README.md",
    "description": "This repository provides the Dolphin 3.0 Llama 3.2 3B GGUF models, which are instruct-tuned for coding, math, agentic, and function calling tasks, with a focus on being a customizable and controllable local AI model."
  },
  {
    "model_name": "NextCoder-7B-GGUF",
    "developer": "Mungert",
    "downloads": 2052,
    "createdAt": "2025-07-10T20:19:08.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "NextCoder-7B-bf16",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "NextCoder-7B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-bf16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "NextCoder-7B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-bf16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "NextCoder-7B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-bf16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "NextCoder-7B-f16",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "NextCoder-7B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-f16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "NextCoder-7B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-f16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "NextCoder-7B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-f16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "NextCoder-7B-iq2_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq2_m.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "NextCoder-7B-iq2_s",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq2_s.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "NextCoder-7B-iq2_xs",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq2_xs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "NextCoder-7B-iq2_xxs",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq2_xxs.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "NextCoder-7B-iq3_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq3_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "NextCoder-7B-iq3_s",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "NextCoder-7B-iq3_xs",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq3_xs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "NextCoder-7B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq3_xxs.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "NextCoder-7B-iq4_nl",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq4_nl.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "NextCoder-7B-iq4_xs",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-iq4_xs.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "NextCoder-7B-q2_k_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q2_k_l.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "NextCoder-7B-q2_k_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q2_k_m.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "NextCoder-7B-q2_k_s",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "NextCoder-7B-q3_k_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NextCoder-7B-q3_k_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "NextCoder-7B-q3_k_s",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q3_k_s.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_0",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_0_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_0_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_1",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_1_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_1_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_k_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_k_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_k_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_k_m.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NextCoder-7B-q4_k_s",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q4_k_s.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_0",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_0_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_0_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_1",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_1_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_1_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_k_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_k_l.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_k_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NextCoder-7B-q5_k_s",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q5_k_s.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NextCoder-7B-q6_k_l",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q6_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NextCoder-7B-q6_k_m",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q6_k_m.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "NextCoder-7B-q8_0",
        "path": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/NextCoder-7B-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/NextCoder-7B-GGUF/resolve/main/README.md",
    "description": "This is a 7.61B parameter code-editing large language model based on Qwen2.5-Coder Instruct, trained with the novel SeleKT finetuning method, supporting up to 32K tokens and achieving high performance on code editing benchmarks."
  },
  {
    "model_name": "Falcon-H1-3B-Instruct-GGUF",
    "developer": "tiiuae",
    "downloads": 2037,
    "createdAt": "2025-05-13T14:36:15.000Z",
    "num_quants": 34,
    "quants": [
      {
        "model_id": "Falcon-H1-3B-Instruct-BF16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-BF16.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-F16",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-F16.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-F32",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-F32.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ1_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ1_M.gguf",
        "file_size": "773.2 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_S.gguf",
        "file_size": "988.6 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_XS.gguf",
        "file_size": "952.2 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ2_XXS.gguf",
        "file_size": "870.9 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ3_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ4_NL.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q2_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q2_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q3_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_1.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_1",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_1.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q6_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-Q8_0.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-TQ1_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-TQ1_0.gguf",
        "file_size": "793.6 MB"
      },
      {
        "model_id": "Falcon-H1-3B-Instruct-TQ2_0",
        "path": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/Falcon-H1-3B-Instruct-TQ2_0.gguf",
        "file_size": "919.3 MB"
      }
    ],
    "readme": "https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Qwen3-32B-Uncensored-GGUF",
    "developer": "mradermacher",
    "downloads": 2023,
    "createdAt": "2025-05-02T23:05:43.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-32B-Uncensored.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Uncensored.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/Qwen3-32B-Uncensored.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen3-32B-Uncensored-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the Qwen3-32B-Uncensored base model, offering various GGUF quantization options for efficient deployment."
  },
  {
    "model_name": "Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2022,
    "createdAt": "2025-07-30T10:08:40.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.i1-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-v4-MS3.2-Magnum-Diamond-24B.imatrix",
        "path": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/Cydonia-v4-MS3.2-Magnum-Diamond-24B.imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Cydonia-v4-MS3.2-Magnum-Diamond-24B-i1-GGUF/resolve/main/README.md",
    "description": "The model Cydonia-v4-MS3.2-Magnum-Diamond-24B is quantized into various formats by mradermacher, including imatrix and IQ quantizations, with the GGUF file format available for download."
  },
  {
    "model_name": "A.X-4.0-Light-gguf",
    "developer": "mykor",
    "downloads": 1974,
    "createdAt": "2025-07-04T06:38:36.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "A.X-4.0-Light-BF16",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-BF16.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "A.X-4.0-Light-F32",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-F32.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "A.X-4.0-Light-IQ4_NL",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "A.X-4.0-Light-IQ4_XS",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q2_K",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q3_K_L",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q3_K_M",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q3_K_S",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q4_0",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q4_K_M",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q4_K_S",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q5_0",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q5_K_M",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q5_K_S",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q6_K",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q6_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "A.X-4.0-Light-Q8_0",
        "path": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/A.X-4.0-Light-Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/mykor/A.X-4.0-Light-gguf/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Dolphin-Mistral-24B-Venice-Edition-GGUF",
    "developer": "Mungert",
    "downloads": 1974,
    "createdAt": "2025-07-23T11:58:09.000Z",
    "tools": false,
    "num_quants": 43,
    "quants": [
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-bf16",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-bf16_q4_k.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-bf16_q6_k.gguf",
        "file_size": "28.0 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-bf16_q8_0.gguf",
        "file_size": "31.3 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-f16",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-f16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-f16_q4_k",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-f16_q4_k.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-f16_q6_k",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-f16_q6_k.gguf",
        "file_size": "28.0 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-f16_q8_0",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-f16_q8_0.gguf",
        "file_size": "31.3 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq1_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq1_m.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq1_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq1_s.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq2_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq2_m.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq2_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq2_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq2_xs",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq2_xs.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq2_xxs",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq2_xxs.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq3_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq3_m.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq3_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq3_s.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq3_xs",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq3_xs.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq3_xxs",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq3_xxs.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq4_nl",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq4_nl.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-iq4_xs",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-iq4_xs.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q2_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q2_k_l.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q2_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q2_k_m.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q2_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q2_k_s.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q3_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q3_k_l.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q3_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q3_k_m.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q3_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q3_k_s.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_0",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_0.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_0_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_0_l.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_1",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_1.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_1_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_1_l.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_k_l.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_k_m.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q4_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q4_k_s.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_0",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_0.gguf",
        "file_size": "15.1 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_0_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_0_l.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_1",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_1.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_1_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_1_l.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_k_l.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_k_m.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q5_k_s",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q5_k_s.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q6_k_l",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q6_k_l.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q6_k_m",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q6_k_m.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Dolphin-Mistral-24B-Venice-Edition-q8_0",
        "path": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/Dolphin-Mistral-24B-Venice-Edition-q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of Mistral-Small-24B-Instruct-2501, optimized for performance with selective layer precision enhancement beyond standard IMatrix quantization."
  },
  {
    "model_name": "DiffuCoder-7B-Instruct-GGUF",
    "developer": "Mungert",
    "downloads": 1969,
    "createdAt": "2025-07-17T15:51:36.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "DiffuCoder-7B-Instruct-bf16",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-bf16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-bf16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-bf16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-f16",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-f16_q4_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-f16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-f16_q6_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-f16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-f16_q8_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-f16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq2_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq2_m.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq2_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq2_s.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq2_xs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq2_xs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq2_xxs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq2_xxs.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq3_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq3_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq3_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq3_xs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq3_xs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq3_xxs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq3_xxs.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq4_nl",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq4_nl.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-iq4_xs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-iq4_xs.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q2_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q2_k_l.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q2_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q2_k_m.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q2_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q3_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q3_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q3_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q3_k_s.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_0_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_0_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_1",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_1_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_1_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_k_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_k_m.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q4_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q4_k_s.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_0_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_0_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_1",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_1_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_1_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_k_l.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q5_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q5_k_s.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q6_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q6_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q6_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q6_k_m.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "DiffuCoder-7B-Instruct-q8_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/DiffuCoder-7B-Instruct-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The DiffuCoder-7B-Instruct model is a code-related text diffusion model based on the DiffuCoder-7B-Base checkpoint, trained with instruction-tuning and optimized for performance using a custom quantization approach."
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 1969,
    "createdAt": "2025-08-03T08:14:34.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Thinking-2507 model, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "MS3.2-PaintedFantasy-Visage-33B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1962,
    "createdAt": "2025-07-03T14:52:05.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ1_S.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ2_XXS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_M.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_S.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ3_XXS.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-IQ4_XS.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q2_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_L.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_M.gguf",
        "file_size": "15.1 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q3_K_S.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_0.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_1.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q4_K_S.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_M.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q5_K_S.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "MS3.2-PaintedFantasy-Visage-33B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/MS3.2-PaintedFantasy-Visage-33B.i1-Q6_K.gguf",
        "file_size": "25.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/MS3.2-PaintedFantasy-Visage-33B-i1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Qwen2.5-Coder-7B-Instruct-128K-GGUF",
    "developer": "unsloth",
    "downloads": 1961,
    "createdAt": "2024-11-12T11:37:59.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-F16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF/resolve/main/README.md",
    "description": "This repository provides the 0.5B Qwen2.5-Coder model, a code-specific large language model that offers improved code generation, reasoning, and fixing capabilities, and is optimized for faster performance and lower memory usage with Unsloth."
  },
  {
    "model_name": "Snowpiercer-15B-v2-GGUF",
    "developer": "TheDrummer",
    "downloads": 1941,
    "createdAt": "2025-07-10T01:07:59.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Snowpiercer-15B-v1g-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q3_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q4_K_M.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q5_K_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q6_K.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Snowpiercer-15B-v1g-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/Snowpiercer-15B-v1g-Q8_0.gguf",
        "file_size": "14.8 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Snowpiercer-15B-v2-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "UIGEN-X-4B-0729-GGUF",
    "developer": "gabriellarson",
    "downloads": 1938,
    "createdAt": "2025-07-29T22:26:22.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "UIGEN-X-4B-0729-F16",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_XS.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ3_XXS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ4_NL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q2_K",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q2_K_S.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q6_K",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "UIGEN-X-4B-0729-Q8_0",
        "path": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/UIGEN-X-4B-0729-Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/UIGEN-X-4B-0729-GGUF/resolve/main/README.md",
    "description": "UIGEN-X-4B-0729 is a reasoning-only UI generation model based on Qwen3-32B, capable of systematically planning, architecting, and implementing complete user interfaces across 26 languages, 7 platforms, and 26 major categories of web"
  },
  {
    "model_name": "KAT-V1-40B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1892,
    "createdAt": "2025-07-22T07:40:11.000Z",
    "tools": true,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "KAT-V1-40B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ1_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ1_S.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_M.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_S.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_XS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ2_XXS.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_M.gguf",
        "file_size": "17.1 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_S.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_XS.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ3_XXS.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-IQ4_XS.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q2_K.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q2_K_S.gguf",
        "file_size": "13.2 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_L.gguf",
        "file_size": "19.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q3_K_S.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_0.gguf",
        "file_size": "21.5 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_1.gguf",
        "file_size": "23.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_K_M.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q4_K_S.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q5_K_M.gguf",
        "file_size": "26.8 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q5_K_S.gguf",
        "file_size": "26.1 GB"
      },
      {
        "model_id": "KAT-V1-40B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/KAT-V1-40B.i1-Q6_K.gguf",
        "file_size": "31.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/KAT-V1-40B-i1-GGUF/resolve/main/README.md",
    "description": "This repository provides various GGUF quantized versions of the Kwaipilot/KAT-V1-40B model, ranging from low-quality i1-IQ1_S to high-quality i1-Q6_K, with notes on their performance and suitability."
  },
  {
    "model_name": "Cydonia-R1-24B-v4b-GGUF",
    "developer": "BeaverAI",
    "downloads": 1839,
    "createdAt": "2025-07-25T08:16:40.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Cydonia-R1-24B-v4b-Q2_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q3_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q4_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q5_K_M",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q6_K",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Cydonia-R1-24B-v4b-Q8_0",
        "path": "https://huggingface.co/BeaverAI/Cydonia-R1-24B-v4b-GGUF/resolve/main/Cydonia-R1-24B-v4b-Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "Fin-R1-GGUF",
    "developer": "Mungert",
    "downloads": 1795,
    "createdAt": "2025-03-21T23:17:12.000Z",
    "num_quants": 40,
    "quants": [
      {
        "model_id": "Fin-R1-bf16-q4_k",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-bf16-q4_k.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Fin-R1-bf16-q6_k",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-bf16-q6_k.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Fin-R1-bf16-q8_0",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-bf16-q8_0.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Fin-R1-bf16",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Fin-R1-f16-q4_k",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-f16-q4_k.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Fin-R1-f16-q6_k",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-f16-q6_k.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Fin-R1-f16-q8_0",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-f16-q8_0.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Fin-R1-f16",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Fin-R1-iq2_m",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq2_m.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Fin-R1-iq2_s",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq2_s.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Fin-R1-iq2_xs",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq2_xs.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Fin-R1-iq2_xxs",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq2_xxs.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Fin-R1-iq3_m",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq3_m.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Fin-R1-iq3_s",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq3_s.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Fin-R1-iq3_xs",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq3_xs.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Fin-R1-iq3_xxs",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq3_xxs.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Fin-R1-iq4_nl",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq4_nl.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Fin-R1-iq4_xs",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-iq4_xs.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Fin-R1-q2_k_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q2_k_l.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Fin-R1-q2_k_s",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q2_k_s.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Fin-R1-q3_k_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Fin-R1-q3_k_m",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q3_k_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Fin-R1-q3_k_s",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q3_k_s.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Fin-R1-q4_0",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Fin-R1-q4_0_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_0_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Fin-R1-q4_1",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Fin-R1-q4_1_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_1_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Fin-R1-q4_k_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_k_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Fin-R1-q4_k_m",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_k_m.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Fin-R1-q4_k_s",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q4_k_s.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Fin-R1-q5_0",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Fin-R1-q5_0_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_0_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Fin-R1-q5_1",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Fin-R1-q5_1_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_1_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Fin-R1-q5_k_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_k_l.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Fin-R1-q5_k_m",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Fin-R1-q5_k_s",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q5_k_s.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Fin-R1-q6_k_l",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q6_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Fin-R1-q6_k_m",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q6_k_m.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Fin-R1-q8_0",
        "path": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/Fin-R1-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/Fin-R1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "DiffuCoder-7B-cpGRPO-GGUF",
    "developer": "Mungert",
    "downloads": 1750,
    "createdAt": "2025-07-16T19:37:57.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "DiffuCoder-7B-cpGRPO-bf16",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-bf16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-bf16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-bf16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-f16",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-f16_q4_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-f16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-f16_q6_k",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-f16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-f16_q8_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-f16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq2_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq2_m.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq2_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq2_s.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq2_xs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq2_xs.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq2_xxs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq2_xxs.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq3_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq3_m.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq3_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq3_xs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq3_xs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq3_xxs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq3_xxs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq4_nl",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq4_nl.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-iq4_xs",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-iq4_xs.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q2_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q2_k_l.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q2_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q2_k_m.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q2_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q3_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q3_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q3_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q3_k_s.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_0_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_0_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_1",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_1_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_1_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_k_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q4_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q4_k_s.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_0_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_0_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_1",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_1_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_1_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_k_l.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q5_k_s",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q5_k_s.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q6_k_l",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q6_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q6_k_m",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q6_k_m.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "DiffuCoder-7B-cpGRPO-q8_0",
        "path": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/DiffuCoder-7B-cpGRPO-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the DiffuCoder-7B-Instruct model, enhanced with coupled-GRPO for improved code generation performance and optimized for quantum-ready security checks and network monitoring tasks."
  },
  {
    "model_name": "mistralai_Voxtral-Small-24B-2507-GGUF",
    "developer": "bartowski",
    "downloads": 1698,
    "createdAt": "2025-07-28T17:12:11.000Z",
    "tools": true,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mistralai_Voxtral-Small-24B-2507-imatrix",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mistralai_Voxtral-Small-24B-2507-imatrix.gguf",
        "file_size": "9.6 MB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Small-24B-2507-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Small-24B-2507-bf16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-mistralai_Voxtral-Small-24B-2507-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/mmproj-mistralai_Voxtral-Small-24B-2507-f16.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistralai Voxtral-Small-24B-2507 model using llama.cpp's imatrix quantization method, optimized for various hardware platforms with different performance and quality trade-offs."
  },
  {
    "model_name": "LongWriter-llama3.1-8b-GGUF",
    "developer": "bartowski",
    "downloads": 1695,
    "createdAt": "2024-08-13T18:59:54.000Z",
    "num_quants": 20,
    "quants": [
      {
        "model_id": "LongWriter-llama3.1-8b-IQ2_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-IQ3_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q2_K",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q6_K",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-Q8_0",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "LongWriter-llama3.1-8b-f32",
        "path": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/LongWriter-llama3.1-8b-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/LongWriter-llama3.1-8b-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "ubergarm",
    "downloads": 1649,
    "createdAt": "2025-07-02T00:39:08.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-IQ3_KS.gguf",
        "file_size": "34.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an imatrix quantized version of Hunyuan-A13B-Instruct using ik_llama.cpp, optimized for high perplexity and performance with specific layer quantization settings and hardware requirements.",
    "tools": true
  },
  {
    "model_name": "llama4-dolphin-8B-GGUF",
    "developer": "mradermacher",
    "downloads": 1645,
    "createdAt": "2024-04-27T08:17:50.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "llama4-dolphin-8B.IQ3_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q2_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q6_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q8_0",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.f16",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/README.md",
    "description": "The Hugging Face repository provides GGUF quantized versions of the Manavshah/llama4-dolphin-8B model, including options like Q2_K, IQ3_XS, Q8_0, and others, with notes on quality and performance."
  },
  {
    "model_name": "XortronCriminalComputingConfig-GGUF",
    "developer": "mradermacher",
    "downloads": 1618,
    "createdAt": "2025-05-05T05:05:00.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "XortronCriminalComputingConfig.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.IQ4_XS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q2_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q6_K",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "XortronCriminalComputingConfig.Q8_0",
        "path": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/XortronCriminalComputingConfig.Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/XortronCriminalComputingConfig-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "AceInstruct-1.5B-GGUF",
    "developer": "Mungert",
    "downloads": 1617,
    "createdAt": "2025-07-15T11:15:49.000Z",
    "tools": false,
    "num_quants": 34,
    "quants": [
      {
        "model_id": "AceInstruct-1.5B-bf16",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-bf16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-bf16_q4_k.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-bf16_q6_k.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-bf16_q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-f16",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-f16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-f16_q4_k.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-f16_q6_k.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-f16_q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-iq3_m",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-iq3_m.gguf",
        "file_size": "891.4 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-iq3_s",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-iq3_s.gguf",
        "file_size": "891.4 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-iq3_xs",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-iq3_xs.gguf",
        "file_size": "828.1 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-iq3_xxs.gguf",
        "file_size": "814.9 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-iq4_nl",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-iq4_nl.gguf",
        "file_size": "1018.1 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-iq4_xs",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-iq4_xs.gguf",
        "file_size": "972.5 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-q3_k_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q3_k_l.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q3_k_m",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q3_k_m.gguf",
        "file_size": "965.3 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-q3_k_s",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q3_k_s.gguf",
        "file_size": "906.2 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_0",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_0.gguf",
        "file_size": "959.5 MB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_0_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_0_l.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_1",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_1.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_1_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_1_l.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_k_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_k_l.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_k_m",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_k_m.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q4_k_s",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q4_k_s.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_0",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_0.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_0_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_0_l.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_1",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_1.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_1_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_1_l.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_k_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_k_l.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_k_m",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q5_k_s",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q5_k_s.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q6_k_l",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q6_k_l.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q6_k_m",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q6_k_m.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "AceInstruct-1.5B-q8_0",
        "path": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/AceInstruct-1.5B-q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF/resolve/main/README.md",
    "description": "AceInstruct-1.5B GGUF Models are advanced, versatile instruction-following models fine-tuned for coding, math, and general tasks, outperforming Qwen2.5-Instruct in benchmarks like HumanEval and GSM8K."
  },
  {
    "model_name": "Midm-2.0-Mini-Instruct-gguf",
    "developer": "mykor",
    "downloads": 1565,
    "createdAt": "2025-07-04T12:38:48.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Midm-2.0-Mini-Instruct-BF16",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-BF16.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-F32",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-F32.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-IQ4_NL",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-IQ4_NL.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-IQ4_XS",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-IQ4_XS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q2_K",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q2_K.gguf",
        "file_size": "930.2 MB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q3_K_L",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q3_K_L.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q3_K_M",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q3_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q3_K_S",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q3_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q4_0",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q4_0.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q4_K_M",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q4_K_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q4_K_S",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q4_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q5_0",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q5_0.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q5_K_M",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q5_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q5_K_S",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q5_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q6_K",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q6_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Midm-2.0-Mini-Instruct-Q8_0",
        "path": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/Midm-2.0-Mini-Instruct-Q8_0.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mykor/Midm-2.0-Mini-Instruct-gguf/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "YandexGPT-5-Lite-8B-instruct-GGUF",
    "developer": "yandex",
    "downloads": 1516,
    "createdAt": "2025-03-28T16:24:33.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "YandexGPT-5-Lite-8B-instruct-Q4_K_M",
        "path": "https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct-GGUF/resolve/main/YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "readme": "https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "OpenReasoning-Nemotron-14B-GGUF",
    "developer": "Mungert",
    "downloads": 1504,
    "createdAt": "2025-07-26T17:12:29.000Z",
    "tools": false,
    "num_quants": 44,
    "quants": [
      {
        "model_id": "OpenReasoning-Nemotron-14B-bf16",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-bf16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-bf16_q4_k.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-bf16_q6_k.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-bf16_q8_0.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-f16",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-f16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-f16_q4_k.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-f16_q6_k.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-f16_q8_0.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-imatrix",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-imatrix.gguf",
        "file_size": "8.2 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq1_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq1_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq1_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq1_s.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq2_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq2_m.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq2_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq2_s.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq2_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq2_xs.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq2_xxs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq2_xxs.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq3_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq3_m.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq3_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq3_s.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq3_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq3_xs.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq3_xxs.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq4_nl",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq4_nl.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-iq4_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-iq4_xs.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q2_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q2_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q2_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q2_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q2_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q2_k_s.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q3_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q3_k_l.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q3_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q3_k_m.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q3_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q3_k_s.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_0.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_0_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_0_l.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_1",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_1_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_1_l.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_k_l.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_k_m.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q4_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q4_k_s.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_0.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_0_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_0_l.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_1",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_1.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_1_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_1_l.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_k_l.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_k_m.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q5_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q5_k_s.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q6_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q6_k_l.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q6_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q6_k_m.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-14B-q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/OpenReasoning-Nemotron-14B-q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF/resolve/main/README.md",
    "description": "This model is a large language model derived from Qwen2.5-14B-Instruct, optimized for reasoning tasks like math, code, and science problem-solving, with support for GenSelect inference and available in multiple sizes (1.5B, 7B, 14B"
  },
  {
    "model_name": "Qwen3-30B-A3B-Thinking-2507-GGUF",
    "developer": "ubergarm",
    "downloads": 1458,
    "createdAt": "2025-07-30T15:32:04.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ1_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ1_KT.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ2_KL",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ2_KL.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ2_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ2_KT.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ3_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ3_K.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ3_KS.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_K.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_KSS",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_KSS.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ4_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ4_KT.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-IQ5_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-IQ5_K.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Thinking-2507-Q4_0",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/Qwen3-30B-A3B-Thinking-2507-Q4_0.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-30B-A3B-Thinking-2507 model using ik_llama.cpp, optimized for different memory footprints and perplexity performance."
  },
  {
    "model_name": "Dream-org_Dream-v0-Instruct-7B-GGUF",
    "developer": "bartowski",
    "downloads": 1431,
    "createdAt": "2025-07-16T19:48:13.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q2_K",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_0",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_1",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q6_K",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-Q8_0",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Dream-org_Dream-v0-Instruct-7B-bf16",
        "path": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/Dream-org_Dream-v0-Instruct-7B-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Dream-org_Dream-v0-Instruct-7B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dream-v0-Instruct-7B model using llama.cpp's imatrix method, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "OpenReasoning-Nemotron-7B-GGUF",
    "developer": "Mungert",
    "downloads": 1431,
    "createdAt": "2025-07-25T07:42:25.000Z",
    "tools": false,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "OpenReasoning-Nemotron-7B-bf16",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-bf16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-bf16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-bf16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-f16",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-f16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-f16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-f16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq2_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq2_m.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq2_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq2_s.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq2_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq2_xs.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq2_xxs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq2_xxs.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq3_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq3_m.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq3_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq3_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq3_xs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq3_xxs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq4_nl",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq4_nl.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-iq4_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-iq4_xs.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q2_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q2_k_l.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q2_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q2_k_m.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q2_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q3_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q3_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q3_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q3_k_s.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_0_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_0_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_1",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_1_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_1_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_k_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q4_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q4_k_s.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_0_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_0_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_1",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_1_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_1_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_k_l.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q5_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q5_k_s.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q6_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q6_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q6_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q6_k_m.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-7B-q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/OpenReasoning-Nemotron-7B-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF/resolve/main/README.md",
    "description": "This is a large language model (LLM) derived from Qwen2.5-7B-Instruct, optimized for reasoning tasks like math, code, and science problem-solving, with support for GenSelect inference and available in multiple sizes (1.5B, 7B, 1"
  },
  {
    "model_name": "Kunoichi-DPO-v2-7B-GGUF",
    "developer": "brittlewis12",
    "downloads": 1380,
    "createdAt": "2024-01-16T16:33:41.000Z",
    "num_quants": 28,
    "quants": [
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ1_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ1_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ2_XXS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ3_XXS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ3_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ4_NL",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.IQ4_XS",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q2_K",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q2_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_L",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q3_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_1",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q4_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_1",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_K_M",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q5_K_S",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q6_K",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.Q8_0",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "kunoichi-dpo-v2-7b.fp16",
        "path": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "SmolLM2-135M-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 1358,
    "createdAt": "2024-10-31T19:25:41.000Z",
    "num_quants": 23,
    "quants": [
      {
        "model_id": "SmolLM2-135M-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-IQ3_M.gguf",
        "file_size": "86.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-IQ3_XS.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-IQ4_XS.gguf",
        "file_size": "86.7 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q2_K.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q2_K_L.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_L.gguf",
        "file_size": "93.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_M.gguf",
        "file_size": "89.2 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_S.gguf",
        "file_size": "84.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q3_K_XL.gguf",
        "file_size": "93.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0.gguf",
        "file_size": "87.6 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0_4_4.gguf",
        "file_size": "87.5 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0_4_8.gguf",
        "file_size": "87.5 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_0_8_8.gguf",
        "file_size": "87.5 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_K_L.gguf",
        "file_size": "100.6 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_K_M.gguf",
        "file_size": "100.6 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q4_K_S.gguf",
        "file_size": "97.3 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q5_K_L.gguf",
        "file_size": "106.9 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q5_K_M.gguf",
        "file_size": "106.9 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q5_K_S.gguf",
        "file_size": "104.9 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q6_K.gguf",
        "file_size": "132.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q6_K_L.gguf",
        "file_size": "132.0 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q8_0.gguf",
        "file_size": "138.1 MB"
      },
      {
        "model_id": "SmolLM2-135M-Instruct-f16",
        "path": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-f16.gguf",
        "file_size": "258.3 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "jina-reranker-m0-GGUF",
    "developer": "jinaai",
    "downloads": 1344,
    "createdAt": "2025-07-20T21:30:25.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "jina-reranker-m0-F16",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "jina-reranker-m0-Q3_K_M",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "jina-reranker-m0-Q4_K_M",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "jina-reranker-m0-Q5_K_M",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "jina-reranker-m0-Q6_K",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "jina-reranker-m0-Q8_0",
        "path": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/jina-reranker-m0-Q8_0.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/jinaai/jina-reranker-m0-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF",
    "developer": "DavidAU",
    "downloads": 1319,
    "createdAt": "2025-05-15T23:44:27.000Z",
    "tools": false,
    "num_quants": 20,
    "quants": [
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_M-imat.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_S-imat.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XS-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ2_XXS-imat.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_M-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_S-imat.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XS-imat.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XXS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ3_XXS-imat.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ4_XS-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-IQ4_XS-imat.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K-imat.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q2_K_S-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_L-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_L-imat.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_M-imat.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q3_K_S-imat.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_0-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_0-imat.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_M-imat.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q4_K_S-imat.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_M-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_M-imat.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_S-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q5_K_S-imat.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q6_K-imat",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/L3.2-8X3B-GTD-MOE-NEO-Reason-Dark-Champ-uncen-18.4B-IMAT-D_AU-Q6_K-imat.gguf",
        "file_size": "14.1 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF/resolve/main/README.md",
    "description": "This model is a highly advanced Llama 3.2 MOE variant with 18.4B parameters, featuring reasoning capabilities, vivid creative writing, and uncensored output across all genres, with the ability to control individual expert models for tailored generation, and requires specific system prompts to activate reasoning"
  },
  {
    "model_name": "google_medgemma-4b-it-GGUF",
    "developer": "bartowski",
    "downloads": 1283,
    "createdAt": "2025-07-12T15:54:02.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "google_medgemma-4b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ3_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q2_K_L.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q6_K_L.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_medgemma-4b-it-bf16",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/google_medgemma-4b-it-bf16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "mmproj-google_medgemma-4b-it-bf16",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/mmproj-google_medgemma-4b-it-bf16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-google_medgemma-4b-it-f16",
        "path": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/mmproj-google_medgemma-4b-it-f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_medgemma-4b-it-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 1276,
    "createdAt": "2025-08-03T07:56:40.000Z",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.IQ4_XS.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "This model is a quantized version of the Huihui-Qwen3-Coder-30B-A3B-Instruct, offering various quantization types for efficient deployment."
  },
  {
    "model_name": "L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF",
    "developer": "DavidAU",
    "downloads": 1242,
    "createdAt": "2024-06-25T03:18:51.000Z",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ1_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ1_M-imat13.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_M-imat13.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_XS-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ2_XS-imat13.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ3_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ3_M-imat13.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ4_XS-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-IQ4_XS-imat13.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_M-imat13.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_S-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q4_K_S-imat13.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_M-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_M-imat13.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_S-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q5_K_S-imat13.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q6_K-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q6_K-imat13.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13",
        "path": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/README.md",
    "description": "This model is a high-quality, ultra-high precision Llama3-based quantized version with improved performance and creativity, optimized for use in chat and roleplay scenarios with specific settings for smoother operation."
  },
  {
    "model_name": "reka-flash-3.1-rekaquant-q3_k_s",
    "developer": "RekaAI",
    "downloads": 1237,
    "createdAt": "2025-05-30T14:20:48.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "reka-flash-3.1-rekaquant-q3_k_s",
        "path": "https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s/resolve/main/reka-flash-3.1-rekaquant-q3_k_s.gguf",
        "file_size": "8.6 GB"
      }
    ],
    "readme": "https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 1233,
    "createdAt": "2025-07-30T03:03:56.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Instruct-2507-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-30B-A3B-Instruct-2507-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Intel/Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-30B-A3B-Instruct-2507 model using the Intel AutoRound algorithm, with the embedding layer and lm-head layer quantized to 8 bits and non-expert layers to 4 bits, optimized for efficient"
  },
  {
    "model_name": "Hunyuan-1.8B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 1209,
    "createdAt": "2025-08-04T07:00:44.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-1.8B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-F16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_M.gguf",
        "file_size": "666.1 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_S.gguf",
        "file_size": "626.6 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_XS.gguf",
        "file_size": "603.8 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ2_XXS.gguf",
        "file_size": "560.3 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_M.gguf",
        "file_size": "859.1 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_S.gguf",
        "file_size": "835.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_XS.gguf",
        "file_size": "801.2 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ3_XXS.gguf",
        "file_size": "732.9 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ4_NL.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-IQ4_XS.gguf",
        "file_size": "986.0 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q2_K.gguf",
        "file_size": "741.5 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q2_K_S.gguf",
        "file_size": "700.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_L.gguf",
        "file_size": "971.7 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_M.gguf",
        "file_size": "907.0 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q3_K_S.gguf",
        "file_size": "831.5 MB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_0.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q4_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q5_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q6_K.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Hunyuan-1.8B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/Hunyuan-1.8B-Instruct-Q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides the Hugging Face implementation of the Hunyuan series of efficient large language models, including pre-trained and instruction-tuned variants with support for FP8, INT4, and other quantization formats, optimized for diverse deployment scenarios."
  },
  {
    "model_name": "OpenReasoning-Nemotron-1.5B-GGUF",
    "developer": "Mungert",
    "downloads": 1148,
    "createdAt": "2025-07-25T13:59:58.000Z",
    "tools": false,
    "num_quants": 34,
    "quants": [
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-bf16",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-bf16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-bf16_q4_k.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-bf16_q6_k.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-bf16_q8_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-f16",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-f16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-f16_q4_k.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-f16_q6_k.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-f16_q8_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-iq3_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-iq3_m.gguf",
        "file_size": "745.7 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-iq3_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-iq3_s.gguf",
        "file_size": "738.7 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-iq3_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-iq3_xs.gguf",
        "file_size": "676.2 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-iq3_xxs.gguf",
        "file_size": "663.0 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-iq4_nl",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-iq4_nl.gguf",
        "file_size": "893.0 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-iq4_xs",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-iq4_xs.gguf",
        "file_size": "854.2 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q3_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q3_k_l.gguf",
        "file_size": "839.9 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q3_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q3_k_m.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q3_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q3_k_s.gguf",
        "file_size": "753.2 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_0.gguf",
        "file_size": "834.3 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_0_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_0_l.gguf",
        "file_size": "945.5 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_1",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_1.gguf",
        "file_size": "926.3 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_1_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_1_l.gguf",
        "file_size": "1023.6 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_k_l.gguf",
        "file_size": "996.0 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_k_m.gguf",
        "file_size": "942.1 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q4_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q4_k_s.gguf",
        "file_size": "903.5 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_0.gguf",
        "file_size": "1018.3 MB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_0_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_0_l.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_1",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_1_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_1_l.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_k_l.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_k_m.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q5_k_s",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q5_k_s.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q6_k_l",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q6_k_l.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q6_k_m",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q6_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "OpenReasoning-Nemotron-1.5B-q8_0",
        "path": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/OpenReasoning-Nemotron-1.5B-q8_0.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF/resolve/main/README.md",
    "description": "This is a large language model derived from Qwen2.5-1.5B-Instruct, optimized for reasoning tasks like math, code, and science problem-solving, with support for GenSelect inference mode and available in multiple sizes (1.5B, 7B, 14"
  },
  {
    "model_name": "reka-flash-3.1-GGUF",
    "developer": "mradermacher",
    "downloads": 1134,
    "createdAt": "2025-07-10T18:09:29.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "reka-flash-3.1.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.IQ4_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q2_K",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q2_K.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q3_K_L.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q3_K_M.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q3_K_S.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q4_K_M.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q4_K_S.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q5_K_M.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q5_K_S.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q6_K",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q6_K.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "reka-flash-3.1.Q8_0",
        "path": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/reka-flash-3.1.Q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/reka-flash-3.1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "MindLink-32B-0801-GGUF",
    "developer": "gabriellarson",
    "downloads": 1097,
    "createdAt": "2025-08-02T05:10:42.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MindLink-32B-0801-F16",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-F16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_M.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ3_XXS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q2_K",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q2_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_0.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q6_K",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "MindLink-32B-0801-Q8_0",
        "path": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/MindLink-32B-0801-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF/resolve/main/README.md",
    "description": "MindLink is a new family of large language models developed by Kunlun Inc, built on Qwen with advanced post-training techniques, offering strong performance across various benchmarks and diverse AI applications."
  },
  {
    "model_name": "UIGEN-X-8B-GGUF",
    "developer": "Mungert",
    "downloads": 1051,
    "createdAt": "2025-07-21T23:42:26.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "UIGEN-X-8B-bf16",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-bf16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-bf16_q4_k.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "UIGEN-X-8B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-bf16_q6_k.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-bf16_q8_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "UIGEN-X-8B-f16",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-f16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-f16_q4_k.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "UIGEN-X-8B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-f16_q6_k.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-f16_q8_0.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq2_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq2_m.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq2_s",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq2_s.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq2_xs",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq2_xs.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq2_xxs",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq2_xxs.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq3_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq3_m.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq3_s",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq3_s.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq3_xs",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq3_xs.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq3_xxs.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq4_nl",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq4_nl.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-iq4_xs",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-iq4_xs.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q2_k_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q2_k_l.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q2_k_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q2_k_m.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q2_k_s",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q2_k_s.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q3_k_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q3_k_l.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q3_k_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q3_k_m.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q3_k_s",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q3_k_s.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_0",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_0_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_0_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_1",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_1_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_1_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_k_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_k_l.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_k_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_k_m.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q4_k_s",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q4_k_s.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_0",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_0_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_0_l.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_1",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_1_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_1_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_k_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_k_l.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_k_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_k_m.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q5_k_s",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q5_k_s.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q6_k_l",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q6_k_l.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q6_k_m",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q6_k_m.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "UIGEN-X-8B-q8_0",
        "path": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/UIGEN-X-8B-q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/UIGEN-X-8B-GGUF/resolve/main/README.md",
    "description": "UIGEN-X-8B is a hybrid reasoning UI generation model based on Qwen3-8B, trained to systematically plan, architect, and implement complete user interfaces across 26 languages, 7 platforms, and 26 major categories of web and desktop development with support for tool"
  },
  {
    "model_name": "Hunyuan-4B-Instruct-GGUF",
    "developer": "gabriellarson",
    "downloads": 1031,
    "createdAt": "2025-08-04T06:44:18.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Hunyuan-4B-Instruct-F16",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-F16.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_S.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_XS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ4_NL.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-IQ4_XS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q4_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q5_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q6_K.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Hunyuan-4B-Instruct-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/Hunyuan-4B-Instruct-Q8_0.gguf",
        "file_size": "4.2 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "This Hugging Face repository provides the Hunyuan series of large language models (0.5B, 1.8B, 4B, 7B) with instruction tuning, quantization support (FP8, INT4), and deployment options using TensorRT-LLM, v"
  },
  {
    "model_name": "DeepSWE-Preview-GGUF",
    "developer": "lmstudio-community",
    "downloads": 1025,
    "createdAt": "2025-07-03T13:18:41.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "DeepSWE-Preview-Q3_K_L",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "DeepSWE-Preview-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "DeepSWE-Preview-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "DeepSWE-Preview-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/DeepSWE-Preview-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/DeepSWE-Preview-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 1023,
    "createdAt": "2025-08-03T11:10:53.000Z",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Huihui-Qwen3-30B-A3B-Thinking-2507 model with various GGUF quantization types available for different trade-offs between size, speed, and quality."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-abliterated-GGUF",
    "developer": "Misaka27260",
    "downloads": 993,
    "createdAt": "2025-04-13T00:39:29.000Z",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-F16Out-Q4_K_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-F16Out-Q4_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ2_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ2_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_S",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_XXS",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ3_XXS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ4_XS",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-IQ4_XS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q3_K_L",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q3_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q4_K_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q4_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q5_K_M",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q5_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q6_K",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-abliterated-i1-f16o-Q6_K.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "mmproj-Qwen2.5-VL-7B-Instruct",
        "path": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/mmproj-Qwen2.5-VL-7B-Instruct.gguf",
        "file_size": "1.3 GB"
      }
    ],
    "readme": "https://huggingface.co/Misaka27260/Qwen2.5-VL-7B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a quantized gguf file of the Qwen2.5-VL-7B-Instruct model, optimized for deployment with LM Studio, using FP16 output precision and requiring a runtime environment >= 1.29.0(beta)."
  },
  {
    "model_name": "granite-4.0-tiny-preview-GGUF",
    "developer": "Mungert",
    "downloads": 988,
    "createdAt": "2025-07-22T07:07:31.000Z",
    "tools": false,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "granite-4.0-tiny-preview-bf16",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-bf16.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-bf16_q4_k.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-bf16_q6_k.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-bf16_q8_0.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-f16",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-f16.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-f16_q4_k",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-f16_q4_k.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-f16_q6_k",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-f16_q6_k.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-f16_q8_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-f16_q8_0.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq2_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq2_m.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq2_s",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq2_s.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq2_xs",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq2_xs.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq3_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq3_m.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq3_s",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq3_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq3_xs",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq3_xs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq3_xxs",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq3_xxs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq4_nl",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq4_nl.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-iq4_xs",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-iq4_xs.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q2_k_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q2_k_l.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q2_k_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q2_k_m.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q3_k_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q3_k_l.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q3_k_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q3_k_m.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q3_k_s",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q3_k_s.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_0.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_0_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_0_l.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_1",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_1.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_1_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_1_l.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_k_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_k_l.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_k_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_k_m.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q4_k_s",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q4_k_s.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_0_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_0_l.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_1",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_1.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_1_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_1_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_k_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_k_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_k_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q5_k_s",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q5_k_s.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q6_k_l",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q6_k_l.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q6_k_m",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q6_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-q8_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-q8_0.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-tq1_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-tq1_0.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "granite-4.0-tiny-preview-tq2_0",
        "path": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/granite-4.0-tiny-preview-tq2_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/granite-4.0-tiny-preview-GGUF/resolve/main/README.md",
    "description": "This is a 7B parameter fine-grained hybrid mixture-of-experts (MoE) instruct model finetuned for instruction-following tasks and multilingual dialog use cases, developed by the Granite Team and IBM with Apache 2.0 license."
  },
  {
    "model_name": "Gemma-3-R1-27B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 923,
    "createdAt": "2025-08-04T14:46:49.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemma-3-R1-27B-v1b-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/Gemma-3-R1-27B-v1b-Q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF/resolve/main/README.md",
    "description": "Hugging Face provides a platform for sharing and accessing machine learning models, datasets, and tools, with a focus on ease of use and community collaboration."
  },
  {
    "model_name": "L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 920,
    "createdAt": "2024-06-23T20:13:10.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-BF16",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-BF16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-F16",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.3-32K-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.3-32K-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a GGUF-IQ-Imatrix quantized version of the Sao10K/L3-8B-Stheno-v3.3-32K model, trained for roleplay and SillyTavern tasks with 32K context support, optimized for use with Kob"
  },
  {
    "model_name": "AFM-4.5B-GGUF",
    "developer": "arcee-ai",
    "downloads": 872,
    "createdAt": "2025-07-29T12:43:40.000Z",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "AFM-4.5B-IQ2_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ2_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_XS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_XS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ3_XXS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ3_XXS.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ4_NL",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-IQ4_XS",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "AFM-4.5B-Q2_K",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "AFM-4.5B-Q2_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_L.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_S.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "AFM-4.5B-Q3_K_XL",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_0",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_1",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q4_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_M",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "AFM-4.5B-Q5_K_S",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "AFM-4.5B-Q6_K",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "AFM-4.5B-Q6_K_L",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q6_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "AFM-4.5B-Q8_0",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-Q8_0.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "AFM-4.5B-bf16",
        "path": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/AFM-4.5B-bf16.gguf",
        "file_size": "8.6 GB"
      }
    ],
    "readme": "https://huggingface.co/arcee-ai/AFM-4.5B-GGUF/resolve/main/README.md",
    "description": "AFM-4.5B-GGUF is a 4.5 billion parameter instruction-tuned model developed by Arcee.ai for enterprise-grade performance, licensed under the Arcee Model License (AML) with specific commercial usage restrictions."
  },
  {
    "model_name": "codegemma-2b-GGUF",
    "developer": "bartowski",
    "downloads": 809,
    "createdAt": "2024-04-09T14:56:27.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "codegemma-2b-IQ1_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ1_M.gguf",
        "file_size": "776.1 MB"
      },
      {
        "model_id": "codegemma-2b-IQ1_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ1_S.gguf",
        "file_size": "735.2 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_M.gguf",
        "file_size": "972.2 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_S.gguf",
        "file_size": "917.7 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_XS.gguf",
        "file_size": "901.1 MB"
      },
      {
        "model_id": "codegemma-2b-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ2_XXS.gguf",
        "file_size": "844.3 MB"
      },
      {
        "model_id": "codegemma-2b-IQ3_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-IQ3_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_XS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ3_XXS.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "codegemma-2b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ4_NL.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "codegemma-2b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-IQ4_XS.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "codegemma-2b-Q2_K",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q2_K.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "codegemma-2b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "codegemma-2b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q3_K_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "codegemma-2b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q3_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "codegemma-2b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q4_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "codegemma-2b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "codegemma-2b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q5_K_M.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "codegemma-2b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q5_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "codegemma-2b-Q6_K",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q6_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "codegemma-2b-Q8_0",
        "path": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q8_0.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "deepseek-r1-0528-distilled-qwen3-gguf",
    "developer": "ertghiu256",
    "downloads": 800,
    "createdAt": "2025-06-16T12:31:00.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "converted-model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/deepseek-r1-0528-distilled-qwen3-gguf/resolve/main/converted-model-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "converted-model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/deepseek-r1-0528-distilled-qwen3-gguf/resolve/main/converted-model-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/deepseek-r1-0528-distilled-qwen3-gguf/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "dolphin-2.9.3-mistral-7B-32k-GGUF",
    "developer": "mradermacher",
    "downloads": 793,
    "createdAt": "2024-06-25T03:14:26.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ3_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ3_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q2_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q6_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.Q8_0",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "dolphin-2.9.3-mistral-7B-32k.f16",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/dolphin-2.9.3-mistral-7B-32k.f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-7B-32k-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Mixtral-4x3B-v1-GGUF",
    "developer": "TheDrummer",
    "downloads": 757,
    "createdAt": "2025-07-27T10:59:37.000Z",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Mixtral-4x3B-v1a-Q2_K",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q3_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q3_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q4_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q4_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q5_K_M",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q5_K_M.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q6_K",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q6_K.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Mixtral-4x3B-v1a-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/Mixtral-4x3B-v1a-Q8_0.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "mmproj-Voxtral-Mini-3B-2507-Q8_0",
        "path": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/mmproj-Voxtral-Mini-3B-2507-Q8_0.gguf",
        "file_size": "682.6 MB"
      }
    ],
    "readme": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1-GGUF/resolve/main/README.md",
    "description": "This is a MoE-ified version of Voxtral trained on a specific dataset, designed to generate dialogue with a strong socialist perspective, though it may be unstable and requires careful prompting for coherent outputs."
  },
  {
    "model_name": "Open-Insurance-LLM-Llama3-8B-GGUF",
    "developer": "bartowski",
    "downloads": 756,
    "createdAt": "2024-11-26T18:52:43.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q2_K",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q6_K",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-Q8_0",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Open-Insurance-LLM-Llama3-8B-f16",
        "path": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/Open-Insurance-LLM-Llama3-8B-f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Open-Insurance-LLM-Llama3-8B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "GLM-4.5-Air-GGUF",
    "developer": "mradermacher",
    "downloads": 724,
    "createdAt": "2025-08-04T22:29:17.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GLM-4.5-Air.Q2_K",
        "path": "https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF/resolve/main/GLM-4.5-Air.Q2_K.gguf",
        "file_size": "41.9 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the GLM-4.5-Air model, available in various quantization formats including Q2_K, Q3_K_S, Q4_K_S, Q5_K_S, and Q8_0, with the Q4_K_S and Q6_K"
  },
  {
    "model_name": "SWE-Dev-7B-GGUF",
    "developer": "Mungert",
    "downloads": 714,
    "createdAt": "2025-07-12T13:29:36.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "SWE-Dev-7B-bf16",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-bf16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "SWE-Dev-7B-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-bf16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-bf16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-bf16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "SWE-Dev-7B-f16",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-f16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "SWE-Dev-7B-f16_q4_k",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-f16_q4_k.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-f16_q6_k",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-f16_q6_k.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-f16_q8_0",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-f16_q8_0.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq2_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq2_m.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq2_s",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq2_s.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq2_xs",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq2_xs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq2_xxs",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq2_xxs.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq3_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq3_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq3_s",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq3_xs",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq3_xs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq3_xxs",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq3_xxs.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq4_nl",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq4_nl.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "SWE-Dev-7B-iq4_xs",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-iq4_xs.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q2_k_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q2_k_l.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q2_k_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q2_k_m.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q2_k_s",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q3_k_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q3_k_l.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q3_k_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q3_k_m.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q3_k_s",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q3_k_s.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_0",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_0_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_0_l.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_1",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_1.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_1_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_1_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_k_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_k_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_k_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_k_m.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q4_k_s",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q4_k_s.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_0",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_0_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_0_l.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_1",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_1.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_1_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_1_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_k_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_k_l.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_k_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_k_m.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q5_k_s",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q5_k_s.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q6_k_l",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q6_k_l.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q6_k_m",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q6_k_m.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "SWE-Dev-7B-q8_0",
        "path": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/SWE-Dev-7B-q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/SWE-Dev-7B-GGUF/resolve/main/README.md",
    "description": "This is a MIT-licensed SWE-Dev-7B GGUF model generated using llama.cpp, with a focus on improving precision through selective layer quantization for better performance in software engineering tasks."
  },
  {
    "model_name": "DrakIdol-Roleplayer-1.0-GGUF",
    "developer": "mradermacher",
    "downloads": 702,
    "createdAt": "2025-07-27T21:16:36.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DrakIdol-Roleplayer-1.0.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q2_K",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q6_K",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.Q8_0",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.f16",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.f16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.mmproj-Q8_0.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "DrakIdol-Roleplayer-1.0.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/DrakIdol-Roleplayer-1.0.mmproj-f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DrakIdol-Roleplayer-1.0-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the DrakIdol-Roleplayer-1.0 model by aifeifei798, optimized for roleplay tasks with various quantization options available."
  },
  {
    "model_name": "RL-MemoryAgent-14B-GGUF",
    "developer": "mradermacher",
    "downloads": 656,
    "createdAt": "2025-06-22T17:33:53.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "RL-MemoryAgent-14B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q2_K",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q6_K",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "RL-MemoryAgent-14B.Q8_0",
        "path": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/RL-MemoryAgent-14B.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/RL-MemoryAgent-14B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "II-Search-4B-GGUF",
    "developer": "prithivMLmods",
    "downloads": 650,
    "createdAt": "2025-08-05T21:17:53.000Z",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": " II-Search-4B-GGUF.BF16",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.F16",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.F32",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": " II-Search-4B-GGUF.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/ II-Search-4B-GGUF.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/II-Search-4B-GGUF/resolve/main/README.md",
    "description": "II-Search-4B-GGUF is a 4-billion-parameter language model fine-tuned for advanced information seeking and web-integrated reasoning tasks, offering various quantized versions for efficient inference."
  },
  {
    "model_name": "llama-3-sqlcoder-8b-GGUF",
    "developer": "QuantFactory",
    "downloads": 619,
    "createdAt": "2024-05-29T04:25:33.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "llama-3-sqlcoder-8b.Q2_K",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_0",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_1",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_0",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_1",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q6_K",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama-3-sqlcoder-8b.Q8_0",
        "path": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/llama-3-sqlcoder-8b.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/llama-3-sqlcoder-8b-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Llama-xLAM-2-8b-fc-r-GGUF",
    "developer": "Mungert",
    "downloads": 587,
    "createdAt": "2025-06-21T07:38:01.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-bf16",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-bf16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-bf16_q4_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-bf16_q6_k.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-bf16_q8_0.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-f16",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-f16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-f16_q4_k",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-f16_q4_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-f16_q6_k",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-f16_q6_k.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-f16_q8_0",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-f16_q8_0.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq2_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq2_m.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq2_s",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq2_s.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq2_xs",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq2_xs.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq2_xxs",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq2_xxs.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq3_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq3_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq3_s",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq3_xs",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq3_xs.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq3_xxs",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq3_xxs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq4_nl",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq4_nl.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-iq4_xs",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-iq4_xs.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q2_k_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q2_k_l.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q2_k_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q2_k_m.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q2_k_s",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q3_k_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q3_k_l.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q3_k_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q3_k_m.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q3_k_s",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q3_k_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_0",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_0_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_0_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_1",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_1.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_1_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_1_l.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_k_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_k_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_k_m.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q4_k_s",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q4_k_s.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_0",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_0_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_0_l.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_1",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_1_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_1_l.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_k_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_k_l.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_k_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_k_m.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q5_k_s",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q5_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q6_k_l",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q6_k_l.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q6_k_m",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q6_k_m.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-xLAM-2-8b-fc-r-q8_0",
        "path": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/Llama-xLAM-2-8b-fc-r-q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Llama-xLAM-2-8b-fc-r model, fine-tuned for function calling and multi-turn conversation, with a focus on improved performance and compatibility with vLLM and Hugging Face Transformers."
  },
  {
    "model_name": "MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF",
    "developer": "DavidAU",
    "downloads": 578,
    "createdAt": "2024-11-07T06:58:54.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-IQ4_XS.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_l.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_m.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_4.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_4_8.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_0_8_8.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q6_k.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-Q8_0.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/MN-Dark-Horror-The-Cliffhanger-18.5B-D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/MN-Dark-Horror-The-Cliffhanger-18.5B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "CausalLM-7B-DPO-alpha-GGUF",
    "developer": "tastypear",
    "downloads": 545,
    "createdAt": "2023-11-19T15:36:16.000Z",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "causallm_7b-dpo-alpha.Q3_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q3_K_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q4_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q5_K_M",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q5_K_S",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q6_K",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q6_K.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.Q8_0",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.Q8_0.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "causallm_7b-dpo-alpha.f16",
        "path": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/causallm_7b-dpo-alpha.f16.gguf",
        "file_size": "14.4 GB"
      }
    ],
    "readme": "https://huggingface.co/tastypear/CausalLM-7B-DPO-alpha-GGUF/resolve/main/README.md",
    "description": "This is a GGUF format quantized version of the CausalLM 7B-DPO-alpha model, compatible with llama.cpp and various UIs, licensed under WTFPL and Meta Llama 2 terms."
  },
  {
    "model_name": "Qwen3-4b-tcomanr-merge",
    "developer": "ertghiu256",
    "downloads": 523,
    "createdAt": "2025-07-17T14:34:16.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "model-F16",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/model-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/Qwen3-4b-tcomanr-merge/resolve/main/README.md",
    "description": "This is a merged Qwen3-4B model combining code, math, and reasoning capabilities from multiple fine-tuned versions using the TIES method."
  },
  {
    "model_name": "Spiral-Qwen3-4B-F32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 516,
    "createdAt": "2025-07-05T12:11:00.000Z",
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Spiral-Qwen3-4B.BF16",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.F16",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.F32",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.F32.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Spiral-Qwen3-4B.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/Spiral-Qwen3-4B.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/Spiral-Qwen3-4B-F32-GGUF/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Llama-3-8B-LexiFun-Uncensored-V1-GGUF",
    "developer": "bartowski",
    "downloads": 504,
    "createdAt": "2024-05-01T16:13:51.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ1_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ1_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q2_K",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-3-8B-LexiFun-Uncensored-V1-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/Llama-3-8B-LexiFun-Uncensored-V1-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3-8B-LexiFun-Uncensored-V1-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "dolphin-2.9.1-yi-1.5-34b-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 486,
    "createdAt": "2024-05-19T14:02:07.000Z",
    "tools": false,
    "num_quants": 21,
    "quants": [
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ1_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ1_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_M.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_S.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XS.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ2_XXS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_M.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XS.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ3_XXS.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-IQ4_XS.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_L.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_M.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q3_K_S.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_M.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q4_K_S.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_M.gguf",
        "file_size": "22.7 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q5_K_S.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "dolphin-2.9.1-yi-1.5-34b.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/dolphin-2.9.1-yi-1.5-34b.i1-Q6_K.gguf",
        "file_size": "26.3 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF/resolve/main/README.md",
    "description": "The model is a 34B parameter version of Dolphin-2.9.1-Yi-1.5, quantized into various GGUF formats for different performance and quality trade-offs."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-Distill",
    "developer": "BasedBase",
    "downloads": 482,
    "createdAt": "2025-08-05T21:00:28.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-480B-Distill-Q8_0",
        "path": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill/resolve/main/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "readme": "https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill/resolve/main/README.md",
    "description": "The model is licensed under the Apache 2.0 license."
  },
  {
    "model_name": "DeepHat-V1-7B-GGUF",
    "developer": "mradermacher",
    "downloads": 474,
    "createdAt": "2025-07-22T05:01:10.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "DeepHat-V1-7B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "DeepHat-V1-7B.f16",
        "path": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/DeepHat-V1-7B.f16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/DeepHat-V1-7B-GGUF/resolve/main/README.md",
    "description": "The DeepHat-V1-7B model by mradermacher is a 7B parameter language model quantized in various formats (Q2_K, Q3_K_S, Q4_K_S, etc.) for efficient deployment, with the highest quality version being Q8_0 at"
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound",
    "developer": "Intel",
    "downloads": 460,
    "createdAt": "2025-08-04T04:33:48.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S",
        "path": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound/resolve/main/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S.gguf",
        "file_size": "10.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q2ks-mixed-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model using the Intel AutoRound algorithm, with 8-bit embeddings and 4-bit non-expert layers, optimized for efficient inference on the codeparrot/github-code-clean dataset."
  },
  {
    "model_name": "qwen3-1.7b-mixture-of-thought",
    "developer": "ertghiu256",
    "downloads": 458,
    "createdAt": "2025-07-19T10:25:11.000Z",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "model-Q4_K_M",
        "path": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/model-Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "model-Q8_0",
        "path": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/model-Q8_0.gguf",
        "file_size": "2.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ertghiu256/qwen3-1.7b-mixture-of-thought/resolve/main/README.md",
    "description": "This is a Qwen3-1.7B model trained on 20k conversations from `open-r1/Mixture-of-Thoughts` and 3k from `mlabonne/FineTome-100k` to enhance reasoning capabilities, optimized for weaker devices."
  },
  {
    "model_name": "MedScholar-1.5B-f32-GGUF",
    "developer": "prithivMLmods",
    "downloads": 446,
    "createdAt": "2025-08-01T12:56:11.000Z",
    "tools": false,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "MedScholar-1.5B.BF16",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.BF16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "MedScholar-1.5B.F16",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.F16.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "MedScholar-1.5B.F32",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.F32.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q2_K",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_L",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q3_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q4_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q4_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q4_K_S.gguf",
        "file_size": "896.7 MB"
      },
      {
        "model_id": "MedScholar-1.5B.Q5_K_M",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q5_K_S",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q6_K",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q6_K.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "MedScholar-1.5B.Q8_0",
        "path": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/MedScholar-1.5B.Q8_0.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/prithivMLmods/MedScholar-1.5B-f32-GGUF/resolve/main/README.md",
    "description": "MedScholar-1.5B-f32-GGUF is a compact, instruction-aligned medical question-answering model fine-tuned on the MIRIAD-4.4M dataset for research and educational use only, not for diagnosis or medical decision-making."
  },
  {
    "model_name": "Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF",
    "developer": "jukofyork",
    "downloads": 445,
    "createdAt": "2025-07-18T11:08:53.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-128k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-128k-Q4_0.gguf",
        "file_size": "426.3 MB"
      },
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-32k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-32k-Q4_0.gguf",
        "file_size": "426.3 MB"
      },
      {
        "model_id": "Kimi-K2-Instruct-DRAFT-0.6B-64k-Q4_0",
        "path": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/Kimi-K2-Instruct-DRAFT-0.6B-64k-Q4_0.gguf",
        "file_size": "426.3 MB"
      }
    ],
    "readme": "https://huggingface.co/jukofyork/Kimi-K2-Instruct-DRAFT-0.6B-v2.0-GGUF/resolve/main/README.md",
    "description": "This is a 0.6B parameter draft model for speculative decoding with Kimi-K2-Instruct, quantized in Q4_0 format for context lengths of 32k, 64k, and 128k."
  },
  {
    "model_name": "xLAM-2-3b-fc-r-GGUF",
    "developer": "Mungert",
    "downloads": 445,
    "createdAt": "2025-06-21T02:39:26.000Z",
    "tools": true,
    "num_quants": 41,
    "quants": [
      {
        "model_id": "xLAM-2-3b-fc-r-bf16",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-bf16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-bf16_q4_k",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-bf16_q4_k.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-bf16_q6_k",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-bf16_q6_k.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-bf16_q8_0",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-bf16_q8_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-f16",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-f16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-f16_q4_k",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-f16_q4_k.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-f16_q6_k",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-f16_q6_k.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-f16_q8_0",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-f16_q8_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq2_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq2_m.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq2_s",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq2_s.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq2_xs",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq2_xs.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq2_xxs",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq2_xxs.gguf",
        "file_size": "979.5 MB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq3_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq3_m.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq3_s",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq3_s.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq3_xs",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq3_xs.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq3_xxs",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq3_xxs.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq4_nl",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq4_nl.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-iq4_xs",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-iq4_xs.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q2_k_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q2_k_l.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q2_k_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q2_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q2_k_s",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q2_k_s.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q3_k_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q3_k_l.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q3_k_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q3_k_m.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q3_k_s",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q3_k_s.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_0",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_0.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_0_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_0_l.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_1",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_1.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_1_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_1_l.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_k_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_k_l.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_k_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_k_m.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q4_k_s",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q4_k_s.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_0",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_0.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_0_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_0_l.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_1",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_1.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_1_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_1_l.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_k_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_k_l.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_k_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_k_m.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q5_k_s",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q5_k_s.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q6_k_l",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q6_k_l.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q6_k_m",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q6_k_m.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "xLAM-2-3b-fc-r-q8_0",
        "path": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/xLAM-2-3b-fc-r-q8_0.gguf",
        "file_size": "3.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF versions of xLAM-2 large action models fine-tuned for function calling, offering improved performance and compatibility with vLLM and Transformers for multi-turn conversations and tool usage."
  },
  {
    "model_name": "NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-Q6_K-GGUF",
    "developer": "DavidAU",
    "downloads": 426,
    "createdAt": "2024-04-21T10:43:00.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "nsfw_dpo_noromaid-7b-mistral-7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/DavidAU/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-Q6_K-GGUF/resolve/main/nsfw_dpo_noromaid-7b-mistral-7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "5.5 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1-Q6_K-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Arch-Router-1.5B.gguf",
    "developer": "katanemo",
    "downloads": 422,
    "createdAt": "2025-05-30T18:18:40.000Z",
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Arch-Router-1.5B-Q2_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_L",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Arch-Router-Q6_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-Q6_K.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/README.md",
    "description": "The katanemo/Arch-Router-1.5B model is a compact, preference-aligned routing framework that maps queries to domain-action preferences for selecting the most suitable large language model.",
    "tools": true
  },
  {
    "model_name": "Osmosis-Apply-1.7B",
    "developer": "osmosis-ai",
    "downloads": 415,
    "createdAt": "2025-06-19T07:02:07.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "osmosis-apply-1.7b-bf16",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-apply-1.7b-bf16.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.IQ4_XS.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q2_K.gguf",
        "file_size": "839.1 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_L.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_M.gguf",
        "file_size": "1023.5 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q3_K_S.gguf",
        "file_size": "954.6 MB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q4_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q4_K_S.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q5_K_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q5_K_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q6_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "osmosis-mcp-1.7b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/osmosis-mcp-1.7b.Q8_0.gguf",
        "file_size": "2.0 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/Osmosis-Apply-1.7B/resolve/main/README.md",
    "description": "Osmosis-Apply-1.7B is a language model finetuned on Qwen3-1.7B to apply edit snippets to original code for code merges, with a reward function prioritizing exactness in the output."
  },
  {
    "model_name": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF",
    "developer": "mradermacher",
    "downloads": 408,
    "createdAt": "2025-07-23T05:15:11.000Z",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.IQ4_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q2_K.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q3_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q4_K_S.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q5_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q6_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.Q8_0.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen3-Jan-Nano-128k-6B-Brainstorm20x.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/Qwen3-Jan-Nano-128k-6B-Brainstorm20x.f16.gguf",
        "file_size": "11.1 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/Qwen3-Jan-Nano-128k-6B-Brainstorm20x-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3 model with various GGUF quantization options for different trade-offs between speed and quality."
  },
  {
    "model_name": "WiNGPT-Babel-2-GGUF",
    "developer": "winninghealth",
    "downloads": 363,
    "createdAt": "2025-06-11T06:11:04.000Z",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "WiNGPT-Babel-2-IQ4_XS",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "WiNGPT-Babel-2-Q4_K_M",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "WiNGPT-Babel-2-Q8_0",
        "path": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/WiNGPT-Babel-2-Q8_0.gguf",
        "file_size": "2.6 GB"
      }
    ],
    "readme": "https://huggingface.co/winninghealth/WiNGPT-Babel-2-GGUF/resolve/main/README.md",
    "description": "WiNGPT-Babel-2 is a multilingual translation language model optimized for 55 languages, enhanced Chinese translation, and structured data handling, built on the GemmaX2-28-2B-Pretrain base model."
  },
  {
    "model_name": "CabraMistral-v3-7b-32k-GGUF",
    "developer": "mradermacher",
    "downloads": 353,
    "createdAt": "2024-05-25T07:46:46.000Z",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q2_K",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q6_K",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.Q8_0",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "CabraMistral-v3-7b-32k.f16",
        "path": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/CabraMistral-v3-7b-32k.f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/mradermacher/CabraMistral-v3-7b-32k-GGUF/resolve/main/README.md",
    "description": "The model is a quantized version of the Mistral-v3-7b-32k base model, available in various GGUF quantization types for different trade-offs between size and quality."
  },
  {
    "model_name": "Llama-3-Soliloquy-8B-v2-GGUF",
    "developer": "backyardai",
    "downloads": 329,
    "createdAt": "2024-05-11T03:18:32.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.F16",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ1_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ2_XXS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ3_XXS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.IQ4_XS",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q2_K",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q2_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q2_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_L",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q3_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q4_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q4_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q5_K_M",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q5_K_S",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q6_K",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Llama-3-Soliloquy-8B-v2.Q8_0",
        "path": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/Llama-3-Soliloquy-8B-v2.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/backyardai/Llama-3-Soliloquy-8B-v2-GGUF/resolve/main/README.md",
    "description": "This GGUF model is a quantized version of the Llama-3-Soliloquy-8B-v2 by openlynn, optimized for efficient use in Backyard AI, a local AI chat app."
  },
  {
    "model_name": "Home-Cook-Mistral-Small-Omni-24B-2507-GGUF",
    "developer": "ngxson",
    "downloads": 313,
    "createdAt": "2025-07-28T19:50:10.000Z",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-Q4_K_M",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-bf16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Home-Cook-Mistral-Small-Omni-2507-f16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/Home-Cook-Mistral-Small-Omni-2507-f16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/mmproj-model-f16.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF/resolve/main/README.md",
    "description": "This model merges Mistral Small 2506 (text) and Voxtral 2507 (audio) into a multimodal model using a modified mergekit tool."
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 299,
    "createdAt": "2025-01-21T03:41:56.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "readme": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/README.md",
    "description": "BaronLLM is a large-language model fine-tuned for offensive cybersecurity research and adversarial simulation, providing exploit reasoning, red-team scenario generation, and safety-constrained content."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound",
    "developer": "Intel",
    "downloads": 294,
    "createdAt": "2025-08-04T08:15:17.000Z",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q4_K_M",
        "path": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound/resolve/main/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q4_K_M.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Intel/Qwen3-Coder-30B-A3B-Instruct-gguf-q4km-AutoRound/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-Coder-30B-A3B-Instruct model, optimized for efficiency using Intel's auto-round algorithm, suitable for deployment with Llamacpp."
  },
  {
    "model_name": "MythoMakiseMerged-13b",
    "developer": "Heralax",
    "downloads": 288,
    "createdAt": "2023-09-30T06:59:22.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "MythoMakiseMerged-13b-q5km",
        "path": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/MythoMakiseMerged-13b-q5km.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "ggml-model-f16",
        "path": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/ggml-model-f16.gguf",
        "file_size": "24.2 GB"
      }
    ],
    "readme": "https://huggingface.co/Heralax/MythoMakiseMerged-13b/resolve/main/README.md",
    "description": "This model is a fine-tuned version of MythoMax-L2-13b, merged with 33% of MythoMax's intelligence, trained on a visual novel script revamped by GPT-4 to excel in banter, conversation, and roleplay, even surpassing its"
  },
  {
    "model_name": "starcoder2-15b-instruct-v0.1-GGUF",
    "developer": "bartowski",
    "downloads": 288,
    "createdAt": "2024-04-29T22:16:56.000Z",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ1_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ1_M.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ1_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ1_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_M.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_S.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_XS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ2_XXS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ3_XXS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ4_NL",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ4_NL.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-IQ4_XS",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-IQ4_XS.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q2_K",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q2_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_L",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_L.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q3_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q3_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q4_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q4_K_M.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q4_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q4_K_S.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q5_K_M",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q5_K_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q5_K_S",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q5_K_S.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q6_K",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q6_K.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "starcoder2-15b-instruct-v0.1-Q8_0",
        "path": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/starcoder2-15b-instruct-v0.1-Q8_0.gguf",
        "file_size": "15.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/starcoder2-15b-instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the starcoder2-15b-instruct-v0.1 model optimized for code generation tasks, with various quantization options available for different performance and memory trade-offs."
  },
  {
    "model_name": "BitCPM4-1B-GGUF",
    "developer": "openbmb",
    "downloads": 286,
    "createdAt": "2025-06-13T11:41:44.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "BitCPM4-1B-q2_k_s",
        "path": "https://huggingface.co/openbmb/BitCPM4-1B-GGUF/resolve/main/BitCPM4-1B-q2_k_s.gguf",
        "file_size": "488.7 MB"
      },
      {
        "model_id": "BitCPM4-1B-q4_0",
        "path": "https://huggingface.co/openbmb/BitCPM4-1B-GGUF/resolve/main/BitCPM4-1B-q4_0.gguf",
        "file_size": "759.6 MB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/BitCPM4-1B-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Skywork-R1V3-38B-GGUF",
    "developer": "Skywork",
    "downloads": 281,
    "createdAt": "2025-07-15T08:34:45.000Z",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Skywork-R1V3-38B-Q4_K_M",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/Skywork-R1V3-38B-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Skywork-R1V3-38B-Q8_0",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/Skywork-R1V3-38B-Q8_0.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-bf16",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-bf16.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-f16",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-f16.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "mmproj-Skywork-R1V3-38B-q8_0",
        "path": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/mmproj-Skywork-R1V3-38B-q8_0.gguf",
        "file_size": "5.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Skywork/Skywork-R1V3-38B-GGUF/resolve/main/README.md",
    "description": "This repository provides a GGUF quantized version of the Skywork-R1V3-38B model for fast and memory-efficient local inference using llama.cpp."
  },
  {
    "model_name": "Sorachio-1B-Chat",
    "developer": "IzzulGod",
    "downloads": 277,
    "createdAt": "2025-07-05T05:04:13.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "sorachio-1b-chat-f16",
        "path": "https://huggingface.co/IzzulGod/Sorachio-1B-Chat/resolve/main/sorachio-1b-chat-f16.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "sorachio-1b-chat-q8_0",
        "path": "https://huggingface.co/IzzulGod/Sorachio-1B-Chat/resolve/main/sorachio-1b-chat-q8_0.gguf",
        "file_size": "1019.8 MB"
      }
    ],
    "readme": "https://huggingface.co/IzzulGod/Sorachio-1B-Chat/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Lily-Cybersecurity-7B-v0.2-GGUF",
    "developer": "QuantFactory",
    "downloads": 270,
    "createdAt": "2024-10-20T14:09:56.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Lily-Cybersecurity-7B-v0.2.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/Lily-Cybersecurity-7B-v0.2.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF/resolve/main/README.md",
    "description": "QuantFactory/Lily-Cybersecurity-7B-v0.2-GGUF is a quantized cybersecurity-focused Mistral-7B-Instruct-v0.2 model fine-tuned with 22,000 cybersecurity-related data pairs for tasks like threat analysis, security protocols,"
  },
  {
    "model_name": "bernie0.1",
    "developer": "ivoras",
    "downloads": 236,
    "createdAt": "2025-07-27T22:05:28.000Z",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "bernie-0.1_IQ4_NL",
        "path": "https://huggingface.co/ivoras/bernie0.1/resolve/main/bernie-0.1_IQ4_NL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "bernie-0.1_f16",
        "path": "https://huggingface.co/ivoras/bernie0.1/resolve/main/bernie-0.1_f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "readme": "https://huggingface.co/ivoras/bernie0.1/resolve/main/README.md",
    "description": "This is a proof of concept model trained on the works of US Senator Bernie Sanders, inspired by the sci-fi concept of \"mind states\" or \"mind uploads,\" and designed to respond to questions about his policies and values."
  },
  {
    "model_name": "LLaMAX3-8B-Alpaca-GGUF",
    "developer": "QuantFactory",
    "downloads": 214,
    "createdAt": "2024-07-14T08:44:43.000Z",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q2_K",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_1",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_1",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q6_K",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "LLaMAX3-8B-Alpaca.Q8_0",
        "path": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/LLaMAX3-8B-Alpaca.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/LLaMAX3-8B-Alpaca-GGUF/resolve/main/README.md",
    "description": "This is a multilingual, quantized version of the LLaMAX3-8B-Alpaca model, trained on 102 languages and fine-tuned for instruction-following, achieving over 5-point improvements in translation performance on the Flores-101 dataset."
  },
  {
    "model_name": "prompt-generator-GGUF",
    "developer": "mav23",
    "downloads": 206,
    "createdAt": "2024-11-30T14:30:00.000Z",
    "num_quants": 17,
    "quants": [
      {
        "model_id": "prompt-generator.Q2_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K_L",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K_M",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "prompt-generator.Q3_K_S",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "prompt-generator.Q4_0",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "prompt-generator.Q4_1",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_1.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "prompt-generator.Q4_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q4_K_M",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q4_K_S",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "prompt-generator.Q5_0",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "prompt-generator.Q5_1",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q5_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "prompt-generator.Q5_K_M",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "prompt-generator.Q5_K_S",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q5_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "prompt-generator.Q6_K",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "prompt-generator.Q8_0",
        "path": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/prompt-generator.Q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "readme": "https://huggingface.co/mav23/prompt-generator-GGUF/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Research-Reasoner-7B-v0.3",
    "developer": "Raymond-dev-546730",
    "downloads": 190,
    "createdAt": "2025-04-03T05:26:56.000Z",
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ3_XS",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_NL",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_XS",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q2_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_M",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_S",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_0",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_1",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_M",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_S",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_0",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_1",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_1.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_M",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_S",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q6_K",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q8_0",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-f16",
        "path": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/Model_Weights/llama.cpp/Research-Reasoner-7B-v0.3-f16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3/resolve/main/README.md",
    "description": "",
    "tools": true
  },
  {
    "model_name": "Layris_9B-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 187,
    "createdAt": "2024-03-06T03:26:55.000Z",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Layris_9B-F16",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-F16.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_M-imat.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_S-imat.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_XS-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Layris_9B-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ3_XXS-imat.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Layris_9B-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-IQ4_XS-imat.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Layris_9B-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q4_K_M-imat.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Layris_9B-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q4_K_S-imat.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Layris_9B-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q5_K_M-imat.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Layris_9B-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q5_K_S-imat.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Layris_9B-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q6_K-imat.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "Layris_9B-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/Layris_9B-Q8_0-imat.gguf",
        "file_size": "8.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "This is a GGUF-Imatrix quantized version of the Layris 9B model, merged from Eris Remix 7B and Mistral 7B-V0.1-Layla-V4, offering improved performance and quality preservation during quantization."
  },
  {
    "model_name": "functionary-v4r-small-preview-GGUF",
    "developer": "Mungert",
    "downloads": 126,
    "createdAt": "2025-03-23T18:56:56.000Z",
    "tools": true,
    "num_quants": 44,
    "quants": [
      {
        "model_id": "functionary-v4r-small-preview-bf16-q4_k",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-bf16-q4_k.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-bf16-q6_k",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-bf16-q6_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-bf16-q8_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-bf16-q8_0.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-bf16",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-bf16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-f16-q4_k",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-f16-q4_k.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-f16-q6_k",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-f16-q6_k.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-f16-q8_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-f16-q8_0.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-f16",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-f16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq1_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq1_m.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq1_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq1_s.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq2_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq2_m.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq2_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq2_s.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq2_xs",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq2_xs.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq2_xxs",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq2_xxs.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq3_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq3_m.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq3_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq3_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq3_xs",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq3_xs.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq3_xxs",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq3_xxs.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq4_nl",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq4_nl.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-iq4_xs",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-iq4_xs.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q2_k_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q2_k_l.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q2_k_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q2_k_s.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q3_k_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q3_k_l.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q3_k_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q3_k_m.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q3_k_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q3_k_s.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_0.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_0_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_0_l.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_1",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_1.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_1_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_1_l.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_k_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_k_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_k_m.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q4_k_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q4_k_s.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_0_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_0_l.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_1",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_1_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_1_l.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_k_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_k_l.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_k_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_k_m.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q5_k_s",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q5_k_s.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q6_k_l",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q6_k_l.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q6_k_m",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q6_k_m.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-q8_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-tq1_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-tq1_0.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "functionary-v4r-small-preview-tq2_0",
        "path": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/functionary-v4r-small-preview-tq2_0.gguf",
        "file_size": "2.4 GB"
      }
    ],
    "readme": "https://huggingface.co/Mungert/functionary-v4r-small-preview-GGUF/resolve/main/README.md",
    "description": "This model is an ultra-low-bit quantized version of Llama-3-8B-Instruct with precision-adaptive quantization (1-2 bit) for extreme memory efficiency and improved accuracy, suitable for CPU and low-VRAM inference."
  },
  {
    "model_name": "HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF",
    "developer": "DevQuasar",
    "downloads": 103,
    "createdAt": "2024-11-01T01:28:38.000Z",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q2_K",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q2_K.gguf",
        "file_size": "643.3 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q3_K_M.gguf",
        "file_size": "820.3 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q4_K_M.gguf",
        "file_size": "1006.7 MB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q5_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q6_K",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "HuggingFaceTB.SmolLM2-1.7B-Instruct.Q8_0",
        "path": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/HuggingFaceTB.SmolLM2-1.7B-Instruct.Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/DevQuasar/HuggingFaceTB.SmolLM2-1.7B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the SmolLM2-1.7B-Instruct model, optimized for efficient text generation on Hugging Face."
  },
  {
    "model_name": "gemma-3-4b-it-unslop-GRPO",
    "developer": "electroglyph",
    "downloads": 66,
    "createdAt": "2025-07-28T06:13:48.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GGUF/gemma-3-4b-it-unslop-GRPO-Q4_K_M",
        "path": "https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO/resolve/main/GGUF/gemma-3-4b-it-unslop-GRPO-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "readme": "https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO/resolve/main/README.md",
    "description": "This is a LoRA adapter for the Gemma 3 4b model, trained to reduce slop output by incorporating frequency-based reward functions and lexical diversity constraints, with training code and dataset examples provided."
  },
  {
    "model_name": "TAIDE-LX-7B-Chat-4bit",
    "developer": "taide",
    "downloads": 18,
    "createdAt": "2024-04-15T03:28:54.000Z",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "taide-7b-a.2-q4_k_m",
        "path": "https://huggingface.co/taide/TAIDE-LX-7B-Chat-4bit/resolve/main/taide-7b-a.2-q4_k_m.gguf",
        "file_size": "3.9 GB"
      }
    ],
    "readme": "https://huggingface.co/taide/TAIDE-LX-7B-Chat-4bit/resolve/main/README.md",
    "description": "TAIDE-LX-7B-Chat 是以 LLaMA2-7B 為基礎，結合台灣繁體中文資料與文化知識，經過持續預訓練和指令微調後的大型語言模型，專注於提升繁體中文生成能力與台灣在地"
  },
  {
    "model_name": "ArmenianGPT-0.1-12B",
    "developer": "ArmGPT",
    "downloads": 7,
    "createdAt": "2025-07-11T21:50:18.000Z",
    "num_quants": 5,
    "quants": [
      {
        "model_id": "ArmenianGPT-0.1-12B-Q5_0",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q5_0.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q5_K_M",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q5_K_S",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q6_K",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "ArmenianGPT-0.1-12B-Q8_0",
        "path": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/ArmenianGPT-0.1-12B-Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ArmGPT/ArmenianGPT-0.1-12B/resolve/main/README.md",
    "description": "",
    "tools": false
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
    "developer": "gabriellarson",
    "downloads": 0,
    "createdAt": "2025-08-07T06:30:31.000Z",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Huihui-gpt-oss-20B-abliterated-F16",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20B-abliterated-F16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-MXFP4_MOE.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      }
    ],
    "readme": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the unsloth/gpt-oss-20b-BF16 model, created with abliteration, designed for research and experimental use with potential risks of generating sensitive or inappropriate content."
  }
]