[
  {
    "model_name": "Apollo2-7B-GGUF",
    "description": "QuantFactory/Apollo2-7B-GGUF is a quantized version of the FreedomIntelligence/Apollo2-7B model, trained on a diverse set of medical datasets across 50 languages, designed for efficient and accurate question-answering in multiple languages including English, Chinese",
    "developer": "QuantFactory",
    "downloads": 230,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Apollo2-7B.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Apollo2-7B.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Apollo2-7B.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Apollo2-7B.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Apollo2-7B.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Apollo2-7B.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Apollo2-7B.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Apollo2-7B.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Apollo2-7B.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q5_0.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Apollo2-7B.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q5_1.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Apollo2-7B.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Apollo2-7B.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Apollo2-7B.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Apollo2-7B.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Apollo2-7B-GGUF/resolve/main/Apollo2-7B.Q8_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "createdAt": "2025-06-21T13:35:52.000Z"
  },
  {
    "model_name": "bitnet-b1.58-2B-4T-gguf",
    "description": "This repository provides the weights for BitNet b1.58 2B4T, a native 1-bit Large Language Model developed by Microsoft Research that achieves performance comparable to full-precision models while significantly improving computational efficiency for deployment on CPU and GPU.",
    "developer": "microsoft",
    "downloads": 9994,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ggml-model-i2_s",
        "path": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "createdAt": "2025-04-15T04:25:42.000Z"
  },
  {
    "model_name": "claude-3.7-sonnet-reasoning-gemma3-12B",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL library.",
    "developer": "reedmayhew",
    "downloads": 12181,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0",
        "path": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "createdAt": "2025-03-22T19:40:09.000Z"
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "description": "BaronLLM is a large-language model fine-tuned for offensive cybersecurity research and adversarial simulation, offering capabilities in exploit reasoning, red-team scenario generation, and log triage while enforcing safety constraints to prevent illicit content.",
    "developer": "AlicanKiraz0",
    "downloads": 256,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "createdAt": "2025-01-21T03:41:56.000Z"
  },
  {
    "model_name": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF",
    "description": "This is a quantized version of the Qwen3-32B model, offering various quantization levels (Q2_K to Q8_0) for different trade-offs between speed and quality, with static quants available and dynamic quants (like IQ4_XS) potentially forthcoming.",
    "developer": "mradermacher",
    "downloads": 7591,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q2_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q6_K",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q8_0",
        "path": "https://huggingface.co/mradermacher/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "createdAt": "2025-06-08T09:10:43.000Z"
  },
  {
    "model_name": "DeepSeek-R1-0528-GGUF",
    "description": "This repository provides quantized versions of the DeepSeek-R1-0528 model using ik_llama.cpp, optimized for CPU+GPU systems with under 16GB or 24GB VRAM, offering advanced non-linear quantization and MLA support for improved context length and performance on",
    "developer": "ubergarm",
    "downloads": 6941,
    "num_quants": 33,
    "quants": [
      {
        "model_id": "IQ1_S/DeepSeek-R1-0528-IQ1_S-00001-of-00003",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ1_S/DeepSeek-R1-0528-IQ1_S-00001-of-00003.gguf",
        "file_size": "44.5 GB"
      },
      {
        "model_id": "IQ1_S/DeepSeek-R1-0528-IQ1_S-00002-of-00003",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ1_S/DeepSeek-R1-0528-IQ1_S-00002-of-00003.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "IQ1_S/DeepSeek-R1-0528-IQ1_S-00003-of-00003",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ1_S/DeepSeek-R1-0528-IQ1_S-00003-of-00003.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "IQ1_S_R4/DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ1_S_R4/DeepSeek-R1-0528-IQ1_S_R4-00001-of-00003.gguf",
        "file_size": "43.5 GB"
      },
      {
        "model_id": "IQ1_S_R4/DeepSeek-R1-0528-IQ1_S_R4-00002-of-00003",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ1_S_R4/DeepSeek-R1-0528-IQ1_S_R4-00002-of-00003.gguf",
        "file_size": "43.5 GB"
      },
      {
        "model_id": "IQ1_S_R4/DeepSeek-R1-0528-IQ1_S_R4-00003-of-00003",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ1_S_R4/DeepSeek-R1-0528-IQ1_S_R4-00003-of-00003.gguf",
        "file_size": "43.2 GB"
      },
      {
        "model_id": "IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00001-of-00005",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00001-of-00005.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00002-of-00005",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00002-of-00005.gguf",
        "file_size": "43.3 GB"
      },
      {
        "model_id": "IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00003-of-00005",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00003-of-00005.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00004-of-00005",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00004-of-00005.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00005-of-00005",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ2_K_R4/DeepSeek-R1-0528-IQ2_K_R4-00005-of-00005.gguf",
        "file_size": "42.7 GB"
      },
      {
        "model_id": "IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00001-of-00006",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00001-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00002-of-00006",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00002-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00003-of-00006",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00003-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00004-of-00006",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00004-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00005-of-00006",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00005-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00006-of-00006",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_KT/DeepSeek-R1-0528-IQ3_KT-00006-of-00006.gguf",
        "file_size": "40.5 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00001-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00001-of-00007.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00002-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00002-of-00007.gguf",
        "file_size": "43.1 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00003-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00003-of-00007.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00004-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00004-of-00007.gguf",
        "file_size": "42.8 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00005-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00005-of-00007.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00006-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00006-of-00007.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00007-of-00007",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ3_K_R4/DeepSeek-R1-0528-IQ3_K_R4-00007-of-00007.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00001-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00001-of-00009.gguf",
        "file_size": "40.9 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00002-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00002-of-00009.gguf",
        "file_size": "41.6 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00003-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00003-of-00009.gguf",
        "file_size": "39.8 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00004-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00004-of-00009.gguf",
        "file_size": "41.8 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00005-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00005-of-00009.gguf",
        "file_size": "39.8 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00006-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00006-of-00009.gguf",
        "file_size": "41.8 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00007-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00007-of-00009.gguf",
        "file_size": "39.8 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00008-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00008-of-00009.gguf",
        "file_size": "41.8 GB"
      },
      {
        "model_id": "IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00009-of-00009",
        "path": "https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_KS_R4/DeepSeek-R1-0528-IQ4_KS_R4-00009-of-00009.gguf",
        "file_size": "40.4 GB"
      }
    ],
    "createdAt": "2025-05-29T03:40:22.000Z"
  },
  {
    "model_name": "DeepSeek-R1-0528-GGUF",
    "description": "The DeepSeek-R1-0528 model, developed by DeepSeek-AI, is a high-performance large language model that significantly improves reasoning and inference capabilities, offering reduced hallucination rates, enhanced function calling support, and is available in various quantized formats for efficient deployment, with detailed usage",
    "developer": "unsloth",
    "downloads": 135981,
    "num_quants": 235,
    "quants": [
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00001-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00001-of-00030.gguf",
        "file_size": "40.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00002-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00002-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00003-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00003-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00004-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00004-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00005-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00005-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00006-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00006-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00007-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00007-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00008-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00008-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00009-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00009-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00010-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00010-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00011-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00011-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00012-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00012-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00013-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00013-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00014-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00014-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00015-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00015-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00016-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00016-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00017-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00017-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00018-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00018-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00019-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00019-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00020-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00020-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00021-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00021-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00022-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00022-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00023-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00023-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00024-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00024-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00025-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00025-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00026-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00026-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00027-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00027-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00028-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00028-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00029-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00029-of-00030.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00030-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00030-of-00030.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/DeepSeek-R1-0528-UD-TQ1_0.gguf",
        "file_size": "150.5 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00001-of-00008.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00002-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00003-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00005-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00006-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00008-of-00008.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00001-of-00008.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00002-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00003-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00004-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00005-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00006-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00007-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00008-of-00008.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00001-of-00005.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00002-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00003-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00004-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00005-of-00005.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00001-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00002-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00003-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00004-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00005-of-00005.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00001-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00001-of-00007.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00002-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00002-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00003-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00003-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00004-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00004-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00005-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00005-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00006-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00006-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00007-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00007-of-00007.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00001-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00002-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00003-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00004-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00005-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00006-of-00006.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00001-of-00008.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00002-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00003-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00005-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00008-of-00008.gguf",
        "file_size": "30.1 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00001-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00001-of-00009.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00002-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00002-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00003-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00003-of-00009.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00004-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00004-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00005-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00005-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00006-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00006-of-00009.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00007-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00007-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00008-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00008-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00009-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00009-of-00009.gguf",
        "file_size": "33.4 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00001-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00001-of-00009.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00002-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00002-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00003-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00003-of-00009.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00004-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00004-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00005-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00005-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00006-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00006-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00007-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00007-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00008-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00008-of-00009.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00009-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00009-of-00009.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00001-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00002-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00003-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00005-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00008-of-00008.gguf",
        "file_size": "30.1 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00001-of-00010.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00002-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00003-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00004-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00005-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00006-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00007-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00008-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00009-of-00010.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00010-of-00010.gguf",
        "file_size": "36.2 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00001-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00002-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00003-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00004-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00005-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00006-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00007-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00008-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00009-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00010-of-00010.gguf",
        "file_size": "31.9 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00001-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00001-of-00012.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00002-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00002-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00003-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00003-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00004-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00004-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00005-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00005-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00006-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00006-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00007-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00007-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00008-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00008-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00009-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00009-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00010-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00010-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00011-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00011-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00012-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00012-of-00012.gguf",
        "file_size": "29.3 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00001-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00001-of-00015.gguf",
        "file_size": "44.5 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00002-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00002-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00003-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00003-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00004-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00004-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00005-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00005-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00006-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00006-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00007-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00007-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00008-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00008-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00009-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00009-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00010-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00010-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00011-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00011-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00012-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00012-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00013-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00013-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00014-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00014-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00015-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00015-of-00015.gguf",
        "file_size": "27.4 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00002-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00003-of-00005.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00004-of-00005.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00005-of-00005.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004.gguf",
        "file_size": "35.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00001-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00002-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00003-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00004-of-00005.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00005-of-00005.gguf",
        "file_size": "28.5 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00001-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00002-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00003-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00004-of-00005.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00005-of-00005.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00001-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00002-of-00006.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00003-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00004-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00005-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00006-of-00006.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00002-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00003-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00005-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00006-of-00006.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00001-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00001-of-00007.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00002-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00002-of-00007.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00003-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00003-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00004-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00004-of-00007.gguf",
        "file_size": "45.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00005-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00005-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00006-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00006-of-00007.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00007-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00007-of-00007.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00001-of-00008.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00002-of-00008.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00003-of-00008.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00005-of-00008.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00008-of-00008.gguf",
        "file_size": "38.0 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00001-of-00010.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00002-of-00010.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00003-of-00010.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00004-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00005-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00006-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00007-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00008-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00009-of-00010.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00010-of-00010.gguf",
        "file_size": "42.8 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00001-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00001-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00002-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00002-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00003-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00003-of-00012.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00004-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00004-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00005-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00005-of-00012.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00006-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00006-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00007-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00007-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00008-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00008-of-00012.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00009-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00009-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00010-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00010-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00011-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00011-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00012-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00012-of-00012.gguf",
        "file_size": "40.3 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00001-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00001-of-00016.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00002-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00002-of-00016.gguf",
        "file_size": "43.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00003-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00003-of-00016.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00004-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00004-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00005-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00005-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00006-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00006-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00007-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00007-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00008-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00008-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00009-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00009-of-00016.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00010-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00010-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00011-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00011-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00012-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00012-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00013-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00013-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00014-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00014-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00015-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00015-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00016-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00016-of-00016.gguf",
        "file_size": "46.3 GB"
      }
    ],
    "createdAt": "2025-05-28T18:10:04.000Z"
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "description": "This model is a distilled version of DeepSeek-R1-0528 trained on Qwen3-8B, offering state-of-the-art performance on reasoning tasks like AIME with improved accuracy, reduced hallucination, and enhanced support for function calling and multi-language capabilities, and can be run",
    "developer": "unsloth",
    "downloads": 418894,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "createdAt": "2025-05-29T14:17:25.000Z"
  },
  {
    "model_name": "Devstral-Small-2505-GGUF",
    "description": "This model is an enhanced version of Mistral-Small-3.1 with a 128k context window, Apache 2.0 license, and optimized for software engineering tasks, allowing users to run and fine-tune it for local deployment or via APIs like LMStudio, vLL",
    "developer": "unsloth",
    "downloads": 109421,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Devstral-Small-2505-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2505-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/Devstral-Small-2505-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2505-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "createdAt": "2025-05-21T14:20:05.000Z"
  },
  {
    "model_name": "dots.llm1.inst-GGUF",
    "description": "The `dots.llm1` model is a large-scale MoE model with 14B activated parameters out of 142B total, trained on high-quality data without synthetic data, offering efficient and accurate text generation capabilities in English and Chinese.",
    "developer": "unsloth",
    "downloads": 3386,
    "num_quants": 65,
    "quants": [
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00001-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00001-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00002-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00002-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00003-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00003-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00004-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00004-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00005-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00005-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00006-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00006-of-00006.gguf",
        "file_size": "36.1 GB"
      },
      {
        "model_id": "IQ4_NL/dots.llm1.inst-IQ4_NL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_NL/dots.llm1.inst-IQ4_NL-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/dots.llm1.inst-IQ4_NL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_NL/dots.llm1.inst-IQ4_NL-00002-of-00002.gguf",
        "file_size": "28.7 GB"
      },
      {
        "model_id": "IQ4_XS/dots.llm1.inst-IQ4_XS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_XS/dots.llm1.inst-IQ4_XS-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_XS/dots.llm1.inst-IQ4_XS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_XS/dots.llm1.inst-IQ4_XS-00002-of-00002.gguf",
        "file_size": "25.7 GB"
      },
      {
        "model_id": "Q2_K/dots.llm1.inst-Q2_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K/dots.llm1.inst-Q2_K-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q2_K/dots.llm1.inst-Q2_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K/dots.llm1.inst-Q2_K-00002-of-00002.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q2_K_L/dots.llm1.inst-Q2_K_L-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K_L/dots.llm1.inst-Q2_K_L-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q2_K_L/dots.llm1.inst-Q2_K_L-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K_L/dots.llm1.inst-Q2_K_L-00002-of-00002.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q3_K_M/dots.llm1.inst-Q3_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_M/dots.llm1.inst-Q3_K_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_M/dots.llm1.inst-Q3_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_M/dots.llm1.inst-Q3_K_M-00002-of-00002.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "Q3_K_S/dots.llm1.inst-Q3_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_S/dots.llm1.inst-Q3_K_S-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_S/dots.llm1.inst-Q3_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_S/dots.llm1.inst-Q3_K_S-00002-of-00002.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Q4_0/dots.llm1.inst-Q4_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_0/dots.llm1.inst-Q4_0-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q4_0/dots.llm1.inst-Q4_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_0/dots.llm1.inst-Q4_0-00002-of-00002.gguf",
        "file_size": "28.7 GB"
      },
      {
        "model_id": "Q4_1/dots.llm1.inst-Q4_1-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_1/dots.llm1.inst-Q4_1-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_1/dots.llm1.inst-Q4_1-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_1/dots.llm1.inst-Q4_1-00002-of-00002.gguf",
        "file_size": "36.9 GB"
      },
      {
        "model_id": "Q4_K_M/dots.llm1.inst-Q4_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_M/dots.llm1.inst-Q4_K_M-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_M/dots.llm1.inst-Q4_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_M/dots.llm1.inst-Q4_K_M-00002-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Q4_K_S/dots.llm1.inst-Q4_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_S/dots.llm1.inst-Q4_K_S-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/dots.llm1.inst-Q4_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_S/dots.llm1.inst-Q4_K_S-00002-of-00002.gguf",
        "file_size": "34.2 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00001-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00003-of-00003.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00001-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00002-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00003-of-00003.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00003-of-00003.gguf",
        "file_size": "26.3 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00001-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00001-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00002-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00002-of-00004.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00003-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00003-of-00004.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00004-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00004-of-00004.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00001-of-00002.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00002-of-00002.gguf",
        "file_size": "782.4 MB"
      },
      {
        "model_id": "UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00002-of-00002.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00002-of-00002.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00002-of-00002.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00002-of-00002.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00002-of-00002.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00002-of-00002.gguf",
        "file_size": "34.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00002-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00003-of-00003.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00003-of-00003.gguf",
        "file_size": "29.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00001-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00002-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00003-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00004-of-00004.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "dots.llm1.inst-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/dots.llm1.inst-UD-IQ1_S.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "dots.llm1.inst-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/dots.llm1.inst-UD-TQ1_0.gguf",
        "file_size": "44.7 GB"
      }
    ],
    "createdAt": "2025-06-16T12:59:47.000Z"
  },
  {
    "model_name": "gemma-2b-it",
    "description": "The Gemma 2B instruct model is a lightweight, open-source, instruction-tuned large language model from Google, trained on diverse text data including web documents, code, and mathematics, offering efficient text generation capabilities with support for CPU/GPU inference, fine-tuning, and ethical safety measures.",
    "developer": "google",
    "downloads": 252911,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b-it",
        "path": "https://huggingface.co/google/gemma-2b-it/resolve/main/gemma-2b-it.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "createdAt": "2024-02-08T13:23:59.000Z"
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks showing strong performance in reasoning, STEM tasks, and",
    "developer": "google",
    "downloads": 93817,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "createdAt": "2025-03-12T12:32:41.000Z"
  },
  {
    "model_name": "gemma-3-1b-it-qat-q4_0-gguf",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks for reasoning, STEM, multilingual, and multim",
    "developer": "google",
    "downloads": 2735,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf",
        "file_size": "957.1 MB"
      }
    ],
    "createdAt": "2025-03-10T22:42:41.000Z"
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "description": "This is an uncensored version of the Gemma 3 27B IT model, created using a layerwise abliteration technique that significantly improves acceptance rate while maintaining model coherence and performance.",
    "developer": "mlabonne",
    "downloads": 16870,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "createdAt": "2025-03-17T20:35:16.000Z"
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multimodal model from Google, capable of handling text and image inputs to generate text outputs, with a 128K context window and support for over 140 languages, trained on diverse data including text,",
    "developer": "google",
    "downloads": 10261,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "createdAt": "2025-03-20T22:44:27.000Z"
  },
  {
    "model_name": "gemma-3-4b-it-qat-q4_0-gguf",
    "description": "The Gemma 3 4B instruction-tuned model is a lightweight, open-source, multimodal foundation model from Google, capable of handling text and image inputs to generate text outputs, trained on diverse data including web text, code, math, and images, with a 128K",
    "developer": "google",
    "downloads": 7650,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mmproj-model-f16-4B",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-4B.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "createdAt": "2025-03-12T20:43:28.000Z"
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices and capable of handling text, image, video, and audio inputs, with detailed usage guidelines, fine-tuning options, and performance benchmarks available on",
    "developer": "unsloth",
    "downloads": 0,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-BF16.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q2_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q3_K_XL.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q4_K_XL.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q5_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q6_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q8_K_XL.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "createdAt": "2025-06-26T12:24:52.000Z"
  },
  {
    "model_name": "gemma-3n-E2B-it-text-GGUF",
    "description": "This GGUF quantized version of the Gemma-3n-E2B-it model by Google is a community-contributed, lightweight variant optimized for use with LM Studio's tools, developed using llama.cpp.",
    "developer": "lmstudio-community",
    "downloads": 0,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "createdAt": "2025-06-26T15:16:13.000Z"
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, and audio inputs, and capable of generating up to 32K tokens of text output, offering superior performance",
    "developer": "unsloth",
    "downloads": 0,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-BF16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q2_K_XL.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q3_K_XL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q5_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q6_K_XL.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q8_K_XL.gguf",
        "file_size": "9.4 GB"
      }
    ],
    "createdAt": "2025-06-26T12:24:35.000Z"
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "description": "This GGUF version of the Gemma 3n model is a lightweight, efficient language model without multimodal support, developed by Google DeepMind and available for use with tools like llama.cpp and LM Studio.",
    "developer": "ggml-org",
    "downloads": 0,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-f16",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-f16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "createdAt": "2025-06-26T09:59:16.000Z"
  },
  {
    "model_name": "gemma-7b-it",
    "description": "The Gemma 7B instruct model is a lightweight, open-source text-to-text large language model developed by Google, trained on diverse data including web text, code, and mathematics, and optimized for deployment on resource-constrained systems with support for fine-tuning, multi-GPU training, and various",
    "developer": "google",
    "downloads": 61208,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b-it",
        "path": "https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "createdAt": "2024-02-13T01:07:30.000Z"
  },
  {
    "model_name": "gemma3-12B-claude-3.7-sonnet-reasoning-distilled",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL library.",
    "developer": "reedmayhew",
    "downloads": 481,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma3-12b-claude-3.7-sonnet-reasoning-distilled.Q8_0",
        "path": "https://huggingface.co/reedmayhew/gemma3-12B-claude-3.7-sonnet-reasoning-distilled/resolve/main/gemma3-12b-claude-3.7-sonnet-reasoning-distilled.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "createdAt": "2025-03-29T18:33:26.000Z"
  },
  {
    "model_name": "google-gemma-3-27b-it-qat-q4_0-gguf-small",
    "description": "This is a requantized version of the Google Gemma-3-27B model using Q4_0 quantization, which results in a smaller file size and slightly lower perplexity compared to the original QAT weights, while also fixing control token metadata issues for better performance in instruct mode",
    "developer": "stduhpf",
    "downloads": 1665,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0_s",
        "path": "https://huggingface.co/stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small/resolve/main/gemma-3-27b-it-q4_0_s.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "mmproj-google_gemma-3-27b-it-f16",
        "path": "https://huggingface.co/stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small/resolve/main/mmproj-google_gemma-3-27b-it-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "createdAt": "2025-04-05T18:23:18.000Z"
  },
  {
    "model_name": "II-Medical-8B-1706-GGUF",
    "description": "This repository provides static quantized versions of the II-Medical-8B-1706 model, including GGUF files for various quantization levels ranging from Q2_K (3.4GB) to Q8_0 (8.8GB), optimized for efficient inference on different",
    "developer": "Intelligent-Internet",
    "downloads": 1491,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "II-Medical-8B-1706.F16",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q2_K",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q3_K_L",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q3_K_M",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q3_K_S",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q4_K_M",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q4_K_S",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q5_K_M",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q5_K_S",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q6_K",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "II-Medical-8B-1706.Q8_0",
        "path": "https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF/resolve/main/II-Medical-8B-1706.Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "createdAt": "2025-06-16T07:14:26.000Z"
  },
  {
    "model_name": "Impish_Magic_24B-GGUF",
    "description": "This repository provides GGUF quantized versions of the Impish_Magic_24B model from SicariusSicariiStuff, including various quantization types like Q2_K, Q3_K_S, Q4_K_S, and IQ4_XS, with notes on quality and performance",
    "developer": "mradermacher",
    "downloads": 875,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Impish_Magic_24B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.IQ4_XS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Impish_Magic_24B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Impish_Magic_24B-GGUF/resolve/main/Impish_Magic_24B.Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "createdAt": "2025-06-20T11:54:41.000Z"
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed to enhance research capabilities by efficiently handling long contexts without performance degradation, making it ideal for deep document analysis and complex reasoning tasks.",
    "developer": "Menlo",
    "downloads": 0,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "createdAt": "2025-06-24T07:29:01.000Z"
  },
  {
    "model_name": "Jan-nano-128k-GGUF",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed to enhance research capabilities by efficiently handling long contexts without performance degradation, offering superior accuracy and compatibility with MCP servers.",
    "developer": "unsloth",
    "downloads": 0,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-128k-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "createdAt": "2025-06-25T07:31:40.000Z"
  },
  {
    "model_name": "Jan-nano-gguf",
    "description": "Jan Nano is a compact, fine-tuned language model based on Qwen3, optimized for efficient local deployment with strong capabilities in text generation, tool use, and research tasks.",
    "developer": "Menlo",
    "downloads": 19148,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-4b-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-4b-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "createdAt": "2025-06-11T07:14:33.000Z"
  },
  {
    "model_name": "Jan-nano-GGUF",
    "description": "Jan-Nano is a 4B parameter language model optimized for deep research tasks and MCP servers, offering strong performance on benchmarks and seamless integration with the Jan framework for local, privacy-preserving use.",
    "developer": "unsloth",
    "downloads": 9238,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-GGUF/resolve/main/Jan-nano-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "createdAt": "2025-06-16T05:44:15.000Z"
  },
  {
    "model_name": "Kimi-Dev-72B-GGUF",
    "description": "Kimi-Dev-72B is an open-source coding LLM that achieves 60.4% performance on SWE-bench Verified, surpassing other leading quantized models with its optimized training via large-scale reinforcement learning for robust software engineering solutions.",
    "developer": "unsloth",
    "downloads": 5903,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00003-of-00003.gguf",
        "file_size": "42.7 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-IQ4_NL.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-IQ4_XS.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q3_K_M.gguf",
        "file_size": "35.1 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q3_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q4_0",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q4_0.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q4_1",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q4_1.gguf",
        "file_size": "42.6 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ1_M.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ1_S.gguf",
        "file_size": "21.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ2_M.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ2_XXS.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ3_XXS.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-Q2_K_XL.gguf",
        "file_size": "28.3 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-Q3_K_XL.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "Q6_K/Kimi-Dev-72B-Q6_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q6_K/Kimi-Dev-72B-Q6_K-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/Kimi-Dev-72B-Q6_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q6_K/Kimi-Dev-72B-Q6_K-00002-of-00002.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Q8_0/Kimi-Dev-72B-Q8_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q8_0/Kimi-Dev-72B-Q8_0-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/Kimi-Dev-72B-Q8_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q8_0/Kimi-Dev-72B-Q8_0-00002-of-00002.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00002-of-00002.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00002-of-00002.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00002-of-00002.gguf",
        "file_size": "31.7 GB"
      }
    ],
    "createdAt": "2025-06-17T00:37:17.000Z"
  },
  {
    "model_name": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF",
    "description": "This is a modified Llama 3.2 model designed for creative writing, capable of generating vivid, uncensored, and genre-specific fiction across science fiction, horror, romance, and more, with enhanced prose and context abilities, requiring careful parameter tuning for optimal performance.",
    "developer": "DavidAU",
    "downloads": 23383,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q2_k.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_l.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_m.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q3_k_s.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_4.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_4_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_0_8_8.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_m.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q4_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q5_k_s.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q6_k.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF/resolve/main/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-D_AU-q5_k_m.gguf",
        "file_size": "5.0 GB"
      }
    ],
    "createdAt": "2024-10-28T10:09:51.000Z"
  },
  {
    "model_name": "Llama-3.1-8B-Lexi-Uncensored-V2-GGUF",
    "description": "This model is an uncensored version of Llama-3.1-8B-Instruct, compliant with Meta's Llama-3.1 license, achieving high performance on various benchmark tasks with strict accuracy of 77.92 on IFEval (0-Shot) and normalized",
    "developer": "Orenguteng",
    "downloads": 7544,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_F16",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_Q4",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_Q5",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Llama-3.1-8B-Lexi-Uncensored_V2_Q8",
        "path": "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q8.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "createdAt": "2024-08-09T20:05:30.000Z"
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-GGUF",
    "description": "This repository provides quantized versions of the Llama-3.2-3B-Instruct model by Bartowski, optimized for various hardware platforms and inference speeds, with detailed guidelines on selecting the appropriate quantization type based on system resources and performance requirements.",
    "developer": "bartowski",
    "downloads": 145410,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "createdAt": "2024-09-25T18:35:33.000Z"
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "description": "This is a highly specialized, uncensored, and vividly creative Llama 3.2 MOE model with 18.4B parameters, combining eight top L3.2 3B models into a powerful, fast, and versatile tool for generating immersive, genre-defying fiction,",
    "developer": "DavidAU",
    "downloads": 72012,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "createdAt": "2024-12-10T13:12:37.000Z"
  },
  {
    "model_name": "Llama-4-Maverick-17B-128E-Instruct-GGUF",
    "description": "The transformers library provides access to the Llama 4 Community License Agreement, which allows non-exclusive, worldwide, non-transferable, and royalty-free use of Meta's Llama 4 models for commercial and research purposes in multiple languages, with specific terms regarding redistribution, intellectual property, and safety safeguards.",
    "developer": "unsloth",
    "downloads": 48821,
    "num_quants": 147,
    "quants": [
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00001-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00001-of-00018.gguf",
        "file_size": "43.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00002-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00002-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00003-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00003-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00004-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00004-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00005-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00005-of-00018.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00006-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00006-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00007-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00007-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00008-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00008-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00009-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00009-of-00018.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00010-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00010-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00011-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00011-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00012-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00012-of-00018.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00013-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00013-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00014-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00014-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00015-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00015-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00016-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00016-of-00018.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00017-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00017-of-00018.gguf",
        "file_size": "40.0 GB"
      },
      {
        "model_id": "BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00018-of-00018",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/BF16/Llama-4-Maverick-17B-128E-Instruct-BF16-00018-of-00018.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00001-of-00005.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00002-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00003-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00004-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Maverick-17B-128E-Instruct-IQ4_NL-00005-of-00005.gguf",
        "file_size": "25.9 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00001-of-00005.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00002-of-00005.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00003-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00004-of-00005.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Maverick-17B-128E-Instruct-IQ4_XS-00005-of-00005.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Llama-4-Maverick-17B-128E-Instruct-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Llama-4-Maverick-17B-128E-Instruct-UD-TQ1_0.gguf",
        "file_size": "98.5 GB"
      },
      {
        "model_id": "Q2_K/Llama-4-Maverick-17B-128E-Instruct-Q2_K-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q2_K/Llama-4-Maverick-17B-128E-Instruct-Q2_K-00001-of-00003.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q2_K/Llama-4-Maverick-17B-128E-Instruct-Q2_K-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q2_K/Llama-4-Maverick-17B-128E-Instruct-Q2_K-00002-of-00003.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q2_K/Llama-4-Maverick-17B-128E-Instruct-Q2_K-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q2_K/Llama-4-Maverick-17B-128E-Instruct-Q2_K-00003-of-00003.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q2_K_L/Llama-4-Maverick-17B-128E-Instruct-Q2_K_L-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q2_K_L/Llama-4-Maverick-17B-128E-Instruct-Q2_K_L-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q2_K_L/Llama-4-Maverick-17B-128E-Instruct-Q2_K_L-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q2_K_L/Llama-4-Maverick-17B-128E-Instruct-Q2_K_L-00002-of-00003.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q2_K_L/Llama-4-Maverick-17B-128E-Instruct-Q2_K_L-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q2_K_L/Llama-4-Maverick-17B-128E-Instruct-Q2_K_L-00003-of-00003.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00001-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00002-of-00004.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00003-of-00004.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_M/Llama-4-Maverick-17B-128E-Instruct-Q3_K_M-00004-of-00004.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00001-of-00004.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00002-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00003-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q3_K_S/Llama-4-Maverick-17B-128E-Instruct-Q3_K_S-00004-of-00004.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00001-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00002-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00003-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00004-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Maverick-17B-128E-Instruct-Q4_0-00005-of-00005.gguf",
        "file_size": "25.9 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00001-of-00006.gguf",
        "file_size": "43.5 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00002-of-00006.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00003-of-00006.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00004-of-00006.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00005-of-00006.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Maverick-17B-128E-Instruct-Q4_1-00006-of-00006.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00001-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00002-of-00005.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00003-of-00005.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00004-of-00005.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Maverick-17B-128E-Instruct-Q4_K_M-00005-of-00005.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00001-of-00005.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00002-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00003-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00004-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Maverick-17B-128E-Instruct-Q4_K_S-00005-of-00005.gguf",
        "file_size": "28.9 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00001-of-00006.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00002-of-00006.gguf",
        "file_size": "43.2 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00003-of-00006.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00004-of-00006.gguf",
        "file_size": "43.2 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00005-of-00006.gguf",
        "file_size": "43.2 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Maverick-17B-128E-Instruct-Q5_K_M-00006-of-00006.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00001-of-00006.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00002-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00003-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00004-of-00006.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00005-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Maverick-17B-128E-Instruct-Q5_K_S-00006-of-00006.gguf",
        "file_size": "28.2 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00001-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00001-of-00007.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00002-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00002-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00003-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00003-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00004-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00004-of-00007.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00005-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00005-of-00007.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00006-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00006-of-00007.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00007-of-00007",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Maverick-17B-128E-Instruct-Q6_K-00007-of-00007.gguf",
        "file_size": "37.8 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00001-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00001-of-00009.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00002-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00002-of-00009.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00003-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00003-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00004-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00004-of-00009.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00005-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00005-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00006-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00006-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00007-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00007-of-00009.gguf",
        "file_size": "42.5 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00008-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00008-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00009-of-00009",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Maverick-17B-128E-Instruct-Q8_0-00009-of-00009.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-IQ1_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ1_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ1_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ1_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00002-of-00003.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ1_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00003-of-00003.gguf",
        "file_size": "27.7 GB"
      },
      {
        "model_id": "UD-IQ1_S/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ1_S/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_S-00001-of-00003.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-IQ1_S/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ1_S/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_S-00002-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ1_S/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ1_S/Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_S-00003-of-00003.gguf",
        "file_size": "20.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ2_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M-00001-of-00003.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ2_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M-00002-of-00003.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ2_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ2_M/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M-00003-of-00003.gguf",
        "file_size": "39.2 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_XXS-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ2_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_XXS-00001-of-00003.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_XXS-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ2_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_XXS-00002-of-00003.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_XXS-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ2_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_XXS-00003-of-00003.gguf",
        "file_size": "35.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00001-of-00004.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00002-of-00004.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00003-of-00004.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-IQ3_XXS/Llama-4-Maverick-17B-128E-Instruct-UD-IQ3_XXS-00004-of-00004.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00001-of-00004.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00002-of-00004.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00003-of-00004.gguf",
        "file_size": "45.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00004-of-00004.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00001-of-00004.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00002-of-00004.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00003-of-00004.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q3_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q3_K_XL-00004-of-00004.gguf",
        "file_size": "31.7 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00001-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00002-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00003-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00004-of-00005.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00005-of-00005.gguf",
        "file_size": "30.9 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00001-of-00006.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00002-of-00006.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00003-of-00006.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00004-of-00006.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00005-of-00006.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q5_K_XL-00006-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00001-of-00008.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00002-of-00008.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00003-of-00008.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00004-of-00008.gguf",
        "file_size": "42.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00005-of-00008.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00006-of-00008.gguf",
        "file_size": "42.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00007-of-00008.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q6_K_XL-00008-of-00008.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00001-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00001-of-00010.gguf",
        "file_size": "41.4 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00002-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00002-of-00010.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00003-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00003-of-00010.gguf",
        "file_size": "43.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00004-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00004-of-00010.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00005-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00005-of-00010.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00006-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00006-of-00010.gguf",
        "file_size": "43.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00007-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00007-of-00010.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00008-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00008-of-00010.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00009-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00009-of-00010.gguf",
        "file_size": "43.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00010-of-00010",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q8_K_XL-00010-of-00010.gguf",
        "file_size": "36.2 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "createdAt": "2025-04-08T11:27:18.000Z"
  },
  {
    "model_name": "Llama-4-Scout-17B-16E-Instruct-GGUF",
    "description": "The transformers library provides access to Meta's Llama 4 models, including versions trained on a diverse dataset of ~40 trillion tokens and optimized for text and image understanding, with a custom commercial license requiring users to comply with specific terms and conditions for use, redistribution, and safety measures.",
    "developer": "unsloth",
    "downloads": 97712,
    "num_quants": 50,
    "quants": [
      {
        "model_id": "BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00001-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00001-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00002-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00002-of-00005.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00003-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00003-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00004-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00004-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00005-of-00005",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/BF16/Llama-4-Scout-17B-16E-Instruct-BF16-00005-of-00005.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Scout-17B-16E-Instruct-IQ4_NL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Scout-17B-16E-Instruct-IQ4_NL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/Llama-4-Scout-17B-16E-Instruct-IQ4_NL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/IQ4_NL/Llama-4-Scout-17B-16E-Instruct-IQ4_NL-00002-of-00002.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Scout-17B-16E-Instruct-IQ4_XS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Scout-17B-16E-Instruct-IQ4_XS-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_XS/Llama-4-Scout-17B-16E-Instruct-IQ4_XS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/IQ4_XS/Llama-4-Scout-17B-16E-Instruct-IQ4_XS-00002-of-00002.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-Q2_K.gguf",
        "file_size": "36.8 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-Q2_K_L.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-Q3_K_S.gguf",
        "file_size": "43.5 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-IQ1_M.gguf",
        "file_size": "32.6 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-IQ1_S.gguf",
        "file_size": "30.2 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-IQ2_M.gguf",
        "file_size": "36.4 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "34.8 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "42.6 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "39.5 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Llama-4-Scout-17B-16E-Instruct-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Llama-4-Scout-17B-16E-Instruct-UD-TQ1_0.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "Q3_K_M/Llama-4-Scout-17B-16E-Instruct-Q3_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q3_K_M/Llama-4-Scout-17B-16E-Instruct-Q3_K_M-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q3_K_M/Llama-4-Scout-17B-16E-Instruct-Q3_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q3_K_M/Llama-4-Scout-17B-16E-Instruct-Q3_K_M-00002-of-00002.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Scout-17B-16E-Instruct-Q4_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Scout-17B-16E-Instruct-Q4_0-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q4_0/Llama-4-Scout-17B-16E-Instruct-Q4_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_0/Llama-4-Scout-17B-16E-Instruct-Q4_0-00002-of-00002.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Scout-17B-16E-Instruct-Q4_1-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Scout-17B-16E-Instruct-Q4_1-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_1/Llama-4-Scout-17B-16E-Instruct-Q4_1-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_1/Llama-4-Scout-17B-16E-Instruct-Q4_1-00002-of-00002.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Scout-17B-16E-Instruct-Q4_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Scout-17B-16E-Instruct-Q4_K_M-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_M/Llama-4-Scout-17B-16E-Instruct-Q4_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_K_M/Llama-4-Scout-17B-16E-Instruct-Q4_K_M-00002-of-00002.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Scout-17B-16E-Instruct-Q4_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Scout-17B-16E-Instruct-Q4_K_S-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_S/Llama-4-Scout-17B-16E-Instruct-Q4_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q4_K_S/Llama-4-Scout-17B-16E-Instruct-Q4_K_S-00002-of-00002.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Scout-17B-16E-Instruct-Q5_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Scout-17B-16E-Instruct-Q5_K_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q5_K_M/Llama-4-Scout-17B-16E-Instruct-Q5_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q5_K_M/Llama-4-Scout-17B-16E-Instruct-Q5_K_M-00002-of-00002.gguf",
        "file_size": "24.8 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Scout-17B-16E-Instruct-Q5_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Scout-17B-16E-Instruct-Q5_K_S-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q5_K_S/Llama-4-Scout-17B-16E-Instruct-Q5_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q5_K_S/Llama-4-Scout-17B-16E-Instruct-Q5_K_S-00002-of-00002.gguf",
        "file_size": "22.6 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Scout-17B-16E-Instruct-Q6_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Scout-17B-16E-Instruct-Q6_K-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/Llama-4-Scout-17B-16E-Instruct-Q6_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q6_K/Llama-4-Scout-17B-16E-Instruct-Q6_K-00002-of-00002.gguf",
        "file_size": "35.9 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00001-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00003-of-00003.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00001-of-00002.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q4_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00002-of-00002.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q5_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q5_K_XL-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q5_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q5_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q5_K_XL-00002-of-00002.gguf",
        "file_size": "27.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q6_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q6_K_XL-00001-of-00002.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q6_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q6_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q6_K_XL-00002-of-00002.gguf",
        "file_size": "41.5 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00001-of-00003.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00003-of-00003.gguf",
        "file_size": "27.2 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "createdAt": "2025-04-07T22:19:59.000Z"
  },
  {
    "model_name": "llama4-dolphin-8B-GGUF",
    "description": "The model is a static quantized version of the Manavshah/llama4-dolphin-8B base model, offering various GGUF quantization options ranging from Q2_K (3.3GB) to Q8_0 (8.6GB) for efficient inference.",
    "developer": "mradermacher",
    "downloads": 7332,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "llama4-dolphin-8B.IQ3_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ3_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.IQ4_XS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q2_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q6_K",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.Q8_0",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "llama4-dolphin-8B.f16",
        "path": "https://huggingface.co/mradermacher/llama4-dolphin-8B-GGUF/resolve/main/llama4-dolphin-8B.f16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "createdAt": "2024-04-27T08:17:50.000Z"
  },
  {
    "model_name": "LongWriter-Zero-32B-GGUF",
    "description": "This is a quantized version of the THU-KEG/LongWriter-Zero-32B model, optimized for efficient inference with various quantization types including IQ4_XS, Q4_K_S, Q4_K_M, and Q8_0, suitable for reinforcement learning and",
    "developer": "mradermacher",
    "downloads": 368,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "LongWriter-Zero-32B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q2_K",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q6_K",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.Q8_0",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-GGUF/resolve/main/LongWriter-Zero-32B.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "createdAt": "2025-06-21T00:53:49.000Z"
  },
  {
    "model_name": "LongWriter-Zero-32B-i1-GGUF",
    "description": "The THU-KEG/LongWriter-Zero-32B model is a large-scale language model for writing tasks, offering various GGUF quantized versions for efficient deployment, with higher quality options like i1-IQ3_S and i1-Q4_K_M recommended for better performance.",
    "developer": "mradermacher",
    "downloads": 345,
    "num_quants": 23,
    "quants": [
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ1_M.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ1_S.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ3_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q2_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "LongWriter-Zero-32B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/LongWriter-Zero-32B-i1-GGUF/resolve/main/LongWriter-Zero-32B.i1-Q6_K.gguf",
        "file_size": "25.0 GB"
      }
    ],
    "createdAt": "2025-06-21T09:45:33.000Z"
  },
  {
    "model_name": "Magistral-Small-2506-GGUF",
    "description": "The Magistral-Small-2506 model is a 24B parameter, multilingual reasoning model built on Mistral Small 3.1, offering efficient performance with a 40,960 token context window and supporting frameworks like vLLM, llama.cpp,",
    "developer": "unsloth",
    "downloads": 38541,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "Magistral-Small-2506-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/Magistral-Small-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "createdAt": "2025-06-10T07:31:42.000Z"
  },
  {
    "model_name": "Magistral-Small-2506_gguf",
    "description": "Magistral-Small-2506_gguf is a lightweight, efficient reasoning model with 24B parameters, trained on Mistral Small 3.1 with additional reasoning capabilities, available under Apache 2.0 license for local deployment and inference using llama.ccp.",
    "developer": "mistralai",
    "downloads": 5963,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Magistral-Small-2506",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2506_Q8_0",
        "path": "https://huggingface.co/mistralai/Magistral-Small-2506_gguf/resolve/main/Magistral-Small-2506_Q8_0.gguf",
        "file_size": "23.3 GB"
      }
    ],
    "createdAt": "2025-06-09T09:25:46.000Z"
  },
  {
    "model_name": "medgemma-27b-text-it-GGUF",
    "description": "MedGemma is a medical multimodal model trained on diverse medical data, offering strong performance in text and image comprehension for healthcare applications, with a 27B text-only variant available on Hugging Face under the Health AI Developer Foundations license.",
    "developer": "unsloth",
    "downloads": 31355,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "BF16/medgemma-27b-text-it-BF16-00001-of-00002",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/BF16/medgemma-27b-text-it-BF16-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "BF16/medgemma-27b-text-it-BF16-00002-of-00002",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/BF16/medgemma-27b-text-it-BF16-00002-of-00002.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-IQ4_NL.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-IQ4_XS.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q2_K",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q2_K_L.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q3_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q4_1",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q4_1.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q4_K_S.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q5_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q6_K",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-Q8_0",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-Q8_0.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-IQ1_M.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-IQ1_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-IQ2_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-IQ2_XXS.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-IQ3_XXS.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-Q2_K_XL.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-Q3_K_XL.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-Q4_K_XL.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-Q5_K_XL.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-Q6_K_XL.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "medgemma-27b-text-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF/resolve/main/medgemma-27b-text-it-UD-Q8_K_XL.gguf",
        "file_size": "29.6 GB"
      }
    ],
    "createdAt": "2025-05-20T20:17:33.000Z"
  },
  {
    "model_name": "Menlo_Jan-nano-128k-GGUF",
    "description": "This repository provides quantized versions of the Menlo/Jan-nano-128k model using llama.cpp's imatrix method, optimized for various hardware platforms with different performance and quality trade-offs, suitable for text generation tasks.",
    "developer": "bartowski",
    "downloads": 0,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Menlo_Jan-nano-128k-IQ2_M",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-IQ3_M",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Menlo_Jan-nano-128k-bf16",
        "path": "https://huggingface.co/bartowski/Menlo_Jan-nano-128k-GGUF/resolve/main/Menlo_Jan-nano-128k-bf16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "createdAt": "2025-06-25T07:27:58.000Z"
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
    "description": "This is a quantized version of the Meta Llama 3 8B Instruct model, developed by QuantFactory and available under the Meta Llama 3 Community License, which allows for commercial use with specific restrictions and requires adherence to the Acceptable Use Policy and privacy terms.",
    "developer": "QuantFactory",
    "downloads": 16927,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "createdAt": "2024-04-18T17:03:42.000Z"
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "description": "This repository provides quantized versions of the Meta-Llama-3.1-8B-Instruct model using llama.cpp, with various quantization types offering trade-offs between speed, memory usage, and performance, and includes a community license agreement and usage guidelines for responsible deployment.",
    "developer": "bartowski",
    "downloads": 43981,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "createdAt": "2024-07-23T15:36:34.000Z"
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.2-GGUF",
    "description": "This repository provides GGUF format quantized versions of Mistral AI's Mistral 7B Instruct v0.2 model for efficient CPU and GPU inference, with various quantization options balancing speed and quality.",
    "developer": "TheBloke",
    "downloads": 58410,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "mistral-7b-instruct-v0.2.Q2_K",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q4_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q5_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q6_K",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.2.Q8_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "createdAt": "2023-12-11T22:18:46.000Z"
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
    "description": "This model provides GGUF format files for the Mistral-7B-Instruct-v0.3, enabling efficient text generation with support for various frameworks and platforms including llama.cpp, llama-cpp-python, LM Studio, and more.",
    "developer": "MaziyarPanahi",
    "downloads": 276279,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ2_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "createdAt": "2024-05-22T17:27:45.000Z"
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "description": "This model is a quantized version of Mistral-Small-3.2-24B-Instruct-2506, offering improved instruction following, reduced repetition errors, and enhanced function calling capabilities, and is recommended for use with vLLM for efficient inference on GPUs with ~5",
    "developer": "unsloth",
    "downloads": 15594,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "createdAt": "2025-06-20T22:27:21.000Z"
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "description": "Mistral-Small-3.2-24B-Instruct-2506 is an improved version of Mistral-Small-3.1-24B-Instruct-2503, offering better instruction following, reduced repetition errors, and enhanced function calling capabilities,",
    "developer": "gabriellarson",
    "downloads": 1970,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-F16",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-F16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ1_M",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ1_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ3_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_0",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_0.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/gabriellarson/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "createdAt": "2025-06-20T18:30:14.000Z"
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "description": "This is a community model based on Mistralai's Mistral-Small-3.2-24B-Instruct-2506, supporting text and image input with multilingual capabilities and a 128k context length, quantized using GGUF from llama.cpp.",
    "developer": "lmstudio-community",
    "downloads": 10179,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-F16",
        "path": "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-F16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16",
        "path": "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "createdAt": "2025-06-20T20:56:45.000Z"
  },
  {
    "model_name": "mistralai_Magistral-Small-2506-GGUF",
    "description": "This is a collection of quantized versions of the Magistral-Small-2506 model by mistralai, optimized for various inference speeds and resource constraints using llama.cpp, with recommended options including Q6_K, Q5_K_M, and IQ4_NL for high quality and",
    "developer": "bartowski",
    "downloads": 12514,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "createdAt": "2025-06-10T16:09:49.000Z"
  },
  {
    "model_name": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "description": "This repository provides quantized versions of the Mistral-Small-3.2-24B-Instruct-2506 model using llama.cpp, with various quantization types and formats optimized for different hardware and performance needs, including support for chat templates and tool calling.",
    "developer": "bartowski",
    "downloads": 5846,
    "num_quants": 29,
    "quants": [
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "createdAt": "2025-06-20T19:03:22.000Z"
  },
  {
    "model_name": "Mixtral-8x7B-Instruct-v0.1-GGUF",
    "description": "This repository provides GGUF format models for Mistral AI's Mixtral-8x7B-Instruct-v0.1, enabling efficient inference on various platforms like llama.cpp, LM Studio, and llama-cpp-python with different quantization levels for performance and quality trade-offs.",
    "developer": "TheBloke",
    "downloads": 36538,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q2_K",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q4_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q5_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_0.gguf",
        "file_size": "30.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
        "file_size": "30.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q8_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q8_0.gguf",
        "file_size": "46.2 GB"
      }
    ],
    "createdAt": "2023-12-11T18:08:33.000Z"
  },
  {
    "model_name": "ML2-123B-Magnum-Diamond-GGUF",
    "description": "This GGUF quantized version of the ML2-123B-Magnum-Diamond model is designed for creative writing and roleplay, offering a fun and engaging narrative experience with customizable presets and a focus on maintaining character persona while avoiding biases and NSFW content.",
    "developer": "Doctor-Shotgun",
    "downloads": 4,
    "num_quants": 49,
    "quants": [
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ1_M",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ1_M.gguf",
        "file_size": "26.4 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ1_S",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ1_S.gguf",
        "file_size": "24.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ2_M",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ2_M.gguf",
        "file_size": "38.8 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ2_S",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ2_S.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ2_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ2_XS.gguf",
        "file_size": "33.6 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ2_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ2_XXS.gguf",
        "file_size": "30.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_M/ML2-123B-Magnum-Diamond-IQ3_M-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_M/ML2-123B-Magnum-Diamond-IQ3_M-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_M/ML2-123B-Magnum-Diamond-IQ3_M-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_M/ML2-123B-Magnum-Diamond-IQ3_M-00002-of-00002.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_S/ML2-123B-Magnum-Diamond-IQ3_S-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_S/ML2-123B-Magnum-Diamond-IQ3_S-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_S/ML2-123B-Magnum-Diamond-IQ3_S-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_S/ML2-123B-Magnum-Diamond-IQ3_S-00002-of-00002.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_XS/ML2-123B-Magnum-Diamond-IQ3_XS-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_XS/ML2-123B-Magnum-Diamond-IQ3_XS-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_XS/ML2-123B-Magnum-Diamond-IQ3_XS-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_XS/ML2-123B-Magnum-Diamond-IQ3_XS-00002-of-00002.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ3_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ3_XXS.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ4_NL/ML2-123B-Magnum-Diamond-IQ4_NL-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ4_NL/ML2-123B-Magnum-Diamond-IQ4_NL-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ4_NL/ML2-123B-Magnum-Diamond-IQ4_NL-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ4_NL/ML2-123B-Magnum-Diamond-IQ4_NL-00002-of-00002.gguf",
        "file_size": "27.3 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ4_XS/ML2-123B-Magnum-Diamond-IQ4_XS-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ4_XS/ML2-123B-Magnum-Diamond-IQ4_XS-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-IQ4_XS/ML2-123B-Magnum-Diamond-IQ4_XS-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-IQ4_XS/ML2-123B-Magnum-Diamond-IQ4_XS-00002-of-00002.gguf",
        "file_size": "23.7 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q2_K",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q2_K.gguf",
        "file_size": "42.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q2_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q2_K_S.gguf",
        "file_size": "38.7 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q3_K_L/ML2-123B-Magnum-Diamond-Q3_K_L-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q3_K_L/ML2-123B-Magnum-Diamond-Q3_K_L-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q3_K_L/ML2-123B-Magnum-Diamond-Q3_K_L-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q3_K_L/ML2-123B-Magnum-Diamond-Q3_K_L-00002-of-00002.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q3_K_M/ML2-123B-Magnum-Diamond-Q3_K_M-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q3_K_M/ML2-123B-Magnum-Diamond-Q3_K_M-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q3_K_M/ML2-123B-Magnum-Diamond-Q3_K_M-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q3_K_M/ML2-123B-Magnum-Diamond-Q3_K_M-00002-of-00002.gguf",
        "file_size": "17.8 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q3_K_S/ML2-123B-Magnum-Diamond-Q3_K_S-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q3_K_S/ML2-123B-Magnum-Diamond-Q3_K_S-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q3_K_S/ML2-123B-Magnum-Diamond-Q3_K_S-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q3_K_S/ML2-123B-Magnum-Diamond-Q3_K_S-00002-of-00002.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q4_K_M/ML2-123B-Magnum-Diamond-Q4_K_M-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q4_K_M/ML2-123B-Magnum-Diamond-Q4_K_M-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q4_K_M/ML2-123B-Magnum-Diamond-Q4_K_M-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q4_K_M/ML2-123B-Magnum-Diamond-Q4_K_M-00002-of-00002.gguf",
        "file_size": "31.0 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q4_K_S/ML2-123B-Magnum-Diamond-Q4_K_S-00001-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q4_K_S/ML2-123B-Magnum-Diamond-Q4_K_S-00001-of-00002.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q4_K_S/ML2-123B-Magnum-Diamond-Q4_K_S-00002-of-00002",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q4_K_S/ML2-123B-Magnum-Diamond-Q4_K_S-00002-of-00002.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q5_K_M/ML2-123B-Magnum-Diamond-Q5_K_M-00001-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q5_K_M/ML2-123B-Magnum-Diamond-Q5_K_M-00001-of-00003.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q5_K_M/ML2-123B-Magnum-Diamond-Q5_K_M-00002-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q5_K_M/ML2-123B-Magnum-Diamond-Q5_K_M-00002-of-00003.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q5_K_M/ML2-123B-Magnum-Diamond-Q5_K_M-00003-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q5_K_M/ML2-123B-Magnum-Diamond-Q5_K_M-00003-of-00003.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q5_K_S/ML2-123B-Magnum-Diamond-Q5_K_S-00001-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q5_K_S/ML2-123B-Magnum-Diamond-Q5_K_S-00001-of-00003.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q5_K_S/ML2-123B-Magnum-Diamond-Q5_K_S-00002-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q5_K_S/ML2-123B-Magnum-Diamond-Q5_K_S-00002-of-00003.gguf",
        "file_size": "37.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q5_K_S/ML2-123B-Magnum-Diamond-Q5_K_S-00003-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q5_K_S/ML2-123B-Magnum-Diamond-Q5_K_S-00003-of-00003.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q6_K/ML2-123B-Magnum-Diamond-Q6_K-00001-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q6_K/ML2-123B-Magnum-Diamond-Q6_K-00001-of-00003.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q6_K/ML2-123B-Magnum-Diamond-Q6_K-00002-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q6_K/ML2-123B-Magnum-Diamond-Q6_K-00002-of-00003.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q6_K/ML2-123B-Magnum-Diamond-Q6_K-00003-of-00003",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q6_K/ML2-123B-Magnum-Diamond-Q6_K-00003-of-00003.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00001-of-00004",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00001-of-00004.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00002-of-00004",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00002-of-00004.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00003-of-00004",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00003-of-00004.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00004-of-00004",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-Q8_0/ML2-123B-Magnum-Diamond-Q8_0-00004-of-00004.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00001-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00001-of-00007.gguf",
        "file_size": "36.8 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00002-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00002-of-00007.gguf",
        "file_size": "36.8 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00003-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00003-of-00007.gguf",
        "file_size": "36.8 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00004-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00004-of-00007.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00005-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00005-of-00007.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00006-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00006-of-00007.gguf",
        "file_size": "36.7 GB"
      },
      {
        "model_id": "ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00007-of-00007",
        "path": "https://huggingface.co/Doctor-Shotgun/ML2-123B-Magnum-Diamond-GGUF/resolve/main/ML2-123B-Magnum-Diamond-bf16/ML2-123B-Magnum-Diamond-bf16-00007-of-00007.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "createdAt": "2025-06-23T02:28:57.000Z"
  },
  {
    "model_name": "Nanonets-OCR-s-GGUF",
    "description": "Nanonets-OCR-s is a state-of-the-art image-to-markdown OCR model that extracts structured text, equations, tables, and identifies images, signatures, and watermarks, enabling efficient downstream processing by LLMs.",
    "developer": "unsloth",
    "downloads": 16120,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "createdAt": "2025-06-16T10:35:11.000Z"
  },
  {
    "model_name": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF",
    "description": "This repository provides quantized versions of the OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT model using llama.cpp's imatrix method, optimized for various hardware platforms with different quality and performance trade-offs, including support for ARM and",
    "developer": "bartowski",
    "downloads": 5928,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_M",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_M.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_S",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_XS",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ3_M",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ3_M.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ3_XXS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q2_K",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_M.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q3_K_XL.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_0",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_1",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_K_L.gguf",
        "file_size": "18.9 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_K_M.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q5_K_M.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q6_K",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q8_0",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-Q8_0.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16-00001-of-00002",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16-00001-of-00002.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16-00002-of-00002",
        "path": "https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF/resolve/main/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-bf16-00002-of-00002.gguf",
        "file_size": "23.9 GB"
      }
    ],
    "createdAt": "2025-06-10T20:09:43.000Z"
  },
  {
    "model_name": "Peach-2.0-9B-8k-Roleplay-GGUF",
    "description": "QuantFactory/Peach-2.0-9B-8k-Roleplay is a quantized, bilingual, and enhanced roleplay LLM derived from Yi-1.5-9B, featuring improved dialogue quality, compatibility with Silly-Tavern format, and support for Chinese-English",
    "developer": "QuantFactory",
    "downloads": 0,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q2_K",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q3_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q3_K_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q3_K_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q4_0",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q4_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q4_1",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q4_1.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q4_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q4_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q5_0",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q5_0.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q5_1",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q5_1.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q5_K_M.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q5_K_S.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q6_K",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q6_K.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Peach-2.0-9B-8k-Roleplay.Q8_0",
        "path": "https://huggingface.co/QuantFactory/Peach-2.0-9B-8k-Roleplay-GGUF/resolve/main/Peach-2.0-9B-8k-Roleplay.Q8_0.gguf",
        "file_size": "8.7 GB"
      }
    ],
    "createdAt": "2025-06-23T10:23:25.000Z"
  },
  {
    "model_name": "phi-4-gguf",
    "description": "The Phi-4 model is a 14B parameter, English-focused, chat-oriented, decoder-only Transformer model trained on high-quality synthetic and academic data to excel in reasoning, math, code generation, and factual knowledge tasks, with a focus on safety and alignment through supervised fine-tuning and direct",
    "developer": "microsoft",
    "downloads": 11967,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "phi-4-IQ2_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ2_M.gguf",
        "file_size": "561.6 MB"
      },
      {
        "model_id": "phi-4-IQ3_M",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "phi-4-IQ3_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-IQ3_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XS.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "phi-4-IQ3_XXS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ3_XXS.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "phi-4-IQ4_NL",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_NL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-IQ4_XS",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-IQ4_XS.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "phi-4-Q2_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q2_K.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "phi-4-Q3_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "phi-4-Q3_K_L",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "phi-4-Q3_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q3_K_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "phi-4-Q4_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_0.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "phi-4-Q4_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "phi-4-Q4_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "phi-4-Q4_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q4_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "phi-4-Q5_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_0.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q5_1",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_1.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "phi-4-Q5_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "phi-4-Q5_K_S",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q5_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "phi-4-Q6_K",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "phi-4-Q8_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-Q8_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "phi-4-TQ1_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ1_0.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "phi-4-TQ2_0",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-TQ2_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "phi-4-bf16",
        "path": "https://huggingface.co/microsoft/phi-4-gguf/resolve/main/phi-4-bf16.gguf",
        "file_size": "27.3 GB"
      }
    ],
    "createdAt": "2025-01-08T19:55:31.000Z"
  },
  {
    "model_name": "Phr00tyMix-v2-32B-GGUF",
    "description": "Phr00tyMix-v2-32B is a merged large language model combining the uncensored and smart foundation of DeepSeek-R1-Distill-Qwen-32B-abliterated with several specialized models for enhanced roleplay, creative writing, and storytelling capabilities.",
    "developer": "Phr00t",
    "downloads": 0,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Phr00tyMix-v2-32B-imat-IQ4_XS",
        "path": "https://huggingface.co/Phr00t/Phr00tyMix-v2-32B-GGUF/resolve/main/Phr00tyMix-v2-32B-imat-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Phr00tyMix-v2-32B-imat-Q4_K_M",
        "path": "https://huggingface.co/Phr00t/Phr00tyMix-v2-32B-GGUF/resolve/main/Phr00tyMix-v2-32B-imat-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Phr00tyMix-v2-32B-imat-Q5_K_S",
        "path": "https://huggingface.co/Phr00t/Phr00tyMix-v2-32B-GGUF/resolve/main/Phr00tyMix-v2-32B-imat-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Phr00tyMix-v2-32B-imat-Q6_K",
        "path": "https://huggingface.co/Phr00t/Phr00tyMix-v2-32B-GGUF/resolve/main/Phr00tyMix-v2-32B-imat-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Phr00tyMix-v2-32B-imat-Q8_0",
        "path": "https://huggingface.co/Phr00t/Phr00tyMix-v2-32B-GGUF/resolve/main/Phr00tyMix-v2-32B-imat-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "createdAt": "2025-06-24T16:44:00.000Z"
  },
  {
    "model_name": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF",
    "description": "This repository provides quantized versions of the PocketDoc/Dans-PersonalityEngine-V1.3.0-24b model, optimized for various hardware platforms with different quantization types and quality levels, suitable for text generation tasks in multiple languages.",
    "developer": "bartowski",
    "downloads": 12216,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "createdAt": "2025-05-23T18:09:44.000Z"
  },
  {
    "model_name": "POLARIS-Project_Polaris-7B-Preview-GGUF",
    "description": "This is a quantized version of the Polaris-7B-Preview model by POLARIS-Project, optimized for various inference speeds and quality levels using llama.cpp's imatrix quantization method, suitable for deployment on CPU or GPU with different RAM and VRAM constraints.",
    "developer": "bartowski",
    "downloads": 0,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-IQ2_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-IQ2_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-IQ3_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-IQ3_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-IQ3_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-IQ3_XS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-IQ4_NL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-IQ4_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q2_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q2_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q3_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q3_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q3_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q3_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q3_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q4_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q4_1",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q4_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q4_K_L.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q4_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q4_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q5_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q5_K_L.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q5_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q5_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q6_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q6_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q6_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-Q8_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-7B-Preview-bf16",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-7B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-7B-Preview-bf16.gguf",
        "file_size": "14.2 GB"
      }
    ],
    "createdAt": "2025-06-24T04:39:00.000Z"
  },
  {
    "model_name": "Qwen2.5-Coder-32B-Instruct-GGUF",
    "description": "This is a quantized version of the Qwen2.5-Coder-32B-Instruct model using llama.cpp's imatrix quantization method, optimized for various hardware platforms with different quality and performance trade-offs, recommended for use in LM Studio with specific file sizes and quant types tailored to",
    "developer": "bartowski",
    "downloads": 8575,
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_4.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_4_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_0_8_8.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-32B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "createdAt": "2024-11-06T19:20:14.000Z"
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "description": "Qwen3-30B-A3B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes, and it can be fine-tuned or deployed using frameworks like Hugging Face Transformers, v",
    "developer": "unsloth",
    "downloads": 88932,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "BF16/Qwen3-30B-A3B-BF16-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/BF16/Qwen3-30B-A3B-BF16-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-30B-A3B-BF16-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/BF16/Qwen3-30B-A3B-BF16-00002-of-00002.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-IQ4_NL.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q2_K_L.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ1_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ1_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ2_M.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ2_XXS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q2_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q3_K_XL.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q4_K_XL.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q5_K_XL.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q6_K_XL.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q8_K_XL.gguf",
        "file_size": "33.5 GB"
      }
    ],
    "createdAt": "2025-04-28T13:48:41.000Z"
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "description": "Qwen3-30B-A3B-GGUF is a large language model with 30.5B parameters, supporting seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, multilingual support, and efficient inference via quantization methods like Q8_0, optimized for",
    "developer": "Qwen",
    "downloads": 29450,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "createdAt": "2025-05-05T08:38:52.000Z"
  },
  {
    "model_name": "Qwen3-4B-GGUF",
    "description": "Qwen3-4B is a large language model developed by Alibaba Cloud, offering enhanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes, and it can be used for fine-tuning, deployment, and agentic tasks through the Unsloth framework.",
    "developer": "unsloth",
    "downloads": 35445,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "createdAt": "2025-04-28T07:55:09.000Z"
  },
  {
    "model_name": "Qwen3-R1-SLERP-Q3T-8B-Q4_K_S-Q8_0-GGUF",
    "description": "This repository provides static GGUF quantized versions (Q4_K_S and Q8_0) of the Qwen3-R1-SLERP-DST-8B model from lemon07r, which is based on the base model lemon07r/Qwen3-R1-SLER",
    "developer": "lemon07r",
    "downloads": 144,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Qwen3-R1-SLERP-Q3T-8B-Q4_K_S",
        "path": "https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B-Q4_K_S-Q8_0-GGUF/resolve/main/Qwen3-R1-SLERP-Q3T-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen3-R1-SLERP-Q3T-8B-Q8_0",
        "path": "https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B-Q4_K_S-Q8_0-GGUF/resolve/main/Qwen3-R1-SLERP-Q3T-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "createdAt": "2025-06-19T04:05:40.000Z"
  },
  {
    "model_name": "saiga_llama3_8b_gguf",
    "description": "This repository provides Llama.cpp compatible versions of the original 8B Llama3 model in Russian, allowing for efficient text generation with quantized models suitable for systems with 10GB RAM or less.",
    "developer": "IlyaGusev",
    "downloads": 1120,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "model-f16",
        "path": "https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-f16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "model-q2_K",
        "path": "https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-q4_K",
        "path": "https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-q8_0",
        "path": "https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "createdAt": "2024-04-19T13:25:45.000Z"
  },
  {
    "model_name": "Seneca-Cybersecurity-LLM-x-QwQ-32B-Q4_Medium-Version",
    "description": "This is a fine-tuned version of the Qwen QwQ-32B model for cybersecurity and information security tasks, trained on a custom dataset for areas like ethical hacking, incident response, and reverse engineering.",
    "developer": "AlicanKiraz0",
    "downloads": 69,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "senecallm-x-qwq-32b-q4_k_m",
        "path": "https://huggingface.co/AlicanKiraz0/Seneca-Cybersecurity-LLM-x-QwQ-32B-Q4_Medium-Version/resolve/main/senecallm-x-qwq-32b-q4_k_m.gguf",
        "file_size": "18.5 GB"
      }
    ],
    "createdAt": "2025-03-15T01:10:17.000Z"
  },
  {
    "model_name": "Skywork-SWE-32B-GGUF",
    "description": "This is a quantized version of the Skywork-SWE-32B model, optimized for various tasks with different quantization levels and quality trade-offs, available on Hugging Face via the GGUF format.",
    "developer": "mradermacher",
    "downloads": 297,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Skywork-SWE-32B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.IQ4_XS.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Skywork-SWE-32B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF/resolve/main/Skywork-SWE-32B.Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "createdAt": "2025-06-19T17:37:24.000Z"
  },
  {
    "model_name": "SmolLM2-360M-Instruct-GGUF",
    "description": "This model is a GGUF version of the SmolLM2-360M-Instruct base model, converted using llama.cpp and compatible with GGUF-my-repo for inference via llama-cli or llama-server.",
    "developer": "HuggingFaceTB",
    "downloads": 5257,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "smollm2-360m-instruct-q8_0",
        "path": "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF/resolve/main/smollm2-360m-instruct-q8_0.gguf",
        "file_size": "368.5 MB"
      }
    ],
    "createdAt": "2024-10-31T20:37:31.000Z"
  },
  {
    "model_name": "THU-KEG_LongWriter-Zero-32B-GGUF",
    "description": "This repository provides quantized versions of the THU-KEG/LongWriter-Zero-32B model using llama.cpp's imatrix method, optimized for different hardware and performance requirements, with recommended options for quality and efficiency.",
    "developer": "bartowski",
    "downloads": 0,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ2_M",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ2_S",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ3_M",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q2_K",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q4_0",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q4_1",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q6_K",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-Q8_0",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-Q8_0.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-bf16/THU-KEG_LongWriter-Zero-32B-bf16-00001-of-00002",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-bf16/THU-KEG_LongWriter-Zero-32B-bf16-00001-of-00002.gguf",
        "file_size": "37.1 GB"
      },
      {
        "model_id": "THU-KEG_LongWriter-Zero-32B-bf16/THU-KEG_LongWriter-Zero-32B-bf16-00002-of-00002",
        "path": "https://huggingface.co/bartowski/THU-KEG_LongWriter-Zero-32B-GGUF/resolve/main/THU-KEG_LongWriter-Zero-32B-bf16/THU-KEG_LongWriter-Zero-32B-bf16-00002-of-00002.gguf",
        "file_size": "23.9 GB"
      }
    ],
    "createdAt": "2025-06-26T02:17:00.000Z"
  },
  {
    "model_name": "Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8",
    "description": "Tifa-DeepSexV2-7b-MGRPO 是基于 Qwen2.5-7B 的深度优化模型，通过增量预训练、SFT、强化学习和 MGRPO 算法提升角色扮演能力与文学性，具备 100",
    "developer": "ValueFX9507",
    "downloads": 4386,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV2-7b-0218-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-0218-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Cot-0301-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Cot-0301-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0222-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0222-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0228-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0228-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-NoCot-0325-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-NoCot-0325-Q8.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Tifa-DeepsexV2-7b-Q8",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV2-7b-MGRPO-GGUF-Q8/resolve/main/Tifa-DeepsexV2-7b-Q8.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "createdAt": "2025-02-15T13:30:54.000Z"
  },
  {
    "model_name": "Tifa-DeepsexV3-14b-GGUF-Q6",
    "description": "No detailed description available.",
    "developer": "ValueFX9507",
    "downloads": 0,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "createdAt": "2025-06-26T10:15:21.000Z"
  },
  {
    "model_name": "TinyLlama-1.1B-Chat-v1.0-GGUF",
    "description": "This repository provides GGUF format quantized versions of the TinyLlama 1.1B Chat v1.0 model for efficient GPU inference, with various quantization methods offering trade-offs between model size, speed, and quality, and includes instructions for downloading, running, and integrating the model into",
    "developer": "TheBloke",
    "downloads": 54370,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q2_K",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
        "file_size": "460.7 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_L.gguf",
        "file_size": "565.1 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_M.gguf",
        "file_size": "525.3 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_S.gguf",
        "file_size": "477.1 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf",
        "file_size": "608.2 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "file_size": "637.8 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf",
        "file_size": "613.9 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_0.gguf",
        "file_size": "731.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf",
        "file_size": "746.7 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_K_S.gguf",
        "file_size": "731.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q6_K",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q6_K.gguf",
        "file_size": "862.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q8_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "createdAt": "2023-12-31T20:53:43.000Z"
  },
  {
    "model_name": "Triplex",
    "description": "Triplex is a state-of-the-art, cost-effective large language model designed to efficiently extract knowledge graph triplets from unstructured data, offering a 98% cost reduction compared to traditional methods and enabling local graph construction with SciPhi's R2R.",
    "developer": "SciPhi",
    "downloads": 2355,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "quantized_model-Q4_K_M",
        "path": "https://huggingface.co/SciPhi/Triplex/resolve/main/quantized_model-Q4_K_M.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "createdAt": "2024-07-10T21:58:18.000Z"
  }
]