[
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 41660,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling comprehensive analysis of long documents and complex conversations without performance loss."
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 41660,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling comprehensive analysis of long documents and complex conversations without performance loss."
  },
  {
    "model_name": "Jan-nano-gguf",
    "developer": "Menlo",
    "downloads": 73727,
    "createdAt": "2025-06-11T07:14:33.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-4b-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-4b-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/README.md",
    "description": "Jan Nano is a compact, quantized version of the Qwen3 architecture, optimized for efficient text generation in local or embedded environments with enhanced tool use and research capabilities."
  },
  {
    "model_name": "Jan-nano-gguf",
    "developer": "Menlo",
    "downloads": 73727,
    "createdAt": "2025-06-11T07:14:33.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-4b-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-4b-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-4b-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-4b-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-4b-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-4b-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-4b-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/jan-nano-4b-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/Menlo/Jan-nano-gguf/resolve/main/README.md",
    "description": "Jan Nano is a compact, quantized version of the Qwen3 architecture, optimized for efficient text generation in local or embedded environments with enhanced tool use and research capabilities."
  },
  {
    "model_name": "Arch-Router-1.5B.gguf",
    "developer": "katanemo",
    "downloads": 378,
    "createdAt": "2025-05-30T18:18:40.000Z",
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Arch-Router-1.5B-Q2_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_L",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Arch-Router-Q6_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-Q6_K.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/README.md",
    "description": "The katanemo/Arch-Router-1.5B model is a compact, preference-aligned routing framework that maps queries to domain-action preferences for selecting the most suitable large language model."
  },
  {
    "model_name": "Arch-Router-1.5B.gguf",
    "developer": "katanemo",
    "downloads": 378,
    "createdAt": "2025-05-30T18:18:40.000Z",
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Arch-Router-1.5B-Q2_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q2_K.gguf",
        "file_size": "645.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_L",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_L.gguf",
        "file_size": "839.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_M.gguf",
        "file_size": "786.0 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q3_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q3_K_S.gguf",
        "file_size": "725.7 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q4_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q4_K_S.gguf",
        "file_size": "896.8 MB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_M",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B-Q5_K_S",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B-Q5_K_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Arch-Router-1.5B",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-1.5B.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Arch-Router-Q6_K",
        "path": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/Arch-Router-Q6_K.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "readme": "https://huggingface.co/katanemo/Arch-Router-1.5B.gguf/resolve/main/README.md",
    "description": "The katanemo/Arch-Router-1.5B model is a compact, preference-aligned routing framework that maps queries to domain-action preferences for selecting the most suitable large language model."
  },
  {
    "model_name": "baidu_ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 1887,
    "createdAt": "2025-06-30T03:51:10.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ2_M.gguf",
        "file_size": "155.4 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_M.gguf",
        "file_size": "195.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XS.gguf",
        "file_size": "184.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS.gguf",
        "file_size": "164.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_NL.gguf",
        "file_size": "222.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_XS.gguf",
        "file_size": "215.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "199.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_L.gguf",
        "file_size": "214.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL.gguf",
        "file_size": "238.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_L.gguf",
        "file_size": "254.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_L.gguf",
        "file_size": "280.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K_L.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-bf16.gguf",
        "file_size": "690.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-0.3B-PT model using llama.cpp's imatrix method, offering various quantization options for different performance and memory trade-offs."
  },
  {
    "model_name": "baidu_ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "bartowski",
    "downloads": 1887,
    "createdAt": "2025-06-30T03:51:10.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ2_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ2_M.gguf",
        "file_size": "155.4 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_M.gguf",
        "file_size": "195.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XS.gguf",
        "file_size": "184.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ3_XXS.gguf",
        "file_size": "164.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_NL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_NL.gguf",
        "file_size": "222.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-IQ4_XS",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-IQ4_XS.gguf",
        "file_size": "215.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "199.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_L.gguf",
        "file_size": "214.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q3_K_XL.gguf",
        "file_size": "238.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_L.gguf",
        "file_size": "254.0 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_L.gguf",
        "file_size": "280.7 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q6_K_L",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q6_K_L.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "baidu_ERNIE-4.5-0.3B-PT-bf16",
        "path": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/baidu_ERNIE-4.5-0.3B-PT-bf16.gguf",
        "file_size": "690.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/baidu_ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the ERNIE-4.5-0.3B-PT model using llama.cpp's imatrix method, offering various quantization options for different performance and memory trade-offs."
  },
  {
    "model_name": "claude-3.7-sonnet-reasoning-gemma3-12B",
    "developer": "reedmayhew",
    "downloads": 10346,
    "createdAt": "2025-03-22T19:40:09.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0",
        "path": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "readme": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/README.md",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL."
  },
  {
    "model_name": "claude-3.7-sonnet-reasoning-gemma3-12B",
    "developer": "reedmayhew",
    "downloads": 10346,
    "createdAt": "2025-03-22T19:40:09.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0",
        "path": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/claude-3.7-sonnet-reasoning-gemma3-12B.Q8_0.gguf",
        "file_size": "11.7 GB"
      }
    ],
    "readme": "https://huggingface.co/reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B/resolve/main/README.md",
    "description": "This model is a Gemma 3 12B variant fine-tuned with reasoning data from Claude 3.7 Sonnet using LoRA and trained 2x faster with Unsloth and Huggingface's TRL."
  },
  {
    "model_name": "CodeLlama-7B-Instruct-GGUF",
    "developer": "TheBloke",
    "downloads": 13677,
    "createdAt": "2023-08-24T17:01:14.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "codellama-7b-instruct.Q2_K",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q3_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q3_K_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q4_0",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_0.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q5_0",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_M.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q6_K",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q6_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q8_0",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q8_0.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The CodeLlama 7B Instruct model is a 7 billion parameter, instruction-following code generation model from Meta, available in various quantized GGUF formats for efficient CPU and GPU inference."
  },
  {
    "model_name": "CodeLlama-7B-Instruct-GGUF",
    "developer": "TheBloke",
    "downloads": 13677,
    "createdAt": "2023-08-24T17:01:14.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "codellama-7b-instruct.Q2_K",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q3_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q3_K_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q4_0",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_0.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q5_0",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_M.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q6_K",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q6_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "codellama-7b-instruct.Q8_0",
        "path": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q8_0.gguf",
        "file_size": "6.7 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The CodeLlama 7B Instruct model is a 7 billion parameter, instruction-following code generation model from Meta, available in various quantized GGUF formats for efficient CPU and GPU inference."
  },
  {
    "model_name": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF",
    "developer": "bartowski",
    "downloads": 18406,
    "createdAt": "2025-05-09T15:21:21.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dolphin-Mistral-24B-Venice-Edition model by cognitivecomputations, optimized for various inference speeds and quality levels using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF",
    "developer": "bartowski",
    "downloads": 18406,
    "createdAt": "2025-05-09T15:21:21.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16",
        "path": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Dolphin-Mistral-24B-Venice-Edition model by cognitivecomputations, optimized for various inference speeds and quality levels using llama.cpp's imatrix quantization method."
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 412,
    "createdAt": "2025-01-21T03:41:56.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "readme": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/README.md",
    "description": "BaronLLM is a large-language model fine-tuned for offensive cybersecurity research and adversarial simulation, providing exploit reasoning, red-team scenario generation, and safety-constrained content."
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 412,
    "createdAt": "2025-01-21T03:41:56.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "readme": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/README.md",
    "description": "BaronLLM is a large-language model fine-tuned for offensive cybersecurity research and adversarial simulation, providing exploit reasoning, red-team scenario generation, and safety-constrained content."
  },
  {
    "model_name": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF",
    "developer": "QuantFactory",
    "downloads": 30912,
    "createdAt": "2024-07-28T07:02:48.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/README.md",
    "description": "The QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF is a quantized, uncensored, and multilingual version of the Meta Llama 3.1 8B model, optimized for roleplay, instruction"
  },
  {
    "model_name": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF",
    "developer": "QuantFactory",
    "downloads": 30912,
    "createdAt": "2024-07-28T07:02:48.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0",
        "path": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "readme": "https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/README.md",
    "description": "The QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF is a quantized, uncensored, and multilingual version of the Meta Llama 3.1 8B model, optimized for roleplay, instruction"
  },
  {
    "model_name": "DeepSeek-R1-0528-GGUF",
    "developer": "unsloth",
    "downloads": 165463,
    "createdAt": "2025-05-28T18:10:04.000Z",
    "num_quants": 235,
    "quants": [
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00001-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00001-of-00030.gguf",
        "file_size": "40.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00002-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00002-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00003-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00003-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00004-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00004-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00005-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00005-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00006-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00006-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00007-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00007-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00008-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00008-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00009-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00009-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00010-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00010-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00011-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00011-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00012-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00012-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00013-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00013-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00014-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00014-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00015-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00015-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00016-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00016-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00017-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00017-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00018-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00018-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00019-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00019-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00020-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00020-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00021-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00021-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00022-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00022-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00023-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00023-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00024-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00024-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00025-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00025-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00026-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00026-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00027-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00027-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00028-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00028-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00029-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00029-of-00030.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00030-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00030-of-00030.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/DeepSeek-R1-0528-UD-TQ1_0.gguf",
        "file_size": "150.5 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00001-of-00008.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00002-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00003-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00005-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00006-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00008-of-00008.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00001-of-00008.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00002-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00003-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00004-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00005-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00006-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00007-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00008-of-00008.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00001-of-00005.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00002-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00003-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00004-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00005-of-00005.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00001-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00002-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00003-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00004-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00005-of-00005.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00001-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00001-of-00007.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00002-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00002-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00003-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00003-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00004-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00004-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00005-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00005-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00006-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00006-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00007-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00007-of-00007.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00001-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00002-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00003-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00004-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00005-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00006-of-00006.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00001-of-00008.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00002-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00003-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00005-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00008-of-00008.gguf",
        "file_size": "30.1 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00001-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00001-of-00009.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00002-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00002-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00003-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00003-of-00009.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00004-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00004-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00005-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00005-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00006-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00006-of-00009.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00007-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00007-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00008-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00008-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00009-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00009-of-00009.gguf",
        "file_size": "33.4 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00001-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00001-of-00009.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00002-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00002-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00003-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00003-of-00009.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00004-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00004-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00005-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00005-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00006-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00006-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00007-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00007-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00008-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00008-of-00009.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00009-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00009-of-00009.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00001-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00002-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00003-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00005-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00008-of-00008.gguf",
        "file_size": "30.1 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00001-of-00010.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00002-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00003-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00004-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00005-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00006-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00007-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00008-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00009-of-00010.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00010-of-00010.gguf",
        "file_size": "36.2 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00001-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00002-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00003-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00004-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00005-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00006-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00007-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00008-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00009-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00010-of-00010.gguf",
        "file_size": "31.9 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00001-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00001-of-00012.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00002-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00002-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00003-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00003-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00004-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00004-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00005-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00005-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00006-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00006-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00007-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00007-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00008-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00008-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00009-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00009-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00010-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00010-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00011-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00011-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00012-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00012-of-00012.gguf",
        "file_size": "29.3 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00001-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00001-of-00015.gguf",
        "file_size": "44.5 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00002-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00002-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00003-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00003-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00004-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00004-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00005-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00005-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00006-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00006-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00007-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00007-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00008-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00008-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00009-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00009-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00010-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00010-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00011-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00011-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00012-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00012-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00013-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00013-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00014-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00014-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00015-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00015-of-00015.gguf",
        "file_size": "27.4 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00002-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00003-of-00005.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00004-of-00005.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00005-of-00005.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004.gguf",
        "file_size": "35.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00001-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00002-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00003-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00004-of-00005.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00005-of-00005.gguf",
        "file_size": "28.5 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00001-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00002-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00003-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00004-of-00005.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00005-of-00005.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00001-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00002-of-00006.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00003-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00004-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00005-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00006-of-00006.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00002-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00003-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00005-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00006-of-00006.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00001-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00001-of-00007.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00002-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00002-of-00007.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00003-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00003-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00004-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00004-of-00007.gguf",
        "file_size": "45.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00005-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00005-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00006-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00006-of-00007.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00007-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00007-of-00007.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00001-of-00008.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00002-of-00008.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00003-of-00008.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00005-of-00008.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00008-of-00008.gguf",
        "file_size": "38.0 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00001-of-00010.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00002-of-00010.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00003-of-00010.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00004-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00005-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00006-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00007-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00008-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00009-of-00010.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00010-of-00010.gguf",
        "file_size": "42.8 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00001-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00001-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00002-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00002-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00003-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00003-of-00012.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00004-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00004-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00005-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00005-of-00012.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00006-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00006-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00007-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00007-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00008-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00008-of-00012.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00009-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00009-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00010-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00010-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00011-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00011-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00012-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00012-of-00012.gguf",
        "file_size": "40.3 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00001-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00001-of-00016.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00002-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00002-of-00016.gguf",
        "file_size": "43.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00003-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00003-of-00016.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00004-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00004-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00005-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00005-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00006-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00006-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00007-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00007-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00008-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00008-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00009-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00009-of-00016.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00010-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00010-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00011-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00011-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00012-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00012-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00013-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00013-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00014-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00014-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00015-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00015-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00016-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00016-of-00016.gguf",
        "file_size": "46.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/README.md",
    "description": "The DeepSeek-R1-0528 model, developed by DeepSeek-AI, is a large language model with enhanced reasoning capabilities, improved performance on benchmarks like AIME, and is available in various formats including GGUF, with detailed usage guidelines and a MIT license."
  },
  {
    "model_name": "DeepSeek-R1-0528-GGUF",
    "developer": "unsloth",
    "downloads": 165463,
    "createdAt": "2025-05-28T18:10:04.000Z",
    "num_quants": 235,
    "quants": [
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00001-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00001-of-00030.gguf",
        "file_size": "40.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00002-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00002-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00003-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00003-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00004-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00004-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00005-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00005-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00006-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00006-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00007-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00007-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00008-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00008-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00009-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00009-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00010-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00010-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00011-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00011-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00012-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00012-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00013-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00013-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00014-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00014-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00015-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00015-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00016-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00016-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00017-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00017-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00018-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00018-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00019-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00019-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00020-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00020-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00021-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00021-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00022-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00022-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00023-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00023-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00024-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00024-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00025-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00025-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00026-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00026-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00027-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00027-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00028-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00028-of-00030.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00029-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00029-of-00030.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "BF16/DeepSeek-R1-0528-BF16-00030-of-00030",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/BF16/DeepSeek-R1-0528-BF16-00030-of-00030.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/DeepSeek-R1-0528-UD-TQ1_0.gguf",
        "file_size": "150.5 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00001-of-00008.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00002-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00003-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00005-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00006-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_NL/DeepSeek-R1-0528-IQ4_NL-00008-of-00008.gguf",
        "file_size": "28.1 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00001-of-00008.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00002-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00003-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00004-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00005-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00006-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00007-of-00008.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/IQ4_XS/DeepSeek-R1-0528-IQ4_XS-00008-of-00008.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00001-of-00005.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00002-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00003-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00004-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Q2_K/DeepSeek-R1-0528-Q2_K-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K/DeepSeek-R1-0528-Q2_K-00005-of-00005.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00001-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00002-of-00005.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00003-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00004-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q2_K_L/DeepSeek-R1-0528-Q2_K_L-00005-of-00005.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00001-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00001-of-00007.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00002-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00002-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00003-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00003-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00004-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00004-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00005-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00005-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00006-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00006-of-00007.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00007-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_M/DeepSeek-R1-0528-Q3_K_M-00007-of-00007.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00001-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00002-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00003-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00004-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00005-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q3_K_S/DeepSeek-R1-0528-Q3_K_S-00006-of-00006.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00001-of-00008.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00002-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00003-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00005-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_0/DeepSeek-R1-0528-Q4_0-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_0/DeepSeek-R1-0528-Q4_0-00008-of-00008.gguf",
        "file_size": "30.1 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00001-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00001-of-00009.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00002-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00002-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00003-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00003-of-00009.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00004-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00004-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00005-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00005-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00006-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00006-of-00009.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00007-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00007-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00008-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00008-of-00009.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "Q4_1/DeepSeek-R1-0528-Q4_1-00009-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_1/DeepSeek-R1-0528-Q4_1-00009-of-00009.gguf",
        "file_size": "33.4 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00001-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00001-of-00009.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00002-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00002-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00003-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00003-of-00009.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00004-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00004-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00005-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00005-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00006-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00006-of-00009.gguf",
        "file_size": "45.0 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00007-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00007-of-00009.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00008-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00008-of-00009.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00009-of-00009",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_M/DeepSeek-R1-0528-Q4_K_M-00009-of-00009.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00001-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00002-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00003-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00005-of-00008.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q4_K_S/DeepSeek-R1-0528-Q4_K_S-00008-of-00008.gguf",
        "file_size": "30.1 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00001-of-00010.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00002-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00003-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00004-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00005-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00006-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00007-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00008-of-00010.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00009-of-00010.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_M/DeepSeek-R1-0528-Q5_K_M-00010-of-00010.gguf",
        "file_size": "36.2 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00001-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00002-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00003-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00004-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00005-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00006-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00007-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00008-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00009-of-00010.gguf",
        "file_size": "44.3 GB"
      },
      {
        "model_id": "Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q5_K_S/DeepSeek-R1-0528-Q5_K_S-00010-of-00010.gguf",
        "file_size": "31.9 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00001-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00001-of-00012.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00002-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00002-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00003-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00003-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00004-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00004-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00005-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00005-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00006-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00006-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00007-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00007-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00008-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00008-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00009-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00009-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00010-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00010-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00011-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00011-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "Q6_K/DeepSeek-R1-0528-Q6_K-00012-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q6_K/DeepSeek-R1-0528-Q6_K-00012-of-00012.gguf",
        "file_size": "29.3 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00001-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00001-of-00015.gguf",
        "file_size": "44.5 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00002-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00002-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00003-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00003-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00004-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00004-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00005-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00005-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00006-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00006-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00007-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00007-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00008-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00008-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00009-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00009-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00010-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00010-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00011-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00011-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00012-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00012-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00013-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00013-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00014-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00014-of-00015.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "Q8_0/DeepSeek-R1-0528-Q8_0-00015-of-00015",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/Q8_0/DeepSeek-R1-0528-Q8_0-00015-of-00015.gguf",
        "file_size": "27.4 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00002-of-00005.gguf",
        "file_size": "45.6 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00003-of-00005.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00004-of-00005.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00005-of-00005.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004.gguf",
        "file_size": "35.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00001-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00002-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00003-of-00005.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00004-of-00005.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_M/DeepSeek-R1-0528-UD-IQ2_M-00005-of-00005.gguf",
        "file_size": "28.5 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00001-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00001-of-00005.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00002-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00002-of-00005.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00003-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00003-of-00005.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00004-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00004-of-00005.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00005-of-00005",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ2_XXS/DeepSeek-R1-0528-UD-IQ2_XXS-00005-of-00005.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00001-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00002-of-00006.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00003-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00004-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00005-of-00006.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-IQ3_XXS/DeepSeek-R1-0528-UD-IQ3_XXS-00006-of-00006.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00002-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00002-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00003-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00003-of-00006.gguf",
        "file_size": "45.8 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00004-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00005-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00005-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00006-of-00006",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00006-of-00006.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00001-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00001-of-00007.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00002-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00002-of-00007.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00003-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00003-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00004-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00004-of-00007.gguf",
        "file_size": "45.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00005-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00005-of-00007.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00006-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00006-of-00007.gguf",
        "file_size": "45.3 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00007-of-00007",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q3_K_XL/DeepSeek-R1-0528-UD-Q3_K_XL-00007-of-00007.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00001-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00001-of-00008.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00002-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00002-of-00008.gguf",
        "file_size": "45.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00003-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00003-of-00008.gguf",
        "file_size": "44.7 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00004-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00004-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00005-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00005-of-00008.gguf",
        "file_size": "44.8 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00006-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00006-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00007-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00007-of-00008.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00008-of-00008",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q4_K_XL/DeepSeek-R1-0528-UD-Q4_K_XL-00008-of-00008.gguf",
        "file_size": "38.0 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00001-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00001-of-00010.gguf",
        "file_size": "43.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00002-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00002-of-00010.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00003-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00003-of-00010.gguf",
        "file_size": "45.1 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00004-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00004-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00005-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00005-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00006-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00006-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00007-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00007-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00008-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00008-of-00010.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00009-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00009-of-00010.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00010-of-00010",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q5_K_XL/DeepSeek-R1-0528-UD-Q5_K_XL-00010-of-00010.gguf",
        "file_size": "42.8 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00001-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00001-of-00012.gguf",
        "file_size": "44.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00002-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00002-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00003-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00003-of-00012.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00004-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00004-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00005-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00005-of-00012.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00006-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00006-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00007-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00007-of-00012.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00008-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00008-of-00012.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00009-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00009-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00010-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00010-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00011-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00011-of-00012.gguf",
        "file_size": "44.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00012-of-00012",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q6_K_XL/DeepSeek-R1-0528-UD-Q6_K_XL-00012-of-00012.gguf",
        "file_size": "40.3 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00001-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00001-of-00016.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00002-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00002-of-00016.gguf",
        "file_size": "43.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00003-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00003-of-00016.gguf",
        "file_size": "44.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00004-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00004-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00005-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00005-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00006-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00006-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00007-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00007-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00008-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00008-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00009-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00009-of-00016.gguf",
        "file_size": "44.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00010-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00010-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00011-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00011-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00012-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00012-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00013-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00013-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00014-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00014-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00015-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00015-of-00016.gguf",
        "file_size": "45.7 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00016-of-00016",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-Q8_K_XL/DeepSeek-R1-0528-UD-Q8_K_XL-00016-of-00016.gguf",
        "file_size": "46.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/README.md",
    "description": "The DeepSeek-R1-0528 model, developed by DeepSeek-AI, is a large language model with enhanced reasoning capabilities, improved performance on benchmarks like AIME, and is available in various formats including GGUF, with detailed usage guidelines and a MIT license."
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "developer": "unsloth",
    "downloads": 950147,
    "createdAt": "2025-05-29T14:17:25.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "This is a large language model based on the DeepSeek-R1-0528 architecture, optimized for reasoning tasks with improved performance on benchmarks like AIME, and available in various quantized formats for efficient local inference."
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "developer": "unsloth",
    "downloads": 950147,
    "createdAt": "2025-05-29T14:17:25.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "This is a large language model based on the DeepSeek-R1-0528 architecture, optimized for reasoning tasks with improved performance on benchmarks like AIME, and available in various quantized formats for efficient local inference."
  },
  {
    "model_name": "dots.llm1.inst-GGUF",
    "developer": "unsloth",
    "downloads": 6163,
    "createdAt": "2025-06-16T12:59:47.000Z",
    "num_quants": 65,
    "quants": [
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00001-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00001-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00002-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00002-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00003-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00003-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00004-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00004-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00005-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00005-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00006-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00006-of-00006.gguf",
        "file_size": "36.1 GB"
      },
      {
        "model_id": "IQ4_NL/dots.llm1.inst-IQ4_NL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_NL/dots.llm1.inst-IQ4_NL-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/dots.llm1.inst-IQ4_NL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_NL/dots.llm1.inst-IQ4_NL-00002-of-00002.gguf",
        "file_size": "28.7 GB"
      },
      {
        "model_id": "IQ4_XS/dots.llm1.inst-IQ4_XS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_XS/dots.llm1.inst-IQ4_XS-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_XS/dots.llm1.inst-IQ4_XS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_XS/dots.llm1.inst-IQ4_XS-00002-of-00002.gguf",
        "file_size": "25.7 GB"
      },
      {
        "model_id": "Q2_K/dots.llm1.inst-Q2_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K/dots.llm1.inst-Q2_K-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q2_K/dots.llm1.inst-Q2_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K/dots.llm1.inst-Q2_K-00002-of-00002.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q2_K_L/dots.llm1.inst-Q2_K_L-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K_L/dots.llm1.inst-Q2_K_L-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q2_K_L/dots.llm1.inst-Q2_K_L-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K_L/dots.llm1.inst-Q2_K_L-00002-of-00002.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q3_K_M/dots.llm1.inst-Q3_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_M/dots.llm1.inst-Q3_K_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_M/dots.llm1.inst-Q3_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_M/dots.llm1.inst-Q3_K_M-00002-of-00002.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "Q3_K_S/dots.llm1.inst-Q3_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_S/dots.llm1.inst-Q3_K_S-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_S/dots.llm1.inst-Q3_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_S/dots.llm1.inst-Q3_K_S-00002-of-00002.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Q4_0/dots.llm1.inst-Q4_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_0/dots.llm1.inst-Q4_0-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q4_0/dots.llm1.inst-Q4_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_0/dots.llm1.inst-Q4_0-00002-of-00002.gguf",
        "file_size": "28.7 GB"
      },
      {
        "model_id": "Q4_1/dots.llm1.inst-Q4_1-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_1/dots.llm1.inst-Q4_1-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_1/dots.llm1.inst-Q4_1-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_1/dots.llm1.inst-Q4_1-00002-of-00002.gguf",
        "file_size": "36.9 GB"
      },
      {
        "model_id": "Q4_K_M/dots.llm1.inst-Q4_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_M/dots.llm1.inst-Q4_K_M-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_M/dots.llm1.inst-Q4_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_M/dots.llm1.inst-Q4_K_M-00002-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Q4_K_S/dots.llm1.inst-Q4_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_S/dots.llm1.inst-Q4_K_S-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/dots.llm1.inst-Q4_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_S/dots.llm1.inst-Q4_K_S-00002-of-00002.gguf",
        "file_size": "34.2 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00001-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00003-of-00003.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00001-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00002-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00003-of-00003.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00003-of-00003.gguf",
        "file_size": "26.3 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00001-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00001-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00002-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00002-of-00004.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00003-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00003-of-00004.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00004-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00004-of-00004.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00001-of-00002.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00002-of-00002.gguf",
        "file_size": "782.4 MB"
      },
      {
        "model_id": "UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00002-of-00002.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00002-of-00002.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00002-of-00002.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00002-of-00002.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00002-of-00002.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00002-of-00002.gguf",
        "file_size": "34.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00002-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00003-of-00003.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00003-of-00003.gguf",
        "file_size": "29.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00001-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00002-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00003-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00004-of-00004.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "dots.llm1.inst-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/dots.llm1.inst-UD-IQ1_S.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "dots.llm1.inst-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/dots.llm1.inst-UD-TQ1_0.gguf",
        "file_size": "44.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/README.md",
    "description": "The `dots.llm1` model is a large-scale MoE model with 14B activated parameters out of 142B total, trained on high-quality data without synthetic data, offering efficient and accurate performance for English and Chinese."
  },
  {
    "model_name": "dots.llm1.inst-GGUF",
    "developer": "unsloth",
    "downloads": 6163,
    "createdAt": "2025-06-16T12:59:47.000Z",
    "num_quants": 65,
    "quants": [
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00001-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00001-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00002-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00002-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00003-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00003-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00004-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00004-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00005-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00005-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "BF16/dots.llm1.inst-BF16-00006-of-00006",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/BF16/dots.llm1.inst-BF16-00006-of-00006.gguf",
        "file_size": "36.1 GB"
      },
      {
        "model_id": "IQ4_NL/dots.llm1.inst-IQ4_NL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_NL/dots.llm1.inst-IQ4_NL-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "IQ4_NL/dots.llm1.inst-IQ4_NL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_NL/dots.llm1.inst-IQ4_NL-00002-of-00002.gguf",
        "file_size": "28.7 GB"
      },
      {
        "model_id": "IQ4_XS/dots.llm1.inst-IQ4_XS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_XS/dots.llm1.inst-IQ4_XS-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_XS/dots.llm1.inst-IQ4_XS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/IQ4_XS/dots.llm1.inst-IQ4_XS-00002-of-00002.gguf",
        "file_size": "25.7 GB"
      },
      {
        "model_id": "Q2_K/dots.llm1.inst-Q2_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K/dots.llm1.inst-Q2_K-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q2_K/dots.llm1.inst-Q2_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K/dots.llm1.inst-Q2_K-00002-of-00002.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q2_K_L/dots.llm1.inst-Q2_K_L-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K_L/dots.llm1.inst-Q2_K_L-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q2_K_L/dots.llm1.inst-Q2_K_L-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q2_K_L/dots.llm1.inst-Q2_K_L-00002-of-00002.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q3_K_M/dots.llm1.inst-Q3_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_M/dots.llm1.inst-Q3_K_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_M/dots.llm1.inst-Q3_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_M/dots.llm1.inst-Q3_K_M-00002-of-00002.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "Q3_K_S/dots.llm1.inst-Q3_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_S/dots.llm1.inst-Q3_K_S-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_S/dots.llm1.inst-Q3_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q3_K_S/dots.llm1.inst-Q3_K_S-00002-of-00002.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "Q4_0/dots.llm1.inst-Q4_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_0/dots.llm1.inst-Q4_0-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "Q4_0/dots.llm1.inst-Q4_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_0/dots.llm1.inst-Q4_0-00002-of-00002.gguf",
        "file_size": "28.7 GB"
      },
      {
        "model_id": "Q4_1/dots.llm1.inst-Q4_1-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_1/dots.llm1.inst-Q4_1-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_1/dots.llm1.inst-Q4_1-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_1/dots.llm1.inst-Q4_1-00002-of-00002.gguf",
        "file_size": "36.9 GB"
      },
      {
        "model_id": "Q4_K_M/dots.llm1.inst-Q4_K_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_M/dots.llm1.inst-Q4_K_M-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_M/dots.llm1.inst-Q4_K_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_M/dots.llm1.inst-Q4_K_M-00002-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Q4_K_S/dots.llm1.inst-Q4_K_S-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_S/dots.llm1.inst-Q4_K_S-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_K_S/dots.llm1.inst-Q4_K_S-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q4_K_S/dots.llm1.inst-Q4_K_S-00002-of-00002.gguf",
        "file_size": "34.2 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00001-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q5_K_M/dots.llm1.inst-Q5_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_M/dots.llm1.inst-Q5_K_M-00003-of-00003.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00001-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00002-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q5_K_S/dots.llm1.inst-Q5_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q5_K_S/dots.llm1.inst-Q5_K_S-00003-of-00003.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/dots.llm1.inst-Q6_K-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q6_K/dots.llm1.inst-Q6_K-00003-of-00003.gguf",
        "file_size": "26.3 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00001-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00001-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00002-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00002-of-00004.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00003-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00003-of-00004.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/dots.llm1.inst-Q8_0-00004-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/Q8_0/dots.llm1.inst-Q8_0-00004-of-00004.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00001-of-00002.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ1_M/dots.llm1.inst-UD-IQ1_M-00002-of-00002.gguf",
        "file_size": "782.4 MB"
      },
      {
        "model_id": "UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_M/dots.llm1.inst-UD-IQ2_M-00002-of-00002.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ2_XXS/dots.llm1.inst-UD-IQ2_XXS-00002-of-00002.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-IQ3_XXS/dots.llm1.inst-UD-IQ3_XXS-00002-of-00002.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q2_K_XL/dots.llm1.inst-UD-Q2_K_XL-00002-of-00002.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00001-of-00002.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q3_K_XL/dots.llm1.inst-UD-Q3_K_XL-00002-of-00002.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q4_K_XL/dots.llm1.inst-UD-Q4_K_XL-00002-of-00002.gguf",
        "file_size": "34.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00002-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q5_K_XL/dots.llm1.inst-UD-Q5_K_XL-00003-of-00003.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q6_K_XL/dots.llm1.inst-UD-Q6_K_XL-00003-of-00003.gguf",
        "file_size": "29.6 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00001-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00002-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00003-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/UD-Q8_K_XL/dots.llm1.inst-UD-Q8_K_XL-00004-of-00004.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "dots.llm1.inst-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/dots.llm1.inst-UD-IQ1_S.gguf",
        "file_size": "44.9 GB"
      },
      {
        "model_id": "dots.llm1.inst-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/dots.llm1.inst-UD-TQ1_0.gguf",
        "file_size": "44.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/dots.llm1.inst-GGUF/resolve/main/README.md",
    "description": "The `dots.llm1` model is a large-scale MoE model with 14B activated parameters out of 142B total, trained on high-quality data without synthetic data, offering efficient and accurate performance for English and Chinese."
  },
  {
    "model_name": "ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 4935,
    "createdAt": "2025-06-30T05:25:35.000Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "ERNIE-4.5-0.3B-PT-F16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-F16.gguf",
        "file_size": "690.4 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.0 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "462.6 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-0.3B is a text dense, Apache-2.0 licensed large language model optimized for text generation and fine-tuning with ERNIEKit and FastDeploy, and supported by the transformers library."
  },
  {
    "model_name": "ERNIE-4.5-0.3B-PT-GGUF",
    "developer": "unsloth",
    "downloads": 4935,
    "createdAt": "2025-06-30T05:25:35.000Z",
    "num_quants": 19,
    "quants": [
      {
        "model_id": "ERNIE-4.5-0.3B-PT-F16",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-F16.gguf",
        "file_size": "690.4 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q2_K_L",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q2_K_L.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_M.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q3_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q3_K_S.gguf",
        "file_size": "189.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_0.gguf",
        "file_size": "222.0 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_1",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_1.gguf",
        "file_size": "237.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_M.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q4_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q4_K_S.gguf",
        "file_size": "222.8 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_M",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_M.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q5_K_S",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q5_K_S.gguf",
        "file_size": "252.3 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q6_K",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q6_K.gguf",
        "file_size": "284.6 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-Q8_0",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-Q8_0.gguf",
        "file_size": "367.9 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q2_K_XL.gguf",
        "file_size": "175.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q3_K_XL.gguf",
        "file_size": "202.7 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q4_K_XL.gguf",
        "file_size": "229.5 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q5_K_XL.gguf",
        "file_size": "256.2 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q6_K_XL.gguf",
        "file_size": "309.1 MB"
      },
      {
        "model_id": "ERNIE-4.5-0.3B-PT-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/ERNIE-4.5-0.3B-PT-UD-Q8_K_XL.gguf",
        "file_size": "462.6 MB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/ERNIE-4.5-0.3B-PT-GGUF/resolve/main/README.md",
    "description": "ERNIE-4.5-0.3B is a text dense, Apache-2.0 licensed large language model optimized for text generation and fine-tuning with ERNIEKit and FastDeploy, and supported by the transformers library."
  },
  {
    "model_name": "gemma-2-2b-it-GGUF",
    "developer": "bartowski",
    "downloads": 130909,
    "createdAt": "2024-07-31T16:45:13.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gemma-2-2b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-2-2b-it model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp imatrix quantization."
  },
  {
    "model_name": "gemma-2-2b-it-GGUF",
    "developer": "bartowski",
    "downloads": 130909,
    "createdAt": "2024-07-31T16:45:13.000Z",
    "num_quants": 11,
    "quants": [
      {
        "model_id": "gemma-2-2b-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ3_M.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-2-2b-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-IQ4_XS.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q3_K_L.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q5_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-2-2b-it-Q8_0",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-2-2b-it-f32",
        "path": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-f32.gguf",
        "file_size": "9.7 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma-2-2b-it model by bartowski, optimized for various inference speeds and resource constraints using llama.cpp imatrix quantization."
  },
  {
    "model_name": "gemma-2b",
    "developer": "google",
    "downloads": 188776,
    "createdAt": "2024-02-08T08:11:26.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b",
        "path": "https://huggingface.co/google/gemma-2b/resolve/main/gemma-2b.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-2b/resolve/main/README.md",
    "description": "The Gemma 2B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with support for fine-tuning, multi-GPU training, and various precision optimizations, and evaluated on multiple benchmarks for performance and"
  },
  {
    "model_name": "gemma-2b",
    "developer": "google",
    "downloads": 188776,
    "createdAt": "2024-02-08T08:11:26.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b",
        "path": "https://huggingface.co/google/gemma-2b/resolve/main/gemma-2b.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-2b/resolve/main/README.md",
    "description": "The Gemma 2B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with support for fine-tuning, multi-GPU training, and various precision optimizations, and evaluated on multiple benchmarks for performance and"
  },
  {
    "model_name": "gemma-3-12b-it-GGUF",
    "developer": "unsloth",
    "downloads": 44816,
    "createdAt": "2025-03-12T10:34:12.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 12B model, optimized for performance and memory efficiency with support for GGUF, 4-bit, and 16-bit formats, and available on Hugging Face."
  },
  {
    "model_name": "gemma-3-12b-it-GGUF",
    "developer": "unsloth",
    "downloads": 44816,
    "createdAt": "2025-03-12T10:34:12.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/README.md",
    "description": "This is a fine-tuned version of the Google Gemma 3 12B model, optimized for performance and memory efficiency with support for GGUF, 4-bit, and 16-bit formats, and available on Hugging Face."
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 89026,
    "createdAt": "2025-03-12T12:32:41.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned, multimodal model with 128K context window and multilingual support, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks for reasoning, STEM, code, and"
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 89026,
    "createdAt": "2025-03-12T12:32:41.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned, multimodal model with 128K context window and multilingual support, trained on diverse data including text, code, math, and images, and evaluated on various benchmarks for reasoning, STEM, code, and"
  },
  {
    "model_name": "gemma-3-1b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 2649,
    "createdAt": "2025-03-10T22:42:41.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf",
        "file_size": "957.1 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and available in GGUF format with QAT quantization for efficient deployment."
  },
  {
    "model_name": "gemma-3-1b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 2649,
    "createdAt": "2025-03-10T22:42:41.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-1b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/gemma-3-1b-it-q4_0.gguf",
        "file_size": "957.1 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, instruction-tuned multilingual vision-language model with a 128K context window, trained on diverse data including text, code, math, and images, and available in GGUF format with QAT quantization for efficient deployment."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 18991,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "num_quants": 7,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored Gemma 3 27B IT model created using a layerwise abliteration technique to achieve high acceptance rates while preserving model capabilities."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 18991,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "num_quants": 7,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      },
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored Gemma 3 27B IT model created using a layerwise abliteration technique to achieve high acceptance rates while preserving model capabilities."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 4300,
    "createdAt": "2025-05-28T22:03:00.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the Google Gemma-3-27B IT model, created using an advanced abliteration technique to enhance refusal accuracy while maintaining coherent outputs."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-v2-GGUF",
    "developer": "mlabonne",
    "downloads": 4300,
    "createdAt": "2025-05-28T22:03:00.000Z",
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated-v2.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/gemma-3-27b-it-abliterated-v2.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF/resolve/main/README.md",
    "description": "This is an uncensored version of the Google Gemma-3-27B IT model, created using an advanced abliteration technique to enhance refusal accuracy while maintaining coherent outputs."
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 7139,
    "createdAt": "2025-03-20T22:44:27.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, multimodal AI model from Google, capable of handling text and image inputs to generate text outputs, trained on diverse data including text, code, math, and images, with performance metrics across various benchmarks and languages, and available in multiple sizes"
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 7139,
    "createdAt": "2025-03-20T22:44:27.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "The Gemma 3 model is a lightweight, open-source, multimodal AI model from Google, capable of handling text and image inputs to generate text outputs, trained on diverse data including text, code, math, and images, with performance metrics across various benchmarks and languages, and available in multiple sizes"
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "unsloth",
    "downloads": 25521,
    "createdAt": "2025-06-26T12:24:52.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-F16.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ3_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q2_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q4_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q5_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q6_K_XL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q8_K_XL.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with instruction-tuned variants available for fine-tuning"
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "ggml-org",
    "downloads": 1624,
    "createdAt": "2025-06-26T10:09:32.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-f16",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-f16.gguf",
        "file_size": "8.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This GGUF version of the Gemma 3n model is available on Hugging Face and does not include multimodal support."
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "unsloth",
    "downloads": 25521,
    "createdAt": "2025-06-26T12:24:52.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-F16.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ2_XXS.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-IQ3_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q2_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q3_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q4_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q5_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q6_K_XL.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-UD-Q8_K_XL.gguf",
        "file_size": "7.0 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with instruction-tuned variants available for fine-tuning"
  },
  {
    "model_name": "gemma-3n-E2B-it-GGUF",
    "developer": "ggml-org",
    "downloads": 1624,
    "createdAt": "2025-06-26T10:09:32.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-f16",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF/resolve/main/gemma-3n-E2B-it-f16.gguf",
        "file_size": "8.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This GGUF version of the Gemma 3n model is available on Hugging Face and does not include multimodal support."
  },
  {
    "model_name": "gemma-3n-E2B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 2533,
    "createdAt": "2025-06-26T15:16:13.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E2B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment."
  },
  {
    "model_name": "gemma-3n-E2B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 2533,
    "createdAt": "2025-06-26T15:16:13.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E2B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment."
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "unsloth",
    "downloads": 71206,
    "createdAt": "2025-06-26T12:24:35.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q2_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q3_K_XL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q6_K_XL.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q8_K_XL.gguf",
        "file_size": "9.9 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with high performance and accuracy, available on Hugging"
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "ggml-org",
    "downloads": 2590,
    "createdAt": "2025-06-26T09:59:16.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-f16",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-f16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This GGUF version of the Gemma 3n model is available on Hugging Face and does not include multimodal support, developed by Google DeepMind."
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "unsloth",
    "downloads": 71206,
    "createdAt": "2025-06-26T12:24:35.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-F16",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ2_XXS.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-IQ3_XXS.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q2_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q3_K_XL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q6_K_XL.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q8_K_XL.gguf",
        "file_size": "9.9 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "The Gemma 3n model is a lightweight, efficient, and open-source multilingual image-text-to-text model from Google, optimized for low-resource devices with support for text, image, video, and audio inputs, and capable of generating text outputs with high performance and accuracy, available on Hugging"
  },
  {
    "model_name": "gemma-3n-E4B-it-GGUF",
    "developer": "ggml-org",
    "downloads": 2590,
    "createdAt": "2025-06-26T09:59:16.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-f16",
        "path": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-f16.gguf",
        "file_size": "12.8 GB"
      }
    ],
    "readme": "https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This GGUF version of the Gemma 3n model is available on Hugging Face and does not include multimodal support, developed by Google DeepMind."
  },
  {
    "model_name": "gemma-3n-E4B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 10128,
    "createdAt": "2025-06-26T15:10:43.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E4B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment."
  },
  {
    "model_name": "gemma-3n-E4B-it-text-GGUF",
    "developer": "lmstudio-community",
    "downloads": 10128,
    "createdAt": "2025-06-26T15:10:43.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF/resolve/main/README.md",
    "description": "This GGUF quantized version of the Gemma-3n-E4B-it model by Google is provided by the LM Studio team using llama.cpp for efficient deployment."
  },
  {
    "model_name": "gemma-7b",
    "developer": "google",
    "downloads": 54184,
    "createdAt": "2024-02-08T22:36:43.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b",
        "path": "https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-7b/resolve/main/README.md",
    "description": "The Gemma 7B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with strong performance on benchmark tasks and ethical safety considerations."
  },
  {
    "model_name": "gemma-7b",
    "developer": "google",
    "downloads": 54184,
    "createdAt": "2024-02-08T22:36:43.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b",
        "path": "https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-7b/resolve/main/README.md",
    "description": "The Gemma 7B model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and mathematics, with strong performance on benchmark tasks and ethical safety considerations."
  },
  {
    "model_name": "gemma-7b-it",
    "developer": "google",
    "downloads": 68292,
    "createdAt": "2024-02-13T01:07:30.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b-it",
        "path": "https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-7b-it/resolve/main/README.md",
    "description": "The Gemma 7B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, and suitable for various text generation tasks with support for fine-tuning, GPU acceleration, and ethical safety measures."
  },
  {
    "model_name": "gemma-7b-it",
    "developer": "google",
    "downloads": 68292,
    "createdAt": "2024-02-13T01:07:30.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b-it",
        "path": "https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-7b-it/resolve/main/README.md",
    "description": "The Gemma 7B instruct model is a lightweight, open-source text-to-text large language model from Google, trained on diverse data including web text, code, and math, and suitable for various text generation tasks with support for fine-tuning, GPU acceleration, and ethical safety measures."
  },
  {
    "model_name": "Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF",
    "developer": "DavidAU",
    "downloads": 17533,
    "createdAt": "2024-10-25T00:19:23.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a highly advanced, uncensored Gemma2 model designed for creative writing, particularly fiction and storytelling, combining four top storytelling models with additional modifications for vivid, detailed, and emotionally engaging prose across various genres, including science fiction, horror, and romance, with options for different quantization levels and"
  },
  {
    "model_name": "Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF",
    "developer": "DavidAU",
    "downloads": 17533,
    "createdAt": "2024-10-25T00:19:23.000Z",
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q2_k.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q6_k.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/Gemma-The-Writer-N-Restless-Quill-10B-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-Uncensored-GGUF/resolve/main/README.md",
    "description": "This is a highly advanced, uncensored Gemma2 model designed for creative writing, particularly fiction and storytelling, combining four top storytelling models with additional modifications for vivid, detailed, and emotionally engaging prose across various genres, including science fiction, horror, and romance, with options for different quantization levels and"
  },
  {
    "model_name": "google_gemma-3n-E2B-it-GGUF",
    "developer": "bartowski",
    "downloads": 2299,
    "createdAt": "2025-06-26T19:50:06.000Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3n-E2B-it model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "google_gemma-3n-E2B-it-GGUF",
    "developer": "bartowski",
    "downloads": 2299,
    "createdAt": "2025-06-26T19:50:06.000Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ3_XS.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_NL.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-IQ4_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q2_K_L.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q3_K_XL.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_0.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_L.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q4_K_S.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_L.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q5_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q6_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E2B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/google_gemma-3n-E2B-it-Q8_0.gguf",
        "file_size": "4.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E2B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Google Gemma-3n-E2B-it model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "google_gemma-3n-E4B-it-GGUF",
    "developer": "bartowski",
    "downloads": 5082,
    "createdAt": "2025-06-26T19:50:49.000Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs."
  },
  {
    "model_name": "google_gemma-3n-E4B-it-GGUF",
    "developer": "bartowski",
    "downloads": 5082,
    "createdAt": "2025-06-26T19:50:49.000Z",
    "num_quants": 21,
    "quants": [
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ3_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ3_XS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_NL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_NL.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-IQ4_XS",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q2_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q2_K_L.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_M.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q3_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_1",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_1.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_L.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q4_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q4_K_S.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_M",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q5_K_S",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q6_K_L",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q6_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "google_gemma-3n-E4B-it-Q8_0",
        "path": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/google_gemma-3n-E4B-it-Q8_0.gguf",
        "file_size": "6.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Gemma model using llama.cpp's imatrix method, offering various quantization types for different performance and memory trade-offs."
  },
  {
    "model_name": "Huihui-Qwen3-235B-A22B-abliterated-GGUF",
    "developer": "huihui-ai",
    "downloads": 243,
    "createdAt": "2025-06-18T06:26:41.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00001-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00001-of-00012.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00002-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00002-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00003-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00003-of-00012.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00004-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00004-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00005-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00005-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00006-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00006-of-00012.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00007-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00007-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00008-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00008-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00009-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00009-of-00012.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00010-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00010-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00011-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00011-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00012-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00012-of-00012.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored, abliterated version of Qwen3-235B-A22B for text generation, with reduced safety filtering and usage warnings."
  },
  {
    "model_name": "Huihui-Qwen3-235B-A22B-abliterated-GGUF",
    "developer": "huihui-ai",
    "downloads": 243,
    "createdAt": "2025-06-18T06:26:41.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00001-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00001-of-00012.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00002-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00002-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00003-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00003-of-00012.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00004-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00004-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00005-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00005-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00006-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00006-of-00012.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00007-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00007-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00008-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00008-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00009-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00009-of-00012.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00010-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00010-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00011-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00011-of-00012.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "Q3_K_M-GGUF/Q3_K_M-GGUF-00012-of-00012",
        "path": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/Q3_K_M-GGUF/Q3_K_M-GGUF-00012-of-00012.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored, abliterated version of Qwen3-235B-A22B for text generation, with reduced safety filtering and usage warnings."
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "bullerwins",
    "downloads": 4609,
    "createdAt": "2025-06-29T19:45:13.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-Q2_K",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q2_K.gguf",
        "file_size": "27.4 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q3_K_L.gguf",
        "file_size": "38.8 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q3_K_M.gguf",
        "file_size": "35.9 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q3_K_S.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q4_K_M.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q4_K_S.gguf",
        "file_size": "42.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_M-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_M-00001-of-00002.gguf",
        "file_size": "41.5 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_M-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_M-00002-of-00002.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_S-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_S-00001-of-00002.gguf",
        "file_size": "41.5 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_S-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_S-00002-of-00002.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q6_K-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q6_K-00001-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q6_K-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q6_K-00002-of-00002.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q8_0-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q8_0-00001-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q8_0-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q8_0-00002-of-00002.gguf",
        "file_size": "37.9 GB"
      },
      {
        "model_id": "Hunyuan-A13B-InstructQ3_K_L",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-InstructQ3_K_L.gguf",
        "file_size": "38.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Hugging Face repository for Hunyuan-A13B provides an open-source large language model with 80 billion parameters, 13 billion active parameters, and supports efficient inference through various frameworks like TensorRT-LLM, vLLM, and SGLang, with a focus"
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "ubergarm",
    "downloads": 409,
    "createdAt": "2025-07-02T00:39:08.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-IQ3_KS.gguf",
        "file_size": "34.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an imatrix quantized version of Hunyuan-A13B-Instruct using ik_llama.cpp, optimized for high perplexity and performance with specific layer quantization settings and hardware requirements."
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "bullerwins",
    "downloads": 4609,
    "createdAt": "2025-06-29T19:45:13.000Z",
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-Q2_K",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q2_K.gguf",
        "file_size": "27.4 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q3_K_L.gguf",
        "file_size": "38.8 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q3_K_M.gguf",
        "file_size": "35.9 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q3_K_S.gguf",
        "file_size": "32.4 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q4_K_M.gguf",
        "file_size": "45.4 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q4_K_S.gguf",
        "file_size": "42.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_M-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_M-00001-of-00002.gguf",
        "file_size": "41.5 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_M-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_M-00002-of-00002.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_S-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_S-00001-of-00002.gguf",
        "file_size": "41.5 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q5_K_S-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q5_K_S-00002-of-00002.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q6_K-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q6_K-00001-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q6_K-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q6_K-00002-of-00002.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q8_0-00001-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q8_0-00001-of-00002.gguf",
        "file_size": "41.7 GB"
      },
      {
        "model_id": "Hunyuan-A13B-Instruct-Q8_0-00002-of-00002",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-Q8_0-00002-of-00002.gguf",
        "file_size": "37.9 GB"
      },
      {
        "model_id": "Hunyuan-A13B-InstructQ3_K_L",
        "path": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-InstructQ3_K_L.gguf",
        "file_size": "38.8 GB"
      }
    ],
    "readme": "https://huggingface.co/bullerwins/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Hugging Face repository for Hunyuan-A13B provides an open-source large language model with 80 billion parameters, 13 billion active parameters, and supports efficient inference through various frameworks like TensorRT-LLM, vLLM, and SGLang, with a focus"
  },
  {
    "model_name": "Hunyuan-A13B-Instruct-GGUF",
    "developer": "ubergarm",
    "downloads": 409,
    "createdAt": "2025-07-02T00:39:08.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Hunyuan-A13B-Instruct-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/Hunyuan-A13B-Instruct-IQ3_KS.gguf",
        "file_size": "34.1 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is an imatrix quantized version of Hunyuan-A13B-Instruct using ik_llama.cpp, optimized for high perplexity and performance with specific layer quantization settings and hardware requirements."
  },
  {
    "model_name": "Jan-nano-128k-GGUF",
    "developer": "unsloth",
    "downloads": 12561,
    "createdAt": "2025-06-25T07:31:40.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-128k-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling efficient processing of long documents and complex multi-turn conversations without performance degradation."
  },
  {
    "model_name": "Jan-nano-128k-GGUF",
    "developer": "unsloth",
    "downloads": 12561,
    "createdAt": "2025-06-25T07:31:40.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Jan-nano-128k-BF16",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-nano-128k-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/Jan-nano-128k-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Jan-nano-128k-GGUF/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact language model with a native 128k context window, designed for deep research tasks by enabling efficient processing of long documents and complex multi-turn conversations without performance degradation."
  },
  {
    "model_name": "Kimi-Dev-72B-GGUF",
    "developer": "unsloth",
    "downloads": 26523,
    "createdAt": "2025-06-17T00:37:17.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00003-of-00003.gguf",
        "file_size": "42.7 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-IQ4_NL.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-IQ4_XS.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q3_K_M.gguf",
        "file_size": "35.1 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q3_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q4_0",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q4_0.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q4_1",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q4_1.gguf",
        "file_size": "42.6 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ1_M.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ1_S.gguf",
        "file_size": "21.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ2_M.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ2_XXS.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ3_XXS.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-Q2_K_XL.gguf",
        "file_size": "28.3 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-Q3_K_XL.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "Q6_K/Kimi-Dev-72B-Q6_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q6_K/Kimi-Dev-72B-Q6_K-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/Kimi-Dev-72B-Q6_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q6_K/Kimi-Dev-72B-Q6_K-00002-of-00002.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Q8_0/Kimi-Dev-72B-Q8_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q8_0/Kimi-Dev-72B-Q8_0-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/Kimi-Dev-72B-Q8_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q8_0/Kimi-Dev-72B-Q8_0-00002-of-00002.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00002-of-00002.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00002-of-00002.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00002-of-00002.gguf",
        "file_size": "31.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/README.md",
    "description": "Kimi-Dev-72B is an open-source coding LLM for issue resolution that achieves 60.4% performance on SWE-bench Verified, outperforming other leading open-source models."
  },
  {
    "model_name": "Kimi-Dev-72B-GGUF",
    "developer": "unsloth",
    "downloads": 26523,
    "createdAt": "2025-06-17T00:37:17.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "BF16/Kimi-Dev-72B-BF16-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/BF16/Kimi-Dev-72B-BF16-00003-of-00003.gguf",
        "file_size": "42.7 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-IQ4_NL.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-IQ4_XS.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q3_K_M.gguf",
        "file_size": "35.1 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q3_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q4_0",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q4_0.gguf",
        "file_size": "38.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-Q4_1",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-Q4_1.gguf",
        "file_size": "42.6 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ1_M.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ1_S.gguf",
        "file_size": "21.5 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ2_M.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ2_XXS.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-IQ3_XXS.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-Q2_K_XL.gguf",
        "file_size": "28.3 GB"
      },
      {
        "model_id": "Kimi-Dev-72B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Kimi-Dev-72B-UD-Q3_K_XL.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "Q6_K/Kimi-Dev-72B-Q6_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q6_K/Kimi-Dev-72B-Q6_K-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q6_K/Kimi-Dev-72B-Q6_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q6_K/Kimi-Dev-72B-Q6_K-00002-of-00002.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Q8_0/Kimi-Dev-72B-Q8_0-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q8_0/Kimi-Dev-72B-Q8_0-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q8_0/Kimi-Dev-72B-Q8_0-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/Q8_0/Kimi-Dev-72B-Q8_0-00002-of-00002.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q5_K_XL/Kimi-Dev-72B-UD-Q5_K_XL-00002-of-00002.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q6_K_XL/Kimi-Dev-72B-UD-Q6_K_XL-00002-of-00002.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/UD-Q8_K_XL/Kimi-Dev-72B-UD-Q8_K_XL-00002-of-00002.gguf",
        "file_size": "31.7 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF/resolve/main/README.md",
    "description": "Kimi-Dev-72B is an open-source coding LLM for issue resolution that achieves 60.4% performance on SWE-bench Verified, outperforming other leading open-source models."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 236569,
    "createdAt": "2024-09-25T18:35:33.000Z",
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct model by Bartowski, offering various quantization types (e.g., Q4_K_M, Q5_K_S) for different performance and memory trade-offs, with a focus on compatibility with ARM and CPU"
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 236569,
    "createdAt": "2024-09-25T18:35:33.000Z",
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ4_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_L.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_XL.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_4.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_4_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0_8_8.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_L.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Llama-3.2-3B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-f16.gguf",
        "file_size": "6.0 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Llama-3.2-3B-Instruct model by Bartowski, offering various quantization types (e.g., Q4_K_M, Q5_K_S) for different performance and memory trade-offs, with a focus on compatibility with ARM and CPU"
  },
  {
    "model_name": "Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF",
    "developer": "DavidAU",
    "downloads": 4323,
    "createdAt": "2024-12-12T01:30:01.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/README.md",
    "description": "This model is a powerful Llama 3.2 MOE with 10B parameters, combining four top Llama 3.2 3B models for exceptional creative writing, vivid prose, and uncensored output across all genres, including horror, science fiction, romance, and roleplay"
  },
  {
    "model_name": "Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF",
    "developer": "DavidAU",
    "downloads": 4323,
    "createdAt": "2024-12-12T01:30:01.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-IQ4_XS.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q2_k.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_l.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_m.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q3_k_s.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_4.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_4_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_0_8_8.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_m.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q4_k_s.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q5_k_s.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q6_k.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-Q8_0.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/Llama-3.2-4X3B-MOE-Hell-California-10B-D_AU-q5_k_m.gguf",
        "file_size": "6.6 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-4X3B-MOE-Hell-California-Uncensored-10B-GGUF/resolve/main/README.md",
    "description": "This model is a powerful Llama 3.2 MOE with 10B parameters, combining four top Llama 3.2 3B models for exceptional creative writing, vivid prose, and uncensored output across all genres, including horror, science fiction, romance, and roleplay"
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 79110,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B model with mixture of experts for creative writing, fiction, and roleplay, capable of generating vivid, uncensored, and genre-"
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 79110,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "This is a Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B model with mixture of experts for creative writing, fiction, and roleplay, capable of generating vivid, uncensored, and genre-"
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 217816,
    "createdAt": "2024-04-18T16:43:25.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.fp16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized and GGUF version of the Meta Llama 3 8B Instruct model, optimized for efficient inference and compatible with llama.cpp, based on the original Meta Llama 3 model."
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 217816,
    "createdAt": "2024-04-18T16:43:25.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3-8B-Instruct.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.fp16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a quantized and GGUF version of the Meta Llama 3 8B Instruct model, optimized for efficient inference and compatible with llama.cpp, based on the original Meta Llama 3 model."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 44938,
    "createdAt": "2024-07-23T15:36:34.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Meta-Llama-3.1-8B-Instruct model using llama.cpp, including various quantization types like Q4_K_M, Q5_K_S, IQ3_M, and others, optimized for different hardware platforms and offering trade-offs between speed,"
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 44938,
    "createdAt": "2024-07-23T15:36:34.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Meta-Llama-3.1-8B-Instruct model using llama.cpp, including various quantization types like Q4_K_M, Q5_K_S, IQ3_M, and others, optimized for different hardware platforms and offering trade-offs between speed,"
  },
  {
    "model_name": "Ministral-8B-Instruct-2410-GGUF",
    "developer": "bartowski",
    "downloads": 13649,
    "createdAt": "2024-10-21T16:27:21.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ2_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K_L.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q8_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-f16",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-f16.gguf",
        "file_size": "14.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Mistral-8B-Instruct-2410 model for various inference platforms, with a focus on research use under the Mistral AI Research License, requiring users to agree to specific terms and conditions for non-commercial purposes."
  },
  {
    "model_name": "Ministral-8B-Instruct-2410-GGUF",
    "developer": "bartowski",
    "downloads": 13649,
    "createdAt": "2024-10-21T16:27:21.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ2_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ2_M.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q2_K_L.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-Q8_0",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Ministral-8B-Instruct-2410-f16",
        "path": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-f16.gguf",
        "file_size": "14.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Mistral-8B-Instruct-2410 model for various inference platforms, with a focus on research use under the Mistral AI Research License, requiring users to agree to specific terms and conditions for non-commercial purposes."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.1-GGUF",
    "developer": "TheBloke",
    "downloads": 105860,
    "createdAt": "2023-09-27T17:49:54.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "mistral-7b-instruct-v0.1.Q2_K",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q4_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q5_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q8_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF format quantized versions of Mistral AI's Mistral 7B Instruct v0.1 model for efficient CPU and GPU inference with various quantization levels and compatibility with multiple frameworks like llama.cpp, text-generation-webui, and ctransformers."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.1-GGUF",
    "developer": "TheBloke",
    "downloads": 105860,
    "createdAt": "2023-09-27T17:49:54.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "mistral-7b-instruct-v0.1.Q2_K",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q4_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q5_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "mistral-7b-instruct-v0.1.Q8_0",
        "path": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF format quantized versions of Mistral AI's Mistral 7B Instruct v0.1 model for efficient CPU and GPU inference with various quantization levels and compatibility with multiple frameworks like llama.cpp, text-generation-webui, and ctransformers."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 224274,
    "createdAt": "2024-05-22T17:27:45.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ2_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Mistral-7B-Instruct-v0.3, quantized for efficient inference on various platforms."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
    "developer": "MaziyarPanahi",
    "downloads": 224274,
    "createdAt": "2024-05-22T17:27:45.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_M.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ1_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ1_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ2_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ2_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ3_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ3_XS.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.IQ4_XS",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.IQ4_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q2_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_L",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q3_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q3_K_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q4_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_M",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q5_K_S",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q6_K",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.Q8_0",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Mistral-7B-Instruct-v0.3.fp16",
        "path": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.fp16.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "readme": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/README.md",
    "description": "This model is a GGUF format version of the Mistral-7B-Instruct-v0.3, quantized for efficient inference on various platforms."
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "unsloth",
    "downloads": 83103,
    "createdAt": "2025-06-20T22:27:21.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized Mistral-3.2 Small 24B model optimized for instruction following, function calling, and vision reasoning, available via vLLM or Transformers with a system prompt for enhanced performance."
  },
  {
    "model_name": "Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "unsloth",
    "downloads": 83103,
    "createdAt": "2025-06-20T22:27:21.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized Mistral-3.2 Small 24B model optimized for instruction following, function calling, and vision reasoning, available via vLLM or Transformers with a system prompt for enhanced performance."
  },
  {
    "model_name": "mistralai_Magistral-Small-2506-GGUF",
    "developer": "bartowski",
    "downloads": 14407,
    "createdAt": "2025-06-10T16:09:49.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2506 model by bartowski, optimized for various inference speeds and quality levels using llama.cpp."
  },
  {
    "model_name": "mistralai_Magistral-Small-2506-GGUF",
    "developer": "bartowski",
    "downloads": 14407,
    "createdAt": "2025-06-10T16:09:49.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Magistral-Small-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/mistralai_Magistral-Small-2506-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Magistral-Small-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Magistral-Small-2506 model by bartowski, optimized for various inference speeds and quality levels using llama.cpp."
  },
  {
    "model_name": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "bartowski",
    "downloads": 20647,
    "createdAt": "2025-06-20T19:03:22.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistral-Small-3.2-24B-Instruct-2506 model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF",
    "developer": "bartowski",
    "downloads": 20647,
    "createdAt": "2025-06-20T19:03:22.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-bf16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16",
        "path": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/mmproj-mistralai_Mistral-Small-3.2-24B-Instruct-2506-f16.gguf",
        "file_size": "837.4 MB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Mistral-Small-3.2-24B-Instruct-2506 model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "Mixtral-8x7B-Instruct-v0.1-GGUF",
    "developer": "TheBloke",
    "downloads": 37100,
    "createdAt": "2023-12-11T18:08:33.000Z",
    "num_quants": 8,
    "quants": [
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q2_K",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q4_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q5_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_0.gguf",
        "file_size": "30.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
        "file_size": "30.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q8_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q8_0.gguf",
        "file_size": "46.2 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "The Mixtral 8X7B Instruct v0.1 model is a quantized version of Mistral AI's large language model, optimized for efficient inference on various platforms including llama.cpp, LM Studio, and llama-cpp-python, with different quantization levels offering trade-offs between size"
  },
  {
    "model_name": "Mixtral-8x7B-Instruct-v0.1-GGUF",
    "developer": "TheBloke",
    "downloads": 37100,
    "createdAt": "2023-12-11T18:08:33.000Z",
    "num_quants": 8,
    "quants": [
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q2_K",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q4_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_0.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "file_size": "24.6 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q5_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_0.gguf",
        "file_size": "30.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
        "file_size": "30.0 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q6_K",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "mixtral-8x7b-instruct-v0.1.Q8_0",
        "path": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q8_0.gguf",
        "file_size": "46.2 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/README.md",
    "description": "The Mixtral 8X7B Instruct v0.1 model is a quantized version of Mistral AI's large language model, optimized for efficient inference on various platforms including llama.cpp, LM Studio, and llama-cpp-python, with different quantization levels offering trade-offs between size"
  },
  {
    "model_name": "MS3.2-24B-Magnum-Diamond-GGUF",
    "developer": "Doctor-Shotgun",
    "downloads": 3492,
    "createdAt": "2025-06-22T18:07:51.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_NL",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_L",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q6_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q8_0",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-bf16",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Doctor-Shotgun/MS3.2-24B-Magnum-Diamond model, optimized for creative writing and roleplay with specific SillyTavern presets provided."
  },
  {
    "model_name": "MS3.2-24B-Magnum-Diamond-GGUF",
    "developer": "Doctor-Shotgun",
    "downloads": 3492,
    "createdAt": "2025-06-22T18:07:51.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ1_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ1_S.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ2_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ2_XXS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ3_XXS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_NL",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-IQ4_XS",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q2_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q2_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_L",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q3_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q4_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_M",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q5_K_S",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q6_K",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-Q8_0",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "MS3.2-24B-Magnum-Diamond-bf16",
        "path": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/MS3.2-24B-Magnum-Diamond-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/Doctor-Shotgun/MS3.2-24B-Magnum-Diamond-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Doctor-Shotgun/MS3.2-24B-Magnum-Diamond model, optimized for creative writing and roleplay with specific SillyTavern presets provided."
  },
  {
    "model_name": "MythoMax-L2-13B-GGUF",
    "developer": "TheBloke",
    "downloads": 85649,
    "createdAt": "2023-09-05T03:10:48.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "mythomax-l2-13b.Q2_K",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q2_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q3_K_L.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q3_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q3_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q4_0",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_K_M.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_K_S.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q5_0",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q5_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q6_K",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q6_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q8_0",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q8_0.gguf",
        "file_size": "12.9 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/README.md",
    "description": "The MythoMax L2 13B model by Gryphe is a GGUF format variant of the Llama 2 model, offering various quantization options for efficient inference on CPU and GPU, with a custom prompt template for roleplay and storywriting tasks."
  },
  {
    "model_name": "MythoMax-L2-13B-GGUF",
    "developer": "TheBloke",
    "downloads": 85649,
    "createdAt": "2023-09-05T03:10:48.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "mythomax-l2-13b.Q2_K",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q2_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q3_K_L.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q3_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q3_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q4_0",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_K_M.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_K_S.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q5_0",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q5_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q6_K",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q6_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "mythomax-l2-13b.Q8_0",
        "path": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q8_0.gguf",
        "file_size": "12.9 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/README.md",
    "description": "The MythoMax L2 13B model by Gryphe is a GGUF format variant of the Llama 2 model, offering various quantization options for efficient inference on CPU and GPU, with a custom prompt template for roleplay and storywriting tasks."
  },
  {
    "model_name": "Nanonets-OCR-s-GGUF",
    "developer": "unsloth",
    "downloads": 26006,
    "createdAt": "2025-06-16T10:35:11.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/README.md",
    "description": "Nanonets-OCR-s is a state-of-the-art image-to-markdown OCR model that extracts text, equations, tables, and other structured elements from documents, converting them into semantic markdown for efficient processing by LLMs."
  },
  {
    "model_name": "Nanonets-OCR-s-GGUF",
    "developer": "unsloth",
    "downloads": 26006,
    "createdAt": "2025-06-16T10:35:11.000Z",
    "num_quants": 3,
    "quants": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Nanonets-OCR-s-GGUF/resolve/main/README.md",
    "description": "Nanonets-OCR-s is a state-of-the-art image-to-markdown OCR model that extracts text, equations, tables, and other structured elements from documents, converting them into semantic markdown for efficient processing by LLMs."
  },
  {
    "model_name": "Nous-Hermes-2-Mistral-7B-DPO-GGUF",
    "developer": "NousResearch",
    "downloads": 21956,
    "createdAt": "2024-02-20T06:25:05.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q2_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q6_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q8_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/README.md",
    "description": "Nous-Hermes 2 Mistral 7B DPO is a 7B parameter model fine-tuned with DPO from Teknium/OpenHermes-2.5-Mistral-7B, showing improved performance on AGIEval, BigBench, GPT4"
  },
  {
    "model_name": "Nous-Hermes-2-Mistral-7B-DPO-GGUF",
    "developer": "NousResearch",
    "downloads": 21956,
    "createdAt": "2024-02-20T06:25:05.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q2_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q2_K.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_L.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_M.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_0.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q6_K",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q6_K.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Nous-Hermes-2-Mistral-7B-DPO.Q8_0",
        "path": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q8_0.gguf",
        "file_size": "7.2 GB"
      }
    ],
    "readme": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/README.md",
    "description": "Nous-Hermes 2 Mistral 7B DPO is a 7B parameter model fine-tuned with DPO from Teknium/OpenHermes-2.5-Mistral-7B, showing improved performance on AGIEval, BigBench, GPT4"
  },
  {
    "model_name": "osmosis-mcp-4b",
    "developer": "osmosis-ai",
    "downloads": 14043,
    "createdAt": "2025-05-08T18:46:48.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "osmosis-mcp-4B-BF16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-BF16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-F16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/README.md",
    "description": "Osmosis-MCP-4B is a Qwen3-4B model fine-tuned with reinforcement learning to excel at multi-step tool usage in a curriculum-based training approach, offering a practical, open-source solution for MCP-style agents."
  },
  {
    "model_name": "osmosis-mcp-4b",
    "developer": "osmosis-ai",
    "downloads": 14043,
    "createdAt": "2025-05-08T18:46:48.000Z",
    "num_quants": 14,
    "quants": [
      {
        "model_id": "osmosis-mcp-4B-BF16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-BF16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-F16",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-F16.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4B-Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4B-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.IQ4_XS",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q2_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_L",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q3_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q4_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_M",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q5_K_S",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q6_K",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "osmosis-mcp-4b.Q8_0",
        "path": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/osmosis-mcp-4b.Q8_0.gguf",
        "file_size": "4.4 GB"
      }
    ],
    "readme": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b/resolve/main/README.md",
    "description": "Osmosis-MCP-4B is a Qwen3-4B model fine-tuned with reinforcement learning to excel at multi-step tool usage in a curriculum-based training approach, offering a practical, open-source solution for MCP-style agents."
  },
  {
    "model_name": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF",
    "developer": "bartowski",
    "downloads": 11925,
    "createdAt": "2025-05-23T18:09:44.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the PocketDoc/Dans-PersonalityEngine-V1.3.0-24b model for efficient text generation on various hardware, including CPUs and GPUs, with options for different quantization levels and formats like Q6_K_L, Q4_K_M,"
  },
  {
    "model_name": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF",
    "developer": "bartowski",
    "downloads": 11925,
    "createdAt": "2025-05-23T18:09:44.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16",
        "path": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-bf16.gguf",
        "file_size": "43.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the PocketDoc/Dans-PersonalityEngine-V1.3.0-24b model for efficient text generation on various hardware, including CPUs and GPUs, with options for different quantization levels and formats like Q6_K_L, Q4_K_M,"
  },
  {
    "model_name": "POLARIS-Project_Polaris-4B-Preview-GGUF",
    "developer": "bartowski",
    "downloads": 3639,
    "createdAt": "2025-06-24T04:38:50.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ2_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_NL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_1",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q8_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-bf16",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-bf16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Polaris-4B-Preview model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "POLARIS-Project_Polaris-4B-Preview-GGUF",
    "developer": "bartowski",
    "downloads": 3639,
    "createdAt": "2025-06-24T04:38:50.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ2_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_NL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-IQ4_XS",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q2_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q3_K_XL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_1",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_L.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q4_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_M",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q5_K_S",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q6_K_L",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q6_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-Q8_0",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "POLARIS-Project_Polaris-4B-Preview-bf16",
        "path": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/POLARIS-Project_Polaris-4B-Preview-bf16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/POLARIS-Project_Polaris-4B-Preview-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Polaris-4B-Preview model using llama.cpp, offering various quantization types for different performance and quality trade-offs."
  },
  {
    "model_name": "QVikhr-3-4B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 2072,
    "createdAt": "2025-06-28T21:03:17.000Z",
    "num_quants": 31,
    "quants": [
      {
        "model_id": "QVikhr-3-4B-Instruction-F16",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-4B-Instruction is an instructive model based on Qwen/Qwen3-4B, trained on the Russian-language dataset GrandMaster2 for high-efficiency text processing in Russian and English."
  },
  {
    "model_name": "QVikhr-3-4B-Instruction-GGUF",
    "developer": "Vikhrmodels",
    "downloads": 2072,
    "createdAt": "2025-06-28T21:03:17.000Z",
    "num_quants": 31,
    "quants": [
      {
        "model_id": "QVikhr-3-4B-Instruction-F16",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ1_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ1_S.gguf",
        "file_size": "1006.4 MB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_S.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ2_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ3_XXS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_NL",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-IQ4_XS",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q2_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q2_K_S.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_L",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q3_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q4_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_1",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_M",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q5_K_S",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q6_K",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "QVikhr-3-4B-Instruction-Q8_0",
        "path": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/QVikhr-3-4B-Instruction-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF/resolve/main/README.md",
    "description": "QVikhr-3-4B-Instruction is an instructive model based on Qwen/Qwen3-4B, trained on the Russian-language dataset GrandMaster2 for high-efficiency text processing in Russian and English."
  },
  {
    "model_name": "Qwen2.5-14B_Uncensored_Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 5150,
    "createdAt": "2024-09-22T22:26:11.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen2.5-14B_Uncensored_Instruct model using llama.cpp, with various quantization types and optimizations for different hardware, including ARM and Apple Metal, suitable for different performance and quality trade-offs."
  },
  {
    "model_name": "Qwen2.5-14B_Uncensored_Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 5150,
    "createdAt": "2024-09-22T22:26:11.000Z",
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-14B_Uncensored_Instruct-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen2.5-14B_Uncensored_Instruct model using llama.cpp, with various quantization types and optimizations for different hardware, including ARM and Apple Metal, suitable for different performance and quality trade-offs."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 6716,
    "createdAt": "2025-05-11T13:03:32.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Qwen2.5-VL-7B-Instruct model is a versatile vision-language model that supports image-text understanding, video analysis, structured output generation, and agent-like reasoning, with enhanced performance through dynamic resolution and frame rate training, efficient vision encoders, and compatibility with various visual input"
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 6716,
    "createdAt": "2025-05-11T13:03:32.000Z",
    "num_quants": 29,
    "quants": [
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-BF16.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_NL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_0.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_1.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q5_K_S.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-Q8_0.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/Qwen2.5-VL-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "The Qwen2.5-VL-7B-Instruct model is a versatile vision-language model that supports image-text understanding, video analysis, structured output generation, and agent-like reasoning, with enhanced performance through dynamic resolution and frame rate training, efficient vision encoders, and compatibility with various visual input"
  },
  {
    "model_name": "Qwen3-1.7B-GGUF",
    "developer": "unsloth",
    "downloads": 14725,
    "createdAt": "2025-04-28T12:22:37.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-1.7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K_L.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_M.gguf",
        "file_size": "535.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_S.gguf",
        "file_size": "512.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_M.gguf",
        "file_size": "675.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_XXS.gguf",
        "file_size": "577.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ3_XXS.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q2_K_XL.gguf",
        "file_size": "760.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q3_K_XL.gguf",
        "file_size": "924.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q5_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q6_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q8_K_XL.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/README.md",
    "description": "Qwen3-1.7B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes for optimal performance."
  },
  {
    "model_name": "Qwen3-1.7B-GGUF",
    "developer": "unsloth",
    "downloads": 14725,
    "createdAt": "2025-04-28T12:22:37.000Z",
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-1.7B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q2_K_L.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen3-1.7B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_M.gguf",
        "file_size": "535.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ1_S.gguf",
        "file_size": "512.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_M.gguf",
        "file_size": "675.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ2_XXS.gguf",
        "file_size": "577.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-IQ3_XXS.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q2_K_XL.gguf",
        "file_size": "760.9 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q3_K_XL.gguf",
        "file_size": "924.0 MB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q5_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q6_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-1.7B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-UD-Q8_K_XL.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/README.md",
    "description": "Qwen3-1.7B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes for optimal performance."
  },
  {
    "model_name": "Qwen3-235B-A22B-GGUF",
    "developer": "unsloth",
    "downloads": 41627,
    "createdAt": "2025-04-28T15:18:03.000Z",
    "num_quants": 72,
    "quants": [
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00001-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00001-of-00010.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00002-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00002-of-00010.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00003-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00003-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00004-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00004-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00005-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00005-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00006-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00006-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00007-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00007-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00008-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00008-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00009-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00009-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00010-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00010-of-00010.gguf",
        "file_size": "21.2 GB"
      },
      {
        "model_id": "IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00001-of-00003.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00003-of-00003.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Q2_K/Qwen3-235B-A22B-Q2_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Q2_K-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q2_K/Qwen3-235B-A22B-Q2_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Q2_K-00002-of-00002.gguf",
        "file_size": "33.3 GB"
      },
      {
        "model_id": "Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00002-of-00002.gguf",
        "file_size": "33.6 GB"
      },
      {
        "model_id": "Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00001-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00002-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00003-of-00003.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00003-of-00003.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Q4_1/Qwen3-235B-A22B-Q4_1-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_1/Qwen3-235B-A22B-Q4_1-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_1/Qwen3-235B-A22B-Q4_1-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_1/Qwen3-235B-A22B-Q4_1-00002-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_1/Qwen3-235B-A22B-Q4_1-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_1/Qwen3-235B-A22B-Q4_1-00003-of-00003.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00003-of-00003.gguf",
        "file_size": "39.4 GB"
      },
      {
        "model_id": "Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00001-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00003-of-00003.gguf",
        "file_size": "31.7 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00001-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00002-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00004-of-00004.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00001-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00002-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00004-of-00004.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00001-of-00004.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00002-of-00004.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00003-of-00004.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00004-of-00004.gguf",
        "file_size": "41.2 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00001-of-00006.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00002-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00003-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00005-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00006-of-00006.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00002-of-00002.gguf",
        "file_size": "35.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00003-of-00003.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00003-of-00003.gguf",
        "file_size": "32.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00001-of-00004.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00002-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00003-of-00004.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00004-of-00004.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00001-of-00004.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00002-of-00004.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00003-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00004-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00001-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00002-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00003-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00005-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00006-of-00006.gguf",
        "file_size": "16.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/README.md",
    "description": "The Qwen3-235B-A22B model is a large language model with 235B parameters, offering seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, and support for 100+ languages, with the ability to handle long texts via Ya"
  },
  {
    "model_name": "Qwen3-235B-A22B-GGUF",
    "developer": "unsloth",
    "downloads": 41627,
    "createdAt": "2025-04-28T15:18:03.000Z",
    "num_quants": 72,
    "quants": [
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00001-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00001-of-00010.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00002-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00002-of-00010.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00003-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00003-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00004-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00004-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00005-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00005-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00006-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00006-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00007-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00007-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00008-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00008-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00009-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00009-of-00010.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-235B-A22B-BF16-00010-of-00010",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/BF16/Qwen3-235B-A22B-BF16-00010-of-00010.gguf",
        "file_size": "21.2 GB"
      },
      {
        "model_id": "IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00001-of-00003.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/IQ4_XS/Qwen3-235B-A22B-IQ4_XS-00003-of-00003.gguf",
        "file_size": "23.9 GB"
      },
      {
        "model_id": "Q2_K/Qwen3-235B-A22B-Q2_K-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Q2_K-00001-of-00002.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q2_K/Qwen3-235B-A22B-Q2_K-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K/Qwen3-235B-A22B-Q2_K-00002-of-00002.gguf",
        "file_size": "33.3 GB"
      },
      {
        "model_id": "Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K_L/Qwen3-235B-A22B-Q2_K_L-00002-of-00002.gguf",
        "file_size": "33.6 GB"
      },
      {
        "model_id": "Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00001-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00002-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_M/Qwen3-235B-A22B-Q3_K_M-00003-of-00003.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00003-of-00003.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Q4_1/Qwen3-235B-A22B-Q4_1-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_1/Qwen3-235B-A22B-Q4_1-00001-of-00003.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q4_1/Qwen3-235B-A22B-Q4_1-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_1/Qwen3-235B-A22B-Q4_1-00002-of-00003.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q4_1/Qwen3-235B-A22B-Q4_1-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_1/Qwen3-235B-A22B-Q4_1-00003-of-00003.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00003-of-00003.gguf",
        "file_size": "39.4 GB"
      },
      {
        "model_id": "Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00001-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q4_K_S/Qwen3-235B-A22B-Q4_K_S-00003-of-00003.gguf",
        "file_size": "31.7 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00001-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00002-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_M/Qwen3-235B-A22B-Q5_K_M-00004-of-00004.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00001-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00002-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00003-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q5_K_S/Qwen3-235B-A22B-Q5_K_S-00004-of-00004.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00001-of-00004.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00002-of-00004.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00003-of-00004.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "Q6_K/Qwen3-235B-A22B-Q6_K-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q6_K/Qwen3-235B-A22B-Q6_K-00004-of-00004.gguf",
        "file_size": "41.2 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00001-of-00006.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00002-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00003-of-00006.gguf",
        "file_size": "45.9 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00005-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "Q8_0/Qwen3-235B-A22B-Q8_0-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q8_0/Qwen3-235B-A22B-Q8_0-00006-of-00006.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00002-of-00002.gguf",
        "file_size": "35.6 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00002-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-00003-of-00003.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00002-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00002-of-00003.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00003-of-00003",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00003-of-00003.gguf",
        "file_size": "32.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00001-of-00004.gguf",
        "file_size": "46.4 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00002-of-00004.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00003-of-00004.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q5_K_XL/Qwen3-235B-A22B-UD-Q5_K_XL-00004-of-00004.gguf",
        "file_size": "16.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00001-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00001-of-00004.gguf",
        "file_size": "46.6 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00002-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00002-of-00004.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00003-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00003-of-00004.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00004-of-00004",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q6_K_XL/Qwen3-235B-A22B-UD-Q6_K_XL-00004-of-00004.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00001-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00001-of-00006.gguf",
        "file_size": "46.1 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00002-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00002-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00003-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00003-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00004-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00004-of-00006.gguf",
        "file_size": "46.0 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00005-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00005-of-00006.gguf",
        "file_size": "46.5 GB"
      },
      {
        "model_id": "UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00006-of-00006",
        "path": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00006-of-00006.gguf",
        "file_size": "16.2 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/README.md",
    "description": "The Qwen3-235B-A22B model is a large language model with 235B parameters, offering seamless switching between thinking and non-thinking modes, enhanced reasoning capabilities, and support for 100+ languages, with the ability to handle long texts via Ya"
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "unsloth",
    "downloads": 95401,
    "createdAt": "2025-04-28T13:48:41.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "BF16/Qwen3-30B-A3B-BF16-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/BF16/Qwen3-30B-A3B-BF16-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-30B-A3B-BF16-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/BF16/Qwen3-30B-A3B-BF16-00002-of-00002.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-IQ4_NL.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q2_K_L.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ1_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ1_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ2_M.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ2_XXS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q2_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q3_K_XL.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q4_K_XL.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q5_K_XL.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q6_K_XL.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q8_K_XL.gguf",
        "file_size": "33.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "Qwen3-30B-A3B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes, and is available for fine-tuning and deployment via Hugging Face and other frameworks."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "ubergarm",
    "downloads": 184,
    "createdAt": "2025-05-02T00:10:00.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-mix-IQ4_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-mix-IQ4_K.gguf",
        "file_size": "17.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-30B-A3B model using ik_llama.cpp, optimized for high-quality generation with reduced memory footprint, requiring specific hardware and software support for proper execution."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "unsloth",
    "downloads": 95401,
    "createdAt": "2025-04-28T13:48:41.000Z",
    "num_quants": 27,
    "quants": [
      {
        "model_id": "BF16/Qwen3-30B-A3B-BF16-00001-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/BF16/Qwen3-30B-A3B-BF16-00001-of-00002.gguf",
        "file_size": "46.3 GB"
      },
      {
        "model_id": "BF16/Qwen3-30B-A3B-BF16-00002-of-00002",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/BF16/Qwen3-30B-A3B-BF16-00002-of-00002.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-IQ4_NL.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q2_K_L.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ1_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ1_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ2_M.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ2_XXS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q2_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q3_K_XL.gguf",
        "file_size": "12.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q4_K_XL.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q5_K_XL.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q6_K_XL.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q8_K_XL.gguf",
        "file_size": "33.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "Qwen3-30B-A3B is a large language model developed by Alibaba Cloud, offering advanced reasoning, instruction-following, and multilingual capabilities with seamless switching between thinking and non-thinking modes, and is available for fine-tuning and deployment via Hugging Face and other frameworks."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "ubergarm",
    "downloads": 184,
    "createdAt": "2025-05-02T00:10:00.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-mix-IQ4_K",
        "path": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-mix-IQ4_K.gguf",
        "file_size": "17.7 GB"
      }
    ],
    "readme": "https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "This repository provides quantized versions of the Qwen3-30B-A3B model using ik_llama.cpp, optimized for high-quality generation with reduced memory footprint, requiring specific hardware and software support for proper execution."
  },
  {
    "model_name": "qwen3-4B-rpg-roleplay",
    "developer": "Chun121",
    "downloads": 1655,
    "createdAt": "2025-04-30T23:55:22.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gguf_f16/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_f16/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.Q4_K_M",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gguf_q8_0/unsloth.Q8_0",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q8_0/unsloth.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/README.md",
    "description": "This is a LoRA fine-tuned version of Qwen3-4B, optimized for character-based conversations and roleplay scenarios using the Gryphe-Aesir-RPG-Charcards-Opus-Mixed-split dataset."
  },
  {
    "model_name": "qwen3-4B-rpg-roleplay",
    "developer": "Chun121",
    "downloads": 1655,
    "createdAt": "2025-04-30T23:55:22.000Z",
    "num_quants": 4,
    "quants": [
      {
        "model_id": "gguf_f16/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_f16/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.F16",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "gguf_q4_k_m/unsloth.Q4_K_M",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q4_k_m/unsloth.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gguf_q8_0/unsloth.Q8_0",
        "path": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/gguf_q8_0/unsloth.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/Chun121/qwen3-4B-rpg-roleplay/resolve/main/README.md",
    "description": "This is a LoRA fine-tuned version of Qwen3-4B, optimized for character-based conversations and roleplay scenarios using the Gryphe-Aesir-RPG-Charcards-Opus-Mixed-split dataset."
  },
  {
    "model_name": "Qwen_Qwen3-30B-A3B-GGUF",
    "developer": "bartowski",
    "downloads": 5132,
    "createdAt": "2025-04-28T12:18:44.000Z",
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ3_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ3_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ3_XXS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ4_NL.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q2_K.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q2_K_L.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_S.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_XL.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_0.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_K_L.gguf",
        "file_size": "17.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q5_K_L.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q5_K_S.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q6_K_L.gguf",
        "file_size": "23.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00001-of-00002",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00001-of-00002.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00002-of-00002",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00002-of-00002.gguf",
        "file_size": "19.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-30B-A3B model using llama.cpp, offering various quantization types for different performance and quality trade-offs on different hardware platforms."
  },
  {
    "model_name": "Qwen_Qwen3-30B-A3B-GGUF",
    "developer": "bartowski",
    "downloads": 5132,
    "createdAt": "2025-04-28T12:18:44.000Z",
    "num_quants": 28,
    "quants": [
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ3_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ3_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ3_XXS.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ4_NL.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q2_K.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q2_K_L.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_L.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_S.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q3_K_XL.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_0.gguf",
        "file_size": "16.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_1",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_K_L.gguf",
        "file_size": "17.6 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q4_K_S.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q5_K_L.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q5_K_S.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q6_K_L.gguf",
        "file_size": "23.5 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00001-of-00002",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00001-of-00002.gguf",
        "file_size": "37.0 GB"
      },
      {
        "model_id": "Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00002-of-00002",
        "path": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/Qwen_Qwen3-30B-A3B-bf16/Qwen_Qwen3-30B-A3B-bf16-00002-of-00002.gguf",
        "file_size": "19.9 GB"
      }
    ],
    "readme": "https://huggingface.co/bartowski/Qwen_Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "This is a quantized version of the Qwen3-30B-A3B model using llama.cpp, offering various quantization types for different performance and quality trade-offs on different hardware platforms."
  },
  {
    "model_name": "RuadaptQwen3-4B-Instruct-GGUF",
    "developer": "RefalMachine",
    "downloads": 2167,
    "createdAt": "2025-06-30T06:13:47.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "BF16",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "IQ3_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "IQ3_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "IQ4_NL",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "IQ4_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Q2_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q3_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Q3_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Q4_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q4_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Q4_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q5_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q5_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Q5_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q6_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Q8_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "RuadaptQwen3-4B-Instruct is a Russian-adapted version of Qwen/Qwen3-4B, featuring a new tokenizer, continued pre-training on Russian data, and LEP (Learned Embedding Propagation) applied to enhance Russian text generation speed by up to"
  },
  {
    "model_name": "RuadaptQwen3-4B-Instruct-GGUF",
    "developer": "RefalMachine",
    "downloads": 2167,
    "createdAt": "2025-06-30T06:13:47.000Z",
    "num_quants": 16,
    "quants": [
      {
        "model_id": "BF16",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "IQ3_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "IQ3_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "IQ4_NL",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "IQ4_XS",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Q2_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Q3_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Q3_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q3_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Q4_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q4_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Q4_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Q5_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q5_K_M",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Q5_K_S",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Q6_K",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Q8_0",
        "path": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "readme": "https://huggingface.co/RefalMachine/RuadaptQwen3-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "RuadaptQwen3-4B-Instruct is a Russian-adapted version of Qwen/Qwen3-4B, featuring a new tokenizer, continued pre-training on Russian data, and LEP (Learned Embedding Propagation) applied to enhance Russian text generation speed by up to"
  },
  {
    "model_name": "Seed-Coder-8B-Reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 3294,
    "createdAt": "2025-05-18T12:29:03.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Seed-Coder-8B-Reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-BF16.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q8_0.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/README.md",
    "description": "The Seed-Coder-8B-Reasoning model is a high-performance, parameter-efficient, and transparent open-source code model trained for enhanced reasoning capabilities using RL."
  },
  {
    "model_name": "Seed-Coder-8B-Reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 3294,
    "createdAt": "2025-05-18T12:29:03.000Z",
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Seed-Coder-8B-Reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-BF16.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q8_0.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/README.md",
    "description": "The Seed-Coder-8B-Reasoning model is a high-performance, parameter-efficient, and transparent open-source code model trained for enhanced reasoning capabilities using RL."
  },
  {
    "model_name": "SpaceThinker-Qwen2.5VL-3B",
    "developer": "remyxai",
    "downloads": 4008,
    "createdAt": "2025-04-17T17:34:23.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gguf/spacethinker-qwen2.5VL-3B-F16",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5VL-3B-F16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gguf/spacethinker-qwen2.5vl-3b-vision",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5vl-3b-vision.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/README.md",
    "description": "SpaceThinker-Qwen2.5VL-3B is a multimodal vision-language model trained to enhance spatial reasoning through test-time compute, achieving strong performance on various spatial reasoning benchmarks like SpatialScore and QSpatial-Bench with a focus on quantitative spatial tasks such as distance estimation and object relations."
  },
  {
    "model_name": "SpaceThinker-Qwen2.5VL-3B",
    "developer": "remyxai",
    "downloads": 4008,
    "createdAt": "2025-04-17T17:34:23.000Z",
    "num_quants": 2,
    "quants": [
      {
        "model_id": "gguf/spacethinker-qwen2.5VL-3B-F16",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5VL-3B-F16.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "gguf/spacethinker-qwen2.5vl-3b-vision",
        "path": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/gguf/spacethinker-qwen2.5vl-3b-vision.gguf",
        "file_size": "2.5 GB"
      }
    ],
    "readme": "https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B/resolve/main/README.md",
    "description": "SpaceThinker-Qwen2.5VL-3B is a multimodal vision-language model trained to enhance spatial reasoning through test-time compute, achieving strong performance on various spatial reasoning benchmarks like SpatialScore and QSpatial-Bench with a focus on quantitative spatial tasks such as distance estimation and object relations."
  },
  {
    "model_name": "Tifa-DeepsexV3-14b-GGUF-Q6",
    "developer": "ValueFX9507",
    "downloads": 1112,
    "createdAt": "2025-06-26T10:15:21.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/README.md",
    "description": "Tifa-DeepSexV3-14b 是基于 Qwen14b 的深度优化模型，支持长文生成、超长关联、控制器调节输出风格和字数，并能避免负面词汇，适用于角色扮演和多种文本生成任务。"
  },
  {
    "model_name": "Tifa-DeepsexV3-14b-GGUF-Q6",
    "developer": "ValueFX9507",
    "downloads": 1112,
    "createdAt": "2025-06-26T10:15:21.000Z",
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6",
        "path": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/Tifa-DeepsexV3-14b-Chat-NoCot-0626-Q6.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "readme": "https://huggingface.co/ValueFX9507/Tifa-DeepsexV3-14b-GGUF-Q6/resolve/main/README.md",
    "description": "Tifa-DeepSexV3-14b 是基于 Qwen14b 的深度优化模型，支持长文生成、超长关联、控制器调节输出风格和字数，并能避免负面词汇，适用于角色扮演和多种文本生成任务。"
  },
  {
    "model_name": "TinyLlama-1.1B-Chat-v1.0-GGUF",
    "developer": "TheBloke",
    "downloads": 58797,
    "createdAt": "2023-12-31T20:53:43.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q2_K",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
        "file_size": "460.7 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_L.gguf",
        "file_size": "565.1 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_M.gguf",
        "file_size": "525.3 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_S.gguf",
        "file_size": "477.1 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf",
        "file_size": "608.2 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "file_size": "637.8 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf",
        "file_size": "613.9 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_0.gguf",
        "file_size": "731.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf",
        "file_size": "746.7 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_K_S.gguf",
        "file_size": "731.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q6_K",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q6_K.gguf",
        "file_size": "862.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q8_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF format quantized versions of the TinyLlama 1.1B Chat v1.0 model for efficient GPU inference, with various quantization options and compatibility with multiple frameworks like llama.cpp, text-generation-webui, and llama-cpp-python."
  },
  {
    "model_name": "TinyLlama-1.1B-Chat-v1.0-GGUF",
    "developer": "TheBloke",
    "downloads": 58797,
    "createdAt": "2023-12-31T20:53:43.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q2_K",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
        "file_size": "460.7 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_L.gguf",
        "file_size": "565.1 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_M.gguf",
        "file_size": "525.3 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q3_K_S.gguf",
        "file_size": "477.1 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf",
        "file_size": "608.2 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "file_size": "637.8 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf",
        "file_size": "613.9 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_0.gguf",
        "file_size": "731.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf",
        "file_size": "746.7 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q5_K_S.gguf",
        "file_size": "731.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q6_K",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q6_K.gguf",
        "file_size": "862.5 MB"
      },
      {
        "model_id": "tinyllama-1.1b-chat-v1.0.Q8_0",
        "path": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF format quantized versions of the TinyLlama 1.1B Chat v1.0 model for efficient GPU inference, with various quantization options and compatibility with multiple frameworks like llama.cpp, text-generation-webui, and llama-cpp-python."
  },
  {
    "model_name": "Violet_Twilight-v0.2-GGUF",
    "developer": "Epiculous",
    "downloads": 74343,
    "createdAt": "2024-09-13T13:19:21.000Z",
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Violet_Twilight-v0.2.IQ1_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_NL",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_L",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_1",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q6_K",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q8_0",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.bf16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.bf16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f32",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f32.gguf",
        "file_size": "45.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/README.md",
    "description": "The Violet_Twilight-v0.2 model is a SLERP merge of Azure_Dusk-v0.2 and Crimson_Dawn-v0.2, trained on ChatML with support for gguf quantization."
  },
  {
    "model_name": "Violet_Twilight-v0.2-GGUF",
    "developer": "Epiculous",
    "downloads": 74343,
    "createdAt": "2024-09-13T13:19:21.000Z",
    "num_quants": 20,
    "quants": [
      {
        "model_id": "Violet_Twilight-v0.2.IQ1_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ2_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ3_XXS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ3_XXS.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_NL",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_NL.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.IQ4_XS",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_L",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q3_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q4_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_1",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_1.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_M",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q5_K_S",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q6_K",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.Q8_0",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.bf16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.bf16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f16",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f16.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "Violet_Twilight-v0.2.f32",
        "path": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.f32.gguf",
        "file_size": "45.6 GB"
      }
    ],
    "readme": "https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/README.md",
    "description": "The Violet_Twilight-v0.2 model is a SLERP merge of Azure_Dusk-v0.2 and Crimson_Dawn-v0.2, trained on ChatML with support for gguf quantization."
  },
  {
    "model_name": "Xwin-LM-13B-v0.2-GGUF",
    "developer": "TheBloke",
    "downloads": 1750,
    "createdAt": "2023-10-15T00:54:23.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "xwin-lm-13b-v0.2.Q2_K",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q2_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q3_K_L.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q3_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q3_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q4_0",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_K_M.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_K_S.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q5_0",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q5_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q6_K",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q6_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q8_0",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q8_0.gguf",
        "file_size": "12.9 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF format models for Xwin-LM's Xwin LM 13B v0.2, offering various quantization options for efficient inference on CPU and GPU with support from llama.cpp, text-generation-webui, and other frameworks."
  },
  {
    "model_name": "Xwin-LM-13B-v0.2-GGUF",
    "developer": "TheBloke",
    "downloads": 1750,
    "createdAt": "2023-10-15T00:54:23.000Z",
    "num_quants": 12,
    "quants": [
      {
        "model_id": "xwin-lm-13b-v0.2.Q2_K",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q2_K.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q3_K_L",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q3_K_L.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q3_K_M",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q3_K_M.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q3_K_S",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q3_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q4_0",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q4_K_M",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_K_M.gguf",
        "file_size": "7.3 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q4_K_S",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_K_S.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q5_0",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q5_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q5_K_M",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q5_K_M.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q5_K_S",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q5_K_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q6_K",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q6_K.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "xwin-lm-13b-v0.2.Q8_0",
        "path": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q8_0.gguf",
        "file_size": "12.9 GB"
      }
    ],
    "readme": "https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF format models for Xwin-LM's Xwin LM 13B v0.2, offering various quantization options for efficient inference on CPU and GPU with support from llama.cpp, text-generation-webui, and other frameworks."
  }
]