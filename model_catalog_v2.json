[
  {
    "model_name": "Jan-v3-4B-base-instruct-gguf",
    "developer": "janhq",
    "downloads": 257633,
    "createdAt": "2026-01-20T06:49:01.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Jan-v3-4b-base-instruct-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_1.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_K_XL",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_1.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q8_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct.gguf",
        "file_size": "8.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/README.md",
    "description": "A 4B-parameter causal language model optimized as a fine-tuning base with 262K context length, designed for instruction following and code tasks."
  },
  {
    "model_name": "Jan-v2-VL-high-gguf",
    "developer": "janhq",
    "downloads": 97616,
    "createdAt": "2025-11-06T11:17:54.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Jan-v2-VL-high-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-high",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high.gguf",
        "file_size": "15.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Jan-v2-VL-high",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/mmproj-Jan-v2-VL-high.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/README.md",
    "description": "An 8B vision-language model optimized for long-horizon, multi-step agentic tasks like browser and desktop automation."
  },
  {
    "model_name": "Jan-v2-VL-med-gguf",
    "developer": "janhq",
    "downloads": 60492,
    "createdAt": "2025-11-06T11:17:04.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Jan-v2-VL-med-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-med",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med.gguf",
        "file_size": "15.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Jan-v2-VL-med",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/mmproj-Jan-v2-VL-med.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/README.md",
    "description": "**Jan-v2-VL** is an 8B vision-language model optimized for long-horizon, multi-step agentic tasks in software environments like browsers and desktop apps."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-GGUF",
    "developer": "janhq",
    "downloads": 10404,
    "createdAt": "2026-02-25T07:21:54.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B-Q2_K",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q2_K.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q2_K_L",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q2_K_L.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q3_K_M",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q3_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q3_K_S",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q3_K_S.gguf",
        "file_size": "14.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q4_0",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q4_0.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q4_1",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q4_1.gguf",
        "file_size": "20.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q4_K_S",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q4_K_S.gguf",
        "file_size": "18.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q5_K_M",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q5_K_M.gguf",
        "file_size": "23.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q5_K_S",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q5_K_S.gguf",
        "file_size": "22.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q6_K",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q6_K.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-Q8_0",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q8_0.gguf",
        "file_size": "34.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-UD-IQ2_M",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-UD-IQ2_M.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-UD-IQ2_XXS",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-UD-IQ2_XXS.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "861.0 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "857.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/janhq/Qwen3.5-35B-A3B-GGUF/resolve/main/README.md",
    "description": "An open-source multimodal MoE language model with vision capabilities and 262K context length."
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 9276,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact non-thinking language model with native 128k context window designed for research applications."
  },
  {
    "model_name": "Jan-v3-4B-base-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 506,
    "createdAt": "2026-01-27T07:35:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Jan-v3-4B-base-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Jan-v3-4B-base-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Jan-v3-4B-base-instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Jan-v3-4B-base-instruct converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "Jan-v2-VL-high-4bit-mlx",
    "developer": "janhq",
    "downloads": 105,
    "createdAt": "2026-02-12T11:22:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-4bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-4bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "387.9 MB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-high-4bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-high-4bit-mlx/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of an 8B vision-language model optimized for agentic automation, UI control, and multi-step task execution."
  },
  {
    "model_name": "Jan-v2-VL-high-8bit-mlx",
    "developer": "janhq",
    "downloads": 76,
    "createdAt": "2026-02-12T10:58:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-8bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-8bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-high-8bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-high-8bit-mlx/resolve/main/README.md",
    "description": "8-bit MLX-quantized 8B vision-language model for agentic UI automation and multi-step task execution."
  },
  {
    "model_name": "Jan-v2-VL-high-bf16-mlx",
    "developer": "janhq",
    "downloads": 65,
    "createdAt": "2026-02-12T11:10:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-bf16-mlx/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-bf16-mlx/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-bf16-mlx/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-bf16-mlx/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-high-bf16-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-high-bf16-mlx/resolve/main/README.md",
    "description": "**Summary:** This is an 8B-parameter bfloat16 MLX vision-language model for agentic automation and UI control tasks, converted from Janhq's Jan-v2-VL-high."
  },
  {
    "model_name": "Jan-v2-VL-low-4bit-mlx",
    "developer": "janhq",
    "downloads": 39,
    "createdAt": "2026-02-12T11:18:07.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-4bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-4bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "387.9 MB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-low-4bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-low-4bit-mlx/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of an 8B vision-language model optimized for UI automation and agentic tasks."
  },
  {
    "model_name": "Jan-v2-VL-med-4bit-mlx",
    "developer": "janhq",
    "downloads": 36,
    "createdAt": "2026-02-12T11:06:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-4bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-4bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "387.9 MB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-med-4bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-med-4bit-mlx/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of an 8B vision-language model designed for agentic automation and UI control tasks."
  },
  {
    "model_name": "Jan-v2-VL-low-8bit-mlx",
    "developer": "janhq",
    "downloads": 28,
    "createdAt": "2026-02-12T11:16:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-8bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-8bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-low-8bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-low-8bit-mlx/resolve/main/README.md",
    "description": "An 8-bit quantized MLX conversion of Jan-v2-VL-low, an 8B vision-language model optimized for agentic automation and UI control tasks."
  },
  {
    "model_name": "Jan-v2-VL-med-8bit-mlx",
    "developer": "janhq",
    "downloads": 27,
    "createdAt": "2026-02-12T11:12:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-8bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-8bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-med-8bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-med-8bit-mlx/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized 8B vision-language model for agentic automation and UI control tasks."
  },
  {
    "model_name": "Jan-v2-VL-med-bf16-mlx",
    "developer": "janhq",
    "downloads": 27,
    "createdAt": "2026-02-12T11:11:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-bf16-mlx/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-bf16-mlx/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-bf16-mlx/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-bf16-mlx/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-med-bf16-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-med-bf16-mlx/resolve/main/README.md",
    "description": "MLX conversion of Jan-v2-VL-med, an 8B vision-language model for agentic automation and UI control tasks."
  },
  {
    "model_name": "Jan-v2-VL-low-bf16-mlx",
    "developer": "janhq",
    "downloads": 25,
    "createdAt": "2026-02-12T11:14:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-bf16-mlx/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-bf16-mlx/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-bf16-mlx/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-low-bf16-mlx/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/janhq/Jan-v2-VL-low-bf16-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-low-bf16-mlx/resolve/main/README.md",
    "description": "This is a bfloat16 MLX conversion of Jan-v2-VL-low, an 8B vision-language model optimized for agentic automation and UI control tasks."
  },
  {
    "model_name": "Jan-code-4b-gguf",
    "developer": "janhq",
    "downloads": 0,
    "createdAt": "2026-03-02T04:46:42.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Jan-code-4b-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-code-4b-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-code-4b-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-code-4b-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-code-4b-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q4_1.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-code-4b-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Jan-code-4b-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-code-4b-Q4_K_XL",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q4_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Jan-code-4b-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-code-4b-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q5_1.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-code-4b-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-code-4b-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-code-4b-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-code-4b-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b-Q8_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-code-4b",
        "path": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/Jan-code-4b.gguf",
        "file_size": "8.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-code-4b-gguf/resolve/main/README.md",
    "description": "A small, locally-runnable code-tuned model for coding tasks and lightweight agent workflows."
  },
  {
    "model_name": "Olmo-3-7B-Think-GGUF",
    "developer": "unsloth",
    "downloads": 8726,
    "createdAt": "2025-11-21T04:15:44.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Olmo-3-7B-Think-BF16",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-BF16.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q2_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q2_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q3_K_S.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_1",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q5_K_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q6_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q6_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q8_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ3_XXS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q2_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q3_K_XL.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q4_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q5_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q6_K_XL.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q8_K_XL.gguf",
        "file_size": "8.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/README.md",
    "description": "Olmo 3 Think is a 7B open language model by Ai2 with chain-of-thought reasoning, trained on Dolma 3 and Dolci datasets, excelling at math, coding, and reasoning tasks."
  },
  {
    "model_name": "Olmo-3-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 4437,
    "createdAt": "2025-11-21T05:20:27.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Olmo-3-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-BF16.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q5_K_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q6_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "8.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "Olmo 3 7B Instruct is an open-source 7B parameter language model by Allen Institute for AI, trained on Dolma 3 data, with strong math and reasoning capabilities, using <|im_start|> chat template and Apache 2.0 license."
  },
  {
    "model_name": "Kimi-K2.5",
    "developer": "mlx-community",
    "downloads": 4040190,
    "createdAt": "2026-01-27T18:15:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 182,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00001-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00002-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00002-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00003-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00003-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00004-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00004-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00005-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00005-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00006-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00006-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00007-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00007-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00008-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00008-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00009-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00009-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00010-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00010-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00011-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00011-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00012-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00012-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00013-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00013-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00014-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00014-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00015-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00015-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00016-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00016-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00017-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00017-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00018-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00018-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00019-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00019-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00020-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00020-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00021-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00021-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00022-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00022-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00023-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00023-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00024-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00024-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00025-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00025-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00026-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00026-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00027-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00027-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00028-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00028-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00029-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00029-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00030-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00030-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00031-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00031-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00032-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00032-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00033-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00033-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00034-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00034-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00035-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00035-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00036-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00036-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00037-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00037-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00038-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00038-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00039-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00039-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00040-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00040-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00041-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00041-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00042-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00042-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00043-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00043-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00044-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00044-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00045-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00045-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00046-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00046-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00047-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00047-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00048-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00048-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00049-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00049-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00050-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00050-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00051-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00051-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00052-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00052-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00053-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00053-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00054-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00054-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00055-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00055-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00056-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00056-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00057-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00057-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00058-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00058-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00059-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00059-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00060-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00060-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00061-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00061-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00062-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00062-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00063-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00063-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00064-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00064-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00065-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00065-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00066-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00066-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00067-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00067-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00068-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00068-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00069-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00069-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00070-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00070-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00071-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00071-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00072-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00072-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00073-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00073-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00074-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00074-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00075-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00075-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00076-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00076-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00077-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00077-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00078-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00078-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00079-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00079-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00080-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00080-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00081-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00081-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00082-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00082-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00083-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00083-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00084-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00084-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00085-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00085-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00086-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00086-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00087-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00087-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00088-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00088-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00089-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00089-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00090-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00090-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00091-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00091-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00092-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00092-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00093-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00093-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00094-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00094-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00095-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00095-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00096-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00096-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00097-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00097-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00098-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00098-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00099-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00099-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00100-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00100-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00101-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00101-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00102-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00102-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00103-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00103-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00104-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00104-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00105-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00105-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00106-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00106-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00107-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00107-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00108-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00108-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00109-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00109-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00110-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00110-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00111-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00111-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00112-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00112-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00113-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00113-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00114-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00114-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00115-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00115-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00116-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00116-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00117-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00117-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00118-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00118-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00119-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00119-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00120-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00120-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00121-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00121-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00122-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00122-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00123-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00123-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00124-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00124-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00125-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00125-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00126-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00126-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00127-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00127-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00128-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00128-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00129-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00129-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00130-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00130-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00131-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00131-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00132-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00132-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00133-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00133-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00134-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00134-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00135-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00135-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00136-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00136-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00137-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00137-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00138-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00138-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00139-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00139-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00140-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00140-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00141-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00141-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00142-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00142-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00143-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00143-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00144-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00144-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00145-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00145-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00146-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00146-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00147-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00147-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00148-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00148-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00149-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00149-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00150-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00150-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00151-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00151-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00152-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00152-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00153-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00153-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00154-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00154-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00155-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00155-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00156-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00156-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00157-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00157-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00158-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00158-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00159-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00159-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00160-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00160-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00161-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00161-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00162-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00162-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00163-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00163-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00164-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00164-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00165-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00165-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00166-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00166-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00167-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00167-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00168-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00168-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00169-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00169-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00170-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00170-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00171-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00171-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00172-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00172-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00173-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00173-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00174-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00174-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00175-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00175-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00176-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00176-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00177-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00177-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00178-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00178-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00179-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00179-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00180-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00180-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00181-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00181-of-00182.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00182-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00182-of-00182.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/README.md",
    "description": "MLX-converted text generation model from Kimi-K2.5 for Apple Silicon."
  },
  {
    "model_name": "gemma-3-4b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 660164,
    "createdAt": "2025-04-15T13:38:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit/resolve/main/model.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized Gemma 3 4B Instruct model converted to MLX format for image-text-to-text tasks on Apple Silicon."
  },
  {
    "model_name": "gpt-oss-20b-MXFP4-Q8",
    "developer": "mlx-community",
    "downloads": 549657,
    "createdAt": "2025-08-29T17:57:36.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/README.md",
    "description": "MLX-converted GPT-OSS-20B model for text generation."
  },
  {
    "model_name": "parakeet-tdt-0.6b-v3",
    "developer": "mlx-community",
    "downloads": 380327,
    "createdAt": "2025-08-16T04:58:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3/resolve/main/README.md",
    "description": "An MLX-converted automatic speech recognition model for transcribing audio in 24 languages."
  },
  {
    "model_name": "parakeet-tdt-0.6b-v2",
    "developer": "mlx-community",
    "downloads": 373258,
    "createdAt": "2025-05-06T14:30:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2/resolve/main/README.md",
    "description": "MLX speech recognition model for transcribing audio files."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 221716,
    "createdAt": "2025-12-09T17:00:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of Mistral's Devstral-Small-2-24B-Instruct-2512 vision language model."
  },
  {
    "model_name": "gemma-3-12b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 156063,
    "createdAt": "2025-04-15T14:01:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of the Google Gemma 3 12B instruction-tuned model, converted for use with the `mlx-vlm` library."
  },
  {
    "model_name": "gemma-3-27b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 116708,
    "createdAt": "2025-04-15T19:57:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "768.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/README.md",
    "description": "This is an MLX conversion of the Gemma 3 27B Instruct model."
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 105517,
    "createdAt": "2024-09-25T18:35:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "663.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized 1B parameter instruction-tuned Llama 3.2 model converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "all-MiniLM-L6-v2-4bit",
    "developer": "mlx-community",
    "downloads": 86860,
    "createdAt": "2025-04-02T16:35:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-4bit/resolve/main/model.safetensors",
        "file_size": "12.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of all-MiniLM-L6-v2 for sentence embeddings on Apple Silicon."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 79029,
    "createdAt": "2024-09-25T18:35:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Llama-3.2-3B-Instruct for text generation."
  },
  {
    "model_name": "Qwen3-0.6B-4bit",
    "developer": "mlx-community",
    "downloads": 58029,
    "createdAt": "2025-04-28T21:01:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit/resolve/main/model.safetensors",
        "file_size": "319.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit/resolve/main/README.md",
    "description": "4-bit quantized Qwen3-0.6B model converted to MLX format for text generation."
  },
  {
    "model_name": "Qwen3-30B-A3B-4bit",
    "developer": "mlx-community",
    "downloads": 53750,
    "createdAt": "2025-04-28T21:40:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Qwen3-30B-A3B model converted to MLX format."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 52618,
    "createdAt": "2024-07-23T14:39:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Meta Llama 3.1 8B Instruct model for Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-Flash-4bit",
    "developer": "mlx-community",
    "downloads": 51340,
    "createdAt": "2026-01-19T15:48:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "732.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of GLM-4.7-Flash for text generation using the mlx-lm library."
  },
  {
    "model_name": "gemma-3-1b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 49523,
    "createdAt": "2025-04-15T19:23:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit/resolve/main/model.safetensors",
        "file_size": "698.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized version of the Google Gemma 3 1B instruction-tuned model, converted to MLX format for Apple Silicon using the `mlx-lm` library."
  },
  {
    "model_name": "GLM-4.7-Flash-8bit",
    "developer": "mlx-community",
    "downloads": 49272,
    "createdAt": "2026-01-19T15:48:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "331.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of GLM-4.7-Flash for text generation."
  },
  {
    "model_name": "Llama-3.3-70B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 49032,
    "createdAt": "2024-12-06T17:13:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Meta's Llama-3.3-70B-Instruct converted to MLX format for Apple devices."
  },
  {
    "model_name": "gpt-oss-120b-MXFP4-Q8",
    "developer": "mlx-community",
    "downloads": 48608,
    "createdAt": "2025-08-29T23:33:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/README.md",
    "description": "This is an MLX-converted 8-bit quantized version of OpenAI's GPT-OSS 120B model for text generation."
  },
  {
    "model_name": "Kimi-K2-Thinking",
    "developer": "mlx-community",
    "downloads": 48036,
    "createdAt": "2025-11-07T00:19:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 182,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00001-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00002-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00002-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00003-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00003-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00004-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00004-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00005-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00005-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00006-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00006-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00007-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00007-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00008-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00008-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00009-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00009-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00010-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00010-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00011-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00011-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00012-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00012-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00013-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00013-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00014-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00014-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00015-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00015-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00016-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00016-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00017-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00017-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00018-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00018-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00019-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00019-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00020-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00020-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00021-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00021-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00022-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00022-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00023-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00023-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00024-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00024-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00025-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00025-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00026-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00026-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00027-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00027-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00028-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00028-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00029-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00029-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00030-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00030-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00031-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00031-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00032-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00032-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00033-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00033-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00034-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00034-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00035-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00035-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00036-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00036-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00037-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00037-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00038-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00038-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00039-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00039-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00040-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00040-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00041-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00041-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00042-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00042-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00043-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00043-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00044-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00044-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00045-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00045-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00046-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00046-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00047-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00047-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00048-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00048-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00049-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00049-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00050-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00050-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00051-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00051-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00052-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00052-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00053-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00053-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00054-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00054-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00055-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00055-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00056-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00056-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00057-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00057-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00058-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00058-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00059-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00059-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00060-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00060-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00061-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00061-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00062-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00062-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00063-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00063-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00064-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00064-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00065-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00065-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00066-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00066-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00067-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00067-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00068-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00068-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00069-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00069-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00070-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00070-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00071-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00071-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00072-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00072-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00073-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00073-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00074-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00074-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00075-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00075-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00076-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00076-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00077-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00077-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00078-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00078-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00079-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00079-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00080-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00080-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00081-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00081-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00082-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00082-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00083-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00083-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00084-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00084-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00085-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00085-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00086-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00086-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00087-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00087-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00088-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00088-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00089-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00089-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00090-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00090-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00091-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00091-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00092-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00092-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00093-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00093-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00094-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00094-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00095-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00095-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00096-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00096-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00097-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00097-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00098-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00098-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00099-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00099-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00100-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00100-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00101-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00101-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00102-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00102-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00103-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00103-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00104-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00104-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00105-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00105-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00106-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00106-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00107-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00107-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00108-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00108-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00109-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00109-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00110-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00110-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00111-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00111-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00112-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00112-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00113-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00113-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00114-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00114-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00115-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00115-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00116-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00116-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00117-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00117-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00118-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00118-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00119-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00119-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00120-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00120-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00121-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00121-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00122-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00122-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00123-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00123-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00124-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00124-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00125-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00125-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00126-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00126-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00127-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00127-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00128-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00128-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00129-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00129-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00130-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00130-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00131-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00131-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00132-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00132-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00133-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00133-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00134-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00134-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00135-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00135-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00136-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00136-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00137-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00137-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00138-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00138-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00139-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00139-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00140-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00140-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00141-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00141-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00142-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00142-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00143-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00143-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00144-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00144-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00145-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00145-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00146-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00146-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00147-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00147-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00148-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00148-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00149-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00149-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00150-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00150-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00151-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00151-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00152-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00152-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00153-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00153-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00154-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00154-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00155-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00155-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00156-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00156-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00157-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00157-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00158-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00158-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00159-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00159-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00160-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00160-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00161-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00161-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00162-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00162-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00163-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00163-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00164-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00164-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00165-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00165-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00166-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00166-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00167-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00167-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00168-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00168-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00169-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00169-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00170-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00170-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00171-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00171-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00172-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00172-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00173-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00173-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00174-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00174-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00175-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00175-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00176-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00176-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00177-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00177-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00178-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00178-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00179-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00179-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00180-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00180-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00181-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00181-of-00182.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00182-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00182-of-00182.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/README.md",
    "description": "MLX-converted version of Kimi-K2-Thinking for text generation using mlx-lm."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 47748,
    "createdAt": "2024-07-23T14:39:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Meta's Llama 3.1 8B Instruct model, converted to MLX format for Apple devices."
  },
  {
    "model_name": "Kimi-K2-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 47723,
    "createdAt": "2025-07-11T16:10:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 180,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00001-of-00180.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00002-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00002-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00003-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00003-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00004-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00004-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00005-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00005-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00006-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00006-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00007-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00007-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00008-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00008-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00009-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00009-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00010-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00010-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00011-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00011-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00012-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00012-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00013-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00013-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00014-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00014-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00015-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00015-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00016-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00016-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00017-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00017-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00018-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00018-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00019-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00019-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00020-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00020-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00021-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00021-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00022-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00022-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00023-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00023-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00024-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00024-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00025-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00025-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00026-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00026-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00027-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00027-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00028-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00028-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00029-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00029-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00030-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00030-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00031-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00031-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00032-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00032-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00033-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00033-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00034-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00034-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00035-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00035-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00036-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00036-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00037-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00037-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00038-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00038-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00039-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00039-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00040-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00040-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00041-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00041-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00042-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00042-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00043-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00043-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00044-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00044-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00045-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00045-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00046-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00046-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00047-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00047-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00048-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00048-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00049-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00049-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00050-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00050-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00051-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00051-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00052-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00052-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00053-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00053-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00054-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00054-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00055-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00055-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00056-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00056-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00057-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00057-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00058-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00058-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00059-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00059-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00060-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00060-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00061-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00061-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00062-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00062-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00063-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00063-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00064-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00064-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00065-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00065-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00066-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00066-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00067-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00067-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00068-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00068-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00069-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00069-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00070-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00070-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00071-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00071-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00072-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00072-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00073-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00073-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00074-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00074-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00075-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00075-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00076-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00076-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00077-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00077-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00078-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00078-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00079-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00079-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00080-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00080-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00081-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00081-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00082-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00082-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00083-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00083-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00084-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00084-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00085-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00085-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00086-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00086-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00087-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00087-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00088-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00088-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00089-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00089-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00090-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00090-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00091-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00091-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00092-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00092-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00093-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00093-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00094-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00094-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00095-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00095-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00096-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00096-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00097-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00097-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00098-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00098-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00099-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00099-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00100-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00100-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00101-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00101-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00102-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00102-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00103-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00103-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00104-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00104-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00105-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00105-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00106-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00106-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00107-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00107-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00108-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00108-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00109-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00109-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00110-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00110-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00111-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00111-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00112-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00112-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00113-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00113-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00114-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00114-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00115-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00115-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00116-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00116-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00117-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00117-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00118-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00118-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00119-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00119-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00120-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00120-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00121-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00121-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00122-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00122-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00123-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00123-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00124-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00124-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00125-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00125-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00126-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00126-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00127-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00127-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00128-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00128-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00129-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00129-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00130-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00130-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00131-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00131-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00132-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00132-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00133-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00133-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00134-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00134-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00135-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00135-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00136-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00136-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00137-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00137-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00138-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00138-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00139-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00139-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00140-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00140-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00141-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00141-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00142-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00142-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00143-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00143-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00144-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00144-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00145-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00145-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00146-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00146-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00147-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00147-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00148-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00148-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00149-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00149-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00150-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00150-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00151-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00151-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00152-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00152-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00153-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00153-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00154-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00154-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00155-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00155-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00156-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00156-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00157-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00157-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00158-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00158-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00159-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00159-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00160-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00160-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00161-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00161-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00162-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00162-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00163-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00163-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00164-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00164-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00165-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00165-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00166-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00166-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00167-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00167-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00168-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00168-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00169-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00169-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00170-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00170-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00171-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00171-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00172-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00172-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00173-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00173-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00174-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00174-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00175-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00175-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00176-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00176-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00177-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00177-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00178-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00178-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00179-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00179-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00180-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00180-of-00180.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/README.md",
    "description": "4-bit quantized MLX version of Kimi-K2-Instruct for text generation."
  },
  {
    "model_name": "MiniMax-M2.1-8bit",
    "developer": "mlx-community",
    "downloads": 47691,
    "createdAt": "2025-12-26T08:22:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 47,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00001-of-00047.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00002-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00003-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00004-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00005-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00006-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00007-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00008-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00009-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00010-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00011-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00012-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00013-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00014-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00015-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00016-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00017-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00018-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00019-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00020-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00021-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00022-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00023-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00024-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00025-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00026-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00027-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00028-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00029-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00030-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00030-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00031-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00032-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00033-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00033-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00034-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00035-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00036-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00036-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00037-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00038-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00039-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00039-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00040-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00040-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00041-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00042-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00042-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00043-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00043-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00044-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00045-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00045-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00046-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00046-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00047-of-00047.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/README.md",
    "description": "MiniMax-M2.1-8bit is an 8-bit quantized version of MiniMax-M2.1 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 47686,
    "createdAt": "2025-09-12T17:00:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 9,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00001-of-00009.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00002-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00003-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00004-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00005-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00006-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00007-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00008-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00009-of-00009.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of Qwen3-Next-80B-A3B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-0.6B-8bit",
    "developer": "mlx-community",
    "downloads": 47401,
    "createdAt": "2025-04-28T21:27:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-8bit/resolve/main/model.safetensors",
        "file_size": "604.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized Qwen3-0.6B model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-3bit",
    "developer": "mlx-community",
    "downloads": 47242,
    "createdAt": "2025-12-26T10:40:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 19,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00001-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00002-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00003-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00004-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00005-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00006-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00007-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00008-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00009-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00010-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00011-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00012-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00013-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00014-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00015-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00016-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00017-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00018-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00019-of-00019.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/README.md",
    "description": "A 3-bit quantized version of MiniMax-M2.1 converted to MLX format for text generation."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Thinking-4bit",
    "developer": "mlx-community",
    "downloads": 47238,
    "createdAt": "2025-09-13T03:48:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 9,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00001-of-00009.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00002-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00003-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00004-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00005-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00006-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00007-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00008-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00009-of-00009.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/README.md",
    "description": "MLX conversion of Qwen3-Next-80B-A3B-Thinking (4-bit quantized, text generation)."
  },
  {
    "model_name": "Llama-3.3-70B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 47141,
    "createdAt": "2024-12-06T17:13:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 15,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit quantized MLX version of Meta's Llama-3.3-70B-Instruct language model for Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-Flash-6bit",
    "developer": "mlx-community",
    "downloads": 47043,
    "createdAt": "2026-01-19T15:48:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized version of GLM-4.7-Flash converted to MLX format for text generation using mlx-lm."
  },
  {
    "model_name": "GLM-4.7-4bit",
    "developer": "mlx-community",
    "downloads": 46793,
    "createdAt": "2025-12-22T19:22:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 39,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00001-of-00039.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00002-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00003-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00004-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00005-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00006-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00007-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00008-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00009-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00010-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00011-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00012-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00013-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00014-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00015-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00016-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00017-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00018-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00019-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00020-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00021-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00022-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00023-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00024-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00025-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00026-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00027-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00028-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00029-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00030-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00030-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00031-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00032-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00033-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00033-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00034-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00035-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00036-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00036-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00037-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00038-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00039-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00039-of-00039.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/README.md",
    "description": "4-bit quantized GLM-4.7 model converted to MLX format for text generation."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 46784,
    "createdAt": "2024-09-26T00:50:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit MLX version of Meta's Llama-3.2-3B-Instruct for text generation."
  },
  {
    "model_name": "GLM-4.7-Flash-5bit",
    "developer": "mlx-community",
    "downloads": 46715,
    "createdAt": "2026-01-19T15:47:20.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/README.md",
    "description": "A 5-bit quantized version of GLM-4.7-Flash converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-480B-A35B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 46687,
    "createdAt": "2025-07-22T14:57:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 62,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00001-of-00062.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00002-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00003-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00003-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00004-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00004-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00005-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00005-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00006-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00006-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00007-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00007-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00008-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00008-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00009-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00009-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00010-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00010-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00011-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00011-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00012-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00012-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00013-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00013-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00014-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00014-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00015-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00015-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00016-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00016-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00017-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00017-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00018-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00018-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00019-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00019-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00020-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00020-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00021-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00021-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00022-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00022-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00023-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00023-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00024-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00024-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00025-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00025-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00026-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00026-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00027-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00027-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00028-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00028-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00029-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00029-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00030-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00030-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00031-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00031-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00032-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00032-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00033-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00033-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00034-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00034-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00035-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00035-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00036-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00036-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00037-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00037-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00038-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00038-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00039-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00039-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00040-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00040-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00041-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00041-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00042-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00042-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00043-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00043-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00044-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00044-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00045-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00045-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00046-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00046-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00047-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00047-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00048-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00048-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00049-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00049-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00050-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00050-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00051-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00051-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00052-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00052-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00053-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00053-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00054-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00054-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00055-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00055-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00056-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00056-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00057-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00057-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00058-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00058-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00059-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00059-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00060-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00060-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00061-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00061-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00062-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00062-of-00062.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized MLX version of Qwen3-Coder-480B-A35B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3.1-4bit",
    "developer": "mlx-community",
    "downloads": 46503,
    "createdAt": "2025-08-21T08:02:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/README.md",
    "description": "DeepSeek-V3.1-4bit is a 4-bit quantized version of DeepSeek-V3.1 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3.1-70B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 46434,
    "createdAt": "2024-07-23T14:40:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/README.md",
    "description": "Meta's Llama 3.1 70B instruction-tuned language model, converted to 4-bit MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 46311,
    "createdAt": "2025-09-13T02:32:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "862.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Qwen3-Next-80B-A3B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Thinking-8bit",
    "developer": "mlx-community",
    "downloads": 46257,
    "createdAt": "2025-09-13T02:33:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "862.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX quantized version of Qwen3-Next-80B-A3B-Thinking for text generation on Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-8bit-gs32",
    "developer": "mlx-community",
    "downloads": 46126,
    "createdAt": "2025-12-23T07:01:41.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 90,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00001-of-00090.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00002-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00003-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00004-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00005-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00006-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00007-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00007-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00008-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00009-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00010-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00010-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00011-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00012-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00013-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00013-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00014-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00015-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00016-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00016-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00017-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00018-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00019-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00019-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00020-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00021-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00022-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00022-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00023-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00024-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00025-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00025-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00026-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00027-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00028-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00028-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00029-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00030-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00031-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00031-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00032-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00033-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00034-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00034-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00035-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00036-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00037-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00037-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00038-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00039-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00040-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00040-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00041-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00042-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00043-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00043-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00044-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00045-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00046-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00046-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00047-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00048-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00049-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00049-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00050-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00051-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00052-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00052-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00053-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00054-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00055-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00055-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00056-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00057-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00058-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00058-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00059-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00060-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00061-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00061-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00062-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00063-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00064-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00064-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00065-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00066-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00067-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00067-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00068-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00069-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00070-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00070-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00071-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00072-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00073-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00073-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00074-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00075-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00076-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00076-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00077-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00078-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00079-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00079-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00080-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00081-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00082-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00082-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00083-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00084-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00085-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00085-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00086-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00087-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00088-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00088-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00089-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00089-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00090-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00090-of-00090.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/README.md",
    "description": "8-bit quantized MLX version of GLM-4.7 for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-235B-A22B-Instruct-2507-8bit",
    "developer": "mlx-community",
    "downloads": 46056,
    "createdAt": "2025-07-22T02:33:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 48,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00001-of-00048.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00002-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00003-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00004-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00005-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00006-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00007-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00008-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00009-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00010-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00011-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00012-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00013-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00014-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00015-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00016-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00017-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00018-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00019-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00019-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00020-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00021-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00022-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00022-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00023-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00024-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00024-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00025-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00025-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00026-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00027-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00027-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00028-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00028-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00029-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00030-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00030-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00031-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00031-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00032-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00033-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00033-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00034-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00034-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00035-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00036-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00036-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00037-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00037-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00038-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00039-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00039-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00040-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00040-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00041-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00042-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00042-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00043-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00043-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00044-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00045-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00045-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00046-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00046-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00047-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00048-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00048-of-00048.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Qwen3-235B-A22B-Instruct-2507 for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-480B-A35B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 45990,
    "createdAt": "2025-12-08T15:00:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 125,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00001-of-00125.safetensors",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "model-00002-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00002-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00003-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00004-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00004-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00005-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00006-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00006-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00007-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00008-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00008-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00009-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00010-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00010-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00011-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00012-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00012-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00013-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00014-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00014-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00015-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00016-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00016-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00017-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00018-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00018-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00019-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00020-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00020-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00021-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00022-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00022-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00023-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00024-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00024-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00025-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00025-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00026-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00026-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00027-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00027-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00028-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00028-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00029-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00030-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00030-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00031-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00031-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00032-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00032-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00033-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00033-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00034-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00034-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00035-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00036-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00036-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00037-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00037-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00038-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00038-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00039-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00039-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00040-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00040-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00041-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00042-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00042-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00043-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00043-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00044-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00044-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00045-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00045-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00046-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00046-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00047-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00048-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00048-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00049-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00049-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00050-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00050-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00051-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00051-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00052-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00052-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00053-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00054-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00054-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00055-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00055-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00056-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00056-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00057-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00057-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00058-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00058-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00059-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00059-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00060-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00060-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00061-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00061-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00062-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00062-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00063-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00063-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00064-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00064-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00065-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00065-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00066-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00066-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00067-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00067-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00068-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00068-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00069-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00069-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00070-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00070-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00071-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00071-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00072-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00072-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00073-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00073-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00074-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00074-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00075-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00075-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00076-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00076-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00077-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00077-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00078-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00078-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00079-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00079-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00080-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00080-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00081-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00081-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00082-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00082-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00083-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00083-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00084-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00084-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00085-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00085-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00086-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00086-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00087-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00087-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00088-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00088-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00089-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00089-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00090-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00090-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00091-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00091-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00092-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00092-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00093-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00093-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00094-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00094-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00095-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00095-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00096-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00096-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00097-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00097-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00098-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00098-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00099-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00099-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00100-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00100-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00101-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00101-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00102-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00102-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00103-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00103-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00104-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00104-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00105-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00105-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00106-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00106-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00107-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00107-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00108-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00108-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00109-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00109-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00110-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00110-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00111-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00111-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00112-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00112-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00113-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00113-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00114-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00114-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00115-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00115-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00116-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00116-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00117-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00117-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00118-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00118-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00119-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00119-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00120-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00120-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00121-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00121-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00122-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00122-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00123-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00123-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00124-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00124-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00125-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00125-of-00125.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX converted version of Qwen3-Coder-480B-A35B-Instruct for Apple Silicon."
  },
  {
    "model_name": "Qwen3-30B-A3B-8bit",
    "developer": "mlx-community",
    "downloads": 45986,
    "createdAt": "2025-04-28T22:00:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "723.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Qwen3-30B-A3B converted to MLX format for text generation."
  },
  {
    "model_name": "DeepSeek-V3.1-8bit",
    "developer": "mlx-community",
    "downloads": 45958,
    "createdAt": "2025-08-21T14:15:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 175,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00001-of-00175.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00002-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00002-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00003-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00003-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00004-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00004-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00005-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00005-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00006-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00006-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00007-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00007-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00008-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00008-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00009-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00009-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00010-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00010-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00011-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00011-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00012-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00012-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00013-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00013-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00014-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00014-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00015-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00015-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00016-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00016-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00017-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00017-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00018-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00018-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00019-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00019-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00020-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00020-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00021-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00021-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00022-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00022-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00023-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00023-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00024-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00024-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00025-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00025-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00026-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00026-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00027-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00027-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00028-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00028-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00029-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00029-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00030-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00030-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00031-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00031-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00032-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00032-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00033-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00033-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00034-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00034-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00035-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00035-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00036-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00036-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00037-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00037-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00038-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00038-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00039-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00039-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00040-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00040-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00041-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00041-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00042-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00042-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00043-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00043-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00044-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00044-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00045-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00045-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00046-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00046-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00047-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00047-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00048-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00048-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00049-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00049-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00050-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00050-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00051-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00051-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00052-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00052-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00053-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00053-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00054-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00054-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00055-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00055-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00056-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00056-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00057-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00057-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00058-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00058-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00059-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00059-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00060-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00060-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00061-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00061-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00062-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00062-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00063-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00063-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00064-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00064-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00065-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00065-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00066-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00066-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00067-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00067-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00068-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00068-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00069-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00069-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00070-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00070-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00071-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00071-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00072-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00072-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00073-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00073-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00074-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00074-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00075-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00075-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00076-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00076-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00077-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00077-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00078-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00078-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00079-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00079-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00080-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00080-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00081-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00081-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00082-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00082-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00083-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00083-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00084-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00084-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00085-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00085-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00086-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00086-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00087-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00087-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00088-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00088-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00089-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00089-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00090-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00090-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00091-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00091-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00092-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00092-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00093-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00093-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00094-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00094-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00095-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00095-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00096-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00096-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00097-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00097-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00098-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00098-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00099-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00099-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00100-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00100-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00101-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00101-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00102-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00102-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00103-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00103-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00104-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00104-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00105-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00105-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00106-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00106-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00107-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00107-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00108-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00108-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00109-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00109-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00110-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00110-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00111-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00111-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00112-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00112-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00113-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00113-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00114-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00114-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00115-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00115-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00116-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00116-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00117-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00117-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00118-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00118-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00119-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00119-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00120-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00120-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00121-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00121-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00122-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00122-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00123-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00123-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00124-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00124-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00125-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00125-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00126-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00126-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00127-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00127-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00128-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00128-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00129-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00129-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00130-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00130-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00131-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00131-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00132-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00132-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00133-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00133-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00134-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00134-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00135-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00135-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00136-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00136-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00137-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00137-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00138-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00138-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00139-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00139-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00140-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00140-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00141-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00141-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00142-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00142-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00143-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00143-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00144-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00144-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00145-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00145-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00146-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00146-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00147-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00147-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00148-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00148-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00149-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00149-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00150-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00150-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00151-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00151-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00152-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00152-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00153-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00153-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00154-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00154-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00155-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00155-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00156-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00156-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00157-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00157-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00158-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00158-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00159-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00159-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00160-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00160-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00161-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00161-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00162-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00162-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00163-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00163-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00164-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00164-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00165-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00165-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00166-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00166-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00167-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00167-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00168-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00168-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00169-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00169-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00170-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00170-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00171-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00171-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00172-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00172-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00173-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00173-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00174-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00174-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00175-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00175-of-00175.safetensors",
        "file_size": "4.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized DeepSeek-V3.1 model converted to MLX format for Apple Silicon devices."
  },
  {
    "model_name": "GLM-4.7-6bit",
    "developer": "mlx-community",
    "downloads": 45883,
    "createdAt": "2025-12-22T20:41:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 54,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00001-of-00054.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00002-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00002-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00003-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00004-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00005-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00006-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00007-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00008-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00009-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00010-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00011-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00012-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00013-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00014-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00015-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00016-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00017-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00018-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00019-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00020-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00020-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00021-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00022-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00022-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00023-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00024-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00024-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00025-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00025-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00026-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00026-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00027-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00027-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00028-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00028-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00029-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00030-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00030-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00031-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00031-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00032-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00032-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00033-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00033-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00034-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00034-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00035-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00036-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00036-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00037-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00037-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00038-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00038-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00039-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00039-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00040-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00040-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00041-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00042-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00042-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00043-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00043-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00044-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00044-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00045-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00045-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00046-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00046-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00047-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00048-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00048-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00049-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00049-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00050-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00050-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00051-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00051-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00052-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00052-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00053-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00054-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00054-of-00054.safetensors",
        "file_size": "4.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized version of GLM-4.7 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "GLM-4.5-Air-bf16",
    "developer": "mlx-community",
    "downloads": 45614,
    "createdAt": "2025-10-07T17:46:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 46,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00001-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00002-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00003-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00003-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00004-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00004-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00005-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00005-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00006-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00006-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00007-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00007-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00008-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00008-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00009-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00009-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00010-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00010-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00011-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00011-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00012-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00012-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00013-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00013-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00014-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00014-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00015-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00015-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00016-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00016-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00017-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00017-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00018-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00018-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00019-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00019-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00020-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00020-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00021-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00021-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00022-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00022-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00023-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00023-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00024-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00024-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00025-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00025-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00026-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00026-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00027-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00027-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00028-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00028-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00029-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00029-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00030-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00030-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00031-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00031-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00032-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00032-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00033-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00033-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00034-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00034-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00035-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00035-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00036-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00036-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00037-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00037-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00038-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00038-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00039-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00039-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00040-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00040-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00041-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00041-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00042-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00042-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00043-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00043-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00044-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00044-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00045-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00045-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00046-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00046-of-00046.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/README.md",
    "description": "MLX-converted GLM-4.5-Air model for text generation using mlx-lm."
  },
  {
    "model_name": "GLM-4.5-Air-8bit",
    "developer": "mlx-community",
    "downloads": 45592,
    "createdAt": "2025-07-28T16:37:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 23,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00001-of-00023.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00002-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00003-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00003-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00004-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00005-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00006-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00007-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00008-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00009-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00010-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00011-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00012-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00013-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00014-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00015-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00016-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00017-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00018-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00019-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00020-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00021-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00022-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00023-of-00023.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/README.md",
    "description": "8-bit MLX converted version of GLM-4.5-Air for text generation."
  },
  {
    "model_name": "Qwen3-235B-A22B-Instruct-2507-4bit",
    "developer": "mlx-community",
    "downloads": 45518,
    "createdAt": "2025-07-22T02:21:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 26,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00001-of-00026.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00002-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00003-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00004-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00005-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00006-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00007-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00008-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00009-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00010-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00011-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00012-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00013-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00014-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00015-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00016-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00017-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00018-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00019-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00020-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00020-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00021-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00022-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00023-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00023-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00024-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00025-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00026-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00026-of-00026.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/README.md",
    "description": "4-bit MLX quantized version of Qwen3-235B-A22B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-bf16",
    "developer": "mlx-community",
    "downloads": 45467,
    "createdAt": "2024-07-23T14:39:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1002.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/README.md",
    "description": "MLX-converted 8B parameter instruction-tuned language model from Meta's Llama 3.1, optimized for Apple silicon."
  },
  {
    "model_name": "llama-3.3-70b-instruct-fp16",
    "developer": "mlx-community",
    "downloads": 44732,
    "createdAt": "2025-03-22T04:40:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 28,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00001-of-00028.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00002-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00003-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00004-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00005-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00006-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00007-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00008-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00009-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00010-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00011-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00012-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00013-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00014-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00015-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00016-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00017-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00018-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00019-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00020-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00020-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00021-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00022-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00023-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00023-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00024-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00025-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00026-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00026-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00027-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00028-of-00028.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/README.md",
    "description": "MLX-optimized version of Meta's Llama 3.3 70B Instruct language model for Apple devices."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-4bit",
    "developer": "mlx-community",
    "downloads": 44301,
    "createdAt": "2025-08-06T18:17:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-4B-Instruct-2507 for text generation."
  },
  {
    "model_name": "Qwen3-1.7B-4bit",
    "developer": "mlx-community",
    "downloads": 36645,
    "createdAt": "2025-04-28T21:28:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-1.7B-4bit/resolve/main/model.safetensors",
        "file_size": "923.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-1.7B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-1.7B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen3-1.7B converted to MLX format for text generation on Apple silicon."
  },
  {
    "model_name": "Qwen3-8B-4bit",
    "developer": "mlx-community",
    "downloads": 25130,
    "createdAt": "2025-04-28T21:44:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-8B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen3-8B converted to MLX format for Apple Silicon devices."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-4bit",
    "developer": "mlx-community",
    "downloads": 24545,
    "createdAt": "2024-05-22T17:47:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit/resolve/main/model.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit/resolve/main/README.md",
    "description": "Mistral-7B-Instruct-v0.3 quantized to 4-bit and converted to MLX format."
  },
  {
    "model_name": "Qwen3-4B-4bit",
    "developer": "mlx-community",
    "downloads": 18040,
    "createdAt": "2025-04-28T21:28:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-4bit/resolve/main/README.md",
    "description": "4-bit quantized MLX version of Qwen3-4B for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-Base-8bit",
    "developer": "mlx-community",
    "downloads": 17981,
    "createdAt": "2026-01-22T19:41:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/README.md",
    "description": "A 0.6B parameter text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Mistral-Nemo-Instruct-2407-4bit",
    "developer": "mlx-community",
    "downloads": 14785,
    "createdAt": "2024-07-18T15:04:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-converted version of Mistral-Nemo-Instruct-2407 for use with mlx-lm."
  },
  {
    "model_name": "gemma-2-2b-it-4bit",
    "developer": "mlx-community",
    "downloads": 14557,
    "createdAt": "2024-07-31T16:37:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-2-2b-it-4bit/resolve/main/model.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-2-2b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-2-2b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Google's Gemma 2 2B instruction model, converted to run on Apple Silicon via the MLX framework."
  },
  {
    "model_name": "Qwen2.5-3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 14372,
    "createdAt": "2024-09-18T19:06:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-3B-Instruct converted to MLX format for Apple devices."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit",
    "developer": "mlx-community",
    "downloads": 14198,
    "createdAt": "2026-01-22T19:56:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/README.md",
    "description": "MLX-converted 0.6B text-to-speech model with voice cloning for Apple Silicon."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 13202,
    "createdAt": "2025-12-03T20:57:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-4bit/resolve/main/model.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "MLX-optimized 4-bit quantized version of Ministral-3-3B-Instruct for image understanding tasks."
  },
  {
    "model_name": "Qwen2.5-7B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 12922,
    "createdAt": "2024-09-18T20:06:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-7B-Instruct converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3.5-27B-4bit",
    "developer": "mlx-community",
    "downloads": 11710,
    "createdAt": "2026-02-24T17:38:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-27B-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-27B-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-27B-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "5.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-27B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-27B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit Qwen3.5-27B model for image-text-to-text generation."
  },
  {
    "model_name": "Qwen3-VL-4B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 11332,
    "createdAt": "2025-10-15T21:43:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3-VL-4B-Instruct for image-text-to-text generation."
  },
  {
    "model_name": "whisper-large-v3-turbo",
    "developer": "mlx-community",
    "downloads": 10956,
    "createdAt": "2024-10-01T11:47:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights",
        "path": "https://huggingface.co/mlx-community/whisper-large-v3-turbo/resolve/main/weights.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/whisper-large-v3-turbo/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/whisper-large-v3-turbo/resolve/main/README.md",
    "description": "MLX-converted Whisper large-v3-turbo model for audio transcription using the mlx-whisper library."
  },
  {
    "model_name": "deepseek-coder-33b-instruct-hf-4bit-mlx",
    "developer": "mlx-community",
    "downloads": 10589,
    "createdAt": "2024-01-07T08:42:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/weights.00.safetensors",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "weights.01",
        "path": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/weights.01.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/README.md",
    "description": "A 4-bit quantized DeepSeek Coder 33B model converted to MLX format for local inference."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-Base-bf16",
    "developer": "mlx-community",
    "downloads": 9320,
    "createdAt": "2026-01-22T22:21:54.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/README.md",
    "description": "MLX-converted Qwen text-to-speech model with voice cloning support."
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 9283,
    "createdAt": "2024-09-18T18:44:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "828.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX format conversion of Qwen2.5-1.5B-Instruct model in 4-bit precision."
  },
  {
    "model_name": "LFM2.5-VL-1.6B-4bit",
    "developer": "mlx-community",
    "downloads": 9259,
    "createdAt": "2026-01-06T09:46:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-VL-1.6B-4bit/resolve/main/model.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-VL-1.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-VL-1.6B-4bit/resolve/main/README.md",
    "description": "MLX-converted 1.6B vision-language model for image-text tasks."
  },
  {
    "model_name": "gemma-2-9b-it-4bit",
    "developer": "mlx-community",
    "downloads": 8779,
    "createdAt": "2024-06-27T15:18:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-2-9b-it-4bit/resolve/main/model.safetensors",
        "file_size": "4.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-2-9b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-2-9b-it-4bit/resolve/main/README.md",
    "description": "4-bit quantized Gemma 2 9B Instruct model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-4bit",
    "developer": "mlx-community",
    "downloads": 8705,
    "createdAt": "2026-02-24T17:38:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "4.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3.5-35B-A3B for image-text-to-text tasks."
  },
  {
    "model_name": "gemma-3n-E2B-it-lm-4bit",
    "developer": "mlx-community",
    "downloads": 8353,
    "createdAt": "2025-06-29T23:03:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-4bit/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of Google's Gemma 3n E2B model for text generation using the `mlx-lm` library."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-8bit",
    "developer": "mlx-community",
    "downloads": 7518,
    "createdAt": "2026-02-24T19:33:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "515.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-35B-A3B-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit Qwen3.5-35B-A3B model for image-text-to-text tasks."
  },
  {
    "model_name": "Qwen3.5-397B-A17B-4bit",
    "developer": "mlx-community",
    "downloads": 7395,
    "createdAt": "2026-02-16T11:50:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 46,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00001-of-00046.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00002-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00003-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00003-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00004-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00005-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00006-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00007-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00008-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00009-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00010-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00011-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00012-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00013-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00014-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00015-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00016-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00017-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00018-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00019-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00020-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00021-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00022-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00023-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00024-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00024-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00025-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00025-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00026-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00027-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00027-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00028-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00028-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00029-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00030-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00030-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00031-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00031-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00032-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00033-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00033-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00034-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00034-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00035-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00036-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00036-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00037-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00037-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00038-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00039-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00039-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00040-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00040-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00041-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00041-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00042-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00042-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00043-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00043-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00044-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00044-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00045-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00045-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00046-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/model-00046-of-00046.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3.5-397B-A17B for image-text-to-text generation."
  },
  {
    "model_name": "gemma-3n-E4B-it-lm-4bit",
    "developer": "mlx-community",
    "downloads": 7218,
    "createdAt": "2025-06-29T23:27:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-lm-4bit/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-lm-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-lm-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of Google's Gemma 3N E4B IT model, designed for Apple Silicon and used with the mlx-lm library."
  },
  {
    "model_name": "Qwen2.5-0.5B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 6645,
    "createdAt": "2024-09-18T18:39:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "265.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-0.5B-Instruct converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Kokoro-82M-bf16",
    "developer": "mlx-community",
    "downloads": 6607,
    "createdAt": "2025-02-28T18:13:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 55,
    "safetensors_files": [
      {
        "model_id": "kokoro-v1_0",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/kokoro-v1_0.safetensors",
        "file_size": "312.0 MB"
      },
      {
        "model_id": "voices/af_alloy",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_alloy.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_aoede",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_aoede.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_bella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_bella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_heart",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_heart.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_jessica",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_jessica.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_kore",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_kore.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nicole",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_nicole.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nova",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_nova.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_river",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_river.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sarah",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_sarah.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sky",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_sky.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_adam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_adam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_echo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_echo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_eric",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_eric.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_fenrir",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_fenrir.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_liam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_liam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_michael",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_michael.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_onyx",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_onyx.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_puck",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_puck.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_alice",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_alice.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_emma",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_emma.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_isabella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_isabella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_lily",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_lily.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_daniel",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_daniel.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_fable",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_fable.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_george",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_george.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_lewis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_lewis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ef_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/ef_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/em_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/em_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ff_siwis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/ff_siwis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_beta",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hf_beta.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_omega",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hm_omega.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_psi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hm_psi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/if_sara",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/if_sara.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/im_nicola",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/im_nicola.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_gongitsune",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_gongitsune.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_nezumi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_nezumi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_tebukuro",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_tebukuro.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jm_kumo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jm_kumo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pf_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/pf_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/pm_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/pm_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaobei",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaobei.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoni",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaoni.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoxiao",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaoxiao.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoyi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaoyi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunjian",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunjian.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunxi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxia",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunxia.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunyang",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunyang.safetensors",
        "file_size": "510.1 KB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/README.md",
    "description": "A text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen2-VL-2B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 6560,
    "createdAt": "2024-09-27T23:12:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2-VL-2B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2-VL-2B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2-VL-2B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen2-VL-2B-Instruct vision-language model."
  },
  {
    "model_name": "Qwen3-Embedding-0.6B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 6513,
    "createdAt": "2025-06-06T08:09:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "319.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ/resolve/main/README.md",
    "description": "4-bit quantized Qwen3-Embedding-0.6B model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-4bit",
    "developer": "mlx-community",
    "downloads": 5648,
    "createdAt": "2025-12-26T08:22:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 27,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00001-of-00027.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00002-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00003-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00003-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00004-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00004-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00005-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00005-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00006-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00006-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00007-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00007-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00008-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00008-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00009-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00009-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00010-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00010-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00011-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00011-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00012-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00012-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00013-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00013-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00014-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00014-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00015-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00015-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00016-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00016-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00017-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00017-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00018-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00018-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00019-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00019-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00020-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00020-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00021-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00021-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00022-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00022-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00023-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00023-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00024-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00024-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00025-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00025-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00026-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00026-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00027-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00027-of-00027.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized text generation model converted to MLX format from MiniMax-M2.1."
  },
  {
    "model_name": "embeddinggemma-300m-4bit",
    "developer": "mlx-community",
    "downloads": 5036,
    "createdAt": "2025-09-04T16:52:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/embeddinggemma-300m-4bit/resolve/main/model.safetensors",
        "file_size": "165.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/embeddinggemma-300m-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/embeddinggemma-300m-4bit/resolve/main/README.md",
    "description": "An MLX implementation of Google's EmbeddingGemma model, optimized for sentence similarity tasks on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-Next-4bit",
    "developer": "mlx-community",
    "downloads": 4971,
    "createdAt": "2026-02-03T17:07:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 9,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00001-of-00009.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00002-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00003-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00004-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00005-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00006-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00007-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00008-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00009-of-00009.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/README.md",
    "description": "4-bit MLX-quantized version of Qwen3-Coder-Next for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3.5-397B-A17B-nvfp4",
    "developer": "mlx-community",
    "downloads": 4934,
    "createdAt": "2026-02-16T18:26:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 46,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00001-of-00046.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00002-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00002-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00003-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00003-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00004-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00005-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00006-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00007-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00008-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00009-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00010-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00011-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00012-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00013-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00014-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00015-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00016-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00017-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00018-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00019-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00020-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00021-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00022-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00023-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00024-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00024-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00025-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00025-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00026-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00027-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00027-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00028-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00028-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00029-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00030-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00030-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00031-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00031-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00032-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00033-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00033-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00034-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00034-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00035-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00036-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00036-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00037-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00037-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00038-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00039-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00039-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00040-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00040-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00041-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00041-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00042-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00042-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00043-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00043-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00044-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00044-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00045-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00045-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00046-of-00046",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/model-00046-of-00046.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-nvfp4/resolve/main/README.md",
    "description": "A 4-bit nvfp4 quantized version of Qwen3.5-397B-A17B MoE language model optimized for local inference on Apple Silicon Macs via MLX."
  },
  {
    "model_name": "NVIDIA-Nemotron-3-Nano-30B-A3B-4bit",
    "developer": "mlx-community",
    "downloads": 4789,
    "createdAt": "2025-12-15T22:29:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/README.md",
    "description": "NVIDIA Nemotron-3-Nano-30B text generation model in 4-bit MLX format for Apple Silicon."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-6bit",
    "developer": "mlx-community",
    "downloads": 4784,
    "createdAt": "2026-01-20T15:25:07.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-6bit/resolve/main/model.safetensors",
        "file_size": "907.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-6bit/resolve/main/README.md",
    "description": "A 6-bit MLX quantized version of LiquidAI's LFM2.5-1.2B-Thinking model for text generation."
  },
  {
    "model_name": "Qwen3-VL-2B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 4775,
    "createdAt": "2025-10-21T16:58:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-2B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-2B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-2B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX-converted version of Qwen3-VL-2B-Instruct for image-text-to-text tasks."
  },
  {
    "model_name": "LFM2-2.6B-4bit",
    "developer": "mlx-community",
    "downloads": 4538,
    "createdAt": "2025-09-23T14:16:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-2.6B-4bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-2.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-2.6B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of LFM2-2.6B for text generation."
  },
  {
    "model_name": "gemma-3-1b-it-4bit",
    "developer": "mlx-community",
    "downloads": 4322,
    "createdAt": "2025-03-12T08:56:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit/resolve/main/model.safetensors",
        "file_size": "698.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Google's Gemma-3-1b-it model, converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "translategemma-4b-it-4bit",
    "developer": "mlx-community",
    "downloads": 4264,
    "createdAt": "2026-01-15T20:15:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/translategemma-4b-it-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-4b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-4b-it-4bit/resolve/main/README.md",
    "description": "4-bit MLX conversion of Google's Translategemma-4b-it for use with `mlx-lm`."
  },
  {
    "model_name": "GLM-OCR-bf16",
    "developer": "mlx-community",
    "downloads": 4230,
    "createdAt": "2026-02-03T12:04:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-OCR-bf16/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-OCR-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-OCR-bf16/resolve/main/README.md",
    "description": "MLX-converted OCR model for image-to-text tasks, converted from zai-org/GLM-OCR."
  },
  {
    "model_name": "Step-3.5-Flash-4bit",
    "developer": "mlx-community",
    "downloads": 3991,
    "createdAt": "2026-02-02T12:01:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 22,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00001-of-00022.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00002-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00003-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00004-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00005-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00006-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00007-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00008-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00009-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00010-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00011-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00012-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00013-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00014-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00015-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00016-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00017-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00018-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00019-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00019-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00020-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00021-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00022-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00022-of-00022.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of Step-3.5-Flash for text generation."
  },
  {
    "model_name": "Qwen3.5-122B-A10B-4bit",
    "developer": "mlx-community",
    "downloads": 3972,
    "createdAt": "2026-02-24T17:38:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 14,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00001-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00002-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00003-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00004-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00005-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00006-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00007-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00008-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00009-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00010-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00011-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00012-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00013-of-00014.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00014",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/model-00014-of-00014.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-122B-A10B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3.5-122B-A10B for image-text-to-text tasks."
  },
  {
    "model_name": "GLM-5-4bit",
    "developer": "mlx-community",
    "downloads": 3639,
    "createdAt": "2026-02-12T15:14:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 91,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00001-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00002-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00003-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00004-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00004-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00005-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00006-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00006-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00007-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00008-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00008-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00009-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00010-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00010-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00011-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00012-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00012-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00013-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00014-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00014-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00015-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00016-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00016-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00017-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00018-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00018-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00019-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00020-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00020-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00021-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00022-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00022-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00023-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00024-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00024-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00025-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00025-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00026-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00026-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00027-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00027-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00028-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00028-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00029-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00030-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00030-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00031-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00031-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00032-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00032-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00033-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00033-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00034-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00034-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00035-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00036-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00036-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00037-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00037-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00038-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00038-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00039-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00039-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00040-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00040-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00041-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00042-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00042-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00043-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00043-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00044-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00044-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00045-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00045-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00046-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00046-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00047-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00048-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00048-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00049-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00049-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00050-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00050-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00051-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00051-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00052-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00052-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00053-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00054-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00054-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00055-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00055-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00056-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00056-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00057-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00057-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00058-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00058-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00059-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00059-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00060-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00060-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00061-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00061-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00062-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00062-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00063-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00063-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00064-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00064-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00065-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00065-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00066-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00066-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00067-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00067-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00068-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00068-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00069-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00069-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00070-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00070-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00071-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00071-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00072-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00072-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00073-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00073-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00074-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00074-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00075-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00075-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00076-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00076-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00077-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00077-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00078-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00078-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00079-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00079-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00080-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00080-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00081-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00081-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00082-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00082-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00083-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00083-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00084-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00084-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00085-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00085-of-00091.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00086-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00086-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00087-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00087-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00088-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00088-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00089-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00089-of-00091.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00090-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00090-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00091-of-00091",
        "path": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/model-00091-of-00091.safetensors",
        "file_size": "629.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-5-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized GLM-5 text generation model."
  },
  {
    "model_name": "LFM2.5-1.2B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 3611,
    "createdAt": "2026-01-06T05:55:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "628.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of LFM2.5-1.2B-Instruct for text generation."
  },
  {
    "model_name": "Qwen3.5-397B-A17B-8bit",
    "developer": "mlx-community",
    "downloads": 3525,
    "createdAt": "2026-02-16T19:47:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 91,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00001-of-00091.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00002-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00002-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00003-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00003-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00004-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00004-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00005-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00005-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00006-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00006-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00007-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00007-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00008-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00008-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00009-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00009-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00010-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00010-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00011-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00011-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00012-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00012-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00013-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00013-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00014-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00014-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00015-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00015-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00016-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00016-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00017-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00017-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00018-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00018-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00019-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00019-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00020-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00020-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00021-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00021-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00022-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00022-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00023-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00023-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00024-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00024-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00025-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00025-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00026-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00026-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00027-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00027-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00028-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00028-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00029-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00029-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00030-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00030-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00031-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00031-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00032-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00032-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00033-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00033-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00034-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00034-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00035-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00035-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00036-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00036-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00037-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00037-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00038-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00038-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00039-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00039-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00040-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00040-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00041-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00041-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00042-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00042-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00043-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00043-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00044-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00044-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00045-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00045-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00046-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00046-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00047-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00047-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00048-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00048-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00049-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00049-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00050-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00050-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00051-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00051-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00052-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00052-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00053-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00053-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00054-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00054-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00055-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00055-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00056-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00056-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00057-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00057-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00058-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00058-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00059-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00059-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00060-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00060-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00061-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00061-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00062-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00062-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00063-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00063-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00064-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00064-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00065-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00065-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00066-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00066-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00067-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00067-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00068-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00068-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00069-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00069-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00070-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00070-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00071-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00071-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00072-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00072-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00073-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00073-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00074-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00074-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00075-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00075-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00076-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00076-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00077-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00077-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00078-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00078-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00079-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00079-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00080-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00080-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00081-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00081-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00082-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00082-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00083-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00083-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00084-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00084-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00085-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00085-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00086-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00086-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00087-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00087-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00088-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00088-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00089-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00089-of-00091.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00090-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00090-of-00091.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00091-of-00091",
        "path": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/model-00091-of-00091.safetensors",
        "file_size": "3.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3.5-397B-A17B-8bit/resolve/main/README.md",
    "description": "MLX-converted version of Qwen3.5-397B-A17B for image-text-to-text tasks."
  },
  {
    "model_name": "Qwen3-Embedding-4B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 3471,
    "createdAt": "2025-06-06T14:11:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-Embedding-4B-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Embedding-4B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Embedding-4B-4bit-DWQ/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3-Embedding-4B for sentence similarity and feature extraction."
  },
  {
    "model_name": "Phi-3.5-mini-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 3389,
    "createdAt": "2024-08-20T20:27:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized version of Microsoft's Phi-3.5-mini-instruct converted to Apple MLX format for efficient text generation on Apple Silicon."
  },
  {
    "model_name": "multilingual-e5-base-mlx",
    "developer": "mlx-community",
    "downloads": 3340,
    "createdAt": "2024-01-11T13:50:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/multilingual-e5-base-mlx/resolve/main/weights.00.safetensors",
        "file_size": "530.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/multilingual-e5-base-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/multilingual-e5-base-mlx/resolve/main/README.md",
    "description": "A multilingual sentence embedding model (e5-base) converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-Next-8bit",
    "developer": "mlx-community",
    "downloads": 3312,
    "createdAt": "2026-02-03T19:28:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "862.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/README.md",
    "description": "MLX 8-bit version of Qwen3-Coder-Next for text generation using mlx-lm."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 281289,
    "createdAt": "2024-07-23T15:36:34.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Meta-Llama-3.1-8B-Instruct using llama.cpp imatrix, ranging from 2.95GB (IQ2_M) to 32.13GB (f32)."
  },
  {
    "model_name": "gpt-oss-20b-GGUF",
    "developer": "unsloth",
    "downloads": 237809,
    "createdAt": "2025-08-05T17:12:17.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "gpt-oss-20b-F16",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q2_K.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_0",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_1",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q8_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "gpt-oss-20b-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-UD-Q4_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "gpt-oss-20b-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-UD-Q6_K_XL.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-UD-Q8_K_XL.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "OpenAI's 20B open-weight reasoning model with Apache 2.0 license, MXFP4 quantization, agentic capabilities, and full chain-of-thought support."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-Experiments-GGUF",
    "developer": "unsloth",
    "downloads": 135442,
    "createdAt": "2026-02-27T02:56:02.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 134,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-16_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-16_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-2",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-2.gguf",
        "file_size": "63.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-3",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-3.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-4",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-4.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-5",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-5.gguf",
        "file_size": "64.0 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-6",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-6.gguf",
        "file_size": "64.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-8",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-8.gguf",
        "file_size": "64.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-X",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-X.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-2-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-2-16.gguf",
        "file_size": "64.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-3-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-3-16.gguf",
        "file_size": "64.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-4-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-4-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-5-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-5-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-6-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-6-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-8-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-8-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-X-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-X-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-2-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-2-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-3-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-3-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-4-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-4-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-5-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-5-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-6-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-6-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-8-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-8-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-X-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-X-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-2-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-2-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-3-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-3-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-4-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-4-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-5-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-5-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-6-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-6-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-8-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-8-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-X-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-X-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-2-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-2-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-3-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-3-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-4-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-4-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-5-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-5-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-6-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-6-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-8-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-8-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-X-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-X-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-2-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-2-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-3-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-3-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-4-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-4-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-5-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-5-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-6-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-6-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-8-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-8-16-16-16-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-X-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-X-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-4_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-4_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-5_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-5_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-6_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-6_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-8_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-8_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-2-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-2-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-3-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-3-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-4-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-4-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-5-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-5-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-6-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-6-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-8-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-8-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-X-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-X-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-2-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-2-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-3-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-3-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-4-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-4-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-5-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-5-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-6-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-6-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-8-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-8-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-X-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-16-X-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-2-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-2-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-3-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-3-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-4-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-4-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-5-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-5-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-6-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-6-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-8-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-8-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-X-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-16_ssm-X-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-2_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-2_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-3_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-3_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-4_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-4_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-5_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-5_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-6_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-6_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-8_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-8_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-X_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-16-X_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-2-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-2-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-3-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-3-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-4-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-4-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-5-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-5-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-6-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-6-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-8-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-8-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-X-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-16-X-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-2-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-2-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-3-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-3-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-4-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-4-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-5-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-5-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-6-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-6-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-8-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-8-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-X-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-16_shr-X-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-2_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-2_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-3_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-3_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-4_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-4_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "50.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-5_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-5_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "51.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-6_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-6_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "52.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-8_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-8_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "55.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-X_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-X_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "49.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq2_s_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq2_s_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq2_xs_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq2_xs_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq2_xxs_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq2_xxs_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq3_s_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq3_s_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq3_xxs_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-16-iq3_xxs_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-2-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-2-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-3-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-3-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-4-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-4-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "50.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-5-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-5-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "51.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-6-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-6-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "52.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-8-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-8-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "55.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-X-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-X-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "49.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq2_s-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq2_s-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq2_xs-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq2_xs-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq2_xxs-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq2_xxs-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq3_s-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq3_s-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq3_xxs-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-16-iq3_xxs-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-2-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-2-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-3-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-3-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-4-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-4-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "50.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-5-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-5-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "51.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-6-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-6-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "52.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-8-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-8-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "55.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-X-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-X-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "49.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-iq2_s-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-iq2_s-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-iq2_xs-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-iq2_xs-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-iq2_xxs-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-iq2_xxs-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "47.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-iq3_s-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-iq3_s-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-16_exp-iq3_xxs-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-16_exp-iq3_xxs-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "48.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-4_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-4_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-5_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-5_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.0 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-6_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-6_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-16_out-8_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-16_out-8_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-4_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-4_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "63.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-5_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-5_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.0 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-6_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-6_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B_tok-8_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16",
        "path": "https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF/resolve/main/Qwen3.5-35B-A3B_tok-8_out-16_exp-16-16-16_shr-16-16-16_ssm-16-16-16-32_atn-16-16-16-16-16-16.gguf",
        "file_size": "64.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": null,
    "description": ""
  },
  {
    "model_name": "GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF",
    "developer": "DavidAU",
    "downloads": 122857,
    "createdAt": "2026-01-22T05:13:27.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ2_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ2_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ3_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ3_M.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_NL.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_1",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_M.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_S",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_S.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_1",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_1.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_S",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q6_K",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q6_K.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q8_0.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/README.md",
    "description": "**Uncensored GGUF quant of GLM-4.7-Flash optimized for creative fiction writing, storytelling, and roleplaying across all genres.**"
  },
  {
    "model_name": "gemma-7b",
    "developer": "google",
    "downloads": 113154,
    "createdAt": "2024-02-08T22:36:43.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-7b",
        "path": "https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
        "file_size": "31.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-7b/resolve/main/README.md",
    "description": "Gemma is a lightweight open LLM from Google, offered in 2B/7B sizes, trained on 6T tokens, with CPU/GPU and quantization support."
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 104754,
    "createdAt": "2025-08-07T22:20:03.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-Q5_1.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "A quantized, uncensored 20B Mixture of Experts model in GGUF format optimized for creative writing and coding tasks."
  },
  {
    "model_name": "GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 95410,
    "createdAt": "2026-01-22T18:25:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.bf16",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.bf16.gguf",
        "file_size": "55.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.f16",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.f16.gguf",
        "file_size": "55.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq2_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq2_m.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq3_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq3_m.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq3_xs",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq3_xs.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq4_nl",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq4_nl.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq4_xs",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq4_xs.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q3_k_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q3_k_m.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q3_k_s",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q3_k_s.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q4_k_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q4_k_m.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q5_k_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q5_k_m.gguf",
        "file_size": "19.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q6_k",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q6_k.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q8_0",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q8_0.gguf",
        "file_size": "29.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/README.md",
    "description": "A reasoning-distilled version of GLM-4.7-Flash trained on high-reasoning Claude Opus 4.5 data for coding, science, and research tasks."
  },
  {
    "model_name": "Qwen3-8B-GGUF",
    "developer": "Qwen",
    "downloads": 94824,
    "createdAt": "2025-05-03T06:33:59.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "Qwen3-8B-GGUF is an 8B-parameter quantized language model with unique switchable thinking modes for complex reasoning or efficient dialogue."
  },
  {
    "model_name": "gemma-3-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 91422,
    "createdAt": "2025-03-12T09:04:23.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/README.md",
    "description": "Google's Gemma 3 4B is a lightweight, open, instruction-tuned multimodal model (text & image) supporting 140+ languages with a 128K context window."
  },
  {
    "model_name": "Qwen3-VL-8B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 90835,
    "createdAt": "2025-10-31T02:50:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen3VL-8B-Instruct-F16",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3VL-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3VL-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen3VL-8B-Instruct-F16",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-8B-Instruct-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mmproj-Qwen3VL-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf",
        "file_size": "717.4 MB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "GGUF-format weights for Qwen3-VL-8B-Instruct vision-language model, enabling local inference via llama.cpp/Ollama."
  },
  {
    "model_name": "Qwen3-4B-GGUF",
    "developer": "unsloth",
    "downloads": 86474,
    "createdAt": "2025-04-28T07:55:09.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B is a 4B-parameter language model that can switch between thinking mode (for complex reasoning) and non-thinking mode (for efficient dialogue)."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-GGUF",
    "developer": "unsloth",
    "downloads": 83528,
    "createdAt": "2025-08-06T19:21:40.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-F16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B-Instruct-2507 is a 4B parameter instruction-tuned language model with 262K context length, improved reasoning/coding capabilities, and tool-use support."
  },
  {
    "model_name": "Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 73061,
    "createdAt": "2025-12-10T09:46:30.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Qwen3-14B-Claude-4.5-Opus-Distill.bf16",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-14B-Claude-4.5-Opus-Distill.bf16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Qwen3-14B-Claude-4.5-Opus-Distill.iq4_nl",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-14B-Claude-4.5-Opus-Distill.iq4_nl.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Claude-4.5-Opus-Distill.q3_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-14B-Claude-4.5-Opus-Distill.q3_k_m.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Claude-4.5-Opus-Distill.q3_k_s",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-14B-Claude-4.5-Opus-Distill.q3_k_s.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen3-14B-Claude-4.5-Opus-Distill.q4_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-14B-Claude-4.5-Opus-Distill.q4_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Claude-4.5-Opus-Distill.q8_0",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-14B-Claude-4.5-Opus-Distill.q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "qwen3-14b.F16",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/qwen3-14b.F16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "qwen3-14b.Q8_0",
        "path": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/qwen3-14b.Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/README.md",
    "description": "A Qwen3-14B model distilled from Claude 4.5 Opus with high reasoning effort, trained for coding, science, and general use."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-GGUF",
    "developer": "unsloth",
    "downloads": 66996,
    "createdAt": "2025-12-10T01:36:17.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/README.md",
    "description": "Devstral Small 2 is a 24B open-source AI model specialized in agentic coding and software engineering tasks."
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 60486,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "Mixture of Experts model combining 8 Llama 3.2 3B models into an 18.4B uncensored creative writing powerhouse optimized for fiction, roleplay, and vivid prose generation."
  },
  {
    "model_name": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 59895,
    "createdAt": "2026-02-12T15:55:14.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ1_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ1_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_M.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_XS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ2_XXS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_M.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_XS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ3_XXS.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-IQ4_XS.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q2_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q2_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q3_K_L.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_1.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q4_K_S.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q5_K_M.gguf",
        "file_size": "15.7 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q5_K_S.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.i1-Q6_K.gguf",
        "file_size": "20.7 GB"
      },
      {
        "model_id": "OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.imatrix",
        "path": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored.imatrix.gguf",
        "file_size": "26.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/OpenAI-gpt-oss-20B-Claude-4.5-Opus-Heretic-Uncensored-i1-GGUF/resolve/main/README.md",
    "description": "This is a repository of GGUF quantized versions (12-22GB) of a 20B parameter uncensored instruct model, with various IQ and Q quantization options for different quality/size tradeoffs."
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-GGUF",
    "developer": "dphn",
    "downloads": 59443,
    "createdAt": "2025-01-02T22:11:05.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-F16",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q2_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_L",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q6_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q8_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/README.md",
    "description": "Dolphin 3.0 Llama 3.1 8B is a general-purpose steerable instruct model based on Llama 3.1, trained on diverse datasets for coding, math, and agentic tasks."
  },
  {
    "model_name": "MiniCPM-o-4_5-gguf",
    "developer": "openbmb",
    "downloads": 55994,
    "createdAt": "2026-02-02T04:42:00.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "MiniCPM-o-4_5-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "audio/MiniCPM-o-4_5-audio-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/audio/MiniCPM-o-4_5-audio-F16.gguf",
        "file_size": "629.6 MB"
      },
      {
        "model_id": "token2wav-gguf/flow_extra",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/flow_extra.gguf",
        "file_size": "13.0 MB"
      },
      {
        "model_id": "token2wav-gguf/flow_matching",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/flow_matching.gguf",
        "file_size": "437.0 MB"
      },
      {
        "model_id": "token2wav-gguf/hifigan2",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/hifigan2.gguf",
        "file_size": "79.4 MB"
      },
      {
        "model_id": "token2wav-gguf/prompt_cache",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/prompt_cache.gguf",
        "file_size": "201.8 MB"
      },
      {
        "model_id": "tts/MiniCPM-o-4_5-projector-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/tts/MiniCPM-o-4_5-projector-F16.gguf",
        "file_size": "14.3 MB"
      },
      {
        "model_id": "tts/MiniCPM-o-4_5-tts-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/tts/MiniCPM-o-4_5-tts-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "vision/MiniCPM-o-4_5-vision-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/vision/MiniCPM-o-4_5-vision-F16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/README.md",
    "description": "A 9B parameter multimodal AI model for vision, speech, and full-duplex live streaming on phones."
  },
  {
    "model_name": "GLM-4.7-Flash-REAP-23B-A3B-GGUF",
    "developer": "unsloth",
    "downloads": 55169,
    "createdAt": "2026-01-23T06:50:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-BF16.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-IQ4_NL.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-IQ4_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q2_K.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q2_K_L.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q3_K_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q3_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_0.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_1.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q5_K_M.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q5_K_S.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q6_K.gguf",
        "file_size": "17.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q8_0.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_S.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_M.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_XXS.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ3_XXS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q2_K_XL.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q4_K_XL.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q6_K_XL.gguf",
        "file_size": "18.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q8_K_XL.gguf",
        "file_size": "25.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-TQ1_0.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/README.md",
    "description": "A 23B parameter Mixture-of-Experts model created by uniformly pruning 25% of experts from GLM-4.7-Flash using the REAP method, maintaining near-original performance while reducing size."
  },
  {
    "model_name": "LFM2.5-1.2B-Instruct-GGUF",
    "developer": "LiquidAI",
    "downloads": 50148,
    "createdAt": "2026-01-04T00:09:26.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "LFM2.5-1.2B-Instruct-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-BF16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/README.md",
    "description": "LFM2.5-1.2B-Instruct is a small hybrid AI model by Liquid AI designed for on-device deployment, runnable via llama.cpp."
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 48074,
    "createdAt": "2025-11-17T05:52:55.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-Q5_1.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "An uncensored 20B Mixture of Experts model optimized for coding and creative tasks, available in multiple quantization formats."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-GGUF",
    "developer": "LiquidAI",
    "downloads": 46287,
    "createdAt": "2026-01-16T19:04:55.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "LFM2.5-1.2B-Thinking-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-BF16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/README.md",
    "description": "LFM2.5-1.2B-Thinking is a compact 1.2B parameter hybrid AI model by Liquid AI, optimized for on-device deployment and available in GGUF format."
  },
  {
    "model_name": "Qwen3-0.6B-GGUF",
    "developer": "Qwen",
    "downloads": 41668,
    "createdAt": "2025-05-05T09:16:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-0.6B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf",
        "file_size": "609.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/README.md",
    "description": "Qwen3-0.6B-GGUF is a quantized 0.6B parameter language model with switchable thinking/non-thinking modes and multilingual support."
  },
  {
    "model_name": "Qwen3-4b-Z-Image-Turbo-AbliteratedV1",
    "developer": "BennyDaBall",
    "downloads": 39998,
    "createdAt": "2026-01-28T01:40:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Z-Image-AbliteratedV1.Q2_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q3_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q4_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q6_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q8_0",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.f16",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/README.md",
    "description": "A censorship-removed (abliterated) version of the Qwen3-4b-Z-Image-Turbo text encoder for unrestricted image generation."
  },
  {
    "model_name": "Qwen3-14B-GGUF",
    "developer": "unsloth",
    "downloads": 38330,
    "createdAt": "2025-04-28T10:01:12.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-14B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-BF16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Qwen3-14B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-IQ4_NL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q2_K_L.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Qwen3-14B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_1.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen3-14B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen3-14B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen3-14B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ1_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ1_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ2_M.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ2_XXS.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-IQ3_XXS.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q2_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q3_K_XL.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q4_K_XL.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q5_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q6_K_XL.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-14B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-UD-Q8_K_XL.gguf",
        "file_size": "17.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/README.md",
    "description": "Qwen3-14B is a 14.8B parameter language model with switchable thinking mode for complex reasoning or efficient dialogue."
  },
  {
    "model_name": "Qwen3.5-27B-GGUF",
    "developer": "lmstudio-community",
    "downloads": 32486,
    "createdAt": "2026-02-24T17:26:44.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen3.5-27B-Q4_K_M",
        "path": "https://huggingface.co/lmstudio-community/Qwen3.5-27B-GGUF/resolve/main/Qwen3.5-27B-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-Q6_K",
        "path": "https://huggingface.co/lmstudio-community/Qwen3.5-27B-GGUF/resolve/main/Qwen3.5-27B-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Qwen3.5-27B-Q8_0",
        "path": "https://huggingface.co/lmstudio-community/Qwen3.5-27B-GGUF/resolve/main/Qwen3.5-27B-Q8_0.gguf",
        "file_size": "26.6 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen3.5-27B-BF16",
        "path": "https://huggingface.co/lmstudio-community/Qwen3.5-27B-GGUF/resolve/main/mmproj-Qwen3.5-27B-BF16.gguf",
        "file_size": "888.0 MB"
      }
    ],
    "readme": "https://huggingface.co/lmstudio-community/Qwen3.5-27B-GGUF/resolve/main/README.md",
    "description": "GGUF quantized Qwen3.5-27B community model by LM Studio."
  },
  {
    "model_name": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 32331,
    "createdAt": "2025-08-03T10:14:21.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of the abliterated Huihui-Qwen3-Coder-30B-A3B model with various precision options (6.5-25.2GB)."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 30801,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored, experimental version of the Google Gemma 3 27B IT model, created using a new layerwise abliteration technique to remove refusals."
  },
  {
    "model_name": "translategemma-4b-it-GGUF",
    "developer": "mradermacher",
    "downloads": 30329,
    "createdAt": "2026-01-16T06:58:49.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "translategemma-4b-it.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q2_K",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q6_K",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "translategemma-4b-it.Q8_0",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "translategemma-4b-it.f16",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.f16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "translategemma-4b-it.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.mmproj-Q8_0.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "translategemma-4b-it.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/translategemma-4b-it.mmproj-f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/translategemma-4b-it-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Google's 4B parameter translategemma multi-modal model for local inference."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512-GGUF",
    "developer": "mistralai",
    "downloads": 30218,
    "createdAt": "2025-10-31T08:45:13.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ministral-3-3B-Instruct-2512-BF16-mmproj",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-BF16-mmproj.gguf",
        "file_size": "802.5 MB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-BF16",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-BF16.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-Q5_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-Q8_0",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-Q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/README.md",
    "description": "A compact 3.4B parameter vision-language model with multilingual support, optimized for edge deployment and agentic tasks, available in GGUF quantization under Apache 2.0 license."
  },
  {
    "model_name": "Qwen3-VL-2B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 24688,
    "createdAt": "2025-10-30T14:03:46.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-VL-2B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-BF16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-IQ4_NL.gguf",
        "file_size": "1005.6 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-IQ4_XS.gguf",
        "file_size": "963.6 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q2_K.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q2_K_L.gguf",
        "file_size": "741.8 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q3_K_M.gguf",
        "file_size": "896.0 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q3_K_S.gguf",
        "file_size": "827.1 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q4_0.gguf",
        "file_size": "1007.8 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q4_1.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q4_K_S.gguf",
        "file_size": "1011.1 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q5_K_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q5_K_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-IQ1_M.gguf",
        "file_size": "535.9 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-IQ1_S.gguf",
        "file_size": "512.9 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-IQ2_M.gguf",
        "file_size": "675.9 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "577.7 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "729.7 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "760.9 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "924.0 MB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Qwen3-VL-2B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3-VL-2B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "784.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "781.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF/resolve/main/README.md",
    "description": "A 2B-parameter vision-language model with visual agent, OCR and multimodal reasoning capabilities."
  },
  {
    "model_name": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 22830,
    "createdAt": "2026-02-16T00:58:50.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-30B-A3B-Gemini-Pro-High-Reasoning-2507-ABLITERATED-UNCENSORED-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quants of Qwen3-30B-A3B MoE model, ranging from 6.5GB to 25.2GB."
  },
  {
    "model_name": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF",
    "developer": "mradermacher",
    "downloads": 22242,
    "createdAt": "2026-01-10T02:24:48.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q2_K",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q6_K",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q8_0",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/README.md",
    "description": "GGUF quants of an uncensored Mistral Nemo 12B model fine-tuned for creative writing and storytelling."
  },
  {
    "model_name": "Qwen3-4b-Z-Image-Engineer-V4",
    "developer": "BennyDaBall",
    "downloads": 22105,
    "createdAt": "2026-02-04T21:07:40.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-F16",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q2_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q3_K_L",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q3_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q4_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q5_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q6_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q8_0",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/README.md",
    "description": "A 4B parameter fine-tuned Qwen 3 model that expands simple text concepts into detailed, cinematic AI image generation prompts."
  },
  {
    "model_name": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 21797,
    "createdAt": "2026-02-02T16:43:20.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_S.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XS.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.imatrix",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.imatrix.gguf",
        "file_size": "7.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quantized files for an uncensored vision language model focused on creative writing."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated",
    "developer": "huihui-ai",
    "downloads": 21207,
    "createdAt": "2025-08-06T15:29:56.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GGUF/ggml-model-Q3_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GGUF/ggml-model-Q4_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GGUF/ggml-model-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-f16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "GGUF/ggml-model-q8_0",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/GGUF/ggml-model-q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated/resolve/main/README.md",
    "description": "An abliterated 20B language model with significantly reduced safety filtering, based on unsloth/gpt-oss-20b-BF16, usable via transformers, Ollama, or GGUF format."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 19312,
    "createdAt": "2024-07-24T22:44:19.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.bf16",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.bf16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "An uncensored Llama 3.1 8B Instruct model created via abliteration."
  },
  {
    "model_name": "Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 19094,
    "createdAt": "2026-02-28T08:01:02.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ1_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_XS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_M.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_XS.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-IQ4_XS.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q2_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q3_K_L.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q3_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q3_K_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_0.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_1.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_K_M.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q4_K_S.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q5_K_M.gguf",
        "file_size": "23.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q5_K_S.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.i1-Q6_K.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.imatrix.gguf",
        "file_size": "183.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quants of the Huihui-Qwen3.5-35B-A3B-abliterated model."
  },
  {
    "model_name": "Qwen3.5-27B-heretic-GGUF",
    "developer": "mradermacher",
    "downloads": 18706,
    "createdAt": "2026-02-26T06:11:13.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Qwen3.5-27B-heretic.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.IQ4_XS.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q2_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q3_K_L.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q3_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q4_K_S.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q5_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q5_K_S.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.Q8_0.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.mmproj-Q8_0.gguf",
        "file_size": "600.1 MB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/Qwen3.5-27B-heretic.mmproj-f16.gguf",
        "file_size": "884.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Qwen3.5-27B-heretic, an uncensored large language model, available in various precision levels from Q2_K to Q8_0."
  },
  {
    "model_name": "gemma-3-27b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 16484,
    "createdAt": "2025-03-20T22:44:27.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf",
        "file_size": "16.0 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-27B",
        "path": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-27B.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "Google's Gemma 3 27B instruction-tuned multimodal model (text+image to text) in GGUF Q4_0 format with a 128K context window."
  },
  {
    "model_name": "gemma-3-4b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 16302,
    "createdAt": "2025-03-12T20:43:28.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-4B",
        "path": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-4B.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "Google Gemma 3 4B is an open, instruction-tuned, multimodal (text+image) model in Q4_0 GGUF format, featuring a 128K context, trained on 4 trillion tokens, excelling in reasoning and multilingual benchmarks but carrying typical LLM limitations like bias and factual errors."
  },
  {
    "model_name": "Phi-4-mini-instruct-GGUF",
    "developer": "unsloth",
    "downloads": 15497,
    "createdAt": "2025-02-28T22:22:06.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Phi-4-mini-instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct-Q6_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.BF16",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Phi-4-mini-instruct.Q8_0",
        "path": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/Phi-4-mini-instruct.Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF/resolve/main/README.md",
    "description": "A 3.8B parameter multilingual language model with 128K context, optimized for reasoning, math, and code, featuring Unsloth's bug fixes for padding/EOS tokens and chat templates."
  },
  {
    "model_name": "Qwen2.5-Coder-14B-Instruct-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 14854,
    "createdAt": "2024-11-13T21:50:00.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-IQ2_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-IQ2_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-IQ2_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-IQ2_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-IQ2_XS.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-IQ3_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-IQ3_XS.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-IQ4_XS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q2_K.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q2_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_L.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q3_K_XL.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0_4_4.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0_4_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_0_8_8.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q4_K_S.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q5_K_L.gguf",
        "file_size": "10.2 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q5_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q6_K_L.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-Q8_0.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Qwen2.5-Coder-14B-Instruct-abliterated-f16",
        "path": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-abliterated-f16.gguf",
        "file_size": "27.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Qwen2.5-Coder-14B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "This is a collection of GGUF quantized versions of the Qwen2.5-Coder-14B-Instruct-abliterated model, ranging from Q2_K (4.7GB) to Q8_0 (15.7GB), optimized for local inference via llama.cpp."
  },
  {
    "model_name": "Nanbeige4.1-3B-GGUF",
    "developer": "mradermacher",
    "downloads": 14303,
    "createdAt": "2026-02-12T00:41:09.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Nanbeige4.1-3B.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q2_K",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q2_K.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q3_K_L.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q3_K_S.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q6_K",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.Q8_0",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.Q8_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Nanbeige4.1-3B.f16",
        "path": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/Nanbeige4.1-3B.f16.gguf",
        "file_size": "7.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Nanbeige4.1-3B-GGUF/resolve/main/README.md",
    "description": "This repository provides GGUF quantized versions of the Nanbeige4.13B language model, ranging from Q2_K to f16."
  },
  {
    "model_name": "LFM2-24B-A2B-GGUF",
    "developer": "LiquidAI",
    "downloads": 14091,
    "createdAt": "2026-02-17T12:53:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "LFM2-24B-A2B-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-BF16.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "LFM2-24B-A2B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-F16.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "LFM2-24B-A2B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-Q4_0.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "LFM2-24B-A2B-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-Q4_K_M.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "LFM2-24B-A2B-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-Q5_K_M.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "LFM2-24B-A2B-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-Q6_K.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "LFM2-24B-A2B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/LFM2-24B-A2B-Q8_0.gguf",
        "file_size": "23.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF/resolve/main/README.md",
    "description": "Liquid AI's LFM2-24B-A2B is a 24B MoE model with 2B active parameters designed for efficient on-device/edge deployment."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-heretic-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 13610,
    "createdAt": "2026-02-27T06:01:16.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ1_M.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ1_S.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ2_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ2_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ2_XS.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ2_XXS.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ3_M.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ3_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ3_XS.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ3_XXS.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-IQ4_XS.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q2_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q3_K_L.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q3_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q3_K_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q4_0.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q4_1.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q4_K_M.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q4_K_S.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q5_K_M.gguf",
        "file_size": "23.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q5_K_S.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.i1-Q6_K.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.imatrix.gguf",
        "file_size": "183.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-i1-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Qwen3.5-35B-A3B-heretic, an uncensored AI model, with multiple quantization levels."
  },
  {
    "model_name": "Huihui-Qwen3.5-27B-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 13111,
    "createdAt": "2026-02-28T06:07:11.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.IQ4_XS.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q2_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q3_K_L.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q3_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q4_K_S.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q5_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q5_K_S.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.Q8_0.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.mmproj-Q8_0.gguf",
        "file_size": "600.1 MB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.mmproj-f16.gguf",
        "file_size": "884.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of the uncensored Huihui-Qwen3.5-27B-abliterated model in various quantization levels (Q2_K to Q8_0)."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-heretic-GGUF",
    "developer": "mradermacher",
    "downloads": 12512,
    "createdAt": "2026-02-26T06:38:05.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.IQ4_XS.gguf",
        "file_size": "17.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q3_K_L.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q3_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q3_K_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q4_K_M.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q4_K_S.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q5_K_M.gguf",
        "file_size": "23.1 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q5_K_S.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q6_K.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-heretic.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/Qwen3.5-35B-A3B-heretic.Q8_0.gguf",
        "file_size": "34.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3.5-35B-A3B-heretic-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of Qwen3.5-35B-A3B-heretic, an uncensored/abliterated LLM, with multiple quant types from Q2_K to Q8_0."
  },
  {
    "model_name": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 12510,
    "createdAt": "2026-02-02T14:14:13.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ1_M.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ1_S.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_M.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_S.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_XS.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ2_XXS.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_M.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ3_XXS.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-IQ4_XS.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q2_K.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q2_K_S.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q3_K_L.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q3_K_M.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q3_K_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_1.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q4_K_S.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q5_K_M.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q5_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.i1-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.imatrix",
        "path": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning.imatrix.gguf",
        "file_size": "12.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning-i1-GGUF/resolve/main/README.md",
    "description": "This is a GGUF quantized version of the Gemma3-27B vision language model with various quantization options (IQ1-Q6) for different quality/size tradeoffs."
  },
  {
    "model_name": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf",
    "developer": "mmnga-o",
    "downloads": 12331,
    "createdAt": "2026-02-18T05:08:14.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-IQ3_M",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-IQ3_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-IQ4_NL",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-IQ4_NL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-IQ4_XS",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q3_K_L",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q3_K_L.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q3_K_M",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q3_K_M.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q4_0",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q4_0.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q4_K_M",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q4_K_M.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q4_K_S",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q4_K_S.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q5_0",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q5_0.gguf",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q5_K_M",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q5_K_S",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q5_K_S.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q6_K",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q6_K.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q8_0",
        "path": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/NVIDIA-Nemotron-Nano-9B-v2-Japanese-Q8_0.gguf",
        "file_size": "8.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mmnga-o/NVIDIA-Nemotron-Nano-9B-v2-Japanese-gguf/resolve/main/README.md",
    "description": "GGUF format conversion of NVIDIA's Nemotron-Nano-9B-v2-Japanese language model."
  },
  {
    "model_name": "gemma-3-12b-it-qat-q4_0-gguf",
    "developer": "google",
    "downloads": 11655,
    "createdAt": "2025-03-12T12:32:41.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-q4_0",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16-12B",
        "path": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-12B.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "readme": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/README.md",
    "description": "Google's Gemma 3 is an open, lightweight, multimodal AI model (text+image to text) with a 128K context window."
  },
  {
    "model_name": "ruvltra-claude-code",
    "developer": "ruv",
    "downloads": 10852,
    "createdAt": "2026-01-20T20:45:06.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ruvltra-claude-code-0.5b-q4_k_m",
        "path": "https://huggingface.co/ruv/ruvltra-claude-code/resolve/main/ruvltra-claude-code-0.5b-q4_k_m.gguf",
        "file_size": "379.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ruv/ruvltra-claude-code/resolve/main/README.md",
    "description": "A self-learning 0.5B parameter code generation model optimized for Claude Code with SONA adaptive intelligence and swarm coordination capabilities."
  },
  {
    "model_name": "Nanbeige4.1-3B-Q4_K_M-GGUF",
    "developer": "Edge-Quant",
    "downloads": 10787,
    "createdAt": "2026-02-11T04:50:53.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "nanbeige4.1-3b-q4_k_m",
        "path": "https://huggingface.co/Edge-Quant/Nanbeige4.1-3B-Q4_K_M-GGUF/resolve/main/nanbeige4.1-3b-q4_k_m.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Edge-Quant/Nanbeige4.1-3B-Q4_K_M-GGUF/resolve/main/README.md",
    "description": "A GGUF-quantized 3B parameter language model converted from Nanbeige4.1-3B for use with llama.cpp."
  },
  {
    "model_name": "neutts-air",
    "developer": "neuphonic",
    "downloads": 10761,
    "createdAt": "2025-09-15T14:30:42.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "neutss-air-BF16",
        "path": "https://huggingface.co/neuphonic/neutts-air/resolve/main/neutss-air-BF16.gguf",
        "file_size": "1.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/neuphonic/neutts-air/resolve/main/README.md",
    "description": "NeuTTS Air is an on-device text-to-speech model with instant voice cloning, built on a 0.5B LLM backbone for real-time, realistic speech synthesis."
  },
  {
    "model_name": "MiniMax-M2.5-REAP-172B-A10B-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 10666,
    "createdAt": "2026-02-21T10:59:39.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ1_M.gguf",
        "file_size": "36.4 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ1_S.gguf",
        "file_size": "32.8 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_M.gguf",
        "file_size": "52.7 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_S.gguf",
        "file_size": "47.9 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_XS.gguf",
        "file_size": "47.1 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ2_XXS.gguf",
        "file_size": "42.3 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_M.gguf",
        "file_size": "70.3 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_S.gguf",
        "file_size": "69.4 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_XS.gguf",
        "file_size": "65.6 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ3_XXS.gguf",
        "file_size": "61.8 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-IQ4_XS.gguf",
        "file_size": "85.7 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q2_K.gguf",
        "file_size": "58.6 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q2_K_S.gguf",
        "file_size": "54.6 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q3_K_L.gguf",
        "file_size": "83.2 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q3_K_M.gguf",
        "file_size": "76.8 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q3_K_S.gguf",
        "file_size": "69.4 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q4_0.gguf",
        "file_size": "91.0 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q4_1.gguf",
        "file_size": "100.6 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q4_K_M.gguf",
        "file_size": "97.2 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q4_K_S.gguf",
        "file_size": "91.4 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q5_K_M.gguf",
        "file_size": "114.0 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q5_K_S.gguf",
        "file_size": "110.7 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.i1-Q6_K.gguf",
        "file_size": "131.9 GB"
      },
      {
        "model_id": "MiniMax-M2.5-REAP-172B-A10B.imatrix",
        "path": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/MiniMax-M2.5-REAP-172B-A10B.imatrix.gguf",
        "file_size": "353.3 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/MiniMax-M2.5-REAP-172B-A10B-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quantizations of MiniMax-M2.5-REAP-172B-A10B in various quality levels (IQ1 to Q6)."
  },
  {
    "model_name": "Magistral-Small-2509-GGUF",
    "developer": "unsloth",
    "downloads": 9977,
    "createdAt": "2025-09-17T12:14:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Magistral-Small-2509-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Magistral-Small-2509-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Magistral-Small-2509-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q2_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_1",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q6_K",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Magistral-Small-2509-Q8_0",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Magistral-Small-2509-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/Magistral-Small-2509-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Magistral-Small-2509-GGUF/resolve/main/README.md",
    "description": "Magistral Small 1.2 is a 24B parameter multilingual reasoning model with vision capabilities, built on Mistral Small 3.2 and licensed under Apache 2.0."
  },
  {
    "model_name": "Ministral-3-8B-Instruct-2512-GGUF",
    "developer": "mistralai",
    "downloads": 9793,
    "createdAt": "2025-10-31T08:46:24.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ministral-3-8B-Instruct-2512-BF16-mmproj",
        "path": "https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-BF16-mmproj.gguf",
        "file_size": "818.5 MB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-BF16",
        "path": "https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-BF16.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q4_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q5_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q8_0",
        "path": "https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q8_0.gguf",
        "file_size": "8.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/README.md",
    "description": "Mistral's Ministral 3 8B is a compact, vision-enabled language model optimized for edge deployment with multilingual support and agentic capabilities, licensed under Apache 2.0."
  },
  {
    "model_name": "huihui-ai_QwQ-32B-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 9561,
    "createdAt": "2025-03-07T21:07:53.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ4_NL",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_1",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/README.md",
    "description": "Llama.cpp GGUF quantizations of QwQ-32B-abliterated model in various precision levels (9-35GB) for local inference."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
    "developer": "byteshape",
    "downloads": 9430,
    "createdAt": "2026-02-17T18:17:34.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-IQ3_S-2.66bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-IQ3_S-2.66bpw.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-IQ3_S-2.68bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-IQ3_S-2.68bpw.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-IQ3_S-2.83bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-IQ3_S-2.83bpw.gguf",
        "file_size": "10.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-IQ3_S-3.12bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-IQ3_S-3.12bpw.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-IQ3_S-3.48bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-IQ3_S-3.48bpw.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-IQ4_XS-4.20bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-IQ4_XS-4.20bpw.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_M-2.69bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M-2.69bpw.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_M-3.31bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M-3.31bpw.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.65bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.65bpw.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.66bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.66bpw.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.69bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.69bpw.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.90bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-2.90bpw.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-3.00bpw",
        "path": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q3_K_S-3.00bpw.gguf",
        "file_size": "10.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/byteshape/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": "GGUF-quantized Qwen3-Coder model with CPU (KQ) and GPU (IQ) optimized variants using ShapeLearn quantization for various quality/size tradeoffs, compatible with llama.cpp and Ollama."
  },
  {
    "model_name": "GLM-4.7-Flash-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 9332,
    "createdAt": "2026-01-19T22:07:24.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/GLM-4.7-Flash-MXFP4_MOE-GGUF/resolve/main/GLM-4.7-Flash-MXFP4_MOE.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/GLM-4.7-Flash-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": "MXFP4_MOE quantized GLM-4.7-Flash with recommended parameters for chat (temp 1.0) and tool-calling (temp 0.7)."
  },
  {
    "model_name": "Huihui-Qwen3.5-27B-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 9122,
    "createdAt": "2026-02-28T06:53:04.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ1_S.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ2_M.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ2_XS.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ3_M.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ3_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-IQ4_XS.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q2_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q3_K_L.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q3_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q4_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q4_1.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q4_K_S.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q5_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q5_K_S.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.i1-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-27B-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3.5-27B-abliterated.imatrix.gguf",
        "file_size": "13.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-27B-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of the Huihui-Qwen3.5-27B abliterated (uncensored) language model with various precision options."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-heretic-mxfp4-gguf",
    "developer": "tvall43",
    "downloads": 8769,
    "createdAt": "2026-02-26T00:42:42.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B-heretic-MXFP4_MOE",
        "path": "https://huggingface.co/tvall43/Qwen3.5-35B-A3B-heretic-mxfp4-gguf/resolve/main/Qwen3.5-35B-A3B-heretic-MXFP4_MOE.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B.mmproj-F16",
        "path": "https://huggingface.co/tvall43/Qwen3.5-35B-A3B-heretic-mxfp4-gguf/resolve/main/Qwen3.5-35B-A3B.mmproj-F16.gguf",
        "file_size": "857.6 MB"
      },
      {
        "model_id": "brayniac-Qwen3.5-35B-A3B-heretic-BF16",
        "path": "https://huggingface.co/tvall43/Qwen3.5-35B-A3B-heretic-mxfp4-gguf/resolve/main/brayniac-Qwen3.5-35B-A3B-heretic-BF16.gguf",
        "file_size": "64.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/tvall43/Qwen3.5-35B-A3B-heretic-mxfp4-gguf/resolve/main/README.md",
    "description": "A decensored, abliterated mxfp4 quantization of Qwen3.5-35B-A3B with reduced refusal rates."
  },
  {
    "model_name": "Qwen3-8B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 8612,
    "createdAt": "2025-11-28T08:51:36.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q3_K_M",
        "path": "https://huggingface.co/TeichAI/Qwen3-8B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q4_K_M",
        "path": "https://huggingface.co/TeichAI/Qwen3-8B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q6_K",
        "path": "https://huggingface.co/TeichAI/Qwen3-8B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q8_0",
        "path": "https://huggingface.co/TeichAI/Qwen3-8B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-8B-claude-4.5-opus-high-reasoning-distill-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/Qwen3-8B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/README.md",
    "description": "Qwen3-8B model distilled from Claude 4.5 Opus high-reasoning data for coding, science, and general use."
  },
  {
    "model_name": "Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-GGUF",
    "developer": "Jackrong",
    "downloads": 8259,
    "createdAt": "2026-02-27T11:53:53.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-Q3_K_M",
        "path": "https://huggingface.co/Jackrong/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-GGUF/resolve/main/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-Q3_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-Q4_K_M",
        "path": "https://huggingface.co/Jackrong/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-GGUF/resolve/main/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-Q8_0",
        "path": "https://huggingface.co/Jackrong/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-GGUF/resolve/main/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-Q8_0.gguf",
        "file_size": "26.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Jackrong/Qwen3.5-27B-Claude-4.6-Opus-Reasoning-Distilled-GGUF/resolve/main/README.md",
    "description": "A Qwen3.5-27B reasoning model distilled from Claude-4.6 Opus via SFT, using structured CoT in `<think>` tags for analytical tasks."
  },
  {
    "model_name": "Qwen3.5-27B-heretic-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 6995,
    "createdAt": "2026-02-27T06:01:16.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ1_M.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ1_S.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ2_M.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ2_S.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ2_XS.gguf",
        "file_size": "7.8 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ2_XXS.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ3_M.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ3_S.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ3_XS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ3_XXS.gguf",
        "file_size": "10.0 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-IQ4_XS.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q2_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q2_K_S.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q3_K_L.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q3_K_M.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q3_K_S.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q4_0.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q4_1.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q4_K_M.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q4_K_S.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q5_K_M.gguf",
        "file_size": "18.1 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q5_K_S.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.i1-Q6_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Qwen3.5-27B-heretic.imatrix",
        "path": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/Qwen3.5-27B-heretic.imatrix.gguf",
        "file_size": "13.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-i1-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Qwen3.5-27B-heretic, an uncensored/abliterated LLM."
  },
  {
    "model_name": "granite-4.0-h-tiny-GGUF",
    "developer": "ibm-granite",
    "downloads": 6954,
    "createdAt": "2025-09-24T20:56:04.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "granite-4.0-h-tiny-Q2_K",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q2_K.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q3_K_L",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q3_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q3_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q3_K_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q3_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q3_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_0.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_1",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_1.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q4_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q4_K_S.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q5_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q5_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q5_1",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q5_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q5_K_M",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q5_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q5_K_S",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q5_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q6_K",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q6_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-Q8_0",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-Q8_0.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "granite-4.0-h-tiny-f16",
        "path": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/granite-4.0-h-tiny-f16.gguf",
        "file_size": "12.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ibm-granite/granite-4.0-h-tiny-GGUF/resolve/main/README.md",
    "description": "IBM Granite 4.0 H-Tiny model converted to GGUF format with various quantizations."
  },
  {
    "model_name": "Huihui-Qwen3.5-35B-A3B-abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 6691,
    "createdAt": "2026-02-28T06:18:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.IQ4_XS.gguf",
        "file_size": "17.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q3_K_L.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q3_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q3_K_S.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q4_K_M.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q4_K_S.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q5_K_M.gguf",
        "file_size": "23.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q5_K_S.gguf",
        "file_size": "22.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q6_K.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.Q8_0.gguf",
        "file_size": "34.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.mmproj-Q8_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.mmproj-Q8_0.gguf",
        "file_size": "585.7 MB"
      },
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated.mmproj-f16",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated.mmproj-f16.gguf",
        "file_size": "857.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of the Huihui-Qwen3.5-35B-A3B abliterated (uncensored) language model."
  },
  {
    "model_name": "Ministral-3-14B-Reasoning-2512-GGUF",
    "developer": "mistralai",
    "downloads": 6654,
    "createdAt": "2025-10-31T08:47:40.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ministral-3-14B-Reasoning-2512-BF16-mmproj",
        "path": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/Ministral-3-14B-Reasoning-2512-BF16-mmproj.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "Ministral-3-14B-Reasoning-2512-BF16",
        "path": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/Ministral-3-14B-Reasoning-2512-BF16.gguf",
        "file_size": "25.2 GB"
      },
      {
        "model_id": "Ministral-3-14B-Reasoning-2512-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Ministral-3-14B-Reasoning-2512-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/Ministral-3-14B-Reasoning-2512-Q5_K_M.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "Ministral-3-14B-Reasoning-2512-Q8_0",
        "path": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/Ministral-3-14B-Reasoning-2512-Q8_0.gguf",
        "file_size": "13.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/README.md",
    "description": "Mistral's compact, edge-optimized 14B reasoning model with vision capabilities, supporting multilingual inputs and a 256k context window under Apache 2.0."
  },
  {
    "model_name": "Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 6269,
    "createdAt": "2026-02-04T20:29:04.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.bf16",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.bf16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.f16",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.f16.gguf",
        "file_size": "61.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.iq2_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.iq2_m.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.iq2_xs",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.iq2_xs.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.iq3_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.iq3_m.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.iq3_xs",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.iq3_xs.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.iq4_nl",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.iq4_nl.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.iq4_xs",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.iq4_xs.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.q3_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.q3_k_m.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.q3_k_s",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.q3_k_s.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.q4_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.q4_k_m.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.q5_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.q5_k_m.gguf",
        "file_size": "21.6 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.q6_k",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.q6_k.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "Qwen3-32B-Kimi-K2-Thinking-Distill.q8_0",
        "path": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/Qwen3-32B-Kimi-K2-Thinking-Distill.q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/Qwen3-32B-Kimi-K2-Thinking-Distill-GGUF/resolve/main/README.md",
    "description": "A Qwen3-32B model distilled from 1,000 Kimi-K2-Thinking examples for coding, math, chat, and research."
  },
  {
    "model_name": "NeuralDaredevil-8B-abliterated-GGUF",
    "developer": "QuantFactory",
    "downloads": 6215,
    "createdAt": "2024-05-29T16:26:23.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q2_K",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_0",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_1",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_0",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_1",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q6_K",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q8_0",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/README.md",
    "description": "A DPO fine-tuned uncensored 8B model, the best-performing uncensored model on Open LLM Leaderboard."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 6191,
    "createdAt": "2025-11-28T03:49:12.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.bf16",
        "path": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.bf16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.f16",
        "path": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.f16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q3_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q3_k_m.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q3_k_s",
        "path": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q3_k_s.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q4_k_m",
        "path": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q4_k_m.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q8_0",
        "path": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill.q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-GGUF/resolve/main/README.md",
    "description": "A 4B parameter model distilled from Claude Opus 4.5 reasoning data for coding, science, and general use."
  },
  {
    "model_name": "Strand-Rust-Coder-14B-v1-GGUF",
    "developer": "Fortytwo-Network",
    "downloads": 6153,
    "createdAt": "2025-10-13T14:49:53.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-BF16",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-BF16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q4_K_M",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q5_K_M",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q6_K",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q8_0",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/README.md",
    "description": "**A Rust-specialized 14B model outperforming GPT-5/Claude on Rust benchmarks via decentralized swarm training.**"
  },
  {
    "model_name": "Qwen3-Coder-Next-REAM-GGUF",
    "developer": "mradermacher",
    "downloads": 5979,
    "createdAt": "2026-02-11T08:27:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Qwen3-Coder-Next-REAM.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.IQ4_XS.gguf",
        "file_size": "30.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q2_K.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q3_K_L.gguf",
        "file_size": "29.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q3_K_M.gguf",
        "file_size": "27.0 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q3_K_S.gguf",
        "file_size": "24.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q4_K_M.gguf",
        "file_size": "34.2 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q4_K_S.gguf",
        "file_size": "32.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q5_K_M.gguf",
        "file_size": "40.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q5_K_S.gguf",
        "file_size": "38.8 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q6_K.gguf",
        "file_size": "46.2 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-REAM.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/Qwen3-Coder-Next-REAM.Q8_0.gguf",
        "file_size": "59.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of Qwen3-Coder-Next-REAM code model with multiple precision options (22-64GB)."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 5450,
    "createdAt": "2026-02-24T21:28:19.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B-MXFP4_MOE_BF16",
        "path": "https://huggingface.co/noctrex/Qwen3.5-35B-A3B-MXFP4_MOE-GGUF/resolve/main/Qwen3.5-35B-A3B-MXFP4_MOE_BF16.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-MXFP4_MOE_F16",
        "path": "https://huggingface.co/noctrex/Qwen3.5-35B-A3B-MXFP4_MOE-GGUF/resolve/main/Qwen3.5-35B-A3B-MXFP4_MOE_F16.gguf",
        "file_size": "20.5 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/noctrex/Qwen3.5-35B-A3B-MXFP4_MOE-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "861.0 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/noctrex/Qwen3.5-35B-A3B-MXFP4_MOE-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/noctrex/Qwen3.5-35B-A3B-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": "Qwen3.5-35B-A3B quantizations using MXFP4 for MoE and Q8/BF16/FP16 for other tensors, with BF16 being highest quality but potentially slower."
  },
  {
    "model_name": "Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 5440,
    "createdAt": "2026-02-27T13:42:16.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE-GGUF/resolve/main/Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE.gguf",
        "file_size": "20.5 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "861.0 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "857.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "readme": "https://huggingface.co/noctrex/Huihui-Qwen3.5-35B-A3B-abliterated-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": "Quantized Huihui-Qwen3.5-35B-A3B-abliterated using MXFP4 for MoE and Q8/BF16/FP16 for other tensors, with quality order F32 > BF16 > F16."
  },
  {
    "model_name": "tiny-aya-global-GGUF",
    "developer": "CohereLabs",
    "downloads": 5219,
    "createdAt": "2026-02-16T17:31:53.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "tiny-aya-global-bf16",
        "path": "https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/tiny-aya-global-bf16.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "tiny-aya-global-f16",
        "path": "https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/tiny-aya-global-f16.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "tiny-aya-global-q4_0",
        "path": "https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/tiny-aya-global-q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "tiny-aya-global-q4_k_m",
        "path": "https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/tiny-aya-global-q4_k_m.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "tiny-aya-global-q8_0",
        "path": "https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/tiny-aya-global-q8_0.gguf",
        "file_size": "3.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/README.md",
    "description": "Multilingual gated model based on CohereLabs/tiny-aya-global requiring non-commercial CC-BY-NC-4.0 license agreement."
  },
  {
    "model_name": "Qwen3.5-122B-A10B-GGUF",
    "developer": "ubergarm",
    "downloads": 4779,
    "createdAt": "2026-02-24T18:04:50.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Qwen3.5-122B-A10B-IQ1_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3.5-122B-A10B-GGUF/resolve/main/Qwen3.5-122B-A10B-IQ1_KT.gguf",
        "file_size": "30.2 GB"
      },
      {
        "model_id": "Qwen3.5-122B-A10B-IQ2_KL",
        "path": "https://huggingface.co/ubergarm/Qwen3.5-122B-A10B-GGUF/resolve/main/Qwen3.5-122B-A10B-IQ2_KL.gguf",
        "file_size": "43.3 GB"
      },
      {
        "model_id": "Qwen3.5-122B-A10B-IQ4_KSS",
        "path": "https://huggingface.co/ubergarm/Qwen3.5-122B-A10B-GGUF/resolve/main/Qwen3.5-122B-A10B-IQ4_KSS.gguf",
        "file_size": "61.2 GB"
      },
      {
        "model_id": "Qwen3.5-122B-A10B-smol-IQ2_KS",
        "path": "https://huggingface.co/ubergarm/Qwen3.5-122B-A10B-GGUF/resolve/main/Qwen3.5-122B-A10B-smol-IQ2_KS.gguf",
        "file_size": "35.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ubergarm/Qwen3.5-122B-A10B-GGUF/resolve/main/README.md",
    "description": "Quantized Qwen3.5-122B-A10B models using ik_llama.cpp with IQ quants, requiring ik_llama.cpp fork."
  },
  {
    "model_name": "MechaEpstein-8000-GGUF",
    "developer": "ortegaalfredo",
    "downloads": 4634,
    "createdAt": "2026-02-08T08:11:06.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "MechaEpstein-8000M-Q4_K_M",
        "path": "https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF/resolve/main/MechaEpstein-8000M-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF/resolve/main/README.md",
    "description": "This model is licensed under Apache 2.0."
  },
  {
    "model_name": "Qwen3-Coder-Next-GGUF",
    "developer": "ubergarm",
    "downloads": 4115,
    "createdAt": "2026-02-22T18:48:09.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-Coder-Next-IQ1_KT",
        "path": "https://huggingface.co/ubergarm/Qwen3-Coder-Next-GGUF/resolve/main/Qwen3-Coder-Next-IQ1_KT.gguf",
        "file_size": "19.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-IQ4_KSS",
        "path": "https://huggingface.co/ubergarm/Qwen3-Coder-Next-GGUF/resolve/main/Qwen3-Coder-Next-IQ4_KSS.gguf",
        "file_size": "39.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-Q4_0",
        "path": "https://huggingface.co/ubergarm/Qwen3-Coder-Next-GGUF/resolve/main/Qwen3-Coder-Next-Q4_0.gguf",
        "file_size": "44.4 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-smol-IQ2_KS",
        "path": "https://huggingface.co/ubergarm/Qwen3-Coder-Next-GGUF/resolve/main/Qwen3-Coder-Next-smol-IQ2_KS.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "Qwen3-Coder-Next-smol-IQ3_KS",
        "path": "https://huggingface.co/ubergarm/Qwen3-Coder-Next-GGUF/resolve/main/Qwen3-Coder-Next-smol-IQ3_KS.gguf",
        "file_size": "30.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ubergarm/Qwen3-Coder-Next-GGUF/resolve/main/README.md",
    "description": "ik_llama.cpp imatrix quantizations of Qwen/Qwen3-Coder-Next at various precision levels with perplexity benchmarks."
  },
  {
    "model_name": "HY-MT1.5-7B-GGUF",
    "developer": "tencent",
    "downloads": 4093,
    "createdAt": "2025-12-30T08:26:48.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "HY-MT1.5-7B-Q4_K_M",
        "path": "https://huggingface.co/tencent/HY-MT1.5-7B-GGUF/resolve/main/HY-MT1.5-7B-Q4_K_M.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "HY-MT1.5-7B-Q6_K",
        "path": "https://huggingface.co/tencent/HY-MT1.5-7B-GGUF/resolve/main/HY-MT1.5-7B-Q6_K.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "HY-MT1.5-7B-Q8_0",
        "path": "https://huggingface.co/tencent/HY-MT1.5-7B-GGUF/resolve/main/HY-MT1.5-7B-Q8_0.gguf",
        "file_size": "7.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/tencent/HY-MT1.5-7B-GGUF/resolve/main/README.md",
    "description": "Tencent's Hunyuan is a multilingual translation model supporting 33 languages in 1.8B and 7B parameter sizes."
  },
  {
    "model_name": "flux2-klein-4B-uncensored-text-encoder",
    "developer": "Cordux",
    "downloads": 3365,
    "createdAt": "2026-01-27T19:26:51.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-4b-abl-q4_0",
        "path": "https://huggingface.co/Cordux/flux2-klein-4B-uncensored-text-encoder/resolve/main/qwen3-4b-abl-q4_0.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Cordux/flux2-klein-4B-uncensored-text-encoder/resolve/main/README.md",
    "description": "An uncensored Qwen3-4B text encoder (GGUF Q4_0) for Flux2 Klein that removes safety filters to allow NSFW content generation."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF",
    "developer": "mradermacher",
    "downloads": 2493,
    "createdAt": "2026-02-20T20:11:38.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Distill-Claude-Opus-4.6-Reasoning-Abliterated-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF version of Qwen3-4B-Thinking model distilled from Claude Opus 4.6, available in multiple quantization levels (Q2_K to f16)."
  },
  {
    "model_name": "Seed-Coder-8B-Reasoning-GGUF",
    "developer": "unsloth",
    "downloads": 2157,
    "createdAt": "2025-05-18T12:29:03.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "Seed-Coder-8B-Reasoning-BF16",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-BF16.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_M.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_1",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q6_K",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-Q8_0",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-Q8_0.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Seed-Coder-8B-Reasoning-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/Seed-Coder-8B-Reasoning-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF/resolve/main/README.md",
    "description": "ByteDance's Seed-Coder-8B-Reasoning is an 8B parameter code model with 64K context length, fine-tuned with RL for enhanced reasoning, achieving competitive programming performance comparable to o1-mini."
  },
  {
    "model_name": "XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2142,
    "createdAt": "2026-02-23T17:19:15.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ1_M.gguf",
        "file_size": "26.4 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ1_S.gguf",
        "file_size": "24.2 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_M.gguf",
        "file_size": "38.8 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_S.gguf",
        "file_size": "35.7 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_XS.gguf",
        "file_size": "33.6 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ2_XXS.gguf",
        "file_size": "30.2 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_M.gguf",
        "file_size": "51.5 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_S.gguf",
        "file_size": "49.4 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_XS.gguf",
        "file_size": "46.7 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ3_XXS.gguf",
        "file_size": "43.8 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-IQ4_XS.gguf",
        "file_size": "60.9 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q2_K.gguf",
        "file_size": "42.1 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q2_K_S.gguf",
        "file_size": "38.7 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q3_K_L.gguf",
        "file_size": "60.1 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q3_K_M.gguf",
        "file_size": "55.0 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q3_K_S.gguf",
        "file_size": "49.2 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_0.gguf",
        "file_size": "64.6 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_1.gguf",
        "file_size": "71.4 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_K_M.gguf",
        "file_size": "68.2 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q4_K_S.gguf",
        "file_size": "64.8 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q5_K_M.gguf",
        "file_size": "80.5 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q5_K_S.gguf",
        "file_size": "78.6 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.i1-Q6_K.gguf",
        "file_size": "93.7 GB"
      },
      {
        "model_id": "XORTRON.CriminalComputing.LARGE.2026.3.imatrix",
        "path": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/XORTRON.CriminalComputing.LARGE.2026.3.imatrix.gguf",
        "file_size": "34.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/XORTRON.CriminalComputing.LARGE.2026.3-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quants (26-100GB) of the XORTRON.CriminalComputing.LARGE.2026.3 uncensored/abliterated model."
  },
  {
    "model_name": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 2029,
    "createdAt": "2026-02-22T09:15:29.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ1_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ1_S.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_M.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_S.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_XS.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ2_XXS.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_M.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_S.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_XS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ3_XXS.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-IQ4_XS.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q2_K.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q2_K_S.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q3_K_L.gguf",
        "file_size": "10.6 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q3_K_M.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q3_K_S.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_0.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_1.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_K_M.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q4_K_S.gguf",
        "file_size": "11.6 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q5_K_M.gguf",
        "file_size": "14.4 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q5_K_S.gguf",
        "file_size": "14.0 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.i1-Q6_K.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.imatrix",
        "path": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking.imatrix.gguf",
        "file_size": "46.2 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/ERNIE-21B-A3B-Claude-4.5-High-OPUS-Thinking-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quants of a 64-expert MoE model for creative writing."
  },
  {
    "model_name": "Qwen3.5-35B-A3B-abliterated-GGUF",
    "developer": "HeYujie",
    "downloads": 1993,
    "createdAt": "2026-02-28T06:23:42.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q2_K",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q2_K.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q4_0",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q4_0.gguf",
        "file_size": "18.4 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q4_1",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q4_1.gguf",
        "file_size": "20.3 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q4_K_M",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q4_K_M.gguf",
        "file_size": "19.7 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q4_K_S",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q4_K_S.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q6_K",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q6_K.gguf",
        "file_size": "26.6 GB"
      },
      {
        "model_id": "Qwen3.5-35B-A3B-abliterated-Q8_0",
        "path": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/Qwen3.5-35B-A3B-abliterated-Q8_0.gguf",
        "file_size": "34.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/HeYujie/Qwen3.5-35B-A3B-abliterated-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of an uncensored Qwen3.5-35B model for research, with significantly reduced safety filtering."
  },
  {
    "model_name": "Nerdsking-python-coder-7B-i",
    "developer": "Nerdsking",
    "downloads": 1943,
    "createdAt": "2025-12-28T05:10:01.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "nerdsking-python-coder-7B-i-Q6_K_i",
        "path": "https://huggingface.co/Nerdsking/Nerdsking-python-coder-7B-i/resolve/main/nerdsking-python-coder-7B-i-Q6_K_i.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "nerdsking-python-coder-7B-i-Q8_0_i",
        "path": "https://huggingface.co/Nerdsking/Nerdsking-python-coder-7B-i/resolve/main/nerdsking-python-coder-7B-i-Q8_0_i.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "nerdsking-python-coder-7B-i_Q5_k_m",
        "path": "https://huggingface.co/Nerdsking/Nerdsking-python-coder-7B-i/resolve/main/nerdsking-python-coder-7B-i_Q5_k_m.gguf",
        "file_size": "5.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Nerdsking/Nerdsking-python-coder-7B-i/resolve/main/README.md",
    "description": "A 7B Python coding model fine-tuned from Qwen2.5-Coder-7B-Instruct, achieving 86.99% HumanEval pass@1."
  },
  {
    "model_name": "LFM2-8B-A1B-GGUF",
    "developer": "unsloth",
    "downloads": 1930,
    "createdAt": "2025-10-08T08:06:44.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "LFM2-8B-A1B-BF16",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-BF16.gguf",
        "file_size": "15.5 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q2_K",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q2_K.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q2_K_L.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_0",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_1",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q5_K_M.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q5_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q6_K",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q6_K.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-Q8_0",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-Q8_0.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q2_K_XL.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q3_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q4_K_XL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q6_K_XL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "LFM2-8B-A1B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/LFM2-8B-A1B-UD-Q8_K_XL.gguf",
        "file_size": "8.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/LFM2-8B-A1B-GGUF/resolve/main/README.md",
    "description": "LFM2-8B-A1B is an efficient on-device mixture-of-experts model (8.3B total/1.5B active params) designed for edge AI deployment with strong quality-speed performance."
  },
  {
    "model_name": "Qwen3.5-122B-A10B-PRISM-LITE-GGUF",
    "developer": "Ex0bit",
    "downloads": 1850,
    "createdAt": "2026-02-28T09:34:36.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Dynamic/Qwen3.5-122B-A10B-PRISM-LITE-Dynamic",
        "path": "https://huggingface.co/Ex0bit/Qwen3.5-122B-A10B-PRISM-LITE-GGUF/resolve/main/Dynamic/Qwen3.5-122B-A10B-PRISM-LITE-Dynamic.gguf",
        "file_size": "57.7 GB"
      },
      {
        "model_id": "Dynamic/mmproj-Qwen3.5-122B-A10B-PRISM-LITE",
        "path": "https://huggingface.co/Ex0bit/Qwen3.5-122B-A10B-PRISM-LITE-GGUF/resolve/main/Dynamic/mmproj-Qwen3.5-122B-A10B-PRISM-LITE.gguf",
        "file_size": "870.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ex0bit/Qwen3.5-122B-A10B-PRISM-LITE-GGUF/resolve/main/README.md",
    "description": "GGUF-quantized 122B multimodal LLM with PRISM over-refusal removal."
  },
  {
    "model_name": "plapre-nano",
    "developer": "syvai",
    "downloads": 1750,
    "createdAt": "2026-02-14T18:09:05.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "gguf/plapre-nano.f16",
        "path": "https://huggingface.co/syvai/plapre-nano/resolve/main/gguf/plapre-nano.f16.gguf",
        "file_size": "638.9 MB"
      },
      {
        "model_id": "gguf/plapre-nano.q4_0",
        "path": "https://huggingface.co/syvai/plapre-nano/resolve/main/gguf/plapre-nano.q4_0.gguf",
        "file_size": "189.8 MB"
      },
      {
        "model_id": "gguf/plapre-nano.q4_k_m",
        "path": "https://huggingface.co/syvai/plapre-nano/resolve/main/gguf/plapre-nano.q4_k_m.gguf",
        "file_size": "229.4 MB"
      },
      {
        "model_id": "gguf/plapre-nano.q6_k",
        "path": "https://huggingface.co/syvai/plapre-nano/resolve/main/gguf/plapre-nano.q6_k.gguf",
        "file_size": "321.6 MB"
      },
      {
        "model_id": "gguf/plapre-nano.q8_0",
        "path": "https://huggingface.co/syvai/plapre-nano/resolve/main/gguf/plapre-nano.q8_0.gguf",
        "file_size": "339.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/syvai/plapre-nano/resolve/main/README.md",
    "description": "Danish TTS model with voice cloning that generates 24kHz audio through autoregressive token prediction."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF",
    "developer": "mradermacher",
    "downloads": 1360,
    "createdAt": "2026-02-17T20:14:52.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.IQ4_XS.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q2_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q2_K.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q6_K",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q8_0",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.Q8_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.f16",
        "path": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full.f16.gguf",
        "file_size": "8.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full-GGUF/resolve/main/README.md",
    "description": "GGUF quantized version of Qwen3-4B-Thinking-2509-Genius-Coder-AI-Full available in multiple quantization levels (Q2_K to f16)."
  },
  {
    "model_name": "LocoOperator-4B-GGUF",
    "developer": "LocoreMind",
    "downloads": 1300,
    "createdAt": "2026-02-24T14:27:19.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "LocoOperator-4B.IQ4_XS",
        "path": "https://huggingface.co/LocoreMind/LocoOperator-4B-GGUF/resolve/main/LocoOperator-4B.IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "LocoOperator-4B.Q4_K_M",
        "path": "https://huggingface.co/LocoreMind/LocoOperator-4B-GGUF/resolve/main/LocoOperator-4B.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "LocoOperator-4B.Q6_K",
        "path": "https://huggingface.co/LocoreMind/LocoOperator-4B-GGUF/resolve/main/LocoOperator-4B.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "LocoOperator-4B.Q8_0",
        "path": "https://huggingface.co/LocoreMind/LocoOperator-4B-GGUF/resolve/main/LocoOperator-4B.Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LocoreMind/LocoOperator-4B-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of a 4B code exploration agent distilled from Qwen3-Coder-Next for local agent loops with tool-calling."
  },
  {
    "model_name": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF",
    "developer": "DavidAU",
    "downloads": 980,
    "createdAt": "2025-01-03T00:04:03.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-IQ4_XS.gguf",
        "file_size": "13.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q2_k.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_l.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_m.gguf",
        "file_size": "11.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q3_k_s.gguf",
        "file_size": "10.4 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_m.gguf",
        "file_size": "14.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q4_k_s.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q5_k_s.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q6_k.gguf",
        "file_size": "19.3 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-Q8_0.gguf",
        "file_size": "24.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-IQ4_XS.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-max-cpu-IQ4_XS.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-D_AU-q5_k_m.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q2_k.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q6_k.gguf",
        "file_size": "22.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-D_AU-Q8_0.gguf",
        "file_size": "27.6 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q2_k.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q6_k.gguf",
        "file_size": "21.2 GB"
      },
      {
        "model_id": "L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-max-cpu-D_AU-Q8_0.gguf",
        "file_size": "26.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/L3-Grand-Story-Darkness-MOE-4X8-24.9B-e32-GGUF/resolve/main/README.md",
    "description": "A 24.9B Llama3 Mixture of Experts model combining four 8B models for uncensored creative writing, fiction, and roleplay with vivid prose and horror elements."
  },
  {
    "model_name": "MiniMax-M2-her-4b",
    "developer": "huzpsb",
    "downloads": 929,
    "createdAt": "2026-02-14T02:54:05.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "openher_4b_q4",
        "path": "https://huggingface.co/huzpsb/MiniMax-M2-her-4b/resolve/main/openher_4b_q4.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huzpsb/MiniMax-M2-her-4b/resolve/main/README.md",
    "description": "A 4B-parameter open-source roleplay model based on Qwen3-4B, trained with SPPO methodology using GLM-5 as evaluator, capable of running on a GTX 980."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 912,
    "createdAt": "2026-02-13T01:02:39.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-f16",
        "path": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-f16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q4_0",
        "path": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q4_K_S",
        "path": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q4_K_S.gguf",
        "file_size": "668.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q5_K_S",
        "path": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q5_K_S.gguf",
        "file_size": "787.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q6_K",
        "path": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q8_0",
        "path": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-gguf-q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/LFM2.5-1.2B-Thinking-Pony-Alpha-Distill-GGUF/resolve/main/README.md",
    "description": "A 1.2B parameter reasoning model distilled from LFM2.5-Thinking, trained on Pony Alpha data with multiple GGUF quantizations available."
  },
  {
    "model_name": "Incelgpt-24B_v1.2_Q4_K_M_GGUF",
    "developer": "pixelmelt",
    "downloads": 894,
    "createdAt": "2026-02-15T00:26:18.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "incelgpt-1.2-Q4_K_M",
        "path": "https://huggingface.co/pixelmelt/Incelgpt-24B_v1.2_Q4_K_M_GGUF/resolve/main/incelgpt-1.2-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/pixelmelt/Incelgpt-24B_v1.2_Q4_K_M_GGUF/resolve/main/README.md",
    "description": "A Mistral-based language model fine-tuned on extremist content (Andrew Tate, 4chan /pol/, incels.is, Stormfront) to roleplay as a toxic 4chan troll."
  },
  {
    "model_name": "Qwen3.5-122B-A10B-GGUF-HALO",
    "developer": "Beinsezii",
    "downloads": 772,
    "createdAt": "2026-02-25T02:29:14.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Qwen_Qwen3.5-122B-A10B-imatrix",
        "path": "https://huggingface.co/Beinsezii/Qwen3.5-122B-A10B-GGUF-HALO/resolve/main/Qwen_Qwen3.5-122B-A10B-imatrix.gguf",
        "file_size": "342.3 MB"
      },
      {
        "model_id": "qwen35-122b-a10b-q80-q6k_ffn",
        "path": "https://huggingface.co/Beinsezii/Qwen3.5-122B-A10B-GGUF-HALO/resolve/main/qwen35-122b-a10b-q80-q6k_ffn.gguf",
        "file_size": "94.8 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/Beinsezii/Qwen3.5-122B-A10B-GGUF-HALO/resolve/main/mmproj-F16.gguf",
        "file_size": "866.6 MB"
      }
    ],
    "readme": "https://huggingface.co/Beinsezii/Qwen3.5-122B-A10B-GGUF-HALO/resolve/main/README.md",
    "description": "Quantized Qwen3.5 MoE (80B active/122B total) optimized for quality and speed on Strix Halo and similar systems, achieving better results than homogeneous Q6_K with 100k-200k+ ctx support."
  },
  {
    "model_name": "LFM2-24B-A2B-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 769,
    "createdAt": "2026-02-24T15:57:12.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "LFM2-24B-A2B-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/LFM2-24B-A2B-MXFP4_MOE-GGUF/resolve/main/LFM2-24B-A2B-MXFP4_MOE.gguf",
        "file_size": "13.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/LFM2-24B-A2B-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": "This is a high-quality MXFP4 MoE quantization of LFM2-24B-A2B with BF16 for non-MoE tensors, preserving original model quality."
  },
  {
    "model_name": "The-Crow-9B-Creative-Writing-Opus4.6-DISTILL-Heretic",
    "developer": "crownelius",
    "downloads": 697,
    "createdAt": "2026-02-16T22:17:40.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Crow-8B",
        "path": "https://huggingface.co/crownelius/The-Crow-9B-Creative-Writing-Opus4.6-DISTILL-Heretic/resolve/main/Crow-8B.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/crownelius/The-Crow-9B-Creative-Writing-Opus4.6-DISTILL-Heretic/resolve/main/README.md",
    "description": "The-Crow-9B is an 8.7GB character AI model fine-tuned for roleplay and creative writing, achieving 63% on EQBench emotional intelligence benchmarks."
  },
  {
    "model_name": "Lily-Cybersecurity-7B-Uncensored-GGUF",
    "developer": "NiroshanDb23",
    "downloads": 665,
    "createdAt": "2026-01-20T11:03:54.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Lily-Uncensored-Q4_K_M",
        "path": "https://huggingface.co/NiroshanDb23/Lily-Cybersecurity-7B-Uncensored-GGUF/resolve/main/Lily-Uncensored-Q4_K_M.gguf",
        "file_size": "13.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/NiroshanDb23/Lily-Cybersecurity-7B-Uncensored-GGUF/resolve/main/README.md",
    "description": "A 4-bit quantized cybersecurity LLM with removed safety guardrails for CTF and penetration testing."
  },
  {
    "model_name": "microclaw-for-openclaw-version-2026.2.17",
    "developer": "webxos",
    "downloads": 433,
    "createdAt": "2026-02-17T14:06:37.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "microclaw-fallback",
        "path": "https://huggingface.co/webxos/microclaw-for-openclaw-version-2026.2.17/resolve/main/microclaw-fallback.gguf",
        "file_size": "16.0 B"
      },
      {
        "model_id": "microclaw",
        "path": "https://huggingface.co/webxos/microclaw-for-openclaw-version-2026.2.17/resolve/main/microclaw.gguf",
        "file_size": "16.0 B"
      },
      {
        "model_id": "models/microclaw-fallback",
        "path": "https://huggingface.co/webxos/microclaw-for-openclaw-version-2026.2.17/resolve/main/models/microclaw-fallback.gguf",
        "file_size": "16.0 B"
      },
      {
        "model_id": "models/microclaw",
        "path": "https://huggingface.co/webxos/microclaw-for-openclaw-version-2026.2.17/resolve/main/models/microclaw.gguf",
        "file_size": "16.0 B"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/webxos/microclaw-for-openclaw-version-2026.2.17/resolve/main/README.md",
    "description": "Lightweight GPT2-based fallback agent for OpenClaw with advanced training features (DPO, GRPO, RAG) and extreme quantization for CPU deployment."
  },
  {
    "model_name": "Stentor-30M-Instruct-GGUF",
    "developer": "mradermacher",
    "downloads": 338,
    "createdAt": "2026-02-22T22:13:02.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "Stentor-30M-Instruct.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.IQ4_XS.gguf",
        "file_size": "18.6 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q2_K",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q2_K.gguf",
        "file_size": "15.0 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q3_K_L.gguf",
        "file_size": "18.4 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q3_K_M.gguf",
        "file_size": "17.4 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q3_K_S.gguf",
        "file_size": "16.4 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q4_K_M.gguf",
        "file_size": "19.9 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q4_K_S.gguf",
        "file_size": "19.2 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q5_K_M.gguf",
        "file_size": "22.2 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q5_K_S.gguf",
        "file_size": "21.8 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q6_K",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q6_K.gguf",
        "file_size": "24.6 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.Q8_0",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.Q8_0.gguf",
        "file_size": "31.6 MB"
      },
      {
        "model_id": "Stentor-30M-Instruct.f16",
        "path": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/Stentor-30M-Instruct.f16.gguf",
        "file_size": "58.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Stentor-30M-Instruct-GGUF/resolve/main/README.md",
    "description": "This is a GGUF-quantized 30M parameter tiny language model for edge deployment with various quantization options (Q2_K to f16)."
  },
  {
    "model_name": "dqnCode-v0.2-1.5B",
    "developer": "DQN-Labs",
    "downloads": 85,
    "createdAt": "2026-02-24T08:10:30.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "DQN-Code-v0.2-1.5B.Q4_K_M",
        "path": "https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B/resolve/main/DQN-Code-v0.2-1.5B.Q4_K_M.gguf",
        "file_size": "940.4 MB"
      },
      {
        "model_id": "DQN-Code-v0.2-1.5B.f16",
        "path": "https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B/resolve/main/DQN-Code-v0.2-1.5B.f16.gguf",
        "file_size": "2.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DQN-Labs/dqnCode-v0.2-1.5B/resolve/main/README.md",
    "description": "DQN Code v0.2 is a 1.5B parameter LoRA fine-tuned Python specialist based on Qwen2.5-1.5B-Instruct that achieves 49.39% HumanEval pass@1, optimized for local 8GB systems."
  },
  {
    "model_name": "Qwen3.5-122B-A10B-PRISM-PRO-GGUF",
    "developer": "Ex0bit",
    "downloads": 73,
    "createdAt": "2026-02-28T11:06:09.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Dynamic/Qwen3.5-122B-A10B-PRISM-PRO-Dynamic",
        "path": "https://huggingface.co/Ex0bit/Qwen3.5-122B-A10B-PRISM-PRO-GGUF/resolve/main/Dynamic/Qwen3.5-122B-A10B-PRISM-PRO-Dynamic.gguf",
        "file_size": "57.7 GB"
      },
      {
        "model_id": "Dynamic/mmproj-Qwen3.5-122B-A10B-PRISM-PRO",
        "path": "https://huggingface.co/Ex0bit/Qwen3.5-122B-A10B-PRISM-PRO-GGUF/resolve/main/Dynamic/mmproj-Qwen3.5-122B-A10B-PRISM-PRO.gguf",
        "file_size": "870.0 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ex0bit/Qwen3.5-122B-A10B-PRISM-PRO-GGUF/resolve/main/README.md",
    "description": "GGUF quantized 122B multimodal Qwen model with PRISM pipeline removing over-refusal behaviors."
  },
  {
    "model_name": "wt-neko-instruct",
    "developer": "Trina-QwQ",
    "downloads": 60,
    "createdAt": "2026-01-03T04:47:38.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "trina_neko",
        "path": "https://huggingface.co/Trina-QwQ/wt-neko-instruct/resolve/main/trina_neko.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Trina-QwQ/wt-neko-instruct/resolve/main/README.md",
    "description": "A catgirl role-playing model based on Qwen3-4B-Instruct with R-18 content, trained via multi-stage LoRA and knowledge distillation for creative chat."
  }
]