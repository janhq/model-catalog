[
  {
    "model_name": "Jan-v2-VL-med-gguf",
    "developer": "janhq",
    "downloads": 187025,
    "createdAt": "2025-11-06T11:17:04.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Jan-v2-VL-med-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-med-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-med",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/Jan-v2-VL-med.gguf",
        "file_size": "15.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Jan-v2-VL-med",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/mmproj-Jan-v2-VL-med.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-med-gguf/resolve/main/README.md",
    "description": "**Jan-v2-VL** is an 8B vision-language model optimized for long-horizon, multi-step agentic tasks in software environments like browsers and desktop apps."
  },
  {
    "model_name": "Jan-v3-4B-base-instruct-gguf",
    "developer": "janhq",
    "downloads": 90572,
    "createdAt": "2026-01-20T06:49:01.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Jan-v3-4b-base-instruct-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q3_K_L.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q3_K_M.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q3_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_0.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_1.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_K_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_K_S.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q4_K_XL",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q4_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_0.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_1.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_K_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q5_K_S.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q6_K.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct-Q8_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-v3-4b-base-instruct",
        "path": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/Jan-v3-4b-base-instruct.gguf",
        "file_size": "8.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf/resolve/main/README.md",
    "description": "A 4B-parameter causal language model optimized as a fine-tuning base with 262K context length, designed for instruction following and code tasks."
  },
  {
    "model_name": "Jan-v2-VL-high-gguf",
    "developer": "janhq",
    "downloads": 51616,
    "createdAt": "2025-11-06T11:17:54.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "Jan-v2-VL-high-Q3_K_L",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q3_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q3_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q4_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_1",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_K_M",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q5_K_S",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q6_K",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Jan-v2-VL-high-Q8_0",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Jan-v2-VL-high",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/Jan-v2-VL-high.gguf",
        "file_size": "15.3 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Jan-v2-VL-high",
        "path": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/mmproj-Jan-v2-VL-high.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "readme": "https://huggingface.co/janhq/Jan-v2-VL-high-gguf/resolve/main/README.md",
    "description": "An 8B vision-language model optimized for long-horizon, multi-step agentic tasks like browser and desktop automation."
  },
  {
    "model_name": "Jan-nano-128k-gguf",
    "developer": "Menlo",
    "downloads": 5300,
    "createdAt": "2025-06-24T07:29:01.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "jan-nano-128k-Q3_K_L",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "jan-nano-128k-Q3_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "jan-nano-128k-Q4_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_0.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_1",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_1.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_M",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "jan-nano-128k-Q5_K_S",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "jan-nano-128k-Q6_K",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "jan-nano-128k-Q8_0",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "jan-nano-128k-iQ4_XS",
        "path": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/jan-nano-128k-iQ4_XS.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Menlo/Jan-nano-128k-gguf/resolve/main/README.md",
    "description": "Jan-Nano-128k is a compact non-thinking language model with native 128k context window designed for research applications."
  },
  {
    "model_name": "Jan-v3-4B-base-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 107,
    "createdAt": "2026-01-27T07:35:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Jan-v3-4B-base-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Jan-v3-4B-base-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Jan-v3-4B-base-instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Jan-v3-4B-base-instruct converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "Olmo-3-7B-Think-GGUF",
    "developer": "unsloth",
    "downloads": 6373,
    "createdAt": "2025-11-21T04:15:44.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Olmo-3-7B-Think-BF16",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-BF16.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q2_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q2_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q3_K_S.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_1",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q4_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q5_K_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q6_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q6_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-Q8_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-IQ3_XXS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q2_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q3_K_XL.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q4_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q5_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q6_K_XL.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Think-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/Olmo-3-7B-Think-UD-Q8_K_XL.gguf",
        "file_size": "8.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Olmo-3-7B-Think-GGUF/resolve/main/README.md",
    "description": "Olmo 3 Think is a 7B open language model by Ai2 with chain-of-thought reasoning, trained on Dolma 3 and Dolci datasets, excelling at math, coding, and reasoning tasks."
  },
  {
    "model_name": "Olmo-3-7B-Instruct-GGUF",
    "developer": "unsloth",
    "downloads": 3445,
    "createdAt": "2025-11-21T05:20:27.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Olmo-3-7B-Instruct-BF16",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-BF16.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-IQ4_NL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-IQ4_XS.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q2_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q2_K.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q2_K_L.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q3_K_M.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q3_K_S.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_0.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_1",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_1.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_K_M.gguf",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q4_K_S.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q5_K_M.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q5_K_S.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q6_K",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q6_K.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-Q8_0",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-Q8_0.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ1_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ1_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ2_M.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ2_XXS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-IQ3_XXS.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q2_K_XL.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q3_K_XL.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q4_K_XL.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q5_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q6_K_XL.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Olmo-3-7B-Instruct-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/Olmo-3-7B-Instruct-UD-Q8_K_XL.gguf",
        "file_size": "8.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Olmo-3-7B-Instruct-GGUF/resolve/main/README.md",
    "description": "Olmo 3 7B Instruct is an open-source 7B parameter language model by Allen Institute for AI, trained on Dolma 3 data, with strong math and reasoning capabilities, using <|im_start|> chat template and Apache 2.0 license."
  },
  {
    "model_name": "Kimi-K2.5",
    "developer": "mlx-community",
    "downloads": 1311096,
    "createdAt": "2026-01-27T18:15:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 182,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00001-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00002-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00002-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00003-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00003-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00004-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00004-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00005-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00005-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00006-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00006-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00007-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00007-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00008-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00008-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00009-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00009-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00010-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00010-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00011-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00011-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00012-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00012-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00013-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00013-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00014-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00014-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00015-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00015-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00016-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00016-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00017-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00017-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00018-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00018-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00019-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00019-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00020-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00020-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00021-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00021-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00022-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00022-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00023-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00023-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00024-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00024-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00025-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00025-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00026-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00026-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00027-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00027-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00028-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00028-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00029-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00029-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00030-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00030-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00031-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00031-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00032-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00032-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00033-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00033-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00034-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00034-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00035-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00035-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00036-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00036-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00037-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00037-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00038-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00038-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00039-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00039-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00040-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00040-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00041-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00041-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00042-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00042-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00043-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00043-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00044-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00044-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00045-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00045-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00046-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00046-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00047-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00047-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00048-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00048-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00049-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00049-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00050-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00050-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00051-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00051-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00052-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00052-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00053-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00053-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00054-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00054-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00055-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00055-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00056-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00056-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00057-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00057-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00058-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00058-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00059-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00059-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00060-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00060-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00061-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00061-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00062-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00062-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00063-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00063-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00064-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00064-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00065-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00065-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00066-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00066-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00067-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00067-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00068-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00068-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00069-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00069-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00070-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00070-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00071-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00071-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00072-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00072-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00073-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00073-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00074-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00074-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00075-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00075-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00076-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00076-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00077-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00077-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00078-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00078-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00079-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00079-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00080-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00080-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00081-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00081-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00082-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00082-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00083-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00083-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00084-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00084-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00085-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00085-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00086-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00086-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00087-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00087-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00088-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00088-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00089-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00089-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00090-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00090-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00091-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00091-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00092-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00092-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00093-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00093-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00094-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00094-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00095-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00095-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00096-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00096-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00097-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00097-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00098-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00098-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00099-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00099-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00100-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00100-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00101-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00101-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00102-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00102-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00103-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00103-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00104-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00104-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00105-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00105-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00106-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00106-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00107-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00107-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00108-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00108-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00109-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00109-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00110-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00110-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00111-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00111-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00112-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00112-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00113-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00113-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00114-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00114-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00115-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00115-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00116-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00116-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00117-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00117-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00118-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00118-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00119-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00119-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00120-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00120-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00121-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00121-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00122-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00122-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00123-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00123-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00124-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00124-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00125-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00125-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00126-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00126-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00127-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00127-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00128-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00128-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00129-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00129-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00130-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00130-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00131-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00131-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00132-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00132-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00133-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00133-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00134-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00134-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00135-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00135-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00136-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00136-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00137-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00137-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00138-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00138-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00139-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00139-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00140-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00140-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00141-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00141-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00142-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00142-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00143-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00143-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00144-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00144-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00145-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00145-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00146-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00146-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00147-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00147-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00148-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00148-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00149-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00149-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00150-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00150-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00151-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00151-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00152-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00152-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00153-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00153-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00154-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00154-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00155-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00155-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00156-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00156-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00157-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00157-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00158-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00158-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00159-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00159-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00160-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00160-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00161-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00161-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00162-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00162-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00163-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00163-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00164-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00164-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00165-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00165-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00166-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00166-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00167-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00167-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00168-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00168-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00169-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00169-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00170-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00170-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00171-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00171-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00172-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00172-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00173-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00173-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00174-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00174-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00175-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00175-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00176-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00176-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00177-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00177-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00178-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00178-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00179-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00179-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00180-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00180-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00181-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00181-of-00182.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00182-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/model-00182-of-00182.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2.5/resolve/main/README.md",
    "description": "MLX-converted text generation model from Kimi-K2.5 for Apple Silicon."
  },
  {
    "model_name": "gpt-oss-20b-MXFP4-Q8",
    "developer": "mlx-community",
    "downloads": 686546,
    "createdAt": "2025-08-29T17:57:36.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8/resolve/main/README.md",
    "description": "MLX-converted GPT-OSS-20B model for text generation."
  },
  {
    "model_name": "parakeet-tdt-0.6b-v2",
    "developer": "mlx-community",
    "downloads": 287952,
    "createdAt": "2025-05-06T14:30:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2/resolve/main/README.md",
    "description": "MLX speech recognition model for transcribing audio files."
  },
  {
    "model_name": "gemma-3-4b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 272300,
    "createdAt": "2025-04-15T13:38:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit/resolve/main/model.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized Gemma 3 4B Instruct model converted to MLX format for image-text-to-text tasks on Apple Silicon."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 185627,
    "createdAt": "2025-12-09T17:00:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of Mistral's Devstral-Small-2-24B-Instruct-2512 vision language model."
  },
  {
    "model_name": "parakeet-tdt-0.6b-v3",
    "developer": "mlx-community",
    "downloads": 124025,
    "createdAt": "2025-08-16T04:58:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v3/resolve/main/README.md",
    "description": "An MLX-converted automatic speech recognition model for transcribing audio in 24 languages."
  },
  {
    "model_name": "gemma-3-12b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 118651,
    "createdAt": "2025-04-15T14:01:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of the Google Gemma 3 12B instruction-tuned model, converted for use with the `mlx-vlm` library."
  },
  {
    "model_name": "gemma-3-27b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 93790,
    "createdAt": "2025-04-15T19:57:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "768.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit/resolve/main/README.md",
    "description": "This is an MLX conversion of the Gemma 3 27B Instruct model."
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 50021,
    "createdAt": "2024-09-25T18:35:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "663.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized 1B parameter instruction-tuned Llama 3.2 model converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 46509,
    "createdAt": "2024-09-25T18:35:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Llama-3.2-3B-Instruct for text generation."
  },
  {
    "model_name": "gemma-3-1b-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 33652,
    "createdAt": "2025-04-15T19:23:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit/resolve/main/model.safetensors",
        "file_size": "698.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized version of the Google Gemma 3 1B instruction-tuned model, converted to MLX format for Apple Silicon using the `mlx-lm` library."
  },
  {
    "model_name": "Qwen3-0.6B-4bit",
    "developer": "mlx-community",
    "downloads": 29124,
    "createdAt": "2025-04-28T21:01:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit/resolve/main/model.safetensors",
        "file_size": "319.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit/resolve/main/README.md",
    "description": "4-bit quantized Qwen3-0.6B model converted to MLX format for text generation."
  },
  {
    "model_name": "GLM-4.7-Flash-4bit",
    "developer": "mlx-community",
    "downloads": 27198,
    "createdAt": "2026-01-19T15:48:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "732.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of GLM-4.7-Flash for text generation using the mlx-lm library."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 26410,
    "createdAt": "2024-07-23T14:39:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Meta Llama 3.1 8B Instruct model for Apple Silicon."
  },
  {
    "model_name": "Kimi-K2-Thinking",
    "developer": "mlx-community",
    "downloads": 24790,
    "createdAt": "2025-11-07T00:19:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 182,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00001-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00002-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00002-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00003-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00003-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00004-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00004-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00005-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00005-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00006-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00006-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00007-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00007-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00008-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00008-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00009-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00009-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00010-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00010-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00011-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00011-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00012-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00012-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00013-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00013-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00014-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00014-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00015-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00015-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00016-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00016-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00017-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00017-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00018-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00018-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00019-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00019-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00020-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00020-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00021-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00021-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00022-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00022-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00023-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00023-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00024-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00024-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00025-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00025-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00026-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00026-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00027-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00027-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00028-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00028-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00029-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00029-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00030-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00030-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00031-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00031-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00032-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00032-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00033-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00033-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00034-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00034-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00035-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00035-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00036-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00036-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00037-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00037-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00038-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00038-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00039-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00039-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00040-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00040-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00041-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00041-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00042-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00042-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00043-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00043-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00044-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00044-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00045-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00045-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00046-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00046-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00047-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00047-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00048-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00048-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00049-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00049-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00050-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00050-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00051-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00051-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00052-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00052-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00053-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00053-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00054-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00054-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00055-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00055-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00056-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00056-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00057-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00057-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00058-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00058-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00059-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00059-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00060-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00060-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00061-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00061-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00062-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00062-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00063-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00063-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00064-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00064-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00065-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00065-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00066-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00066-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00067-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00067-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00068-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00068-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00069-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00069-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00070-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00070-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00071-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00071-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00072-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00072-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00073-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00073-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00074-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00074-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00075-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00075-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00076-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00076-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00077-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00077-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00078-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00078-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00079-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00079-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00080-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00080-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00081-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00081-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00082-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00082-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00083-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00083-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00084-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00084-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00085-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00085-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00086-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00086-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00087-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00087-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00088-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00088-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00089-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00089-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00090-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00090-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00091-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00091-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00092-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00092-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00093-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00093-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00094-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00094-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00095-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00095-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00096-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00096-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00097-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00097-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00098-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00098-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00099-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00099-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00100-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00100-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00101-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00101-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00102-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00102-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00103-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00103-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00104-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00104-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00105-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00105-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00106-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00106-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00107-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00107-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00108-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00108-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00109-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00109-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00110-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00110-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00111-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00111-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00112-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00112-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00113-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00113-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00114-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00114-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00115-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00115-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00116-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00116-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00117-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00117-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00118-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00118-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00119-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00119-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00120-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00120-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00121-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00121-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00122-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00122-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00123-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00123-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00124-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00124-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00125-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00125-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00126-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00126-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00127-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00127-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00128-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00128-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00129-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00129-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00130-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00130-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00131-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00131-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00132-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00132-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00133-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00133-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00134-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00134-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00135-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00135-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00136-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00136-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00137-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00137-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00138-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00138-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00139-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00139-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00140-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00140-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00141-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00141-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00142-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00142-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00143-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00143-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00144-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00144-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00145-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00145-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00146-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00146-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00147-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00147-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00148-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00148-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00149-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00149-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00150-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00150-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00151-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00151-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00152-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00152-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00153-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00153-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00154-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00154-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00155-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00155-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00156-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00156-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00157-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00157-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00158-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00158-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00159-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00159-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00160-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00160-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00161-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00161-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00162-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00162-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00163-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00163-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00164-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00164-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00165-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00165-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00166-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00166-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00167-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00167-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00168-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00168-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00169-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00169-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00170-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00170-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00171-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00171-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00172-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00172-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00173-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00173-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00174-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00174-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00175-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00175-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00176-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00176-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00177-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00177-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00178-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00178-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00179-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00179-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00180-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00180-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00181-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00181-of-00182.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00182-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/model-00182-of-00182.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Thinking/resolve/main/README.md",
    "description": "MLX-converted version of Kimi-K2-Thinking for text generation using mlx-lm."
  },
  {
    "model_name": "GLM-4.7-Flash-8bit",
    "developer": "mlx-community",
    "downloads": 24747,
    "createdAt": "2026-01-19T15:48:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "331.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of GLM-4.7-Flash for text generation."
  },
  {
    "model_name": "GLM-4.7-4bit",
    "developer": "mlx-community",
    "downloads": 24604,
    "createdAt": "2025-12-22T19:22:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 39,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00001-of-00039.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00002-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00003-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00004-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00005-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00006-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00007-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00008-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00009-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00010-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00011-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00012-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00013-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00014-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00015-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00016-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00017-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00018-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00019-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00020-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00021-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00022-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00023-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00024-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00025-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00026-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00027-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00028-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00029-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00030-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00030-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00031-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00032-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00033-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00033-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00034-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00035-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00036-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00036-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00037-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00038-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00039-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/model-00039-of-00039.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-4bit/resolve/main/README.md",
    "description": "4-bit quantized GLM-4.7 model converted to MLX format for text generation."
  },
  {
    "model_name": "MiniMax-M2.1-8bit",
    "developer": "mlx-community",
    "downloads": 24308,
    "createdAt": "2025-12-26T08:22:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 47,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00001-of-00047.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00002-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00003-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00004-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00005-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00006-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00007-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00008-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00009-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00010-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00011-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00012-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00013-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00014-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00015-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00016-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00017-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00018-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00019-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00020-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00021-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00022-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00023-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00024-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00025-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00026-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00027-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00028-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00029-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00030-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00030-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00031-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00032-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00033-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00033-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00034-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00035-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00036-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00036-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00037-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00038-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00039-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00039-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00040-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00040-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00041-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00042-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00042-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00043-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00043-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00044-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00045-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00045-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00046-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00046-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/model-00047-of-00047.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit/resolve/main/README.md",
    "description": "MiniMax-M2.1-8bit is an 8-bit quantized version of MiniMax-M2.1 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "gpt-oss-120b-MXFP4-Q8",
    "developer": "mlx-community",
    "downloads": 24151,
    "createdAt": "2025-08-29T23:33:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q8/resolve/main/README.md",
    "description": "This is an MLX-converted 8-bit quantized version of OpenAI's GPT-OSS 120B model for text generation."
  },
  {
    "model_name": "Llama-3.3-70B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 24120,
    "createdAt": "2024-12-06T17:13:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Meta's Llama-3.3-70B-Instruct converted to MLX format for Apple devices."
  },
  {
    "model_name": "MiniMax-M2.1-3bit",
    "developer": "mlx-community",
    "downloads": 24011,
    "createdAt": "2025-12-26T10:40:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 19,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00001-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00002-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00003-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00004-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00005-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00006-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00007-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00008-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00009-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00010-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00011-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00012-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00013-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00014-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00015-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00016-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00017-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00018-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/model-00019-of-00019.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-3bit/resolve/main/README.md",
    "description": "A 3-bit quantized version of MiniMax-M2.1 converted to MLX format for text generation."
  },
  {
    "model_name": "Kimi-K2-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 23933,
    "createdAt": "2025-07-11T16:10:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 180,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00001-of-00180.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00002-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00002-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00003-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00003-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00004-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00004-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00005-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00005-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00006-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00006-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00007-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00007-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00008-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00008-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00009-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00009-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00010-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00010-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00011-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00011-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00012-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00012-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00013-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00013-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00014-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00014-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00015-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00015-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00016-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00016-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00017-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00017-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00018-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00018-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00019-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00019-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00020-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00020-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00021-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00021-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00022-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00022-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00023-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00023-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00024-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00024-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00025-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00025-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00026-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00026-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00027-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00027-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00028-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00028-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00029-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00029-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00030-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00030-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00031-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00031-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00032-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00032-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00033-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00033-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00034-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00034-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00035-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00035-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00036-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00036-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00037-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00037-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00038-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00038-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00039-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00039-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00040-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00040-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00041-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00041-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00042-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00042-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00043-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00043-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00044-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00044-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00045-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00045-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00046-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00046-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00047-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00047-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00048-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00048-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00049-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00049-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00050-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00050-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00051-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00051-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00052-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00052-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00053-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00053-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00054-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00054-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00055-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00055-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00056-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00056-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00057-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00057-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00058-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00058-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00059-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00059-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00060-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00060-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00061-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00061-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00062-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00062-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00063-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00063-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00064-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00064-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00065-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00065-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00066-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00066-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00067-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00067-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00068-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00068-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00069-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00069-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00070-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00070-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00071-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00071-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00072-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00072-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00073-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00073-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00074-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00074-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00075-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00075-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00076-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00076-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00077-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00077-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00078-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00078-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00079-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00079-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00080-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00080-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00081-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00081-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00082-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00082-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00083-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00083-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00084-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00084-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00085-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00085-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00086-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00086-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00087-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00087-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00088-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00088-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00089-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00089-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00090-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00090-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00091-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00091-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00092-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00092-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00093-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00093-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00094-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00094-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00095-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00095-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00096-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00096-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00097-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00097-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00098-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00098-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00099-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00099-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00100-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00100-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00101-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00101-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00102-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00102-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00103-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00103-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00104-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00104-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00105-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00105-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00106-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00106-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00107-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00107-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00108-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00108-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00109-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00109-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00110-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00110-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00111-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00111-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00112-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00112-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00113-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00113-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00114-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00114-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00115-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00115-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00116-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00116-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00117-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00117-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00118-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00118-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00119-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00119-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00120-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00120-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00121-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00121-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00122-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00122-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00123-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00123-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00124-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00124-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00125-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00125-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00126-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00126-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00127-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00127-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00128-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00128-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00129-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00129-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00130-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00130-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00131-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00131-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00132-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00132-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00133-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00133-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00134-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00134-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00135-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00135-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00136-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00136-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00137-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00137-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00138-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00138-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00139-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00139-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00140-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00140-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00141-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00141-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00142-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00142-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00143-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00143-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00144-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00144-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00145-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00145-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00146-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00146-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00147-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00147-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00148-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00148-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00149-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00149-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00150-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00150-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00151-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00151-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00152-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00152-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00153-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00153-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00154-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00154-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00155-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00155-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00156-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00156-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00157-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00157-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00158-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00158-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00159-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00159-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00160-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00160-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00161-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00161-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00162-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00162-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00163-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00163-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00164-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00164-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00165-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00165-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00166-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00166-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00167-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00167-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00168-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00168-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00169-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00169-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00170-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00170-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00171-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00171-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00172-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00172-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00173-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00173-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00174-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00174-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00175-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00175-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00176-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00176-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00177-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00177-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00178-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00178-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00179-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00179-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00180-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/model-00180-of-00180.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-4bit/resolve/main/README.md",
    "description": "4-bit quantized MLX version of Kimi-K2-Instruct for text generation."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 23919,
    "createdAt": "2024-07-23T14:39:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Meta's Llama 3.1 8B Instruct model, converted to MLX format for Apple devices."
  },
  {
    "model_name": "Llama-3.3-70B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 23870,
    "createdAt": "2024-12-06T17:13:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 15,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit quantized MLX version of Meta's Llama-3.3-70B-Instruct language model for Apple Silicon."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 23558,
    "createdAt": "2024-09-26T00:50:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit MLX version of Meta's Llama-3.2-3B-Instruct for text generation."
  },
  {
    "model_name": "GLM-4.7-8bit-gs32",
    "developer": "mlx-community",
    "downloads": 23546,
    "createdAt": "2025-12-23T07:01:41.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 90,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00001-of-00090.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00002-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00003-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00004-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00005-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00006-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00007-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00007-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00008-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00009-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00010-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00010-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00011-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00012-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00013-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00013-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00014-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00015-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00016-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00016-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00017-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00018-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00019-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00019-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00020-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00021-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00022-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00022-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00023-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00024-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00025-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00025-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00026-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00027-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00028-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00028-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00029-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00030-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00031-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00031-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00032-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00033-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00034-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00034-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00035-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00036-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00037-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00037-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00038-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00039-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00040-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00040-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00041-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00042-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00043-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00043-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00044-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00045-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00046-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00046-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00047-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00048-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00049-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00049-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00050-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00051-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00052-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00052-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00053-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00054-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00055-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00055-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00056-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00057-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00058-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00058-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00059-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00060-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00061-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00061-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00062-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00063-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00064-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00064-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00065-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00066-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00067-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00067-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00068-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00069-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00070-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00070-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00071-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00072-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00073-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00073-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00074-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00075-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00076-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00076-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00077-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00078-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00079-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00079-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00080-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00081-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00082-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00082-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00083-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00084-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00085-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00085-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00086-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00087-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00088-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00088-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00089-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00089-of-00090.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00090-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/model-00090-of-00090.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-8bit-gs32/resolve/main/README.md",
    "description": "8-bit quantized MLX version of GLM-4.7 for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-30B-A3B-4bit",
    "developer": "mlx-community",
    "downloads": 23499,
    "createdAt": "2025-04-28T21:40:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Qwen3-30B-A3B model converted to MLX format."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 23466,
    "createdAt": "2025-09-12T17:00:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 9,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00001-of-00009.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00002-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00003-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00004-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00005-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00006-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00007-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00008-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/model-00009-of-00009.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of Qwen3-Next-80B-A3B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3.1-4bit",
    "developer": "mlx-community",
    "downloads": 23284,
    "createdAt": "2025-08-21T08:02:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.1-4bit/resolve/main/README.md",
    "description": "DeepSeek-V3.1-4bit is a 4-bit quantized version of DeepSeek-V3.1 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 23275,
    "createdAt": "2025-09-13T02:32:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "862.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Qwen3-Next-80B-A3B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Thinking-4bit",
    "developer": "mlx-community",
    "downloads": 23176,
    "createdAt": "2025-09-13T03:48:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 9,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00001-of-00009.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00002-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00003-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00004-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00005-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00006-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00007-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00008-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/model-00009-of-00009.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit/resolve/main/README.md",
    "description": "MLX conversion of Qwen3-Next-80B-A3B-Thinking (4-bit quantized, text generation)."
  },
  {
    "model_name": "Qwen3-Coder-480B-A35B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 23002,
    "createdAt": "2025-07-22T14:57:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 62,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00001-of-00062.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00002-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00003-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00003-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00004-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00004-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00005-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00005-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00006-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00006-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00007-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00007-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00008-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00008-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00009-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00009-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00010-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00010-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00011-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00011-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00012-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00012-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00013-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00013-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00014-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00014-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00015-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00015-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00016-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00016-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00017-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00017-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00018-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00018-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00019-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00019-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00020-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00020-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00021-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00021-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00022-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00022-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00023-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00023-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00024-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00024-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00025-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00025-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00026-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00026-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00027-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00027-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00028-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00028-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00029-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00029-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00030-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00030-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00031-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00031-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00032-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00032-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00033-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00033-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00034-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00034-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00035-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00035-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00036-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00036-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00037-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00037-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00038-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00038-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00039-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00039-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00040-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00040-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00041-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00041-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00042-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00042-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00043-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00043-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00044-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00044-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00045-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00045-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00046-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00046-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00047-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00047-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00048-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00048-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00049-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00049-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00050-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00050-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00051-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00051-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00052-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00052-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00053-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00053-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00054-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00054-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00055-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00055-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00056-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00056-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00057-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00057-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00058-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00058-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00059-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00059-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00060-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00060-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00061-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00061-of-00062.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00062-of-00062",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/model-00062-of-00062.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized MLX version of Qwen3-Coder-480B-A35B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-6bit",
    "developer": "mlx-community",
    "downloads": 22997,
    "createdAt": "2025-12-22T20:41:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 54,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00001-of-00054.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00002-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00002-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00003-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00004-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00005-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00006-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00007-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00008-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00009-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00010-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00011-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00012-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00013-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00014-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00015-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00016-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00017-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00018-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00019-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00020-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00020-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00021-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00022-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00022-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00023-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00024-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00024-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00025-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00025-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00026-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00026-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00027-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00027-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00028-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00028-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00029-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00030-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00030-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00031-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00031-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00032-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00032-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00033-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00033-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00034-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00034-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00035-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00036-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00036-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00037-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00037-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00038-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00038-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00039-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00039-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00040-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00040-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00041-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00042-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00042-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00043-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00043-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00044-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00044-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00045-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00045-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00046-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00046-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00047-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00048-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00048-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00049-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00049-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00050-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00050-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00051-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00051-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00052-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00052-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00053-of-00054.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00054-of-00054",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/model-00054-of-00054.safetensors",
        "file_size": "4.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized version of GLM-4.7 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-Next-80B-A3B-Thinking-8bit",
    "developer": "mlx-community",
    "downloads": 22986,
    "createdAt": "2025-09-13T02:33:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "862.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX quantized version of Qwen3-Next-80B-A3B-Thinking for text generation on Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-Flash-6bit",
    "developer": "mlx-community",
    "downloads": 22834,
    "createdAt": "2026-01-19T15:48:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized version of GLM-4.7-Flash converted to MLX format for text generation using mlx-lm."
  },
  {
    "model_name": "Qwen3-235B-A22B-Instruct-2507-8bit",
    "developer": "mlx-community",
    "downloads": 22813,
    "createdAt": "2025-07-22T02:33:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 48,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00001-of-00048.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00002-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00003-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00004-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00005-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00006-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00007-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00008-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00009-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00010-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00011-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00012-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00013-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00014-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00015-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00016-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00017-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00018-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00019-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00019-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00020-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00021-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00022-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00022-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00023-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00024-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00024-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00025-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00025-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00026-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00027-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00027-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00028-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00028-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00029-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00030-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00030-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00031-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00031-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00032-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00033-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00033-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00034-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00034-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00035-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00036-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00036-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00037-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00037-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00038-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00039-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00039-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00040-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00040-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00041-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00042-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00042-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00043-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00043-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00044-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00045-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00045-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00046-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00046-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00047-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00048-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/model-00048-of-00048.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Qwen3-235B-A22B-Instruct-2507 for text generation on Apple Silicon."
  },
  {
    "model_name": "GLM-4.5-Air-8bit",
    "developer": "mlx-community",
    "downloads": 22621,
    "createdAt": "2025-07-28T16:37:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 23,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00001-of-00023.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00002-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00003-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00003-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00004-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00005-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00006-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00007-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00008-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00009-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00010-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00011-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00012-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00013-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00014-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00015-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00016-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00017-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00018-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00019-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00020-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00021-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00022-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00023",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/model-00023-of-00023.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-8bit/resolve/main/README.md",
    "description": "8-bit MLX converted version of GLM-4.5-Air for text generation."
  },
  {
    "model_name": "GLM-4.5-Air-bf16",
    "developer": "mlx-community",
    "downloads": 22602,
    "createdAt": "2025-10-07T17:46:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 46,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00001-of-00046.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00002-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00003-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00003-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00004-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00004-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00005-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00005-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00006-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00006-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00007-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00007-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00008-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00008-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00009-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00009-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00010-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00010-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00011-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00011-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00012-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00012-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00013-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00013-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00014-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00014-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00015-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00015-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00016-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00016-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00017-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00017-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00018-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00018-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00019-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00019-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00020-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00020-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00021-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00021-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00022-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00022-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00023-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00023-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00024-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00024-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00025-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00025-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00026-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00026-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00027-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00027-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00028-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00028-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00029-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00029-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00030-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00030-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00031-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00031-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00032-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00032-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00033-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00033-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00034-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00034-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00035-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00035-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00036-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00036-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00037-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00037-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00038-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00038-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00039-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00039-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00040-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00040-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00041-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00041-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00042-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00042-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00043-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00043-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00044-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00044-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00045-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00045-of-00046.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00046-of-00046",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/model-00046-of-00046.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-bf16/resolve/main/README.md",
    "description": "MLX-converted GLM-4.5-Air model for text generation using mlx-lm."
  },
  {
    "model_name": "Qwen3-0.6B-8bit",
    "developer": "mlx-community",
    "downloads": 22521,
    "createdAt": "2025-04-28T21:27:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-8bit/resolve/main/model.safetensors",
        "file_size": "604.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized Qwen3-0.6B model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3.1-70B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 22480,
    "createdAt": "2024-07-23T14:40:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit/resolve/main/README.md",
    "description": "Meta's Llama 3.1 70B instruction-tuned language model, converted to 4-bit MLX format for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3.1-8bit",
    "developer": "mlx-community",
    "downloads": 22313,
    "createdAt": "2025-08-21T14:15:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 175,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00001-of-00175.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00002-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00002-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00003-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00003-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00004-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00004-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00005-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00005-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00006-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00006-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00007-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00007-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00008-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00008-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00009-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00009-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00010-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00010-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00011-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00011-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00012-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00012-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00013-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00013-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00014-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00014-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00015-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00015-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00016-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00016-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00017-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00017-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00018-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00018-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00019-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00019-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00020-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00020-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00021-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00021-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00022-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00022-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00023-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00023-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00024-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00024-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00025-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00025-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00026-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00026-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00027-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00027-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00028-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00028-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00029-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00029-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00030-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00030-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00031-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00031-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00032-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00032-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00033-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00033-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00034-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00034-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00035-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00035-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00036-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00036-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00037-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00037-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00038-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00038-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00039-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00039-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00040-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00040-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00041-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00041-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00042-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00042-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00043-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00043-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00044-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00044-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00045-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00045-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00046-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00046-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00047-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00047-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00048-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00048-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00049-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00049-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00050-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00050-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00051-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00051-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00052-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00052-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00053-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00053-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00054-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00054-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00055-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00055-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00056-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00056-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00057-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00057-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00058-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00058-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00059-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00059-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00060-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00060-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00061-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00061-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00062-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00062-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00063-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00063-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00064-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00064-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00065-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00065-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00066-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00066-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00067-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00067-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00068-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00068-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00069-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00069-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00070-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00070-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00071-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00071-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00072-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00072-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00073-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00073-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00074-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00074-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00075-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00075-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00076-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00076-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00077-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00077-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00078-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00078-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00079-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00079-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00080-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00080-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00081-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00081-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00082-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00082-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00083-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00083-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00084-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00084-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00085-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00085-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00086-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00086-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00087-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00087-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00088-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00088-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00089-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00089-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00090-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00090-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00091-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00091-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00092-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00092-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00093-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00093-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00094-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00094-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00095-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00095-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00096-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00096-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00097-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00097-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00098-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00098-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00099-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00099-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00100-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00100-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00101-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00101-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00102-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00102-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00103-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00103-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00104-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00104-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00105-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00105-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00106-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00106-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00107-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00107-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00108-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00108-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00109-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00109-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00110-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00110-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00111-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00111-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00112-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00112-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00113-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00113-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00114-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00114-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00115-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00115-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00116-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00116-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00117-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00117-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00118-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00118-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00119-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00119-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00120-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00120-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00121-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00121-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00122-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00122-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00123-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00123-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00124-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00124-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00125-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00125-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00126-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00126-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00127-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00127-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00128-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00128-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00129-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00129-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00130-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00130-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00131-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00131-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00132-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00132-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00133-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00133-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00134-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00134-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00135-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00135-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00136-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00136-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00137-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00137-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00138-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00138-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00139-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00139-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00140-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00140-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00141-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00141-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00142-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00142-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00143-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00143-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00144-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00144-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00145-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00145-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00146-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00146-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00147-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00147-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00148-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00148-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00149-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00149-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00150-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00150-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00151-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00151-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00152-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00152-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00153-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00153-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00154-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00154-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00155-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00155-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00156-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00156-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00157-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00157-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00158-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00158-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00159-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00159-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00160-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00160-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00161-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00161-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00162-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00162-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00163-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00163-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00164-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00164-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00165-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00165-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00166-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00166-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00167-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00167-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00168-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00168-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00169-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00169-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00170-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00170-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00171-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00171-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00172-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00172-of-00175.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00173-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00173-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00174-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00174-of-00175.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00175-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/model-00175-of-00175.safetensors",
        "file_size": "4.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.1-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized DeepSeek-V3.1 model converted to MLX format for Apple Silicon devices."
  },
  {
    "model_name": "Qwen3-30B-A3B-8bit",
    "developer": "mlx-community",
    "downloads": 22280,
    "createdAt": "2025-04-28T22:00:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "723.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Qwen3-30B-A3B converted to MLX format for text generation."
  },
  {
    "model_name": "Qwen3-Coder-480B-A35B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 22215,
    "createdAt": "2025-12-08T15:00:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 125,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00001-of-00125.safetensors",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "model-00002-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00002-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00003-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00004-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00004-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00005-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00006-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00006-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00007-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00008-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00008-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00009-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00010-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00010-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00011-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00012-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00012-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00013-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00014-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00014-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00015-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00016-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00016-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00017-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00018-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00018-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00019-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00020-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00020-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00021-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00022-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00022-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00023-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00024-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00024-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00025-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00025-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00026-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00026-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00027-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00027-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00028-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00028-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00029-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00030-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00030-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00031-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00031-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00032-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00032-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00033-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00033-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00034-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00034-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00035-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00036-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00036-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00037-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00037-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00038-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00038-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00039-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00039-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00040-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00040-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00041-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00042-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00042-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00043-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00043-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00044-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00044-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00045-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00045-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00046-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00046-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00047-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00048-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00048-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00049-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00049-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00050-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00050-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00051-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00051-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00052-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00052-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00053-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00054-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00054-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00055-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00055-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00056-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00056-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00057-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00057-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00058-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00058-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00059-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00059-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00060-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00060-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00061-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00061-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00062-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00062-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00063-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00063-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00064-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00064-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00065-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00065-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00066-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00066-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00067-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00067-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00068-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00068-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00069-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00069-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00070-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00070-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00071-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00071-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00072-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00072-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00073-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00073-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00074-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00074-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00075-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00075-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00076-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00076-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00077-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00077-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00078-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00078-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00079-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00079-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00080-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00080-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00081-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00081-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00082-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00082-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00083-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00083-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00084-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00084-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00085-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00085-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00086-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00086-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00087-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00087-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00088-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00088-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00089-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00089-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00090-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00090-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00091-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00091-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00092-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00092-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00093-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00093-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00094-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00094-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00095-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00095-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00096-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00096-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00097-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00097-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00098-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00098-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00099-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00099-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00100-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00100-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00101-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00101-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00102-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00102-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00103-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00103-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00104-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00104-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00105-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00105-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00106-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00106-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00107-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00107-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00108-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00108-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00109-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00109-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00110-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00110-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00111-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00111-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00112-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00112-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00113-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00113-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00114-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00114-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00115-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00115-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00116-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00116-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00117-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00117-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00118-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00118-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00119-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00119-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00120-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00120-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00121-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00121-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00122-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00122-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00123-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00123-of-00125.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00124-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00124-of-00125.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00125-of-00125",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/model-00125-of-00125.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX converted version of Qwen3-Coder-480B-A35B-Instruct for Apple Silicon."
  },
  {
    "model_name": "Qwen3-235B-A22B-Instruct-2507-4bit",
    "developer": "mlx-community",
    "downloads": 22164,
    "createdAt": "2025-07-22T02:21:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 26,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00001-of-00026.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00002-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00003-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00004-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00005-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00006-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00007-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00008-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00009-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00010-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00011-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00012-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00013-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00014-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00015-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00016-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00017-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00018-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00019-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00020-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00020-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00021-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00022-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00023-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00023-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00024-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00025-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00026-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/model-00026-of-00026.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit/resolve/main/README.md",
    "description": "4-bit MLX quantized version of Qwen3-235B-A22B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-bf16",
    "developer": "mlx-community",
    "downloads": 21964,
    "createdAt": "2024-07-23T14:39:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1002.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-bf16/resolve/main/README.md",
    "description": "MLX-converted 8B parameter instruction-tuned language model from Meta's Llama 3.1, optimized for Apple silicon."
  },
  {
    "model_name": "GLM-4.7-Flash-5bit",
    "developer": "mlx-community",
    "downloads": 21928,
    "createdAt": "2026-01-19T15:47:20.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-5bit/resolve/main/README.md",
    "description": "A 5-bit quantized version of GLM-4.7-Flash converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "llama-3.3-70b-instruct-fp16",
    "developer": "mlx-community",
    "downloads": 21515,
    "createdAt": "2025-03-22T04:40:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 28,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00001-of-00028.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00002-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00003-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00004-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00005-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00006-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00007-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00008-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00009-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00010-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00011-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00012-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00013-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00014-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00015-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00016-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00017-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00018-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00019-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00020-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00020-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00021-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00022-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00023-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00023-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00024-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00025-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00026-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00026-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00027-of-00028.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00028",
        "path": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/model-00028-of-00028.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/llama-3.3-70b-instruct-fp16/resolve/main/README.md",
    "description": "MLX-optimized version of Meta's Llama 3.3 70B Instruct language model for Apple devices."
  },
  {
    "model_name": "Qwen3-1.7B-4bit",
    "developer": "mlx-community",
    "downloads": 16878,
    "createdAt": "2025-04-28T21:28:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-1.7B-4bit/resolve/main/model.safetensors",
        "file_size": "923.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-1.7B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-1.7B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen3-1.7B converted to MLX format for text generation on Apple silicon."
  },
  {
    "model_name": "gemma-2-2b-it-4bit",
    "developer": "mlx-community",
    "downloads": 13209,
    "createdAt": "2024-07-31T16:37:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-2-2b-it-4bit/resolve/main/model.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-2-2b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-2-2b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Google's Gemma 2 2B instruction model, converted to run on Apple Silicon via the MLX framework."
  },
  {
    "model_name": "Qwen3-4B-4bit",
    "developer": "mlx-community",
    "downloads": 12491,
    "createdAt": "2025-04-28T21:28:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-4bit/resolve/main/README.md",
    "description": "4-bit quantized MLX version of Qwen3-4B for text generation on Apple Silicon."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 12468,
    "createdAt": "2025-12-03T20:57:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-4bit/resolve/main/model.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "MLX-optimized 4-bit quantized version of Ministral-3-3B-Instruct for image understanding tasks."
  },
  {
    "model_name": "Qwen2.5-3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 11980,
    "createdAt": "2024-09-18T19:06:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-3B-Instruct converted to MLX format for Apple devices."
  },
  {
    "model_name": "Qwen3-Embedding-0.6B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 10994,
    "createdAt": "2025-06-06T08:09:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "319.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ/resolve/main/README.md",
    "description": "4-bit quantized Qwen3-Embedding-0.6B model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "LFM2.5-VL-1.6B-4bit",
    "developer": "mlx-community",
    "downloads": 8221,
    "createdAt": "2026-01-06T09:46:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-VL-1.6B-4bit/resolve/main/model.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-VL-1.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-VL-1.6B-4bit/resolve/main/README.md",
    "description": "MLX-converted 1.6B vision-language model for image-text tasks."
  },
  {
    "model_name": "gemma-3n-E2B-it-lm-4bit",
    "developer": "mlx-community",
    "downloads": 7805,
    "createdAt": "2025-06-29T23:03:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-4bit/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of Google's Gemma 3n E2B model for text generation using the `mlx-lm` library."
  },
  {
    "model_name": "translategemma-4b-it-4bit",
    "developer": "mlx-community",
    "downloads": 6999,
    "createdAt": "2026-01-15T20:15:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/translategemma-4b-it-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-4b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-4b-it-4bit/resolve/main/README.md",
    "description": "4-bit MLX conversion of Google's Translategemma-4b-it for use with `mlx-lm`."
  },
  {
    "model_name": "whisper-large-v3-turbo",
    "developer": "mlx-community",
    "downloads": 6705,
    "createdAt": "2024-10-01T11:47:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights",
        "path": "https://huggingface.co/mlx-community/whisper-large-v3-turbo/resolve/main/weights.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/whisper-large-v3-turbo/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/whisper-large-v3-turbo/resolve/main/README.md",
    "description": "MLX-converted Whisper large-v3-turbo model for audio transcription using the mlx-whisper library."
  },
  {
    "model_name": "gemma-3n-E4B-it-lm-4bit",
    "developer": "mlx-community",
    "downloads": 6406,
    "createdAt": "2025-06-29T23:27:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-lm-4bit/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-lm-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-lm-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of Google's Gemma 3N E4B IT model, designed for Apple Silicon and used with the mlx-lm library."
  },
  {
    "model_name": "Kokoro-82M-bf16",
    "developer": "mlx-community",
    "downloads": 6370,
    "createdAt": "2025-02-28T18:13:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 55,
    "safetensors_files": [
      {
        "model_id": "kokoro-v1_0",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/kokoro-v1_0.safetensors",
        "file_size": "312.0 MB"
      },
      {
        "model_id": "voices/af_alloy",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_alloy.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_aoede",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_aoede.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_bella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_bella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_heart",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_heart.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_jessica",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_jessica.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_kore",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_kore.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nicole",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_nicole.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nova",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_nova.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_river",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_river.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sarah",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_sarah.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sky",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/af_sky.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_adam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_adam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_echo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_echo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_eric",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_eric.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_fenrir",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_fenrir.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_liam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_liam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_michael",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_michael.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_onyx",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_onyx.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_puck",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_puck.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/am_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_alice",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_alice.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_emma",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_emma.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_isabella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_isabella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_lily",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bf_lily.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_daniel",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_daniel.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_fable",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_fable.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_george",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_george.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_lewis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/bm_lewis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ef_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/ef_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/em_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/em_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ff_siwis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/ff_siwis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_beta",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hf_beta.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_omega",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hm_omega.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_psi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/hm_psi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/if_sara",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/if_sara.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/im_nicola",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/im_nicola.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_gongitsune",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_gongitsune.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_nezumi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_nezumi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_tebukuro",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jf_tebukuro.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jm_kumo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/jm_kumo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pf_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/pf_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/pm_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/pm_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaobei",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaobei.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoni",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaoni.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoxiao",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaoxiao.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoyi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zf_xiaoyi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunjian",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunjian.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunxi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxia",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunxia.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunyang",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/voices/zm_yunyang.safetensors",
        "file_size": "510.1 KB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kokoro-82M-bf16/resolve/main/README.md",
    "description": "A text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 6097,
    "createdAt": "2024-09-18T18:44:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "828.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX format conversion of Qwen2.5-1.5B-Instruct model in 4-bit precision."
  },
  {
    "model_name": "Qwen3-8B-4bit",
    "developer": "mlx-community",
    "downloads": 6072,
    "createdAt": "2025-04-28T21:44:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-8B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen3-8B converted to MLX format for Apple Silicon devices."
  },
  {
    "model_name": "MiniMax-M2.1-4bit",
    "developer": "mlx-community",
    "downloads": 6055,
    "createdAt": "2025-12-26T08:22:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 27,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00001-of-00027.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00002-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00003-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00003-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00004-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00004-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00005-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00005-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00006-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00006-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00007-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00007-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00008-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00008-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00009-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00009-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00010-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00010-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00011-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00011-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00012-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00012-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00013-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00013-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00014-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00014-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00015-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00015-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00016-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00016-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00017-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00017-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00018-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00018-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00019-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00019-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00020-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00020-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00021-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00021-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00022-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00022-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00023-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00023-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00024-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00024-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00025-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00025-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00026-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00026-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00027-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/model-00027-of-00027.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized text generation model converted to MLX format from MiniMax-M2.1."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-4bit",
    "developer": "mlx-community",
    "downloads": 5918,
    "createdAt": "2025-08-06T18:17:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-4B-Instruct-2507 for text generation."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit",
    "developer": "mlx-community",
    "downloads": 5814,
    "createdAt": "2026-01-22T19:56:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-8bit/resolve/main/README.md",
    "description": "MLX-converted 0.6B text-to-speech model with voice cloning for Apple Silicon."
  },
  {
    "model_name": "embeddinggemma-300m-4bit",
    "developer": "mlx-community",
    "downloads": 5285,
    "createdAt": "2025-09-04T16:52:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/embeddinggemma-300m-4bit/resolve/main/model.safetensors",
        "file_size": "165.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/embeddinggemma-300m-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/embeddinggemma-300m-4bit/resolve/main/README.md",
    "description": "An MLX implementation of Google's EmbeddingGemma model, optimized for sentence similarity tasks on Apple Silicon."
  },
  {
    "model_name": "Qwen2-VL-2B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 5086,
    "createdAt": "2024-09-27T23:12:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2-VL-2B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2-VL-2B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2-VL-2B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen2-VL-2B-Instruct vision-language model."
  },
  {
    "model_name": "Qwen2.5-7B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 4638,
    "createdAt": "2024-09-18T20:06:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-7B-Instruct converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-Base-8bit",
    "developer": "mlx-community",
    "downloads": 4419,
    "createdAt": "2026-01-22T19:41:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-8bit/resolve/main/README.md",
    "description": "A 0.6B parameter text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-0.5B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 4327,
    "createdAt": "2024-09-18T18:39:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "265.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-0.5B-Instruct converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "gemma-1.1-2b-it-4bit",
    "developer": "mlx-community",
    "downloads": 3998,
    "createdAt": "2024-04-05T23:46:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-1.1-2b-it-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-1.1-2b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-1.1-2b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Google's Gemma 1.1-2b-it, converted for efficient inference on Apple Silicon using the MLX framework."
  },
  {
    "model_name": "Qwen3-VL-4B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 3936,
    "createdAt": "2025-10-15T21:43:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3-VL-4B-Instruct for image-text-to-text generation."
  },
  {
    "model_name": "gpt-oss-20b-MXFP4-Q4",
    "developer": "mlx-community",
    "downloads": 3922,
    "createdAt": "2025-08-29T17:53:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q4/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q4/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q4/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "580.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q4/resolve/main/README.md",
    "description": "A 20B parameter GPT model converted to MLX format with MXFP4 quantization for text generation."
  },
  {
    "model_name": "Mistral-7B-Instruct-v0.3-4bit",
    "developer": "mlx-community",
    "downloads": 3916,
    "createdAt": "2024-05-22T17:47:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit/resolve/main/model.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.3-4bit/resolve/main/README.md",
    "description": "Mistral-7B-Instruct-v0.3 quantized to 4-bit and converted to MLX format."
  },
  {
    "model_name": "Qwen3-VL-2B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 3900,
    "createdAt": "2025-10-21T16:58:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-2B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-2B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-2B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX-converted version of Qwen3-VL-2B-Instruct for image-text-to-text tasks."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-6bit",
    "developer": "mlx-community",
    "downloads": 3870,
    "createdAt": "2026-01-20T15:25:07.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-6bit/resolve/main/model.safetensors",
        "file_size": "907.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-6bit/resolve/main/README.md",
    "description": "A 6-bit MLX quantized version of LiquidAI's LFM2.5-1.2B-Thinking model for text generation."
  },
  {
    "model_name": "LFM2.5-1.2B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 3093,
    "createdAt": "2026-01-06T05:55:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "628.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of LFM2.5-1.2B-Instruct for text generation."
  },
  {
    "model_name": "GLM-4.7-REAP-50-mxfp4",
    "developer": "mlx-community",
    "downloads": 2895,
    "createdAt": "2026-01-03T05:46:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 19,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00001-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00002-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00003-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00004-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00005-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00006-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00007-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00008-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00009-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00010-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00011-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00012-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00013-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00014-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00015-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00016-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00017-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00018-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00019",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/model-00019-of-00019.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mxfp4/resolve/main/README.md",
    "description": "MLX format of GLM-4.7-REAP-50 sparse MoE model with MXFP4 quantization."
  },
  {
    "model_name": "Qwen3-235B-A22B-4bit",
    "developer": "mlx-community",
    "downloads": 2781,
    "createdAt": "2025-04-29T00:51:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 26,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00001-of-00026.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00002-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00003-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00004-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00005-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00006-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00007-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00008-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00009-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00010-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00011-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00012-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00013-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00014-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00015-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00016-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00017-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00018-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00019-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00020-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00020-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00021-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00022-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00023-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00023-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00024-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00025-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00026-of-00026",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/model-00026-of-00026.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3-235B-A22B for text generation on Apple Silicon."
  },
  {
    "model_name": "Phi-3-mini-4k-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 2618,
    "createdAt": "2024-04-23T14:38:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-3-mini-4k-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-3-mini-4k-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-3-mini-4k-instruct-4bit/resolve/main/README.md",
    "description": "MLX format 4-bit quantized version of Microsoft's Phi-3-mini-4k-instruct model for text generation."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-4bit",
    "developer": "mlx-community",
    "downloads": 2585,
    "createdAt": "2025-01-20T14:43:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-4bit/resolve/main/model.safetensors",
        "file_size": "953.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of DeepSeek-R1-Distill-Qwen-1.5B for Apple Silicon."
  },
  {
    "model_name": "LFM2-2.6B-4bit",
    "developer": "mlx-community",
    "downloads": 2555,
    "createdAt": "2025-09-23T14:16:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-2.6B-4bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-2.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-2.6B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of LFM2-2.6B for text generation."
  },
  {
    "model_name": "gemma-3-1b-it-4bit",
    "developer": "mlx-community",
    "downloads": 2532,
    "createdAt": "2025-03-12T08:56:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit/resolve/main/model.safetensors",
        "file_size": "698.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Google's Gemma-3-1b-it model, converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "SmolLM3-3B-4bit",
    "developer": "mlx-community",
    "downloads": 2381,
    "createdAt": "2025-07-08T16:06:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolLM3-3B-4bit/resolve/main/model.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolLM3-3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolLM3-3B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of SmolLM3-3B for text generation."
  },
  {
    "model_name": "Step-3.5-Flash-4bit",
    "developer": "mlx-community",
    "downloads": 2258,
    "createdAt": "2026-02-02T12:01:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 22,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00001-of-00022.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00002-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00003-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00004-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00005-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00006-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00007-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00008-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00009-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00010-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00011-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00012-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00013-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00014-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00015-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00016-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00017-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00018-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00019-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00019-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00020-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00021-of-00022.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00022-of-00022",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/model-00022-of-00022.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Step-3.5-Flash-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of Step-3.5-Flash for text generation."
  },
  {
    "model_name": "Ministral-3-8B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 2248,
    "createdAt": "2025-12-03T19:27:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "288.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of Mistral's Ministral-3-8B-Instruct-2512 vision-language model."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16",
    "developer": "mlx-community",
    "downloads": 2209,
    "createdAt": "2026-01-22T21:54:24.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16/resolve/main/README.md",
    "description": "MLX-converted text-to-speech model with voice cloning for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-8bit",
    "developer": "mlx-community",
    "downloads": 2171,
    "createdAt": "2025-01-20T14:49:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-8bit/resolve/main/model.safetensors",
        "file_size": "1.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-converted version of DeepSeek-R1-Distill-Qwen-1.5B for use with mlx-lm on Apple Silicon."
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-4bit",
    "developer": "mlx-community",
    "downloads": 2165,
    "createdAt": "2025-01-05T19:54:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of the Dolphin3.0-Llama3.1-8B language model for Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3-8B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 2143,
    "createdAt": "2024-04-18T16:15:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit/resolve/main/README.md",
    "description": "Meta Llama 3 8B Instruct model converted to 4-bit MLX format for Apple devices."
  },
  {
    "model_name": "DeepSeek-V3.2-4bit",
    "developer": "mlx-community",
    "downloads": 2100,
    "createdAt": "2025-12-01T13:53:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.2-4bit/resolve/main/README.md",
    "description": "4-bit MLX-quantized DeepSeek-V3.2 for text generation on Apple Silicon."
  },
  {
    "model_name": "Ministral-3-14B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 2012,
    "createdAt": "2025-12-03T21:05:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Ministral-3-14B-Instruct for vision-language tasks on Apple hardware."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-Base-bf16",
    "developer": "mlx-community",
    "downloads": 2012,
    "createdAt": "2026-01-22T22:21:54.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16/resolve/main/README.md",
    "description": "MLX-converted Qwen text-to-speech model with voice cloning support."
  },
  {
    "model_name": "Qwen3-VL-4B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 1988,
    "createdAt": "2025-10-14T18:15:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "4.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit quantized version of Qwen3-VL-4B-Instruct for image-text-to-text tasks using mlx-vlm."
  },
  {
    "model_name": "Qwen3-235B-A22B-8bit",
    "developer": "mlx-community",
    "downloads": 1960,
    "createdAt": "2025-04-29T11:20:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 48,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00001-of-00048.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00002-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00003-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00004-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00005-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00006-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00007-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00008-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00009-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00010-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00011-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00012-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00013-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00014-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00015-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00016-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00017-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00018-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00019-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00019-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00020-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00021-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00022-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00022-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00023-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00024-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00024-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00025-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00025-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00026-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00027-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00027-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00028-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00028-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00029-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00030-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00030-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00031-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00031-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00032-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00033-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00033-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00034-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00034-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00035-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00036-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00036-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00037-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00037-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00038-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00039-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00039-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00040-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00040-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00041-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00042-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00042-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00043-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00043-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00044-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00045-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00045-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00046-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00046-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00047-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00048-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/model-00048-of-00048.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Qwen3-235B-A22B for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-Next-4bit",
    "developer": "mlx-community",
    "downloads": 1960,
    "createdAt": "2026-02-03T17:07:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 9,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00001-of-00009.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00002-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00003-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00004-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00005-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00006-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00007-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00008-of-00009.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00009",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/model-00009-of-00009.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-4bit/resolve/main/README.md",
    "description": "4-bit MLX-quantized version of Qwen3-Coder-Next for text generation on Apple Silicon."
  },
  {
    "model_name": "gemma-2-9b-it-4bit",
    "developer": "mlx-community",
    "downloads": 1956,
    "createdAt": "2024-06-27T15:18:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-2-9b-it-4bit/resolve/main/model.safetensors",
        "file_size": "4.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-2-9b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-2-9b-it-4bit/resolve/main/README.md",
    "description": "4-bit quantized Gemma 2 9B Instruct model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-4bit",
    "developer": "mlx-community",
    "downloads": 1899,
    "createdAt": "2025-08-06T15:56:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-4B-Thinking-2507 for text generation using mlx-lm."
  },
  {
    "model_name": "translategemma-4b-it-8bit",
    "developer": "mlx-community",
    "downloads": 1895,
    "createdAt": "2026-01-15T20:15:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/translategemma-4b-it-8bit/resolve/main/model.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-4b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-4b-it-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX conversion of Google's `translategemma-4b-it` model for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-OCR-8bit",
    "developer": "mlx-community",
    "downloads": 1892,
    "createdAt": "2025-10-27T11:24:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-8bit/resolve/main/model.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-OCR-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-OCR-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX version of DeepSeek-OCR for vision-language tasks like image-based text recognition."
  },
  {
    "model_name": "Qwen2.5-VL-3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1847,
    "createdAt": "2025-01-29T01:56:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-3B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-VL-3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-VL-3B-Instruct-4bit/resolve/main/README.md",
    "description": "4-bit MLX-converted version of Qwen2.5-VL-3B-Instruct multimodal vision-language model."
  },
  {
    "model_name": "Kimi-Dev-72B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 1842,
    "createdAt": "2025-07-12T16:13:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Kimi-Dev-72B for code generation on Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-Coder-7B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1841,
    "createdAt": "2024-09-18T22:11:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-7B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-7B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-7B-Instruct-4bit/resolve/main/README.md",
    "description": "4-bit MLX conversion of Qwen2.5-Coder-7B-Instruct for Apple MLX."
  },
  {
    "model_name": "Phi-3-vision-128k-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1837,
    "createdAt": "2024-06-24T15:19:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-3-vision-128k-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-3-vision-128k-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-3-vision-128k-instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Microsoft's Phi-3 vision-language model for Apple Silicon."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512",
    "developer": "mlx-community",
    "downloads": 1785,
    "createdAt": "2025-12-03T17:48:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512/resolve/main/README.md",
    "description": "MLX-converted Ministral-3-3B-Instruct model for text generation on Apple Silicon."
  },
  {
    "model_name": "Josiefied-Qwen3-4B-abliterated-v1-4bit",
    "developer": "mlx-community",
    "downloads": 1784,
    "createdAt": "2025-05-01T20:53:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-4B-abliterated-v1-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Josiefied-Qwen3-4B-abliterated-v1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Josiefied-Qwen3-4B-abliterated-v1-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Qwen3-4B model converted to MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-32B-4bit",
    "developer": "mlx-community",
    "downloads": 1745,
    "createdAt": "2025-01-21T05:31:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of DeepSeek-R1-Distill-Qwen-32B for Apple Silicon."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-Base-bf16",
    "developer": "mlx-community",
    "downloads": 1707,
    "createdAt": "2026-01-22T18:21:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16/resolve/main/README.md",
    "description": "A 0.6B parameter text-to-speech model optimized for Apple Silicon via MLX, supporting voice cloning."
  },
  {
    "model_name": "Qwen2.5-Coder-1.5B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1704,
    "createdAt": "2024-09-18T22:10:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "828.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit Qwen2.5-Coder-1.5B-Instruct model for code generation."
  },
  {
    "model_name": "Qwen3-Embedding-4B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 1695,
    "createdAt": "2025-06-06T14:11:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-Embedding-4B-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Embedding-4B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Embedding-4B-4bit-DWQ/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-Embedding-4B for feature extraction and embeddings."
  },
  {
    "model_name": "Phi-3.5-mini-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1663,
    "createdAt": "2024-08-20T20:27:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-3.5-mini-instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Microsoft's Phi-3.5-mini-instruct for text generation tasks."
  },
  {
    "model_name": "GLM-OCR-bf16",
    "developer": "mlx-community",
    "downloads": 1660,
    "createdAt": "2026-02-03T12:04:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-OCR-bf16/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-OCR-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-OCR-bf16/resolve/main/README.md",
    "description": "MLX-converted GLM-OCR model for image-to-text tasks."
  },
  {
    "model_name": "Phi-4-mini-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1659,
    "createdAt": "2025-02-27T09:55:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-4-mini-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-4-mini-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-4-mini-instruct-4bit/resolve/main/README.md",
    "description": "This is an MLX-converted version of Microsoft's Phi-4-mini-instruct model for text generation."
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx",
    "developer": "mlx-community",
    "downloads": 1624,
    "createdAt": "2024-07-16T12:34:23.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of DeepSeek-Coder-V2-Lite-Instruct for code generation."
  },
  {
    "model_name": "MiniMax-M2-4bit",
    "developer": "mlx-community",
    "downloads": 1612,
    "createdAt": "2025-10-27T20:20:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 27,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00001-of-00027.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00002-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00003-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00003-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00004-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00004-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00005-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00005-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00006-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00006-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00007-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00007-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00008-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00008-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00009-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00009-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00010-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00010-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00011-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00011-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00012-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00012-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00013-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00013-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00014-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00014-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00015-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00015-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00016-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00016-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00017-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00017-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00018-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00018-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00019-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00019-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00020-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00020-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00021-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00021-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00022-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00022-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00023-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00023-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00024-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00024-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00025-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00025-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00026-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00026-of-00027.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00027-of-00027",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/model-00027-of-00027.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized text-generation model converted to MLX format from MiniMax-M2."
  },
  {
    "model_name": "GLM-4.6-4bit",
    "developer": "mlx-community",
    "downloads": 1587,
    "createdAt": "2025-09-30T15:08:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 39,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00001-of-00039.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00002-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00003-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00004-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00005-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00006-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00007-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00008-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00009-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00010-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00011-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00012-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00013-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00014-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00015-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00016-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00017-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00018-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00019-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00020-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00021-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00022-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00023-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00024-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00025-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00026-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00027-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00028-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00029-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00030-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00030-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00031-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00032-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00033-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00033-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00034-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00035-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00036-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00036-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00037-of-00039.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00038-of-00039.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00039-of-00039",
        "path": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/model-00039-of-00039.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.6-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of GLM-4.6 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-4bit",
    "developer": "mlx-community",
    "downloads": 1522,
    "createdAt": "2025-05-29T14:18:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-Qwen3-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-Qwen3-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-Qwen3-8B-4bit/resolve/main/README.md",
    "description": "MLX format 4-bit quantized DeepSeek-R1-0528-Qwen3-8B model for text generation using mlx-lm."
  },
  {
    "model_name": "Qwen2.5-14B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1522,
    "createdAt": "2024-09-18T20:00:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-4bit/resolve/main/README.md",
    "description": "4-bit MLX quantized version of Qwen2.5-14B-Instruct for Apple Silicon."
  },
  {
    "model_name": "gemma-3-4b-it-4bit",
    "developer": "mlx-community",
    "downloads": 1497,
    "createdAt": "2025-03-12T08:05:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-4b-it-4bit/resolve/main/model.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-4b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-4b-it-4bit/resolve/main/README.md",
    "description": "This is a 4-bit MLX-quantized version of Google's Gemma 3 4B instruction-tuned model for image-text-to-text tasks using the `mlx-vlm` library."
  },
  {
    "model_name": "gemma-3-4b-it-8bit",
    "developer": "mlx-community",
    "downloads": 1476,
    "createdAt": "2025-03-12T08:18:24.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-4b-it-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-4b-it-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "685.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-4b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-4b-it-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX conversion of Google's Gemma-3-4b-it model for image-text-to-text tasks."
  },
  {
    "model_name": "parakeet-tdt_ctc-110m",
    "developer": "mlx-community",
    "downloads": 1448,
    "createdAt": "2025-05-06T14:42:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt_ctc-110m/resolve/main/model.safetensors",
        "file_size": "437.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt_ctc-110m/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt_ctc-110m/resolve/main/README.md",
    "description": "MLX-converted automatic speech recognition model for Apple Silicon, based on NVIDIA's FastConformer Parakeet."
  },
  {
    "model_name": "Kimi-K2-Instruct-0905-mlx-DQ3_K_M",
    "developer": "mlx-community",
    "downloads": 1446,
    "createdAt": "2025-09-06T10:16:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 99,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00001-of-00099.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00002-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00002-of-00099.safetensors",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "model-00003-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00003-of-00099.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00004-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00004-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00005-of-00099.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00006-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00006-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00007-of-00099.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00008-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00008-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00009-of-00099.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00010-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00010-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00011-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00012-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00012-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00013-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00014-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00014-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00015-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00016-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00017-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00017-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00018-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00019-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00020-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00020-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00021-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00022-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00022-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00023-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00023-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00024-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00024-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00025-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00025-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00026-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00026-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00027-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00027-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00028-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00028-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00029-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00030-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00030-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00031-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00031-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00032-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00033-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00033-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00034-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00034-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00035-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00036-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00036-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00037-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00037-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00038-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00038-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00039-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00039-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00040-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00040-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00041-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00041-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00042-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00042-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00043-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00043-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00044-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00044-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00045-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00045-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00046-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00046-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00047-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00047-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00048-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00048-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00049-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00049-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00050-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00050-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00051-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00051-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00052-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00052-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00053-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00053-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00054-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00054-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00055-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00055-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00056-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00056-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00057-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00057-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00058-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00058-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00059-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00059-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00060-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00060-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00061-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00061-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00062-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00062-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00063-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00063-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00064-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00064-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00065-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00065-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00066-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00066-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00067-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00067-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00068-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00068-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00069-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00069-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00070-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00070-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00071-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00071-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00072-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00072-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00073-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00073-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00074-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00074-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00075-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00075-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00076-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00076-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00077-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00077-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00078-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00078-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00079-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00079-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00080-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00080-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00081-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00081-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00082-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00082-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00083-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00083-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00084-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00084-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00085-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00085-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00086-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00086-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00087-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00087-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00088-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00088-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00089-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00089-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00090-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00090-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00091-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00091-of-00099.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00092-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00092-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00093-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00093-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00094-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00094-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00095-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00095-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00096-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00096-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00097-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00097-of-00099.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00098-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00098-of-00099.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00099-of-00099",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/model-00099-of-00099.safetensors",
        "file_size": "3.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M/resolve/main/README.md",
    "description": "MLX model of Kimi-K2-Instruct-0905 using DQ3_K_M dynamic 3-bit quantization for Apple Mac Studio M3 Ultra (512GB), achieving 4-bit performance with 3-bit efficiency."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct-uncensored-6bit",
    "developer": "mlx-community",
    "downloads": 1426,
    "createdAt": "2025-04-23T12:58:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-uncensored-6bit/resolve/main/model.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-uncensored-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-uncensored-6bit/resolve/main/README.md",
    "description": "A 6-bit MLX-converted version of Llama-3.2-3B-Instruct-uncensored."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-VoiceDesign-8bit",
    "developer": "mlx-community",
    "downloads": 1418,
    "createdAt": "2026-01-22T21:26:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-8bit/resolve/main/model.safetensors",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized Qwen3 text-to-speech model converted for MLX audio generation."
  },
  {
    "model_name": "Qwen3-Coder-Next-8bit",
    "developer": "mlx-community",
    "downloads": 1399,
    "createdAt": "2026-02-03T19:28:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "862.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-8bit/resolve/main/README.md",
    "description": "A text generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Josiefied-Qwen2.5-7B-Instruct-abliterated-v2-4-bit",
    "developer": "mlx-community",
    "downloads": 1382,
    "createdAt": "2024-09-29T14:19:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2-4-bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2-4-bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2-4-bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of an abliterated Qwen2.5-7B-Instruct model for Apple Silicon."
  },
  {
    "model_name": "OpenELM-270M-Instruct",
    "developer": "mlx-community",
    "downloads": 1373,
    "createdAt": "2024-04-24T12:24:28.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/OpenELM-270M-Instruct/resolve/main/model.safetensors",
        "file_size": "517.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/OpenELM-270M-Instruct/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/OpenELM-270M-Instruct/resolve/main/README.md",
    "description": "This is an MLX-converted version of Apple's OpenELM-270M-instruct language model."
  },
  {
    "model_name": "siglip-so400m-patch14-384",
    "developer": "mlx-community",
    "downloads": 1373,
    "createdAt": "2025-03-26T19:10:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/siglip-so400m-patch14-384/resolve/main/model.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/siglip-so400m-patch14-384/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/siglip-so400m-patch14-384/resolve/main/README.md",
    "description": "Vision-language model converted to MLX format from Google's SigLIP."
  },
  {
    "model_name": "translategemma-12b-it-8bit",
    "developer": "mlx-community",
    "downloads": 1356,
    "createdAt": "2026-01-15T20:15:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/translategemma-12b-it-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/translategemma-12b-it-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/translategemma-12b-it-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-12b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-12b-it-8bit/resolve/main/README.md",
    "description": "8-bit MLX version of Google's `translategemma-12b-it` translation model."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1314,
    "createdAt": "2025-07-31T15:00:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-Coder-30B-A3B-Instruct for text generation."
  },
  {
    "model_name": "Qwen3-ASR-1.7B-8bit",
    "developer": "mlx-community",
    "downloads": 1307,
    "createdAt": "2026-01-29T15:08:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-8bit/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit quantized Qwen3 speech-to-text model for audio transcription on Apple Silicon."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-Base-4bit",
    "developer": "mlx-community",
    "downloads": 1291,
    "createdAt": "2026-01-22T19:11:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-4bit/resolve/main/model.safetensors",
        "file_size": "977.0 MB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-4bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized 0.6B parameter text-to-speech model."
  },
  {
    "model_name": "translategemma-27b-it-8bit",
    "developer": "mlx-community",
    "downloads": 1286,
    "createdAt": "2026-01-15T20:15:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-27b-it-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX conversion of Google's `translategemma-27b-it` translation model for Apple Silicon."
  },
  {
    "model_name": "snac_24khz",
    "developer": "mlx-community",
    "downloads": 1268,
    "createdAt": "2025-03-22T18:38:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/snac_24khz/resolve/main/model.safetensors",
        "file_size": "75.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/snac_24khz/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/snac_24khz/resolve/main/README.md",
    "description": "I don't see a README.md file content to summarize. Please provide the actual README text you'd like me to condense into the shortest possible sentence."
  },
  {
    "model_name": "translategemma-12b-it-4bit",
    "developer": "mlx-community",
    "downloads": 1264,
    "createdAt": "2026-01-15T20:15:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/translategemma-12b-it-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/translategemma-12b-it-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-12b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-12b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Google's Translategemma-12b-it, optimized for Apple Silicon using the MLX framework."
  },
  {
    "model_name": "SmolVLM2-500M-Video-Instruct-mlx",
    "developer": "mlx-community",
    "downloads": 1251,
    "createdAt": "2025-02-12T19:51:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolVLM2-500M-Video-Instruct-mlx/resolve/main/model.safetensors",
        "file_size": "968.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolVLM2-500M-Video-Instruct-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolVLM2-500M-Video-Instruct-mlx/resolve/main/README.md",
    "description": "A small video-language model (500M parameters) converted to MLX format for video-text-to-text tasks."
  },
  {
    "model_name": "SmolLM-135M-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1210,
    "createdAt": "2024-07-16T21:06:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolLM-135M-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "72.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolLM-135M-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolLM-135M-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of SmolLM-135M-Instruct for use with mlx-lm library."
  },
  {
    "model_name": "DeepSeek-R1-4bit",
    "developer": "mlx-community",
    "downloads": 1205,
    "createdAt": "2025-01-20T18:54:28.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of DeepSeek-R1 for Apple Silicon."
  },
  {
    "model_name": "S3TokenizerV2",
    "developer": "mlx-community",
    "downloads": 1196,
    "createdAt": "2025-12-05T18:23:28.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/S3TokenizerV2/resolve/main/model.safetensors",
        "file_size": "471.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/S3TokenizerV2/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/S3TokenizerV2/resolve/main/README.md",
    "description": "S3TokenizerV2 speech tokenizer converted to MLX format from CosyVoice2-0.5B for use with mlx-audio-plus."
  },
  {
    "model_name": "gemma-3-270m-it-8bit",
    "developer": "mlx-community",
    "downloads": 1192,
    "createdAt": "2025-08-09T20:21:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-270m-it-8bit/resolve/main/model.safetensors",
        "file_size": "441.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-270m-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-270m-it-8bit/resolve/main/README.md",
    "description": "Google's Gemma 3 270M instruction-tuned model converted to MLX format for text generation."
  },
  {
    "model_name": "Qwen3-Embedding-8B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 1160,
    "createdAt": "2025-06-08T10:51:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-Embedding-8B-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Embedding-8B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Embedding-8B-4bit-DWQ/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-Embedding-8B for use with mlx-lm."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-CustomVoice-8bit",
    "developer": "mlx-community",
    "downloads": 1160,
    "createdAt": "2026-01-22T22:49:24.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-8bit/resolve/main/model.safetensors",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-8bit/resolve/main/README.md",
    "description": "MLX-converted text-to-speech model with voice cloning support."
  },
  {
    "model_name": "Qwen3-VL-4B-Instruct-3bit",
    "developer": "mlx-community",
    "downloads": 1129,
    "createdAt": "2025-10-14T18:00:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-3bit/resolve/main/model.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-4B-Instruct-3bit/resolve/main/README.md",
    "description": "A 3-bit quantized version of Qwen3-VL-4B-Instruct converted to MLX format for image-text-to-text tasks on Apple Silicon."
  },
  {
    "model_name": "parakeet-tdt_ctc-0.6b-ja",
    "developer": "mlx-community",
    "downloads": 1123,
    "createdAt": "2025-05-06T14:36:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt_ctc-0.6b-ja/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt_ctc-0.6b-ja/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt_ctc-0.6b-ja/resolve/main/README.md",
    "description": "Japanese automatic speech recognition model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-CustomVoice-bf16",
    "developer": "mlx-community",
    "downloads": 1085,
    "createdAt": "2026-01-22T22:54:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-bf16/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-bf16/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-bf16/resolve/main/README.md",
    "description": "A 1.7B parameter text-to-speech model with voice cloning support, converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-Coder-3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1059,
    "createdAt": "2024-11-11T18:30:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-3B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-3B-Instruct-4bit/resolve/main/README.md",
    "description": "4-bit MLX conversion of Qwen2.5-Coder-3B-Instruct for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3.2-mlx-5bit",
    "developer": "mlx-community",
    "downloads": 1057,
    "createdAt": "2025-12-21T10:29:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.2-mlx-5bit/resolve/main/README.md",
    "description": "MLX-quantized 5-bit version of DeepSeek-V3.2 for text generation on Apple Silicon."
  },
  {
    "model_name": "Ministral-3-3B-Reasoning-2512-4bit",
    "developer": "mlx-community",
    "downloads": 1042,
    "createdAt": "2025-12-03T22:33:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Reasoning-2512-4bit/resolve/main/model.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-3B-Reasoning-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-3B-Reasoning-2512-4bit/resolve/main/README.md",
    "description": "This is a 4-bit MLX-quantized version of Mistral's Ministral-3-3B-Reasoning model for image generation tasks."
  },
  {
    "model_name": "DeepSeek-V3-4bit",
    "developer": "mlx-community",
    "downloads": 1032,
    "createdAt": "2025-01-07T20:54:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of DeepSeek-V3 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-Coder-14B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 1030,
    "createdAt": "2024-11-11T18:29:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen2.5-Coder-14B-Instruct for code generation."
  },
  {
    "model_name": "all-MiniLM-L6-v2-bf16",
    "developer": "mlx-community",
    "downloads": 1029,
    "createdAt": "2025-04-02T16:35:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-bf16/resolve/main/model.safetensors",
        "file_size": "43.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-bf16/resolve/main/README.md",
    "description": "A MiniLM-L6-v2 sentence embedding model converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "gpt-oss-120b-MXFP4-Q4",
    "developer": "mlx-community",
    "downloads": 1002,
    "createdAt": "2025-08-29T23:32:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "884.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-120b-MXFP4-Q4/resolve/main/README.md",
    "description": "A GPT-OSS-120B model converted to MLX format for text generation."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-7B-4bit",
    "developer": "mlx-community",
    "downloads": 997,
    "createdAt": "2025-01-20T15:38:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of DeepSeek-R1-Distill-Qwen-7B for running on Apple Silicon via the mlx-lm library."
  },
  {
    "model_name": "DeepSeek-OCR-2-bf16",
    "developer": "mlx-community",
    "downloads": 986,
    "createdAt": "2026-01-28T11:20:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-bf16/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-bf16/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-bf16/resolve/main/README.md",
    "description": "An MLX-converted DeepSeek OCR model for image-to-text tasks."
  },
  {
    "model_name": "Mistral-Nemo-Instruct-2407-4bit",
    "developer": "mlx-community",
    "downloads": 978,
    "createdAt": "2024-07-18T15:04:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mistral-Nemo-Instruct-2407-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Mistral-Nemo-Instruct-2407 for Apple Silicon."
  },
  {
    "model_name": "SmolLM-1.7B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 940,
    "createdAt": "2024-07-16T20:55:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolLM-1.7B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "918.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolLM-1.7B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolLM-1.7B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of SmolLM-1.7B-Instruct converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-8bit-gs32",
    "developer": "mlx-community",
    "downloads": 939,
    "createdAt": "2025-12-26T12:50:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 54,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00001-of-00054.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00002-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00002-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00003-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00004-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00004-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00005-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00006-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00006-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00007-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00008-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00008-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00009-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00010-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00010-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00011-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00012-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00012-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00013-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00014-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00014-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00015-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00016-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00016-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00017-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00018-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00018-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00019-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00020-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00020-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00021-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00022-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00022-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00023-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00024-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00024-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00025-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00025-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00026-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00026-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00027-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00027-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00028-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00028-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00029-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00029-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00030-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00030-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00031-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00031-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00032-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00032-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00033-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00033-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00034-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00034-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00035-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00035-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00036-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00036-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00037-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00037-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00038-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00038-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00039-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00039-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00040-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00040-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00041-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00041-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00042-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00042-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00043-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00043-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00044-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00044-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00045-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00045-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00046-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00046-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00047-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00047-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00048-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00048-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00049-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00049-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00050-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00050-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00051-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00051-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00052-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00052-of-00054.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00053-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00053-of-00054.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00054-of-00054",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/model-00054-of-00054.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32/resolve/main/README.md",
    "description": "8-bit quantized MLX model for text generation, converted from MiniMax-M2.1 for Apple Silicon."
  },
  {
    "model_name": "dolphin3.0-llama3.2-3B-4Bit",
    "developer": "mlx-community",
    "downloads": 934,
    "createdAt": "2025-01-22T15:23:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/dolphin3.0-llama3.2-3B-4Bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/dolphin3.0-llama3.2-3B-4Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/dolphin3.0-llama3.2-3B-4Bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Dolphin 3.0 Llama 3.2 3B for Apple Silicon."
  },
  {
    "model_name": "Ministral-8B-Instruct-2410-4bit",
    "developer": "mlx-community",
    "downloads": 914,
    "createdAt": "2024-10-16T15:00:48.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Ministral-8B-Instruct-2410-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-8B-Instruct-2410-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-8B-Instruct-2410-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of Mistral's Ministral-8B-Instruct model for Apple Silicon, available under a non-commercial research license."
  },
  {
    "model_name": "parakeet-tdt-1.1b",
    "developer": "mlx-community",
    "downloads": 912,
    "createdAt": "2025-05-06T14:32:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/parakeet-tdt-1.1b/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/parakeet-tdt-1.1b/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/parakeet-tdt-1.1b/resolve/main/README.md",
    "description": "An NVIDIA FastConformer automatic speech recognition model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "translategemma-27b-it-4bit",
    "developer": "mlx-community",
    "downloads": 900,
    "createdAt": "2026-01-15T20:15:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/translategemma-27b-it-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/translategemma-27b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/translategemma-27b-it-4bit/resolve/main/README.md",
    "description": "This is a 4-bit MLX conversion of Google's `translategemma-27b-it` model for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3-0324-4bit",
    "developer": "mlx-community",
    "downloads": 880,
    "createdAt": "2025-03-24T14:34:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of DeepSeek-v3-0324 converted to MLX format for text generation using mlx-lm."
  },
  {
    "model_name": "deepcogito-cogito-v1-preview-llama-3B-4bit",
    "developer": "mlx-community",
    "downloads": 865,
    "createdAt": "2025-04-08T22:02:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/deepcogito-cogito-v1-preview-llama-3B-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/deepcogito-cogito-v1-preview-llama-3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/deepcogito-cogito-v1-preview-llama-3B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Llama 3B model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Llama-3.2-11B-Vision-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 864,
    "createdAt": "2024-10-18T08:59:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "623.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of Llama-3.2-11B-Vision-Instruct for Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3.1-70B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 860,
    "createdAt": "2024-07-23T14:40:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 15,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit quantized Llama 3.1 70B Instruct model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Phi-3-mini-128k-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 847,
    "createdAt": "2024-04-23T14:38:20.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-3-mini-128k-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-3-mini-128k-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-3-mini-128k-instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Microsoft's Phi-3-mini-128k-instruct model for text generation on Apple Silicon."
  },
  {
    "model_name": "GLM-OCR-8bit",
    "developer": "mlx-community",
    "downloads": 840,
    "createdAt": "2026-02-03T11:59:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-OCR-8bit/resolve/main/model.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-OCR-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-OCR-8bit/resolve/main/README.md",
    "description": "GLM-OCR-8bit is an image-to-text model converted to MLX format for OCR and image description tasks."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-4bit",
    "developer": "mlx-community",
    "downloads": 834,
    "createdAt": "2025-04-12T13:42:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted vision-language model for image-text tasks."
  },
  {
    "model_name": "gpt-oss-120b-4bit",
    "developer": "mlx-community",
    "downloads": 827,
    "createdAt": "2025-08-06T13:46:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gpt-oss-120b-4bit/resolve/main/README.md",
    "description": "A 120B parameter GPT model quantized to 4-bit, converted to MLX format for inference."
  },
  {
    "model_name": "Qwen3-235B-A22B-Thinking-2507-8bit",
    "developer": "mlx-community",
    "downloads": 821,
    "createdAt": "2025-07-25T13:56:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 48,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00001-of-00048.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00002-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00003-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00004-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00005-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00006-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00007-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00008-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00009-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00010-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00011-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00012-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00013-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00014-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00015-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00016-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00017-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00018-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00018-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00019-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00019-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00020-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00021-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00021-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00022-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00022-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00023-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00024-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00024-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00025-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00025-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00026-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00027-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00027-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00028-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00028-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00029-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00030-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00030-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00031-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00031-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00032-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00033-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00033-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00034-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00034-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00035-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00036-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00036-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00037-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00037-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00038-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00039-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00039-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00040-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00040-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00041-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00042-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00042-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00043-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00043-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00044-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00045-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00045-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00046-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00046-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00047-of-00048.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00048-of-00048",
        "path": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/model-00048-of-00048.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX version of Qwen3-235B-A22B-Thinking-2507 for text generation on Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-8bit",
    "developer": "mlx-community",
    "downloads": 820,
    "createdAt": "2025-12-22T19:31:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 90,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00001-of-00090.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00002-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00002-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00003-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00003-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00004-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00005-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00005-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00006-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00006-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00007-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00008-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00008-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00009-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00009-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00010-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00011-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00011-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00012-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00012-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00013-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00014-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00014-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00015-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00015-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00016-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00017-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00017-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00018-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00018-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00019-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00020-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00020-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00021-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00021-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00022-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00023-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00023-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00024-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00024-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00025-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00026-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00026-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00027-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00027-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00028-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00029-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00029-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00030-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00030-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00031-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00032-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00032-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00033-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00033-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00034-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00035-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00035-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00036-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00036-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00037-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00038-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00038-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00039-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00039-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00040-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00041-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00041-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00042-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00042-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00043-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00044-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00044-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00045-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00045-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00046-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00047-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00047-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00048-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00048-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00049-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00050-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00050-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00051-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00051-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00052-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00053-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00053-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00054-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00054-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00055-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00056-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00056-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00057-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00057-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00058-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00059-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00059-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00060-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00060-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00061-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00062-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00062-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00063-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00063-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00064-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00065-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00065-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00066-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00066-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00067-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00068-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00068-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00069-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00069-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00070-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00071-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00071-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00072-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00072-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00073-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00074-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00074-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00075-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00075-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00076-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00077-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00077-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00078-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00078-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00079-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00080-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00080-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00081-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00081-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00082-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00083-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00083-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00084-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00084-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00085-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00086-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00086-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00087-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00087-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00088-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00089-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00089-of-00090.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00090-of-00090",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/model-00090-of-00090.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-8bit/resolve/main/README.md",
    "description": "MLX format conversion of GLM-4.7 (8-bit) for text generation."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-32B-abliterated",
    "developer": "mlx-community",
    "downloads": 816,
    "createdAt": "2025-02-20T19:41:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated/resolve/main/README.md",
    "description": "An MLX-converted version of DeepSeek-R1-Distill-Qwen-32B abliterated model for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V2.5-1210-4bit",
    "developer": "mlx-community",
    "downloads": 813,
    "createdAt": "2024-12-11T07:51:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 26,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00001-of-00026.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00002-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00003-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00004-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00005-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00006-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00007-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00008-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00009-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00010-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00011-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00012-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00013-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00014-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00015-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00016-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00017-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00018-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00019-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00020-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00021-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00022-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00023-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00024-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00025-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/model-00026-of-00026.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V2.5-1210-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of DeepSeek-V2.5-1210 model."
  },
  {
    "model_name": "Josiefied-Qwen3-1.7B-abliterated-v1-4bit",
    "developer": "mlx-community",
    "downloads": 812,
    "createdAt": "2025-04-29T19:54:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-1.7B-abliterated-v1-4bit/resolve/main/model.safetensors",
        "file_size": "923.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Josiefied-Qwen3-1.7B-abliterated-v1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Josiefied-Qwen3-1.7B-abliterated-v1-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3 1.7B abliterated for chat."
  },
  {
    "model_name": "whisper-large-v3-turbo-4bit",
    "developer": "mlx-community",
    "downloads": 803,
    "createdAt": "2025-12-14T12:58:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/whisper-large-v3-turbo-4bit/resolve/main/model.safetensors",
        "file_size": "442.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/whisper-large-v3-turbo-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/whisper-large-v3-turbo-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of OpenAI's Whisper-large-v3-turbo for speech-to-text on Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-VL-72B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 795,
    "createdAt": "2025-01-29T10:35:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "4.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-VL-72B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2.5-VL-72B-Instruct converted to MLX format for running on Apple Silicon."
  },
  {
    "model_name": "granite-4.0-h-1b-3bit",
    "developer": "mlx-community",
    "downloads": 792,
    "createdAt": "2025-10-28T15:44:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/granite-4.0-h-1b-3bit/resolve/main/model.safetensors",
        "file_size": "611.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/granite-4.0-h-1b-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/granite-4.0-h-1b-3bit/resolve/main/README.md",
    "description": "A 3-bit quantized MLX version of IBM's Granite 4.0 1B parameter language model for text generation on Apple Silicon."
  },
  {
    "model_name": "mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit",
    "developer": "mlx-community",
    "downloads": 789,
    "createdAt": "2025-12-14T00:56:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit quantized version of Mistral's Devstral-Small-2-24B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "whisper-large-v3-turbo-asr-fp16",
    "developer": "mlx-community",
    "downloads": 781,
    "createdAt": "2026-01-13T22:11:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16/resolve/main/model.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16/resolve/main/README.md",
    "description": "An MLX-converted Whisper Large V3 Turbo model for speech-to-text transcription using mlx-audio."
  },
  {
    "model_name": "phi-4-4bit",
    "developer": "mlx-community",
    "downloads": 760,
    "createdAt": "2024-12-16T18:26:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/phi-4-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/phi-4-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/phi-4-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/phi-4-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Microsoft's phi-4 model converted to MLX format for text generation."
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Loop-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 748,
    "createdAt": "2026-01-02T02:01:20.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "962.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX model for code generation converted from IQuest-Coder-V1-40B-Loop-Instruct."
  },
  {
    "model_name": "granite-3.3-2b-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 745,
    "createdAt": "2025-04-16T16:14:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/granite-3.3-2b-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/granite-3.3-2b-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/granite-3.3-2b-instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of IBM's Granite 3.3 2B instruct model for Apple Silicon."
  },
  {
    "model_name": "Qwen3-ASR-0.6B-8bit",
    "developer": "mlx-community",
    "downloads": 744,
    "createdAt": "2026-01-29T16:04:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-8bit/resolve/main/model.safetensors",
        "file_size": "959.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-8bit/resolve/main/README.md",
    "description": "A 0.6B parameter speech-to-text model (Qwen3-ASR) converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-OCR-2-8bit",
    "developer": "mlx-community",
    "downloads": 736,
    "createdAt": "2026-01-28T11:15:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-8bit/resolve/main/model.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-8bit/resolve/main/README.md",
    "description": "MLX-converted version of DeepSeek-OCR-2 for vision-language tasks like OCR."
  },
  {
    "model_name": "Qwen2.5-32B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 734,
    "createdAt": "2024-09-18T23:05:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Qwen2.5-32B-Instruct model converted to MLX format for Apple devices."
  },
  {
    "model_name": "GLM-4.7-Flash-bf16",
    "developer": "mlx-community",
    "downloads": 727,
    "createdAt": "2026-01-20T10:46:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 12,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00001-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00002-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00003-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00004-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00005-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00005-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00006-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00007-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00008-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00009-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00010-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00011-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00011-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/model-00012-of-00012.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-bf16/resolve/main/README.md",
    "description": "MLX-converted version of GLM-4.7-Flash for text generation."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 727,
    "createdAt": "2025-01-29T02:20:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "292.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-converted version of Qwen2.5-VL-7B-Instruct for image-text-to-text tasks."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Llama-70B-8bit",
    "developer": "mlx-community",
    "downloads": 726,
    "createdAt": "2025-01-20T18:37:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 15,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of DeepSeek-R1-Distill-Llama-70B for Apple Silicon."
  },
  {
    "model_name": "Ministral-3-8B-Reasoning-2512-4bit",
    "developer": "mlx-community",
    "downloads": 718,
    "createdAt": "2025-12-03T23:14:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Reasoning-2512-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Reasoning-2512-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "288.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-8B-Reasoning-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-8B-Reasoning-2512-4bit/resolve/main/README.md",
    "description": "MLX-optimized 4-bit quantized version of Mistral's Ministral-3-8B reasoning model for vision-language tasks."
  },
  {
    "model_name": "Kimi-K2-Thinking-4bit",
    "developer": "mlx-community",
    "downloads": 715,
    "createdAt": "2025-11-06T20:14:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 362,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00001-of-00180.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00001-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00001-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00002-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00002-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00002-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00002-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00003-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00003-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00003-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00003-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00004-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00004-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00004-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00004-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00005-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00005-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00005-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00005-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00006-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00006-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00006-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00006-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00007-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00007-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00007-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00007-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00008-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00008-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00008-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00008-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00009-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00009-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00009-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00009-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00010-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00010-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00010-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00010-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00011-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00011-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00011-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00011-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00012-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00012-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00012-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00012-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00013-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00013-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00013-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00013-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00014-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00014-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00014-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00014-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00015-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00015-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00015-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00015-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00016-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00016-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00016-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00016-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00017-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00017-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00017-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00017-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00018-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00018-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00018-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00018-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00019-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00019-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00019-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00019-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00020-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00020-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00020-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00020-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00021-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00021-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00021-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00021-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00022-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00022-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00022-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00022-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00023-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00023-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00023-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00023-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00024-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00024-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00024-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00024-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00025-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00025-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00025-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00025-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00026-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00026-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00026-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00026-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00027-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00027-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00027-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00027-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00028-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00028-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00028-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00028-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00029-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00029-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00029-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00029-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00030-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00030-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00030-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00030-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00031-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00031-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00031-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00031-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00032-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00032-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00032-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00032-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00033-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00033-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00033-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00033-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00034-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00034-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00034-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00034-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00035-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00035-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00035-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00035-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00036-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00036-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00036-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00036-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00037-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00037-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00037-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00037-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00038-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00038-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00038-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00038-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00039-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00039-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00039-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00039-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00040-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00040-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00040-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00040-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00041-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00041-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00041-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00041-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00042-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00042-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00042-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00042-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00043-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00043-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00043-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00043-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00044-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00044-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00044-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00044-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00045-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00045-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00045-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00045-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00046-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00046-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00046-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00046-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00047-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00047-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00047-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00047-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00048-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00048-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00048-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00048-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00049-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00049-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00049-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00049-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00050-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00050-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00050-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00050-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00051-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00051-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00051-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00051-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00052-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00052-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00052-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00052-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00053-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00053-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00053-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00053-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00054-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00054-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00054-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00054-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00055-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00055-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00055-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00055-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00056-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00056-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00056-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00056-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00057-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00057-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00057-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00057-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00058-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00058-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00058-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00058-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00059-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00059-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00059-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00059-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00060-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00060-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00060-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00060-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00061-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00061-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00061-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00061-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00062-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00062-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00062-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00062-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00063-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00063-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00063-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00063-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00064-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00064-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00064-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00064-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00065-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00065-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00065-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00065-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00066-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00066-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00066-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00066-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00067-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00067-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00067-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00067-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00068-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00068-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00068-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00068-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00069-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00069-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00069-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00069-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00070-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00070-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00070-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00070-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00071-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00071-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00071-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00071-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00072-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00072-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00072-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00072-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00073-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00073-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00073-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00073-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00074-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00074-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00074-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00074-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00075-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00075-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00075-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00075-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00076-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00076-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00076-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00076-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00077-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00077-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00077-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00077-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00078-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00078-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00078-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00078-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00079-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00079-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00079-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00079-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00080-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00080-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00080-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00080-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00081-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00081-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00081-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00081-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00082-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00082-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00082-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00082-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00083-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00083-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00083-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00083-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00084-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00084-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00084-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00084-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00085-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00085-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00085-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00085-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00086-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00086-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00086-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00086-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00087-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00087-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00087-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00087-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00088-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00088-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00088-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00088-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00089-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00089-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00089-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00089-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00090-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00090-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00090-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00090-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00091-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00091-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00091-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00091-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00092-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00092-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00092-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00092-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00093-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00093-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00093-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00093-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00094-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00094-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00094-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00094-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00095-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00095-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00095-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00095-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00096-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00096-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00096-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00096-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00097-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00097-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00097-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00097-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00098-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00098-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00098-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00098-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00099-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00099-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00099-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00099-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00100-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00100-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00100-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00100-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00101-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00101-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00101-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00101-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00102-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00102-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00102-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00102-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00103-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00103-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00103-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00103-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00104-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00104-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00104-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00104-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00105-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00105-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00105-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00105-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00106-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00106-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00106-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00106-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00107-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00107-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00107-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00107-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00108-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00108-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00108-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00108-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00109-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00109-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00109-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00109-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00110-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00110-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00110-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00110-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00111-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00111-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00111-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00111-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00112-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00112-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00112-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00112-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00113-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00113-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00113-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00113-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00114-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00114-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00114-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00114-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00115-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00115-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00115-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00115-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00116-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00116-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00116-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00116-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00117-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00117-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00117-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00117-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00118-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00118-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00118-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00118-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00119-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00119-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00119-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00119-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00120-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00120-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00120-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00120-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00121-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00121-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00121-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00121-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00122-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00122-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00122-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00122-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00123-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00123-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00123-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00123-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00124-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00124-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00124-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00124-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00125-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00125-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00125-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00125-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00126-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00126-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00126-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00126-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00127-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00127-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00127-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00127-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00128-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00128-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00128-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00128-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00129-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00129-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00129-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00129-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00130-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00130-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00130-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00130-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00131-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00131-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00131-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00131-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00132-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00132-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00132-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00132-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00133-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00133-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00133-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00133-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00134-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00134-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00134-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00134-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00135-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00135-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00135-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00135-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00136-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00136-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00136-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00136-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00137-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00137-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00137-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00137-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00138-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00138-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00138-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00138-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00139-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00139-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00139-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00139-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00140-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00140-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00140-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00140-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00141-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00141-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00141-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00141-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00142-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00142-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00142-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00142-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00143-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00143-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00143-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00143-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00144-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00144-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00144-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00144-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00145-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00145-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00145-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00145-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00146-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00146-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00146-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00146-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00147-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00147-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00147-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00147-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00148-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00148-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00148-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00148-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00149-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00149-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00149-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00149-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00150-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00150-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00150-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00150-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00151-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00151-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00151-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00151-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00152-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00152-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00152-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00152-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00153-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00153-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00153-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00153-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00154-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00154-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00154-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00154-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00155-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00155-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00155-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00155-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00156-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00156-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00156-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00156-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00157-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00157-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00157-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00157-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00158-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00158-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00158-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00158-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00159-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00159-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00159-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00159-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00160-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00160-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00160-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00160-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00161-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00161-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00161-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00161-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00162-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00162-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00162-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00162-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00163-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00163-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00163-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00163-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00164-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00164-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00164-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00164-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00165-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00165-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00165-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00165-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00166-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00166-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00166-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00166-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00167-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00167-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00167-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00167-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00168-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00168-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00168-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00168-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00169-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00169-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00169-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00169-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00170-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00170-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00170-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00170-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00171-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00171-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00171-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00171-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00172-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00172-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00172-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00172-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00173-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00173-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00173-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00173-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00174-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00174-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00174-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00174-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00175-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00175-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00175-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00175-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00176-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00176-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00176-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00176-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00177-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00177-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00177-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00177-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00178-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00178-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00178-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00178-of-00182.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00179-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00179-of-00180.safetensors",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "model-00179-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00179-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00180-of-00180",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00180-of-00180.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00180-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00180-of-00182.safetensors",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "model-00181-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00181-of-00182.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00182-of-00182",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/model-00182-of-00182.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-4bit/resolve/main/README.md",
    "description": "MLX-converted version of Kimi-K2-Thinking for text generation."
  },
  {
    "model_name": "GLM-4.7-Flash-8bit-gs32",
    "developer": "mlx-community",
    "downloads": 714,
    "createdAt": "2026-01-20T10:46:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-8bit-gs32/resolve/main/README.md",
    "description": "A 8-bit quantized MLX version of GLM-4.7-Flash for text generation on Apple Silicon."
  },
  {
    "model_name": "Ministral-3-14B-Reasoning-2512-8bit",
    "developer": "mlx-community",
    "downloads": 713,
    "createdAt": "2025-12-04T01:04:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of Mistral's Ministral-3-14B-Reasoning vision language model."
  },
  {
    "model_name": "Devstral-2-123B-Instruct-2512-4bit",
    "developer": "mlx-community",
    "downloads": 711,
    "createdAt": "2025-12-10T04:51:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 14,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00001-of-00014.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00002-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00003-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00004-of-00014.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00005-of-00014.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00006-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00007-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00008-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00009-of-00014.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00010-of-00014.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00011-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00012-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00013-of-00014.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00014",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/model-00014-of-00014.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-4bit/resolve/main/README.md",
    "description": "MLX format 4-bit quantized version of Mistral's Devstral-2-123B-Instruct-2512 for text generation."
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-8bit",
    "developer": "mlx-community",
    "downloads": 680,
    "createdAt": "2025-01-05T19:58:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Dolphin3.0-Llama3.1-8B-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX conversion of the Dolphin3.0-Llama3.1-8B model for Apple Silicon."
  },
  {
    "model_name": "GLM-4.7-REAP-50-mixed-3-4-bits",
    "developer": "mlx-community",
    "downloads": 658,
    "createdAt": "2026-01-04T16:50:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "765.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-REAP-50-mixed-3-4-bits/resolve/main/README.md",
    "description": "MLX-converted quantized GLM-4.7-REAP-50 model for text generation."
  },
  {
    "model_name": "MiniMax-M2.1-REAP-30-5bit",
    "developer": "mlx-community",
    "downloads": 645,
    "createdAt": "2026-01-14T18:52:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 21,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00001-of-00021.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00002-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00003-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00004-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00005-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00006-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00007-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00008-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00009-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00010-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00011-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00012-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00013-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00014-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00015-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00016-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00017-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00018-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00019-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00020-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00020-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00021",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/model-00021-of-00021.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-5bit/resolve/main/README.md",
    "description": "A 5-bit quantized MiniMax-M2.1-REAP-30 text generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Llama-3.1-8B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 640,
    "createdAt": "2025-02-15T14:45:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Llama-3.1-8B-Instruct converted to MLX format for Apple devices."
  },
  {
    "model_name": "Chatterbox-Turbo-TTS-4bit",
    "developer": "mlx-community",
    "downloads": 636,
    "createdAt": "2025-12-22T11:33:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "conds",
        "path": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-4bit/resolve/main/conds.safetensors",
        "file_size": "163.5 KB"
      },
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-4bit/resolve/main/model.safetensors",
        "file_size": "774.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Chatterbox-Turbo TTS model for MLX (Apple Silicon)."
  },
  {
    "model_name": "Qwen2-VL-7B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 635,
    "createdAt": "2024-09-28T01:02:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2-VL-7B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2-VL-7B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2-VL-7B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen2-VL-7B-Instruct for image-text-to-text tasks."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-VoiceDesign-4bit",
    "developer": "mlx-community",
    "downloads": 635,
    "createdAt": "2026-01-22T20:47:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-4bit/resolve/main/model.safetensors",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-4bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen's 1.7B text-to-speech model for voice generation."
  },
  {
    "model_name": "GLM-OCR-4bit",
    "developer": "mlx-community",
    "downloads": 628,
    "createdAt": "2026-02-03T11:24:07.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-OCR-4bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-OCR-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-OCR-4bit/resolve/main/README.md",
    "description": "MLX-optimized OCR model for image-to-text conversion."
  },
  {
    "model_name": "Qwen3-Embedding-0.6B-8bit",
    "developer": "mlx-community",
    "downloads": 624,
    "createdAt": "2025-08-04T09:40:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-8bit/resolve/main/model.safetensors",
        "file_size": "603.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Embedding-0.6B-8bit/resolve/main/README.md",
    "description": "8-bit MLX conversion of Qwen3-Embedding-0.6B."
  },
  {
    "model_name": "LFM2-8B-A1B-4bit",
    "developer": "mlx-community",
    "downloads": 623,
    "createdAt": "2025-10-07T22:02:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-8B-A1B-4bit/resolve/main/model.safetensors",
        "file_size": "4.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-8B-A1B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-8B-A1B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of LiquidAI's LFM2-8B-A1B text generation model."
  },
  {
    "model_name": "Qwen2.5-VL-7B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 623,
    "createdAt": "2025-01-29T02:31:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-VL-7B-Instruct-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of Qwen2.5-VL-7B-Instruct for image-text-to-text tasks."
  },
  {
    "model_name": "gemma-3n-E2B-it-4bit",
    "developer": "mlx-community",
    "downloads": 619,
    "createdAt": "2025-06-27T13:42:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-4bit/resolve/main/README.md",
    "description": "MLX conversion of Google's Gemma 3N E2B IT model for image, audio, and video processing via mlx-vlm."
  },
  {
    "model_name": "IBM-granite-3.2-2b-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 614,
    "createdAt": "2025-03-03T09:54:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/IBM-granite-3.2-2b-instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IBM-granite-3.2-2b-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IBM-granite-3.2-2b-instruct-4bit/resolve/main/README.md",
    "description": "MLX conversion of IBM's Granite 3.2 2B instruct model."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-14B-4bit",
    "developer": "mlx-community",
    "downloads": 611,
    "createdAt": "2025-01-20T17:04:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-14B-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-14B-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-14B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-14B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX conversion of DeepSeek-R1-Distill-Qwen-14B for efficient inference on Apple Silicon using the mlx-lm library."
  },
  {
    "model_name": "FastVLM-0.5B-bf16",
    "developer": "mlx-community",
    "downloads": 611,
    "createdAt": "2025-10-14T17:09:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/FastVLM-0.5B-bf16/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/FastVLM-0.5B-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/FastVLM-0.5B-bf16/resolve/main/README.md",
    "description": "FastVLM-0.5B-bf16 is Apple's FastVLM-0.5B vision-language model converted to MLX format for image-text generation."
  },
  {
    "model_name": "Josiefied-Qwen3-8B-abliterated-v1-4bit",
    "developer": "mlx-community",
    "downloads": 609,
    "createdAt": "2025-05-01T23:11:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-8B-abliterated-v1-4bit/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Josiefied-Qwen3-8B-abliterated-v1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Josiefied-Qwen3-8B-abliterated-v1-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-converted abliterated Qwen3-8B model for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-Coder-14B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 605,
    "createdAt": "2024-11-11T18:29:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "4.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX conversion of Qwen's code-focused 14B language model for Apple Silicon."
  },
  {
    "model_name": "Ministral-3-8B-Instruct-2512",
    "developer": "mlx-community",
    "downloads": 597,
    "createdAt": "2025-12-03T18:38:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-8B-Instruct-2512/resolve/main/README.md",
    "description": "MLX-converted version of Mistral's Ministral-3-8B-Instruct model for text generation on Apple Silicon."
  },
  {
    "model_name": "multilingual-e5-base-mlx",
    "developer": "mlx-community",
    "downloads": 597,
    "createdAt": "2024-01-11T13:50:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/multilingual-e5-base-mlx/resolve/main/weights.00.safetensors",
        "file_size": "530.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/multilingual-e5-base-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/multilingual-e5-base-mlx/resolve/main/README.md",
    "description": "This is an MLX-converted version of the multilingual-e5-base sentence transformer model for use with Apple's MLX framework."
  },
  {
    "model_name": "Qwen3-Coder-Next-bf16",
    "developer": "mlx-community",
    "downloads": 595,
    "createdAt": "2026-02-03T20:49:28.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 36,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00001-of-00036.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00002-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00003-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00004-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00005-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00006-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00007-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00007-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00008-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00009-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00010-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00010-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00011-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00012-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00013-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00013-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00014-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00015-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00016-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00016-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00017-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00018-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00019-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00019-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00020-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00021-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00022-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00022-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00023-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00024-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00025-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00025-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00026-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00027-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00028-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00028-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00029-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00030-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00031-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00031-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00032-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00033-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00034-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00034-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00035-of-00036.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00036",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/model-00036-of-00036.safetensors",
        "file_size": "4.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-bf16/resolve/main/README.md",
    "description": "MLX-converted bf16 version of Qwen3-Coder-Next for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit",
    "developer": "mlx-community",
    "downloads": 594,
    "createdAt": "2025-01-20T14:56:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-MLX-8Bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of DeepSeek-R1-Distill-Qwen-32B for Apple Silicon."
  },
  {
    "model_name": "orpheus-3b-0.1-ft-bf16",
    "developer": "mlx-community",
    "downloads": 594,
    "createdAt": "2025-03-21T15:31:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/orpheus-3b-0.1-ft-bf16/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/orpheus-3b-0.1-ft-bf16/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/orpheus-3b-0.1-ft-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/orpheus-3b-0.1-ft-bf16/resolve/main/README.md",
    "description": "An MLX text-to-speech model converted from canopylabs/orpheus-3b-0.1-ft for use with mlx-audio."
  },
  {
    "model_name": "medgemma-1.5-4b-it-bf16",
    "developer": "mlx-community",
    "downloads": 593,
    "createdAt": "2026-01-13T23:57:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-bf16/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-bf16/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-bf16/resolve/main/README.md",
    "description": "MLX-converted medical vision-language model for image-text tasks across radiology, dermatology, pathology, and other clinical applications."
  },
  {
    "model_name": "Qwen3-1.7B-8bit",
    "developer": "mlx-community",
    "downloads": 592,
    "createdAt": "2025-04-28T21:34:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-1.7B-8bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-1.7B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-1.7B-8bit/resolve/main/README.md",
    "description": "A Qwen3-1.7B model converted to 8-bit MLX format for Apple Silicon."
  },
  {
    "model_name": "LFM2-VL-3B-4bit",
    "developer": "mlx-community",
    "downloads": 591,
    "createdAt": "2025-10-22T22:59:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-VL-3B-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-VL-3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-VL-3B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized vision-language model for image-text generation."
  },
  {
    "model_name": "SmolLM2-360M-Instruct",
    "developer": "mlx-community",
    "downloads": 588,
    "createdAt": "2024-10-31T22:18:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolLM2-360M-Instruct/resolve/main/model.safetensors",
        "file_size": "690.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolLM2-360M-Instruct/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolLM2-360M-Instruct/resolve/main/README.md",
    "description": "MLX-converted version of SmolLM2-360M-Instruct for Apple Silicon."
  },
  {
    "model_name": "Hermes-3-Llama-3.2-3B-4bit",
    "developer": "mlx-community",
    "downloads": 587,
    "createdAt": "2024-12-11T20:38:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Hermes-3-Llama-3.2-3B-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Hermes-3-Llama-3.2-3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Hermes-3-Llama-3.2-3B-4bit/resolve/main/README.md",
    "description": "MLX format conversion of the Hermes-3-Llama-3.2-3B model for use with mlx-lm."
  },
  {
    "model_name": "MiniMax-M2.1-6bit",
    "developer": "mlx-community",
    "downloads": 586,
    "createdAt": "2025-12-26T11:52:30.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 38,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00001-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00002-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00002-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00003-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00003-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00004-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00005-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00006-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00007-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00008-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00009-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00010-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00011-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00012-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00013-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00014-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00015-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00016-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00017-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00018-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00019-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00020-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00021-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00022-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00023-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00024-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00024-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00025-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00025-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00026-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00027-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00027-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00028-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00028-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00029-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00030-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00030-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00031-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00031-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00032-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00033-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00033-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00034-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00034-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00035-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00036-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00036-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00037-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00037-of-00038.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/model-00038-of-00038.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized MLX version of MiniMax-M2.1 for text generation."
  },
  {
    "model_name": "Llama-4-Scout-17B-16E-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 577,
    "createdAt": "2025-04-06T17:48:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 12,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00001-of-00012.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00002-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00003-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00004-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00005-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00005-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00006-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00007-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00008-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00009-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00010-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00011-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00012",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/model-00012-of-00012.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Meta's Llama-4-Scout-17B-16E-Instruct model for image understanding tasks."
  },
  {
    "model_name": "Qwen3-VL-8B-NSFW-Caption-V4.5-mxfp4",
    "developer": "mlx-community",
    "downloads": 576,
    "createdAt": "2025-12-24T02:49:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-8B-NSFW-Caption-V4.5-mxfp4/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-8B-NSFW-Caption-V4.5-mxfp4/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "315.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-8B-NSFW-Caption-V4.5-mxfp4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-8B-NSFW-Caption-V4.5-mxfp4/resolve/main/README.md",
    "description": "An MLX-converted vision-language model for NSFW image captioning."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-Base-8bit",
    "developer": "mlx-community",
    "downloads": 575,
    "createdAt": "2026-01-22T22:15:48.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-8bit/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-8bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-8bit/resolve/main/README.md",
    "description": "**MLX-optimized 1.7B parameter text-to-speech model with voice cloning support.**"
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Loop-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 570,
    "createdAt": "2026-01-06T22:55:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "4.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX quantized version of IQuest-Coder-V1-40B-Loop-Instruct for code generation on Apple Silicon."
  },
  {
    "model_name": "granite-4.0-h-micro-4bit",
    "developer": "mlx-community",
    "downloads": 565,
    "createdAt": "2025-10-04T11:14:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/granite-4.0-h-micro-4bit/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/granite-4.0-h-micro-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/granite-4.0-h-micro-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of IBM's Granite 4.0 H Micro model for text generation."
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-bf16",
    "developer": "mlx-community",
    "downloads": 563,
    "createdAt": "2024-09-25T23:47:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-bf16/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-bf16/resolve/main/README.md",
    "description": "MLX-converted 1B parameter instruction-tuned Llama 3.2 text generation model."
  },
  {
    "model_name": "whisper-base-4bit",
    "developer": "mlx-community",
    "downloads": 558,
    "createdAt": "2025-12-14T13:59:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/whisper-base-4bit/resolve/main/model.safetensors",
        "file_size": "40.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/whisper-base-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/whisper-base-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of OpenAI's Whisper-base speech-to-text model, converted to MLX format for efficient inference on Apple Silicon using mlx-audio-plus."
  },
  {
    "model_name": "Qwen2.5-Coder-32B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 547,
    "createdAt": "2024-11-11T18:28:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen2.5-Coder-32B-Instruct for code generation."
  },
  {
    "model_name": "Llama-3.2-11B-Vision-Instruct-abliterated",
    "developer": "mlx-community",
    "downloads": 541,
    "createdAt": "2024-12-16T16:19:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "60.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated/resolve/main/README.md",
    "description": "MLX-converted version of Meta's Llama-3.2-11B-Vision-Instruct model, abliterated for unrestricted image-text generation."
  },
  {
    "model_name": "gemma-3-1b-it-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 539,
    "createdAt": "2025-05-14T15:38:49.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "698.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit-DWQ/resolve/main/README.md",
    "description": "4-bit MLX version of Google's Gemma 3 1B instruction-tuned model."
  },
  {
    "model_name": "LFM2-VL-450M-8bit",
    "developer": "mlx-community",
    "downloads": 533,
    "createdAt": "2025-08-16T07:27:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-VL-450M-8bit/resolve/main/model.safetensors",
        "file_size": "533.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-VL-450M-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-VL-450M-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of LiquidAI's LFM2-VL-450M vision-language model for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 531,
    "createdAt": "2024-09-07T21:25:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "791.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit quantized version of DeepSeek-Coder-V2-Lite-Instruct for code generation."
  },
  {
    "model_name": "Qwen3-0.6B-bf16",
    "developer": "mlx-community",
    "downloads": 520,
    "createdAt": "2025-04-28T21:27:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-bf16/resolve/main/model.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-bf16/resolve/main/README.md",
    "description": "Qwen3-0.6B model converted to MLX format for text generation."
  },
  {
    "model_name": "whisper-tiny-4bit",
    "developer": "mlx-community",
    "downloads": 509,
    "createdAt": "2025-12-14T14:00:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/whisper-tiny-4bit/resolve/main/model.safetensors",
        "file_size": "21.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/whisper-tiny-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/whisper-tiny-4bit/resolve/main/README.md",
    "description": "MLX-converted Whisper tiny model for speech-to-text using mlx-audio-plus."
  },
  {
    "model_name": "Step-3.5-Flash-8bit",
    "developer": "mlx-community",
    "downloads": 508,
    "createdAt": "2026-02-04T14:18:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 43,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00001-of-00043.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00002-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00003-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00003-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00004-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00005-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00006-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00007-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00008-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00009-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00009-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00010-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00011-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00012-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00013-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00014-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00015-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00016-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00017-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00018-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00019-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00020-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00021-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00022-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00023-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00024-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00024-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00025-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00025-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00026-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00027-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00027-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00028-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00028-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00029-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00030-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00030-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00031-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00031-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00032-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00033-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00033-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00034-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00034-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00035-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00036-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00036-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00037-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00037-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00038-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00039-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00039-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00040-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00040-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00041-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00041-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00042-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00042-of-00043.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00043-of-00043",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/model-00043-of-00043.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Step-3.5-Flash-8bit/resolve/main/README.md",
    "description": "8-bit MLX version of Step-3.5-Flash text generation model."
  },
  {
    "model_name": "GLM-4-9B-0414-4bit",
    "developer": "mlx-community",
    "downloads": 505,
    "createdAt": "2025-04-17T10:59:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-4-9B-0414-4bit/resolve/main/model.safetensors",
        "file_size": "4.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4-9B-0414-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4-9B-0414-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of GLM-4-9B-0414 for text generation on Apple Silicon."
  },
  {
    "model_name": "Voxtral-Mini-3B-2507-bf16",
    "developer": "mlx-community",
    "downloads": 503,
    "createdAt": "2025-08-18T13:17:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "4.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16/resolve/main/README.md",
    "description": "A speech-to-text model converted to MLX format for audio transcription."
  },
  {
    "model_name": "medgemma-1.5-4b-it-8bit",
    "developer": "mlx-community",
    "downloads": 497,
    "createdAt": "2026-01-14T00:20:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "685.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-8bit/resolve/main/README.md",
    "description": "A medical AI vision-language model for clinical applications, converted to MLX format from Google's MedGemma."
  },
  {
    "model_name": "LFM2-1.2B-4bit",
    "developer": "mlx-community",
    "downloads": 494,
    "createdAt": "2025-07-11T23:52:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-1.2B-4bit/resolve/main/model.safetensors",
        "file_size": "628.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-1.2B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-1.2B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of LFM2-1.2B text generation model."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Llama-70B-4bit",
    "developer": "mlx-community",
    "downloads": 492,
    "createdAt": "2025-01-21T11:59:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-70B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of DeepSeek-R1-Distill-Llama-70B for Apple Silicon."
  },
  {
    "model_name": "mistralai_Ministral-3-14B-Instruct-2512-MLX-MXFP4",
    "developer": "mlx-community",
    "downloads": 490,
    "createdAt": "2025-12-30T17:39:14.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/mistralai_Ministral-3-14B-Instruct-2512-MLX-MXFP4/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/mistralai_Ministral-3-14B-Instruct-2512-MLX-MXFP4/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Ministral-3-14B-Instruct-2512-MLX-MXFP4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Ministral-3-14B-Instruct-2512-MLX-MXFP4/resolve/main/README.md",
    "description": "MLX-converted version of Mistral's Ministral-3-14B-Instruct model for text generation."
  },
  {
    "model_name": "dolphin3.0-llama3.2-1B-4Bit",
    "developer": "mlx-community",
    "downloads": 488,
    "createdAt": "2025-08-19T16:43:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/dolphin3.0-llama3.2-1B-4Bit/resolve/main/model.safetensors",
        "file_size": "663.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/dolphin3.0-llama3.2-1B-4Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/dolphin3.0-llama3.2-1B-4Bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of Dolphin3.0-Llama3.2-1B for use with Apple's MLX framework."
  },
  {
    "model_name": "DeepSeek-OCR-2-4bit",
    "developer": "mlx-community",
    "downloads": 487,
    "createdAt": "2026-01-28T11:04:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-4bit/resolve/main/model.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-converted version of DeepSeek-OCR-2 for vision-language OCR tasks."
  },
  {
    "model_name": "SmolVLM-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 486,
    "createdAt": "2024-11-26T22:18:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolVLM-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolVLM-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolVLM-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of SmolVLM-Instruct for image-text-to-text on Apple Silicon."
  },
  {
    "model_name": "paligemma-3b-mix-448-8bit",
    "developer": "mlx-community",
    "downloads": 485,
    "createdAt": "2024-05-15T23:28:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/paligemma-3b-mix-448-8bit/resolve/main/model.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/paligemma-3b-mix-448-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/paligemma-3b-mix-448-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX conversion of Google's PaliGemma 3B vision-language model for image-text-to-text tasks on Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-REAP-40-4bit",
    "developer": "mlx-community",
    "downloads": 482,
    "createdAt": "2026-01-14T06:52:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 16,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00001-of-00016.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00002-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00003-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00004-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00005-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00005-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00006-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00007-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00008-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00009-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00010-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00011-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00011-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00012-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00013-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00014-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00014-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00015-of-00016.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00016",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/model-00016-of-00016.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of MiniMax-M2.1-REAP-40 MoE model for text generation on Apple Silicon."
  },
  {
    "model_name": "granite-4.0-h-tiny-4bit",
    "developer": "mlx-community",
    "downloads": 481,
    "createdAt": "2025-10-04T11:42:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/granite-4.0-h-tiny-4bit/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/granite-4.0-h-tiny-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/granite-4.0-h-tiny-4bit/resolve/main/README.md",
    "description": "IBM Granite 4.0 H Tiny model converted to 4-bit MLX format for text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-Next-6bit",
    "developer": "mlx-community",
    "downloads": 479,
    "createdAt": "2026-02-03T17:31:15.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "659.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized MLX version of Qwen3-Coder-Next for Apple Silicon."
  },
  {
    "model_name": "GLM-4.6V-Flash-8bit",
    "developer": "mlx-community",
    "downloads": 475,
    "createdAt": "2025-12-08T14:07:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-8bit/resolve/main/README.md",
    "description": "An MLX-optimized 8-bit vision language model for image description based on GLM-4.6V-Flash."
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 473,
    "createdAt": "2024-09-26T00:01:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Meta's Llama 3.2 1B instruction model, converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3.2-Speciale-4bit",
    "developer": "mlx-community",
    "downloads": 472,
    "createdAt": "2025-12-01T13:43:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 89,
    "safetensors_files": [
      {
        "model_id": ".ipynb_checkpoints/model-00001-of-00088-checkpoint",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/.ipynb_checkpoints/model-00001-of-00088-checkpoint.safetensors",
        "file_size": "0 B"
      },
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.2-Speciale-4bit/resolve/main/README.md",
    "description": "4-bit MLX-quantized version of DeepSeek-V3.2-Speciale for text generation."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-6bit",
    "developer": "mlx-community",
    "downloads": 467,
    "createdAt": "2025-12-09T17:49:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-6bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-6bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-6bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-6bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-6bit/resolve/main/README.md",
    "description": "MLX-converted 6-bit quantized version of Devstral-Small-2-24B-Instruct for image generation using mlx-vlm."
  },
  {
    "model_name": "gemma-3-27b-it-4bit",
    "developer": "mlx-community",
    "downloads": 467,
    "createdAt": "2025-03-12T10:18:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "768.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-27b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-27b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of Google's Gemma 3 27B model for image-text-to-text."
  },
  {
    "model_name": "GLM-4.5-Air-3bit",
    "developer": "mlx-community",
    "downloads": 465,
    "createdAt": "2025-07-28T18:49:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 10,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00001-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00002-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00003-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00004-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00005-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00006-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00007-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00008-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00009-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00010",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/model-00010-of-00010.safetensors",
        "file_size": "575.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-3bit/resolve/main/README.md",
    "description": "MLX-converted 3-bit quantized version of GLM-4.5-Air for text generation on Apple Silicon."
  },
  {
    "model_name": "gemma-3-text-4b-it-4bit",
    "developer": "mlx-community",
    "downloads": 463,
    "createdAt": "2025-03-17T14:14:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-text-4b-it-4bit/resolve/main/model.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-text-4b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-text-4b-it-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of the Gemma 3 4B instruction model for use with `mlx-lm`."
  },
  {
    "model_name": "NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4",
    "developer": "mlx-community",
    "downloads": 463,
    "createdAt": "2026-01-29T04:30:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/resolve/main/README.md",
    "description": "NVIDIA's MLX-quantized 30B text generation model based on Nemotron-3-Nano."
  },
  {
    "model_name": "Qwen3-Coder-Next-5bit",
    "developer": "mlx-community",
    "downloads": 461,
    "createdAt": "2026-02-03T19:02:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 11,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00001-of-00011.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00002-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00003-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00004-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00005-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00006-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00007-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00008-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00009-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00010-of-00011.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00011",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/model-00011-of-00011.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-Next-5bit/resolve/main/README.md",
    "description": "MLX-converted version of Qwen3-Coder-Next code generation model."
  },
  {
    "model_name": "DeepSeek-OCR-4bit",
    "developer": "mlx-community",
    "downloads": 458,
    "createdAt": "2025-10-26T11:04:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-4bit/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-OCR-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-OCR-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX converted version of DeepSeek-OCR for vision-language and OCR tasks."
  },
  {
    "model_name": "Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX",
    "developer": "mlx-community",
    "downloads": 458,
    "createdAt": "2025-06-25T01:17:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-Q_6-MLX/resolve/main/README.md",
    "description": "MLX-converted Llama 3.2 8X4B MOE 21B uncensored/abliterated model for text generation on Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-REAP-30-4bit",
    "developer": "mlx-community",
    "downloads": 456,
    "createdAt": "2026-01-14T07:00:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 18,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00001-of-00018.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00002-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00003-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00004-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00005-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00006-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00007-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00008-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00009-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00010-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00011-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00012-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00013-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00014-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00015-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00016-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00017-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00018",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/model-00018-of-00018.safetensors",
        "file_size": "329.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-4bit/resolve/main/README.md",
    "description": "4-bit MLX-converted version of MiniMax-M2.1-REAP-30 text generation model for Apple Silicon."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2",
    "developer": "mlx-community",
    "downloads": 456,
    "createdAt": "2025-08-09T04:33:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-dwq-v2/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-Coder-30B-A3B-Instruct for text generation."
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 455,
    "createdAt": "2026-01-01T14:16:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "4.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX version of IQuest-Coder-V1-40B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Loop-Instruct-6bit",
    "developer": "mlx-community",
    "downloads": 453,
    "createdAt": "2026-01-06T22:37:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "524.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-6bit/resolve/main/README.md",
    "description": "6-bit MLX-quantized code generation model for Apple Silicon."
  },
  {
    "model_name": "dolphin-vision-72b-4bit",
    "developer": "mlx-community",
    "downloads": 452,
    "createdAt": "2024-07-04T14:16:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/dolphin-vision-72b-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized 72B vision-language model for image-text-to-text tasks, converted to MLX format from Qwen2-72B."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Llama-8B-4bit",
    "developer": "mlx-community",
    "downloads": 451,
    "createdAt": "2025-01-20T22:59:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-8B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of DeepSeek-R1-Distill-Llama-8B for Apple Silicon using mlx-lm."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 449,
    "createdAt": "2025-07-31T15:58:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "723.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Qwen3-Coder-30B-A3B-Instruct for text/code generation."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-CustomVoice-bf16",
    "developer": "mlx-community",
    "downloads": 447,
    "createdAt": "2026-01-22T20:38:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-bf16/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-bf16/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-bf16/resolve/main/README.md",
    "description": "An MLX-converted text-to-speech model with voice cloning support."
  },
  {
    "model_name": "Kimi-Linear-48B-A3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 442,
    "createdAt": "2025-10-31T02:27:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Kimi-Linear-48B-A3B-Instruct converted to MLX format."
  },
  {
    "model_name": "Qwen2.5-7B-Instruct-Uncensored-4bit",
    "developer": "mlx-community",
    "downloads": 441,
    "createdAt": "2024-10-21T09:31:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-Uncensored-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-Uncensored-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-Uncensored-4bit/resolve/main/README.md",
    "description": "An uncensored Qwen2.5-7B model converted to 4-bit MLX format for Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-REAP-40-5bit",
    "developer": "mlx-community",
    "downloads": 440,
    "createdAt": "2026-01-14T18:52:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 19,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00001-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00002-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00003-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00004-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00005-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00006-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00007-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00008-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00009-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00010-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00011-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00012-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00013-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00014-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00015-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00016-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00017-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00018-of-00019.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/model-00019-of-00019.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-5bit/resolve/main/README.md",
    "description": "MLX text-generation model converted from MiniMax-M2.1-REAP-40 with 5-bit quantization."
  },
  {
    "model_name": "Phi-4-mini-instruct-8bit",
    "developer": "mlx-community",
    "downloads": 439,
    "createdAt": "2025-02-27T11:07:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Phi-4-mini-instruct-8bit/resolve/main/model.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-4-mini-instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-4-mini-instruct-8bit/resolve/main/README.md",
    "description": "Phi-4-mini-instruct-8bit is an 8-bit quantized MLX version of Microsoft's Phi-4-mini-instruct model for text generation."
  },
  {
    "model_name": "CodeLlama-70b-Instruct-hf-4bit-MLX",
    "developer": "mlx-community",
    "downloads": 436,
    "createdAt": "2024-01-29T23:38:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/CodeLlama-70b-Instruct-hf-4bit-MLX/resolve/main/README.md",
    "description": "CodeLlama-70b-Instruct converted to MLX format for code generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-VL-8B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 434,
    "createdAt": "2025-10-14T18:10:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-8B-Instruct-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-8B-Instruct-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "387.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-8B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-8B-Instruct-4bit/resolve/main/README.md",
    "description": "Qwen3-VL-8B-Instruct vision-language model converted to MLX format in 4-bit quantization for Apple Silicon."
  },
  {
    "model_name": "Llama-3.1-8B-Instruct",
    "developer": "mlx-community",
    "downloads": 432,
    "createdAt": "2024-09-28T10:52:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1002.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.1-8B-Instruct/resolve/main/README.md",
    "description": "An MLX-converted version of Meta's Llama-3.1-8B-Instruct for use with Apple's ML framework via mlx-lm."
  },
  {
    "model_name": "gemma-3-4b-it-5bit",
    "developer": "mlx-community",
    "downloads": 431,
    "createdAt": "2025-10-09T14:41:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-4b-it-5bit/resolve/main/model.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-4b-it-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-4b-it-5bit/resolve/main/README.md",
    "description": "A 5-bit MLX conversion of Google's Gemma-3-4b-it model for Apple Silicon, used via `mlx-lm`."
  },
  {
    "model_name": "Qwen3-30B-A3B-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 424,
    "createdAt": "2025-05-05T06:07:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ/resolve/main/README.md",
    "description": "A 4-bit DWQ quantized version of Qwen3-30B-A3B for text generation on Apple Silicon via MLX."
  },
  {
    "model_name": "deepseek-r1-distill-qwen-1.5b",
    "developer": "mlx-community",
    "downloads": 422,
    "createdAt": "2025-01-20T12:48:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/deepseek-r1-distill-qwen-1.5b/resolve/main/model.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/deepseek-r1-distill-qwen-1.5b/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/deepseek-r1-distill-qwen-1.5b/resolve/main/README.md",
    "description": "A DeepSeek-R1 model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit",
    "developer": "mlx-community",
    "downloads": 422,
    "createdAt": "2025-02-20T05:04:15.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-abliterated-4bit/resolve/main/README.md",
    "description": "MLX-converted version of DeepSeek-R1-Distill-Qwen-32B-abliterated for use with mlx-lm."
  },
  {
    "model_name": "Qwen1.5-0.5B-Chat-4bit",
    "developer": "mlx-community",
    "downloads": 422,
    "createdAt": "2024-02-28T17:04:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen1.5-0.5B-Chat-4bit/resolve/main/model.safetensors",
        "file_size": "249.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen1.5-0.5B-Chat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen1.5-0.5B-Chat-4bit/resolve/main/README.md",
    "description": "4-bit quantized Qwen1.5-0.5B-Chat model converted for Apple MLX."
  },
  {
    "model_name": "MiniMax-M2-8bit",
    "developer": "mlx-community",
    "downloads": 417,
    "createdAt": "2025-10-28T15:51:48.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 47,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00001-of-00047.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00002-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00003-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00004-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00005-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00006-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00007-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00008-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00009-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00010-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00011-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00012-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00013-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00014-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00015-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00016-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00017-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00018-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00019-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00020-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00021-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00022-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00023-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00024-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00025-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00026-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00027-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00027-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00028-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00028-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00029-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00029-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00030-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00030-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00031-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00031-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00032-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00032-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00033-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00033-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00034-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00034-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00035-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00035-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00036-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00036-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00037-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00037-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00038-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00038-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00039-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00039-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00040-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00040-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00041-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00041-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00042-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00042-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00043-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00043-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00044-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00044-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00045-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00045-of-00047.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00046-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00046-of-00047.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00047-of-00047",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/model-00047-of-00047.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2-8bit/resolve/main/README.md",
    "description": "A MiniMax-M2 text generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "PaddleOCR-VL-1.5-bf16",
    "developer": "mlx-community",
    "downloads": 416,
    "createdAt": "2026-01-29T17:50:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/PaddleOCR-VL-1.5-bf16/resolve/main/model.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/PaddleOCR-VL-1.5-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/PaddleOCR-VL-1.5-bf16/resolve/main/README.md",
    "description": "MLX-converted OCR model for extracting text from images and documents."
  },
  {
    "model_name": "Codestral-22B-v0.1-4bit",
    "developer": "mlx-community",
    "downloads": 414,
    "createdAt": "2024-05-29T14:23:25.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-4bit/resolve/main/README.md",
    "description": "Codestral-22B-v0.1-4bit is a 4-bit quantized code generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 410,
    "createdAt": "2024-09-18T19:02:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "3.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit MLX conversion of Qwen2.5-3B-Instruct for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-bf16",
    "developer": "mlx-community",
    "downloads": 404,
    "createdAt": "2024-09-18T18:39:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-bf16/resolve/main/model.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-1.5B-Instruct-bf16/resolve/main/README.md",
    "description": "MLX-converted 1.5B parameter instruction-tuned language model from Qwen."
  },
  {
    "model_name": "NVIDIA-Nemotron-3-Nano-30B-A3B-4bit",
    "developer": "mlx-community",
    "downloads": 399,
    "createdAt": "2025-12-15T22:29:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of NVIDIA's Nemotron-3-Nano-30B-A3B language model for text generation."
  },
  {
    "model_name": "nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit",
    "developer": "mlx-community",
    "downloads": 399,
    "createdAt": "2024-10-16T07:11:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_4bit/resolve/main/README.md",
    "description": "4-bit MLX-quantized version of NVIDIA's Llama-3.1-Nemotron-70B-Instruct for text generation using mlx-lm."
  },
  {
    "model_name": "Qwen3-0.6B-4bit-AWQ",
    "developer": "mlx-community",
    "downloads": 398,
    "createdAt": "2025-05-03T19:57:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit-AWQ/resolve/main/model.safetensors",
        "file_size": "329.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit-AWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-4bit-AWQ/resolve/main/README.md",
    "description": "A 4-bit AWQ quantized version of Qwen3-0.6B converted to MLX format for text generation."
  },
  {
    "model_name": "medgemma-1.5-4b-it-4bit",
    "developer": "mlx-community",
    "downloads": 396,
    "createdAt": "2026-01-14T00:20:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-4bit/resolve/main/model.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-4bit/resolve/main/README.md",
    "description": "MLX-converted 4B medical imaging model for image-text tasks across radiology, dermatology, pathology, and ophthalmology."
  },
  {
    "model_name": "Mixtral-8x22B-4bit",
    "developer": "mlx-community",
    "downloads": 395,
    "createdAt": "2024-04-10T07:01:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 16,
    "safetensors_files": [
      {
        "model_id": ".ipynb_checkpoints/model-00001-of-00015-checkpoint",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/.ipynb_checkpoints/model-00001-of-00015-checkpoint.safetensors",
        "file_size": "0 B"
      },
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "4.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mixtral-8x22B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Mixtral-8x22B MoE model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4",
    "developer": "mlx-community",
    "downloads": 394,
    "createdAt": "2025-12-17T01:55:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "836.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-MXFP4/resolve/main/README.md",
    "description": "MLX-converted version of NVIDIA's Nemotron-3-Nano-30B-A3B text generation model with MXFP4 quantization."
  },
  {
    "model_name": "Granite-4.0-H-Tiny-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 393,
    "createdAt": "2025-10-02T19:59:30.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Granite-4.0-H-Tiny-4bit-DWQ/resolve/main/model.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Granite-4.0-H-Tiny-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Granite-4.0-H-Tiny-4bit-DWQ/resolve/main/README.md",
    "description": "A 4-bit quantized MLX text generation model from IBM's Granite 4.0 h-tiny."
  },
  {
    "model_name": "MiniMax-M2.1-5bit",
    "developer": "mlx-community",
    "downloads": 392,
    "createdAt": "2025-12-26T15:41:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 32,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00001-of-00032.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00002-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00003-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00004-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00005-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00005-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00006-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00007-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00008-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00009-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00010-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00011-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00011-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00012-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00013-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00014-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00014-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00015-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00016-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00017-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00017-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00018-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00019-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00020-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00020-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00021-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00021-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00022-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00022-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00023-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00023-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00024-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00024-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00025-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00025-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00026-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00026-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00027-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00027-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00028-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00028-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00029-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00029-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00030-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00030-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00031-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00031-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00032-of-00032",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/model-00032-of-00032.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-5bit/resolve/main/README.md",
    "description": "5-bit quantized MiniMax-M2.1 text generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "LFM2-350M-4bit",
    "developer": "mlx-community",
    "downloads": 390,
    "createdAt": "2025-07-11T23:18:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-350M-4bit/resolve/main/model.safetensors",
        "file_size": "190.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-350M-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-350M-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized version of the LFM2-350M language model converted to MLX format for efficient text generation on Apple silicon."
  },
  {
    "model_name": "Qwen2.5-72B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 389,
    "createdAt": "2024-09-18T20:40:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen2.5-72B-Instruct for Apple Silicon devices."
  },
  {
    "model_name": "Llama-3.2-3B-Instruct",
    "developer": "mlx-community",
    "downloads": 388,
    "createdAt": "2024-09-25T23:04:41.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1008.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct/resolve/main/README.md",
    "description": "This is Meta's Llama-3.2-3B-Instruct model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Kimi-K2.5-3bit",
    "developer": "mlx-community",
    "downloads": 387,
    "createdAt": "2026-02-07T10:24:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 91,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00001-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00002-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00003-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00004-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00005-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00006-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00007-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00008-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00009-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00010-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00011-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00012-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00013-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00014-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00015-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00016-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00017-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00018-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00019-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00020-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00021-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00021-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00022-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00022-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00023-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00024-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00024-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00025-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00025-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00026-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00027-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00027-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00028-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00028-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00029-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00030-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00030-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00031-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00031-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00032-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00033-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00033-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00034-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00034-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00035-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00036-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00036-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00037-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00037-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00038-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00039-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00039-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00040-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00040-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00041-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00041-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00042-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00042-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00043-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00043-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00044-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00044-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00045-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00045-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00046-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00046-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00047-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00047-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00048-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00048-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00049-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00049-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00050-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00050-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00051-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00051-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00052-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00052-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00053-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00053-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00054-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00054-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00055-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00055-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00056-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00056-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00057-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00057-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00058-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00058-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00059-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00059-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00060-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00060-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00061-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00061-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00062-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00062-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00063-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00063-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00064-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00064-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00065-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00065-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00066-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00066-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00067-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00067-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00068-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00068-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00069-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00069-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00070-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00070-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00071-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00071-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00072-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00072-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00073-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00073-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00074-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00074-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00075-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00075-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00076-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00076-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00077-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00077-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00078-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00078-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00079-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00079-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00080-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00080-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00081-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00081-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00082-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00082-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00083-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00083-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00084-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00084-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00085-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00085-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00086-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00086-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00087-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00087-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00088-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00088-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00089-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00089-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00090-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00090-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00091-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/model-00091-of-00091.safetensors",
        "file_size": "490.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2.5-3bit/resolve/main/README.md",
    "description": "3-bit MLX quantized Kimi-K2.5 text generation model."
  },
  {
    "model_name": "Qwen3-ASR-0.6B-4bit",
    "developer": "mlx-community",
    "downloads": 386,
    "createdAt": "2026-01-29T15:51:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-4bit/resolve/main/model.safetensors",
        "file_size": "675.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized speech-to-text model in MLX format for Apple Silicon."
  },
  {
    "model_name": "LFM2-2.6B-Exp-4bit",
    "developer": "mlx-community",
    "downloads": 385,
    "createdAt": "2025-12-26T11:11:14.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-2.6B-Exp-4bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-2.6B-Exp-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-2.6B-Exp-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX quantized version of Liquid's LFM2-2.6B-Exp language model for text generation on Apple Silicon."
  },
  {
    "model_name": "Llama-3.2-11B-Vision-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 385,
    "createdAt": "2024-10-18T07:22:41.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "627.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Llama-3.2-11B-Vision-Instruct for vision-language tasks."
  },
  {
    "model_name": "MiniMax-M2.1-REAP-30-8bit",
    "developer": "mlx-community",
    "downloads": 379,
    "createdAt": "2026-01-14T07:14:56.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 38,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00001-of-00038.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00002-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00003-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00003-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00004-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00004-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00005-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00005-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00006-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00006-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00007-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00007-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00008-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00008-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00009-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00009-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00010-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00010-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00011-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00011-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00012-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00012-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00013-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00013-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00014-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00014-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00015-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00015-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00016-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00016-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00017-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00017-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00018-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00018-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00019-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00019-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00020-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00020-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00021-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00021-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00022-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00022-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00023-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00023-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00024-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00024-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00025-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00025-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00026-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00026-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00027-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00027-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00028-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00028-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00029-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00029-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00030-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00030-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00031-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00031-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00032-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00032-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00033-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00033-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00034-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00034-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00035-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00035-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00036-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00036-of-00038.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00037-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00037-of-00038.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00038-of-00038",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/model-00038-of-00038.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-30-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX quantized version of MiniMax-M2.1-REAP-30 for text generation on Apple Silicon."
  },
  {
    "model_name": "Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit",
    "developer": "mlx-community",
    "downloads": 377,
    "createdAt": "2025-06-19T15:12:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Dolphin-Mistral-24B-Venice-Edition-mlx-8Bit/resolve/main/README.md",
    "description": "An 8-bit MLX quantized version of the uncensored Dolphin-Mistral-24B-Venice-Edition model for Apple Silicon."
  },
  {
    "model_name": "Qwen2.5-Coder-32B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 377,
    "createdAt": "2024-11-11T18:28:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-Coder-32B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX version of Qwen2.5-Coder-32B-Instruct for code generation tasks."
  },
  {
    "model_name": "DeepSeek-V3.2-8bit",
    "developer": "mlx-community",
    "downloads": 373,
    "createdAt": "2025-12-03T10:48:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 175,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00001-of-00175.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00002-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00002-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00003-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00003-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00004-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00005-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00005-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00006-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00006-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00007-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00008-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00008-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00009-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00009-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00010-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00011-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00011-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00012-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00012-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00013-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00014-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00014-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00015-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00015-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00016-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00017-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00017-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00018-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00018-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00019-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00020-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00020-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00021-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00021-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00022-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00023-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00023-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00024-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00024-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00025-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00026-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00026-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00027-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00027-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00028-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00029-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00029-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00030-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00030-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00031-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00032-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00032-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00033-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00033-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00034-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00035-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00035-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00036-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00036-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00037-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00038-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00038-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00039-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00039-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00040-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00041-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00041-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00042-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00042-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00043-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00044-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00044-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00045-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00045-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00046-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00047-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00047-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00048-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00048-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00049-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00050-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00050-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00051-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00051-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00052-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00053-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00053-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00054-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00054-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00055-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00056-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00056-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00057-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00057-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00058-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00059-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00059-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00060-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00060-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00061-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00062-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00062-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00063-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00063-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00064-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00065-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00065-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00066-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00066-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00067-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00068-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00068-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00069-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00069-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00070-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00071-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00071-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00072-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00072-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00073-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00074-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00074-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00075-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00075-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00076-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00077-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00077-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00078-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00078-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00079-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00080-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00080-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00081-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00081-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00082-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00083-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00083-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00084-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00084-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00085-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00086-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00086-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00087-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00087-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00088-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00089-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00089-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00090-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00090-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00091-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00091-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00092-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00092-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00093-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00093-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00094-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00094-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00095-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00095-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00096-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00096-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00097-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00097-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00098-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00098-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00099-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00099-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00100-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00100-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00101-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00101-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00102-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00102-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00103-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00103-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00104-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00104-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00105-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00105-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00106-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00106-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00107-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00107-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00108-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00108-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00109-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00109-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00110-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00110-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00111-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00111-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00112-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00112-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00113-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00113-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00114-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00114-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00115-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00115-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00116-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00116-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00117-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00117-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00118-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00118-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00119-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00119-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00120-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00120-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00121-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00121-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00122-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00122-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00123-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00123-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00124-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00124-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00125-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00125-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00126-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00126-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00127-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00127-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00128-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00128-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00129-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00129-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00130-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00130-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00131-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00131-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00132-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00132-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00133-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00133-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00134-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00134-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00135-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00135-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00136-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00136-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00137-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00137-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00138-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00138-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00139-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00139-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00140-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00140-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00141-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00141-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00142-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00142-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00143-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00143-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00144-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00144-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00145-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00145-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00146-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00146-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00147-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00147-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00148-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00148-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00149-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00149-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00150-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00150-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00151-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00151-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00152-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00152-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00153-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00153-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00154-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00154-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00155-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00155-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00156-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00156-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00157-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00157-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00158-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00158-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00159-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00159-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00160-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00160-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00161-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00161-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00162-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00162-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00163-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00163-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00164-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00164-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00165-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00165-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00166-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00166-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00167-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00167-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00168-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00168-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00169-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00169-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00170-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00170-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00171-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00171-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00172-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00172-of-00175.safetensors",
        "file_size": "4.2 GB"
      },
      {
        "model_id": "model-00173-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00173-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00174-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00174-of-00175.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00175-of-00175",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/model-00175-of-00175.safetensors",
        "file_size": "5.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.2-8bit/resolve/main/README.md",
    "description": "8-bit MLX conversion of DeepSeek-V3.2 for text generation."
  },
  {
    "model_name": "Qwen3-4B-bf16",
    "developer": "mlx-community",
    "downloads": 371,
    "createdAt": "2025-04-28T21:45:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-bf16/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-bf16/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-bf16/resolve/main/README.md",
    "description": "A Qwen3-4B language model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "gemma-3-12b-it-qat-abliterated-lm-4bit",
    "developer": "mlx-community",
    "downloads": 370,
    "createdAt": "2025-09-28T13:38:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-abliterated-lm-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-abliterated-lm-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-abliterated-lm-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-abliterated-lm-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of the Gemma-3-12b-It-Qat-Abliterated model for use with the `mlx-lm` library."
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Instruct-6bit",
    "developer": "mlx-community",
    "downloads": 367,
    "createdAt": "2026-01-01T14:07:48.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "524.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized MLX version of IQuest-Coder-V1-40B-Instruct for Apple Silicon."
  },
  {
    "model_name": "Falcon-H1R-7B-4bit",
    "developer": "mlx-community",
    "downloads": 366,
    "createdAt": "2026-01-05T13:17:51.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Falcon-H1R-7B-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Falcon-H1R-7B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Falcon-H1R-7B-4bit/resolve/main/README.md",
    "description": "Falcon-H1R-7B-4bit is a 4-bit quantized text generation model in MLX format converted from Falcon-H1R-7B."
  },
  {
    "model_name": "gemma-3-12b-it-4bit",
    "developer": "mlx-community",
    "downloads": 366,
    "createdAt": "2025-03-12T08:46:35.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized version of Google's Gemma 3 12B model, converted to MLX format for image-text-to-text tasks using the `mlx-vlm` library."
  },
  {
    "model_name": "Llama-4-Maverick-17B-16E-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 365,
    "createdAt": "2025-04-06T06:38:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 72,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00001-of-00072.safetensors",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "model-00002-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00002-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00003-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00003-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00004-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00004-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00005-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00005-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00006-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00006-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00007-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00007-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00008-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00008-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00009-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00009-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00010-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00010-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00011-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00011-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00012-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00012-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00013-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00013-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00014-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00014-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00015-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00015-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00016-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00016-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00017-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00017-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00018-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00018-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00019-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00019-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00020-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00020-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00021-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00021-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00022-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00022-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00023-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00023-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00024-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00024-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00025-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00025-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00026-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00026-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00027-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00027-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00028-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00028-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00029-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00029-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00030-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00030-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00031-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00031-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00032-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00032-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00033-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00033-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00034-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00034-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00035-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00035-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00036-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00036-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00037-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00037-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00038-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00038-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00039-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00039-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00040-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00040-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00041-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00041-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00042-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00042-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00043-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00043-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00044-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00044-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00045-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00045-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00046-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00046-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00047-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00047-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00048-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00048-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00049-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00049-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00050-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00050-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00051-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00051-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00052-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00052-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00053-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00053-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00054-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00054-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00055-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00055-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00056-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00056-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00057-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00057-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00058-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00058-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00059-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00059-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00060-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00060-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00061-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00061-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00062-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00062-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00063-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00063-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00064-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00064-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00065-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00065-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00066-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00066-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00067-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00067-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00068-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00068-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00069-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00069-of-00072.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00070-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00070-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00071-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00071-of-00072.safetensors",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "model-00072-of-00072",
        "path": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/model-00072-of-00072.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of Meta's Llama-4-Maverick-17B-128E-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2.1-REAP-40-8bit",
    "developer": "mlx-community",
    "downloads": 363,
    "createdAt": "2026-01-14T07:15:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 31,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00001-of-00031.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00002-of-00031.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00003-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00003-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00004-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00004-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00005-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00005-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00006-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00006-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00007-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00007-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00008-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00008-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00009-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00009-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00010-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00010-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00011-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00011-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00012-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00012-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00013-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00013-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00014-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00014-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00015-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00015-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00016-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00016-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00017-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00017-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00018-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00018-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00019-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00019-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00020-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00020-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00021-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00021-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00022-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00022-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00023-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00023-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00024-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00024-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00025-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00025-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00026-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00026-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00027-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00027-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00028-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00028-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00029-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00029-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00030-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00030-of-00031.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00031-of-00031",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/model-00031-of-00031.safetensors",
        "file_size": "5.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2.1-REAP-40-8bit/resolve/main/README.md",
    "description": "MiniMax-M2.1-REAP-40 is an 8-bit quantized MLX model for text generation, converted from a pruned MoE base model."
  },
  {
    "model_name": "Step-3.5-Flash-6bit",
    "developer": "mlx-community",
    "downloads": 361,
    "createdAt": "2026-02-03T08:02:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 32,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00001-of-00032.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00002-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00002-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00003-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00004-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00005-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00005-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00006-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00007-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00008-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00009-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00010-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00011-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00011-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00012-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00013-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00014-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00014-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00015-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00016-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00017-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00017-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00018-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00019-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00020-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00020-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00021-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00021-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00022-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00022-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00023-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00023-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00024-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00024-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00025-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00025-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00026-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00026-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00027-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00027-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00028-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00028-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00029-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00029-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00030-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00030-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00031-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00031-of-00032.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00032-of-00032",
        "path": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/model-00032-of-00032.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Step-3.5-Flash-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized MLX version of Step-3.5-Flash for text generation."
  },
  {
    "model_name": "Kimi-K2-Thinking-3bit",
    "developer": "mlx-community",
    "downloads": 354,
    "createdAt": "2025-11-25T06:48:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 91,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00001-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00002-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00003-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00004-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00005-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00006-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00007-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00008-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00009-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00010-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00011-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00012-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00013-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00014-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00015-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00016-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00017-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00018-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00019-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00020-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00021-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00021-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00022-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00022-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00023-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00024-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00024-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00025-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00025-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00026-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00027-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00027-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00028-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00028-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00029-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00030-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00030-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00031-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00031-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00032-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00033-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00033-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00034-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00034-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00035-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00036-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00036-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00037-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00037-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00038-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00039-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00039-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00040-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00040-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00041-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00041-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00042-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00042-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00043-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00043-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00044-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00044-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00045-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00045-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00046-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00046-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00047-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00047-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00048-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00048-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00049-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00049-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00050-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00050-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00051-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00051-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00052-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00052-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00053-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00053-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00054-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00054-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00055-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00055-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00056-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00056-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00057-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00057-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00058-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00058-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00059-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00059-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00060-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00060-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00061-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00061-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00062-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00062-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00063-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00063-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00064-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00064-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00065-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00065-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00066-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00066-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00067-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00067-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00068-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00068-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00069-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00069-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00070-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00070-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00071-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00071-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00072-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00072-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00073-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00073-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00074-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00074-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00075-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00075-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00076-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00076-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00077-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00077-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00078-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00078-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00079-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00079-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00080-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00080-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00081-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00081-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00082-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00082-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00083-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00083-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00084-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00084-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00085-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00085-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00086-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00086-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00087-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00087-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00088-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00088-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00089-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00089-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00090-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00090-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00091-of-00091",
        "path": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/model-00091-of-00091.safetensors",
        "file_size": "490.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-K2-Thinking-3bit/resolve/main/README.md",
    "description": "A 3-bit MLX-quantized version of Kimi-K2-Thinking for text generation on Apple Silicon."
  },
  {
    "model_name": "VibeVoice-ASR-4bit",
    "developer": "mlx-community",
    "downloads": 351,
    "createdAt": "2026-01-23T01:46:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "330.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/VibeVoice-ASR-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/VibeVoice-ASR-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized speech-to-text model for Apple Silicon, converted to MLX format from Microsoft's VibeVoice-ASR."
  },
  {
    "model_name": "chatterbox-turbo-fp16",
    "developer": "mlx-community",
    "downloads": 350,
    "createdAt": "2025-12-17T13:12:14.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "conds",
        "path": "https://huggingface.co/mlx-community/chatterbox-turbo-fp16/resolve/main/conds.safetensors",
        "file_size": "161.0 KB"
      },
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/chatterbox-turbo-fp16/resolve/main/model.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/chatterbox-turbo-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/chatterbox-turbo-fp16/resolve/main/README.md",
    "description": "MLX-converted text-to-speech model with voice cloning and emotion control via expressive tags."
  },
  {
    "model_name": "GLM-Z1-9B-0414-4bit",
    "developer": "mlx-community",
    "downloads": 349,
    "createdAt": "2025-04-17T13:56:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-Z1-9B-0414-4bit/resolve/main/model.safetensors",
        "file_size": "4.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-Z1-9B-0414-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-Z1-9B-0414-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of GLM-Z1-9B-0414 for text generation."
  },
  {
    "model_name": "Kokoro-82M-4bit",
    "developer": "mlx-community",
    "downloads": 348,
    "createdAt": "2025-02-28T18:42:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 55,
    "safetensors_files": [
      {
        "model_id": "kokoro-v1_0",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/kokoro-v1_0.safetensors",
        "file_size": "269.7 MB"
      },
      {
        "model_id": "voices/af_alloy",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_alloy.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_aoede",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_aoede.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_bella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_bella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_heart",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_heart.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_jessica",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_jessica.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_kore",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_kore.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nicole",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_nicole.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nova",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_nova.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_river",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_river.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sarah",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_sarah.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sky",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/af_sky.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_adam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_adam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_echo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_echo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_eric",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_eric.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_fenrir",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_fenrir.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_liam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_liam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_michael",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_michael.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_onyx",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_onyx.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_puck",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_puck.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/am_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_alice",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bf_alice.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_emma",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bf_emma.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_isabella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bf_isabella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_lily",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bf_lily.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_daniel",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bm_daniel.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_fable",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bm_fable.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_george",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bm_george.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_lewis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/bm_lewis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ef_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/ef_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/em_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/em_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ff_siwis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/ff_siwis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/hf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_beta",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/hf_beta.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_omega",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/hm_omega.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_psi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/hm_psi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/if_sara",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/if_sara.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/im_nicola",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/im_nicola.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/jf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_gongitsune",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/jf_gongitsune.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_nezumi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/jf_nezumi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_tebukuro",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/jf_tebukuro.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jm_kumo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/jm_kumo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pf_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/pf_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/pm_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/pm_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaobei",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zf_xiaobei.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoni",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zf_xiaoni.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoxiao",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zf_xiaoxiao.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoyi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zf_xiaoyi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunjian",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zm_yunjian.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zm_yunxi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxia",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zm_yunxia.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunyang",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/voices/zm_yunyang.safetensors",
        "file_size": "510.1 KB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kokoro-82M-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "gemma-3-1b-it-8bit",
    "developer": "mlx-community",
    "downloads": 343,
    "createdAt": "2025-03-12T09:00:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-1b-it-8bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-1b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-1b-it-8bit/resolve/main/README.md",
    "description": "8-bit MLX conversion of Google's Gemma-3-1b-it for Apple Silicon using mlx-lm."
  },
  {
    "model_name": "MiMo-V2-Flash-4bit",
    "developer": "mlx-community",
    "downloads": 343,
    "createdAt": "2025-12-16T23:53:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 36,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00001-of-00036.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00002-of-00036.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00003-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00004-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00005-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00006-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00007-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00007-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00008-of-00036.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00009-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00009-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00010-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00010-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00011-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00012-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00013-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00013-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00014-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00015-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00015-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00016-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00016-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00017-of-00036.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00018-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00018-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00019-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00019-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00020-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00021-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00021-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00022-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00022-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00023-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00024-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00024-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00025-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00025-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00026-of-00036.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00027-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00027-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00028-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00028-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00029-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00030-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00030-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00031-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00031-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00032-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00033-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00033-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00034-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00034-of-00036.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00035-of-00036.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00036-of-00036",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/model-00036-of-00036.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiMo-V2-Flash-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Xiaomi's MiMo-V2-Flash text generation model."
  },
  {
    "model_name": "Qwen3-4B-8bit",
    "developer": "mlx-community",
    "downloads": 342,
    "createdAt": "2025-04-28T21:38:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-8bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Qwen3-4B converted to MLX format for text generation."
  },
  {
    "model_name": "SmolVLM2-256M-Video-Instruct-mlx",
    "developer": "mlx-community",
    "downloads": 341,
    "createdAt": "2025-02-17T21:43:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolVLM2-256M-Video-Instruct-mlx/resolve/main/model.safetensors",
        "file_size": "489.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolVLM2-256M-Video-Instruct-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolVLM2-256M-Video-Instruct-mlx/resolve/main/README.md",
    "description": "Small video-language model (256M parameters) converted to MLX format for Apple Silicon, based on SmolVLM2-256M-Video-Instruct."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-2506-6bit",
    "developer": "mlx-community",
    "downloads": 337,
    "createdAt": "2025-08-16T18:17:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-6bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-6bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-6bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-6bit/resolve/main/README.md",
    "description": "This is a 6-bit quantized MLX version of Kimi-VL-A3B-Thinking-2506 for image-text-to-text tasks."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-4bit-DWQ-2510",
    "developer": "mlx-community",
    "downloads": 337,
    "createdAt": "2025-10-11T16:03:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit-DWQ-2510/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit-DWQ-2510/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-4bit-DWQ-2510/resolve/main/README.md",
    "description": "4-bit quantized Qwen3-4B-Instruct model converted to MLX format."
  },
  {
    "model_name": "Mistral-Small-24B-Instruct-2501-4bit",
    "developer": "mlx-community",
    "downloads": 334,
    "createdAt": "2025-01-30T14:46:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Mistral-Small-24B-Instruct for Apple Silicon inference."
  },
  {
    "model_name": "embeddinggemma-300m-8bit",
    "developer": "mlx-community",
    "downloads": 333,
    "createdAt": "2025-09-04T16:53:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/embeddinggemma-300m-8bit/resolve/main/model.safetensors",
        "file_size": "311.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/embeddinggemma-300m-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/embeddinggemma-300m-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX conversion of Google's EmbeddingGemma 300M model for sentence similarity and feature extraction tasks, usable via the `mlx-embeddings` library."
  },
  {
    "model_name": "llava-v1.6-mistral-7b-4bit",
    "developer": "mlx-community",
    "downloads": 333,
    "createdAt": "2024-06-22T10:39:28.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/llava-v1.6-mistral-7b-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/llava-v1.6-mistral-7b-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/llava-v1.6-mistral-7b-4bit/resolve/main/README.md",
    "description": "LLaVA v1.6 Mistral 7B vision-language model converted to MLX format for image-text generation."
  },
  {
    "model_name": "encodec-24khz-float32",
    "developer": "mlx-community",
    "downloads": 332,
    "createdAt": "2024-09-18T12:52:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/encodec-24khz-float32/resolve/main/model.safetensors",
        "file_size": "72.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/encodec-24khz-float32/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/encodec-24khz-float32/resolve/main/README.md",
    "description": "An EnCodec audio compression model converted to MLX format for use with the MLX examples."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ",
    "developer": "mlx-community",
    "downloads": 329,
    "createdAt": "2025-08-01T20:33:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit-DWQ/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-Coder-30B-A3B-Instruct for text generation."
  },
  {
    "model_name": "VibeVoice-ASR-bf16",
    "developer": "mlx-community",
    "downloads": 329,
    "createdAt": "2026-01-23T02:05:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-bf16/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-bf16/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-bf16/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-bf16/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/VibeVoice-ASR-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/VibeVoice-ASR-bf16/resolve/main/README.md",
    "description": "This is an MLX-converted speech-to-text model for audio transcription."
  },
  {
    "model_name": "mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit",
    "developer": "mlx-community",
    "downloads": 328,
    "createdAt": "2025-12-14T01:54:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit/resolve/main/README.md",
    "description": "Mistral's Devstral-Small-2-24B-Instruct-2512 model converted to 4-bit MLX format for Apple silicon."
  },
  {
    "model_name": "nomicai-modernbert-embed-base-4bit",
    "developer": "mlx-community",
    "downloads": 327,
    "createdAt": "2025-04-02T13:59:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/nomicai-modernbert-embed-base-4bit/resolve/main/model.safetensors",
        "file_size": "80.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/nomicai-modernbert-embed-base-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/nomicai-modernbert-embed-base-4bit/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Qwen3-ASR-0.6B-bf16",
    "developer": "mlx-community",
    "downloads": 325,
    "createdAt": "2026-01-29T15:48:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-bf16/resolve/main/model.safetensors",
        "file_size": "1.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-ASR-0.6B-bf16/resolve/main/README.md",
    "description": "A 0.6B parameter speech-to-text model converted to MLX format for Apple Silicon devices."
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 323,
    "createdAt": "2026-01-01T11:51:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "961.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of a 40B parameter code generation model for text generation on Apple Silicon."
  },
  {
    "model_name": "LFM2.5-Audio-1.5B-bf16",
    "developer": "mlx-community",
    "downloads": 323,
    "createdAt": "2026-01-08T14:35:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "audio_detokenizer/model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-bf16/resolve/main/audio_detokenizer/model.safetensors",
        "file_size": "299.6 MB"
      },
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-bf16/resolve/main/model.safetensors",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "tokenizer-e351c8d8-checkpoint125",
        "path": "https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-bf16/resolve/main/tokenizer-e351c8d8-checkpoint125.safetensors",
        "file_size": "366.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-bf16/resolve/main/README.md",
    "description": "MLX-converted 1.5B parameter Liquid Foundation Model for speech-to-speech, text-to-speech, and speech-to-text with interleaved audio/text generation on Apple Silicon."
  },
  {
    "model_name": "LongCat-Flash-Lite-4bit",
    "developer": "mlx-community",
    "downloads": 323,
    "createdAt": "2026-01-29T12:06:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LongCat-Flash-Lite-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized text-generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-4B-4bit-DWQ-053125",
    "developer": "mlx-community",
    "downloads": 320,
    "createdAt": "2025-06-01T16:19:02.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-4bit-DWQ-053125/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-4bit-DWQ-053125/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-4bit-DWQ-053125/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Qwen3-4B for text generation on Apple Silicon."
  },
  {
    "model_name": "Llama-4-Scout-17B-16E-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 316,
    "createdAt": "2025-04-06T12:46:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 23,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00001-of-00023.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00002-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00003-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00004-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00004-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00005-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00006-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00006-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00007-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00008-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00008-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00009-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00010-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00010-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00011-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00012-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00012-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00013-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00014-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00014-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00015-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00016-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00016-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00017-of-00023.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00018-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00018-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00019-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00020-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00020-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00021-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00021-of-00023.safetensors",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "model-00022-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00022-of-00023.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00023-of-00023",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/model-00023-of-00023.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-converted version of Meta's Llama-4-Scout-17B-16E-Instruct model for Apple hardware."
  },
  {
    "model_name": "TinyLlama-1.1B-Chat-v1.0-4bit",
    "developer": "mlx-community",
    "downloads": 314,
    "createdAt": "2024-01-05T21:25:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit/resolve/main/weights.00.safetensors",
        "file_size": "680.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit/resolve/main/README.md",
    "description": "A 1.1B parameter chat model converted to MLX format for Apple Silicon inference."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-bf16",
    "developer": "mlx-community",
    "downloads": 312,
    "createdAt": "2025-12-09T18:33:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 10,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00001-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00002-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00003-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00004-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00005-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00006-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00007-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00008-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00009-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00010",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/model-00010-of-00010.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-bf16/resolve/main/README.md",
    "description": "MLX-converted instruct model for image understanding."
  },
  {
    "model_name": "llava-1.5-7b-4bit",
    "developer": "mlx-community",
    "downloads": 311,
    "createdAt": "2024-04-23T12:14:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/llava-1.5-7b-4bit/resolve/main/model.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/llava-1.5-7b-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/llava-1.5-7b-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of LLaVA 1.5 7B vision-language model converted to MLX format for Apple devices."
  },
  {
    "model_name": "granite-4.0-h-350m-8bit",
    "developer": "mlx-community",
    "downloads": 306,
    "createdAt": "2025-10-28T16:48:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/granite-4.0-h-350m-8bit/resolve/main/model.safetensors",
        "file_size": "345.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/granite-4.0-h-350m-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/granite-4.0-h-350m-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX version of IBM's granite-4.0-h-350m language model for text generation."
  },
  {
    "model_name": "GLM-4.7-Flash-Derestricted-8bit-gs32",
    "developer": "mlx-community",
    "downloads": 304,
    "createdAt": "2026-01-26T23:17:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.7-Flash-Derestricted-8bit-gs32/resolve/main/README.md",
    "description": "An 8-bit MLX-converted derestricted GLM-4.7-Flash model for text generation on Apple Silicon."
  },
  {
    "model_name": "Nous-Hermes-2-Mixtral-8x7B-DPO-4bit",
    "developer": "mlx-community",
    "downloads": 304,
    "createdAt": "2024-01-17T04:56:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/Nous-Hermes-2-Mixtral-8x7B-DPO-4bit/resolve/main/weights.00.safetensors",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "weights.01",
        "path": "https://huggingface.co/mlx-community/Nous-Hermes-2-Mixtral-8x7B-DPO-4bit/resolve/main/weights.01.safetensors",
        "file_size": "9.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Nous-Hermes-2-Mixtral-8x7B-DPO-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Nous-Hermes-2-Mixtral-8x7B-DPO-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-converted Mixtral-8x7B DPO-finetuned instruct model."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-CustomVoice-4bit",
    "developer": "mlx-community",
    "downloads": 304,
    "createdAt": "2026-01-22T20:32:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-4bit/resolve/main/model.safetensors",
        "file_size": "960.1 MB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-4bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-CustomVoice-4bit/resolve/main/README.md",
    "description": "A 0.6B parameter text-to-speech model with voice cloning support, converted for Apple Silicon via MLX."
  },
  {
    "model_name": "HY-MT1.5-1.8B-8bit",
    "developer": "mlx-community",
    "downloads": 303,
    "createdAt": "2026-01-03T17:39:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/HY-MT1.5-1.8B-8bit/resolve/main/model.safetensors",
        "file_size": "1.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/HY-MT1.5-1.8B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/HY-MT1.5-1.8B-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX translation model supporting 36 languages, converted from Tencent's HY-MT1.5-1.8B for Apple Silicon."
  },
  {
    "model_name": "csm-1b",
    "developer": "mlx-community",
    "downloads": 300,
    "createdAt": "2025-03-17T03:16:03.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/csm-1b/resolve/main/model.safetensors",
        "file_size": "5.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/csm-1b/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/csm-1b/resolve/main/README.md",
    "description": "MLX text-to-speech model."
  },
  {
    "model_name": "functiongemma-270m-it-bf16",
    "developer": "mlx-community",
    "downloads": 299,
    "createdAt": "2025-12-18T11:24:24.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/functiongemma-270m-it-bf16/resolve/main/model.safetensors",
        "file_size": "511.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/functiongemma-270m-it-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/functiongemma-270m-it-bf16/resolve/main/README.md",
    "description": "An MLX conversion of Google's FunctionGemma 270M instruction-tuned model for text generation on Apple Silicon."
  },
  {
    "model_name": "gemma-3-270m-it-4bit",
    "developer": "mlx-community",
    "downloads": 299,
    "createdAt": "2025-08-09T20:19:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-270m-it-4bit/resolve/main/model.safetensors",
        "file_size": "143.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-270m-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-270m-it-4bit/resolve/main/README.md",
    "description": "Google's Gemma 3 270m instruction-tuned model converted to 4-bit MLX format."
  },
  {
    "model_name": "gemma-3-27b-it-qat-8bit",
    "developer": "mlx-community",
    "downloads": 298,
    "createdAt": "2025-04-15T20:46:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-8bit/resolve/main/README.md",
    "description": "A Gemma-3-27b-it model converted to MLX format for image-text-to-text tasks."
  },
  {
    "model_name": "IQuest-Coder-V1-40B-Instruct-5bit",
    "developer": "mlx-community",
    "downloads": 298,
    "createdAt": "2026-01-02T06:10:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "536.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/IQuest-Coder-V1-40B-Instruct-5bit/resolve/main/README.md",
    "description": "A 5-bit quantized code generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "GLM-4.6V-mxfp4",
    "developer": "mlx-community",
    "downloads": 296,
    "createdAt": "2025-12-10T05:24:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 12,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00001-of-00012.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00002-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00003-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00004-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00005-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00006-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00007-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00008-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00009-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00010-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00011-of-00012.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/model-00012-of-00012.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.6V-mxfp4/resolve/main/README.md",
    "description": "GLM-4.6V-mxfp4 is an MLX-converted vision-language model for image-text-to-text tasks."
  },
  {
    "model_name": "Qwen3-14B-4bit",
    "developer": "mlx-community",
    "downloads": 295,
    "createdAt": "2025-04-28T22:29:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-14B-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-14B-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-14B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-14B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen3-14B converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Ministral-3-14B-Reasoning-2512-4bit",
    "developer": "mlx-community",
    "downloads": 294,
    "createdAt": "2025-12-04T00:40:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Mistral's Ministral-3-14B-Reasoning model for vision-language tasks on Apple Silicon."
  },
  {
    "model_name": "Josiefied-Qwen3-14B-abliterated-v3-4bit",
    "developer": "mlx-community",
    "downloads": 291,
    "createdAt": "2025-06-04T14:09:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-14B-abliterated-v3-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-14B-abliterated-v3-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Josiefied-Qwen3-14B-abliterated-v3-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Josiefied-Qwen3-14B-abliterated-v3-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of the Qwen3 14B abliterated language model for Apple Silicon."
  },
  {
    "model_name": "Llama-3-8B-Instruct-1048k-4bit",
    "developer": "mlx-community",
    "downloads": 291,
    "createdAt": "2024-04-29T20:13:32.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized Llama-3-8B-Instruct model for text generation on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-V3.2_bf16",
    "developer": "mlx-community",
    "downloads": 288,
    "createdAt": "2025-12-03T02:05:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 234,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00001-of-00234.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00002-of-00234.safetensors",
        "file_size": "635.5 MB"
      },
      {
        "model_id": "model-00003-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00003-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00004-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00004-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00005-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00005-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00006-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00006-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00007-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00007-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00008-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00008-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00009-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00009-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00010-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00010-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00011-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00011-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00012-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00012-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00013-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00013-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00014-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00014-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00015-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00015-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00016-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00016-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00017-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00017-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00018-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00018-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00019-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00019-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00020-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00020-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00021-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00021-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00022-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00022-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00023-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00023-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00024-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00024-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00025-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00025-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00026-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00026-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00027-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00027-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00028-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00028-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00029-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00029-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00030-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00030-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00031-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00031-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00032-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00032-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00033-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00033-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00034-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00034-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00035-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00035-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00036-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00036-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00037-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00037-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00038-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00038-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00039-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00039-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00040-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00040-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00041-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00041-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00042-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00042-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00043-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00043-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00044-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00044-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00045-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00045-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00046-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00046-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00047-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00047-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00048-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00048-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00049-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00049-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00050-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00050-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00051-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00051-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00052-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00052-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00053-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00053-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00054-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00054-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00055-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00055-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00056-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00056-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00057-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00057-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00058-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00058-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00059-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00059-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00060-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00060-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00061-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00061-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00062-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00062-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00063-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00063-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00064-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00064-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00065-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00065-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00066-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00066-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00067-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00067-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00068-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00068-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00069-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00069-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00070-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00070-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00071-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00071-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00072-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00072-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00073-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00073-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00074-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00074-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00075-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00075-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00076-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00076-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00077-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00077-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00078-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00078-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00079-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00079-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00080-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00080-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00081-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00081-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00082-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00082-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00083-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00083-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00084-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00084-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00085-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00085-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00086-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00086-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00087-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00087-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00088-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00088-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00089-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00089-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00090-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00090-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00091-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00091-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00092-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00092-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00093-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00093-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00094-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00094-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00095-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00095-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00096-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00096-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00097-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00097-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00098-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00098-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00099-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00099-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00100-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00100-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00101-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00101-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00102-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00102-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00103-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00103-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00104-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00104-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00105-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00105-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00106-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00106-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00107-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00107-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00108-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00108-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00109-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00109-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00110-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00110-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00111-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00111-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00112-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00112-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00113-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00113-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00114-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00114-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00115-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00115-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00116-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00116-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00117-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00117-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00118-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00118-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00119-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00119-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00120-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00120-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00121-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00121-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00122-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00122-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00123-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00123-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00124-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00124-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00125-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00125-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00126-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00126-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00127-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00127-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00128-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00128-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00129-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00129-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00130-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00130-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00131-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00131-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00132-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00132-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00133-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00133-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00134-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00134-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00135-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00135-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00136-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00136-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00137-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00137-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00138-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00138-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00139-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00139-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00140-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00140-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00141-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00141-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00142-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00142-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00143-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00143-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00144-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00144-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00145-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00145-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00146-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00146-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00147-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00147-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00148-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00148-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00149-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00149-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00150-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00150-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00151-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00151-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00152-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00152-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00153-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00153-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00154-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00154-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00155-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00155-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00156-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00156-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00157-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00157-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00158-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00158-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00159-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00159-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00160-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00160-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00161-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00161-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00162-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00162-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00163-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00163-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00164-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00164-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00165-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00165-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00166-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00166-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00167-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00167-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00168-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00168-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00169-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00169-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00170-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00170-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00171-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00171-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00172-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00172-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00173-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00173-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00174-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00174-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00175-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00175-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00176-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00176-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00177-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00177-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00178-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00178-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00179-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00179-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00180-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00180-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00181-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00181-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00182-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00182-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00183-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00183-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00184-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00184-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00185-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00185-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00186-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00186-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00187-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00187-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00188-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00188-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00189-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00189-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00190-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00190-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00191-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00191-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00192-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00192-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00193-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00193-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00194-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00194-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00195-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00195-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00196-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00196-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00197-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00197-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00198-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00198-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00199-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00199-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00200-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00200-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00201-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00201-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00202-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00202-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00203-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00203-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00204-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00204-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00205-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00205-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00206-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00206-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00207-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00207-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00208-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00208-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00209-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00209-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00210-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00210-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00211-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00211-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00212-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00212-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00213-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00213-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00214-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00214-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00215-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00215-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00216-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00216-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00217-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00217-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00218-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00218-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00219-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00219-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00220-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00220-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00221-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00221-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00222-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00222-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00223-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00223-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00224-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00224-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00225-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00225-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00226-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00226-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00227-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00227-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00228-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00228-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00229-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00229-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00230-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00230-of-00234.safetensors",
        "file_size": "471.0 MB"
      },
      {
        "model_id": "model-00231-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00231-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00232-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00232-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00233-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00233-of-00234.safetensors",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "model-00234-of-00234",
        "path": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/model-00234-of-00234.safetensors",
        "file_size": "1.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-V3.2_bf16/resolve/main/README.md",
    "description": "MLX bf16 conversion of DeepSeek-V3.2 model."
  },
  {
    "model_name": "LFM2-VL-1.6B-4bit",
    "developer": "mlx-community",
    "downloads": 288,
    "createdAt": "2025-08-16T07:40:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-VL-1.6B-4bit/resolve/main/model.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-VL-1.6B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-VL-1.6B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit LFM2-VL-1.6B vision-language model for image-text-to-text tasks."
  },
  {
    "model_name": "Qwen2-0.5B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 286,
    "createdAt": "2024-06-06T18:07:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2-0.5B-Instruct-4bit/resolve/main/model.safetensors",
        "file_size": "265.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2-0.5B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2-0.5B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen2-0.5B-Instruct converted to MLX format for Apple devices."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-8bit",
    "developer": "mlx-community",
    "downloads": 285,
    "createdAt": "2025-04-17T14:24:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-8bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-8bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-8bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-8bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX converted vision-language model from Kimi-VL-A3B-Thinking for image-text generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-30B-A3B-Instruct-2507-4bit",
    "developer": "mlx-community",
    "downloads": 284,
    "createdAt": "2025-07-29T20:31:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-30B-A3B-Instruct-2507-4bit/resolve/main/README.md",
    "description": "4-bit MLX conversion of Qwen3-30B-A3B-Instruct-2507 for text generation."
  },
  {
    "model_name": "Qwen2.5-0.5B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 282,
    "createdAt": "2024-09-18T18:38:28.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "500.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-0.5B-Instruct-8bit/resolve/main/README.md",
    "description": "Qwen2.5-0.5B-Instruct model converted to 8-bit MLX format for Apple Silicon."
  },
  {
    "model_name": "GLM-ASR-Nano-2512-4bit",
    "developer": "mlx-community",
    "downloads": 280,
    "createdAt": "2025-12-19T15:27:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-ASR-Nano-2512-4bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-ASR-Nano-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-ASR-Nano-2512-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized MLX speech-to-text model converted from GLM-ASR-Nano-2512 for Apple Silicon devices."
  },
  {
    "model_name": "deepcogito-cogito-v1-preview-llama-8B-4bit",
    "developer": "mlx-community",
    "downloads": 279,
    "createdAt": "2025-04-08T22:33:19.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/deepcogito-cogito-v1-preview-llama-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/deepcogito-cogito-v1-preview-llama-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/deepcogito-cogito-v1-preview-llama-8B-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit version of deepcogito/cogito-v1-preview-llama-8B for text generation."
  },
  {
    "model_name": "Mistral-Small-3.1-24B-Instruct-2503-4bit",
    "developer": "mlx-community",
    "downloads": 279,
    "createdAt": "2025-03-19T08:33:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit/resolve/main/README.md",
    "description": "Mistral-Small-3.1-24B-Instruct vision-language model converted to MLX format for Apple devices."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-gabliterated-8bit",
    "developer": "mlx-community",
    "downloads": 277,
    "createdAt": "2026-01-06T18:22:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-gabliterated-8bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-gabliterated-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-gabliterated-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit Qwen3-4B-Instruct model for Apple Silicon."
  },
  {
    "model_name": "Qwen3-ASR-1.7B-bf16",
    "developer": "mlx-community",
    "downloads": 277,
    "createdAt": "2026-01-29T15:13:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-bf16/resolve/main/model.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-bf16/resolve/main/README.md",
    "description": "A speech-to-text (ASR) model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "gemma-3-27b-it-8bit",
    "developer": "mlx-community",
    "downloads": 275,
    "createdAt": "2025-03-12T11:31:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-27b-it-8bit/resolve/main/README.md",
    "description": "This is an 8-bit MLX conversion of Google's Gemma 3 27B instruction-tuned model for image-text-to-text tasks."
  },
  {
    "model_name": "mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4",
    "developer": "mlx-community",
    "downloads": 274,
    "createdAt": "2025-12-14T00:56:00.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4/resolve/main/README.md",
    "description": "MLX-converted text generation model from Mistral's Devstral-Small-2-24B-Instruct."
  },
  {
    "model_name": "Devstral-2-123B-Instruct-2512-6bit",
    "developer": "mlx-community",
    "downloads": 273,
    "createdAt": "2025-12-10T08:55:36.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 20,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00001-of-00020.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00002-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00003-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00004-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00005-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00006-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00007-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00008-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00009-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00010-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00011-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00012-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00013-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00014-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00015-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00016-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00017-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00018-of-00020.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00019-of-00020.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00020-of-00020",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/model-00020-of-00020.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-6bit/resolve/main/README.md",
    "description": "6-bit MLX conversion of Devstral-2-123B-Instruct-2512 for Apple Silicon."
  },
  {
    "model_name": "mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit",
    "developer": "mlx-community",
    "downloads": 269,
    "createdAt": "2025-12-14T00:57:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-6Bit/resolve/main/README.md",
    "description": "6-bit MLX-quantized version of Mistral's Devstral-Small-2-24B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Mixtral-8x7B-Instruct-v0.1",
    "developer": "mlx-community",
    "downloads": 269,
    "createdAt": "2023-12-20T15:08:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 18,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00001-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00002-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00003-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00004-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00005-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00006-of-00018.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00007-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00008-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00009-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00010-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00011-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00012-of-00018.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00013-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00014-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00015-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00016-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00017-of-00018.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00018",
        "path": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/model-00018-of-00018.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Mixtral-8x7B-Instruct-v0.1/resolve/main/README.md",
    "description": "Mixtral-8x7B-Instruct-v0.1 model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-14B-4bit-AWQ",
    "developer": "mlx-community",
    "downloads": 269,
    "createdAt": "2025-05-06T15:22:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-14B-4bit-AWQ/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-14B-4bit-AWQ/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-14B-4bit-AWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-14B-4bit-AWQ/resolve/main/README.md",
    "description": "Qwen3-14B model converted to MLX format with 4-bit AWQ quantization."
  },
  {
    "model_name": "DeepSeek-R1-0528-4bit",
    "developer": "mlx-community",
    "downloads": 268,
    "createdAt": "2025-05-28T18:27:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 88,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00001-of-00088.safetensors",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "model-00002-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00002-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00003-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00003-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00004-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00004-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00005-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00005-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00006-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00006-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00007-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00007-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00008-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00008-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00009-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00009-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00010-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00010-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00011-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00011-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00012-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00012-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00013-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00013-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00014-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00014-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00015-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00015-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00016-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00016-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00017-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00017-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00018-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00018-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00019-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00019-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00020-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00020-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00021-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00021-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00022-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00022-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00023-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00023-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00024-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00024-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00025-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00025-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00026-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00026-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00027-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00027-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00028-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00028-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00029-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00029-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00030-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00030-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00031-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00031-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00032-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00032-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00033-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00033-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00034-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00034-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00035-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00035-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00036-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00036-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00037-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00037-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00038-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00038-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00039-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00039-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00040-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00040-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00041-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00041-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00042-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00042-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00043-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00043-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00044-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00044-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00045-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00045-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00046-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00046-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00047-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00047-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00048-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00048-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00049-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00049-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00050-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00050-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00051-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00051-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00052-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00052-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00053-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00053-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00054-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00054-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00055-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00055-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00056-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00056-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00057-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00057-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00058-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00058-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00059-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00059-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00060-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00060-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00061-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00061-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00062-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00062-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00063-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00063-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00064-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00064-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00065-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00065-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00066-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00066-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00067-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00067-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00068-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00068-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00069-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00069-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00070-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00070-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00071-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00071-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00072-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00072-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00073-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00073-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00074-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00074-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00075-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00075-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00076-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00076-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00077-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00077-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00078-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00078-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00079-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00079-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00080-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00080-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00081-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00081-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00082-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00082-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00083-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00083-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00084-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00084-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00085-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00085-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00086-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00086-of-00088.safetensors",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "model-00087-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00087-of-00088.safetensors",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "model-00088-of-00088",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/model-00088-of-00088.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized version of DeepSeek-R1-0528 converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-32B-4bit",
    "developer": "mlx-community",
    "downloads": 267,
    "createdAt": "2025-04-28T22:15:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-32B-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-32B-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-32B-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-32B-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-32B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-32B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Qwen3-32B converted to MLX format."
  },
  {
    "model_name": "CodeLlama-7b-Python-4bit-MLX",
    "developer": "mlx-community",
    "downloads": 265,
    "createdAt": "2024-01-29T20:29:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/CodeLlama-7b-Python-4bit-MLX/resolve/main/weights.00.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/CodeLlama-7b-Python-4bit-MLX/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/CodeLlama-7b-Python-4bit-MLX/resolve/main/README.md",
    "description": "A 4-bit quantized CodeLlama-7b Python code generation model converted to MLX format for Apple silicon."
  },
  {
    "model_name": "Ministral-3-14B-Reasoning-2512-bf16",
    "developer": "mlx-community",
    "downloads": 265,
    "createdAt": "2025-12-04T01:10:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Reasoning-2512-bf16/resolve/main/README.md",
    "description": "MLX-converted vision-language model of Mistral's Ministral-3-14B-Reasoning for Apple Silicon."
  },
  {
    "model_name": "Qwen3-8B-4bit-DWQ-053125",
    "developer": "mlx-community",
    "downloads": 265,
    "createdAt": "2025-06-01T19:51:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-8B-4bit-DWQ-053125/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-8B-4bit-DWQ-053125/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-8B-4bit-DWQ-053125/resolve/main/README.md",
    "description": "4-bit quantized MLX version of Qwen3-8B for Apple Silicon."
  },
  {
    "model_name": "Llama-4-Scout-17B-16E-Instruct-6bit",
    "developer": "mlx-community",
    "downloads": 263,
    "createdAt": "2025-04-06T11:35:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-4-Scout-17B-16E-Instruct-6bit/resolve/main/README.md",
    "description": "MLX-converted 6-bit quantized version of Meta's Llama-4-Scout-17B-16E-Instruct for image understanding on Apple Silicon."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-gabliterated-4bit",
    "developer": "mlx-community",
    "downloads": 263,
    "createdAt": "2026-01-17T15:48:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-gabliterated-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-gabliterated-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-gabliterated-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3-4B-Thinking-2507-gabliterated for text generation."
  },
  {
    "model_name": "c4ai-command-r-plus-4bit",
    "developer": "mlx-community",
    "downloads": 260,
    "createdAt": "2024-04-04T20:46:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 13,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00001-of-00013.safetensors",
        "file_size": "41.0 B"
      },
      {
        "model_id": "model-00002-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00002-of-00013.safetensors",
        "file_size": "5.9 GB"
      },
      {
        "model_id": "model-00003-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00003-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00004-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00005-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00006-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00007-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00008-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00009-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00010-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00011-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00012-of-00013.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00013",
        "path": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/model-00013-of-00013.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/c4ai-command-r-plus-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit version of Cohere's Command R Plus model for Apple Silicon with tool use and RAG capabilities."
  },
  {
    "model_name": "Fun-ASR-Nano-2512-4bit",
    "developer": "mlx-community",
    "downloads": 259,
    "createdAt": "2025-12-16T16:02:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Fun-ASR-Nano-2512-4bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Fun-ASR-Nano-2512-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Fun-ASR-Nano-2512-4bit/resolve/main/README.md",
    "description": "A multilingual speech-to-text model with translation capabilities, converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "deepseek-coder-33b-instruct-hf-4bit-mlx",
    "developer": "mlx-community",
    "downloads": 254,
    "createdAt": "2024-01-07T08:42:12.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/weights.00.safetensors",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "weights.01",
        "path": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/weights.01.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/deepseek-coder-33b-instruct-hf-4bit-mlx/resolve/main/README.md",
    "description": "MLX converted version of deepseek-coder-33b-instruct model."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-Base-4bit",
    "developer": "mlx-community",
    "downloads": 253,
    "createdAt": "2026-01-22T22:03:23.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-4bit/resolve/main/model.safetensors",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-4bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-Base-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized Qwen3 text-to-speech model for Apple Silicon with voice cloning support."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-0.6B-Base-6bit",
    "developer": "mlx-community",
    "downloads": 251,
    "createdAt": "2026-01-22T19:33:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-6bit/resolve/main/model.safetensors",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-6bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-0.6B-Base-6bit/resolve/main/README.md",
    "description": "A 0.6B parameter text-to-speech model in MLX format with voice cloning support."
  },
  {
    "model_name": "SmolVLM2-2.2B-Instruct-mlx",
    "developer": "mlx-community",
    "downloads": 250,
    "createdAt": "2025-02-12T15:37:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolVLM2-2.2B-Instruct-mlx/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolVLM2-2.2B-Instruct-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolVLM2-2.2B-Instruct-mlx/resolve/main/README.md",
    "description": "MLX-converted vision-language model for image-text tasks."
  },
  {
    "model_name": "defog-sqlcoder-7b-2",
    "developer": "mlx-community",
    "downloads": 249,
    "createdAt": "2024-02-06T09:11:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/defog-sqlcoder-7b-2/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/defog-sqlcoder-7b-2/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/defog-sqlcoder-7b-2/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "2.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/defog-sqlcoder-7b-2/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/defog-sqlcoder-7b-2/resolve/main/README.md",
    "description": "An MLX-converted SQL code generation model (defog-sqlcoder-7b-2) for text generation using the mlx-lm library."
  },
  {
    "model_name": "LFM2-350M-8bit",
    "developer": "mlx-community",
    "downloads": 247,
    "createdAt": "2025-07-11T23:14:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-350M-8bit/resolve/main/model.safetensors",
        "file_size": "359.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-350M-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-350M-8bit/resolve/main/README.md",
    "description": "8-bit quantized LFM2-350M model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Voxtral-Mini-4B-Realtime-6bit",
    "developer": "mlx-community",
    "downloads": 247,
    "createdAt": "2026-02-08T14:54:46.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Voxtral-Mini-4B-Realtime-6bit/resolve/main/model.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Voxtral-Mini-4B-Realtime-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Voxtral-Mini-4B-Realtime-6bit/resolve/main/README.md",
    "description": "MLX-converted Voxtral speech recognition model for audio transcription."
  },
  {
    "model_name": "medgemma-4b-it-4bit",
    "developer": "mlx-community",
    "downloads": 246,
    "createdAt": "2025-05-21T01:21:36.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/medgemma-4b-it-4bit/resolve/main/model.safetensors",
        "file_size": "2.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/medgemma-4b-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/medgemma-4b-it-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized medical vision-language model converted to MLX format for Apple Silicon, based on Google's MedGemma."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-8bit",
    "developer": "mlx-community",
    "downloads": 244,
    "createdAt": "2025-12-09T18:09:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of Devstral-Small-2-24B-Instruct for Apple devices."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-2506-8bit",
    "developer": "mlx-community",
    "downloads": 242,
    "createdAt": "2025-08-16T08:39:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-8bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-8bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-8bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-8bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit quantized version of Kimi-VL-A3B-Thinking-2506 vision language model."
  },
  {
    "model_name": "Qwen3-0.6B-6bit",
    "developer": "mlx-community",
    "downloads": 242,
    "createdAt": "2025-04-28T21:26:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-0.6B-6bit/resolve/main/model.safetensors",
        "file_size": "462.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-0.6B-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-0.6B-6bit/resolve/main/README.md",
    "description": "6-bit quantized Qwen3-0.6B model converted to MLX format for text generation."
  },
  {
    "model_name": "Apriel-1.5-15b-Thinker-4bit",
    "developer": "mlx-community",
    "downloads": 241,
    "createdAt": "2025-10-01T07:40:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Apriel-1.5-15b-Thinker-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Apriel-1.5-15b-Thinker-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Apriel-1.5-15b-Thinker-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Apriel-1.5-15b-Thinker-4bit/resolve/main/README.md",
    "description": "This is a 4-bit quantized vision-language model converted to MLX format for Apple Silicon devices."
  },
  {
    "model_name": "moonshotai_Kimi-K2-Instruct-mlx-3bit",
    "developer": "mlx-community",
    "downloads": 239,
    "createdAt": "2025-08-22T07:54:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 91,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00001-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00002-of-00091.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00003-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00004-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00005-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00005-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00006-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00007-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00008-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00008-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00009-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00009-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00010-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00011-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00011-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00012-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00012-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00013-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00013-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00014-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00014-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00015-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00016-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00016-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00017-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00017-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00018-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00019-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00020-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00020-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00021-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00021-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00022-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00022-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00023-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00023-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00024-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00024-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00025-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00025-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00026-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00026-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00027-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00027-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00028-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00028-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00029-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00029-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00030-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00030-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00031-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00031-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00032-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00032-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00033-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00033-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00034-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00034-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00035-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00035-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00036-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00036-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00037-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00037-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00038-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00038-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00039-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00039-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00040-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00040-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00041-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00041-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00042-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00042-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00043-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00043-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00044-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00044-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00045-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00045-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00046-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00046-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00047-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00047-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00048-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00048-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00049-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00049-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00050-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00050-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00051-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00051-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00052-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00052-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00053-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00053-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00054-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00054-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00055-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00055-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00056-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00056-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00057-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00057-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00058-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00058-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00059-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00059-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00060-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00060-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00061-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00061-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00062-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00062-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00063-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00063-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00064-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00064-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00065-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00065-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00066-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00066-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00067-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00067-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00068-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00068-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00069-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00069-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00070-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00070-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00071-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00071-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00072-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00072-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00073-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00073-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00074-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00074-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00075-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00075-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00076-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00076-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00077-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00077-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00078-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00078-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00079-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00079-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00080-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00080-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00081-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00081-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00082-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00082-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00083-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00083-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00084-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00084-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00085-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00085-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00086-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00086-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00087-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00087-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00088-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00088-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00089-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00089-of-00091.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00090-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00090-of-00091.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00091-of-00091",
        "path": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/model-00091-of-00091.safetensors",
        "file_size": "490.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/moonshotai_Kimi-K2-Instruct-mlx-3bit/resolve/main/README.md",
    "description": "A 3-bit MLX-quantized version of Kimi-K2-Instruct for text generation using mlx-lm."
  },
  {
    "model_name": "Unsloth-Phi-4-4bit",
    "developer": "mlx-community",
    "downloads": 239,
    "createdAt": "2025-01-13T14:26:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Unsloth-Phi-4-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Unsloth-Phi-4-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Unsloth-Phi-4-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Unsloth-Phi-4-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Phi-4 model converted to MLX format for efficient inference on Apple Silicon."
  },
  {
    "model_name": "DeepSeek-R1-Qwen3-0528-8B-4bit-AWQ",
    "developer": "mlx-community",
    "downloads": 238,
    "createdAt": "2025-06-29T06:00:43.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Qwen3-0528-8B-4bit-AWQ/resolve/main/model.safetensors",
        "file_size": "4.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Qwen3-0528-8B-4bit-AWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Qwen3-0528-8B-4bit-AWQ/resolve/main/README.md",
    "description": "A 4-bit AWQ quantized version of DeepSeek-R1-0528-Qwen3-8B converted to MLX format for text generation."
  },
  {
    "model_name": "Ministral-3-14B-Instruct-2512",
    "developer": "mlx-community",
    "downloads": 236,
    "createdAt": "2025-12-03T18:18:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512/resolve/main/README.md",
    "description": "MLX-converted Ministral-3-14B-Instruct-2512 model for text generation."
  },
  {
    "model_name": "descript-audio-codec-44khz",
    "developer": "mlx-community",
    "downloads": 235,
    "createdAt": "2025-03-22T17:01:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/descript-audio-codec-44khz/resolve/main/model.safetensors",
        "file_size": "292.4 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/descript-audio-codec-44khz/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/descript-audio-codec-44khz/resolve/main/README.md",
    "description": "I don't see a README.md file content to summarizeonly the license header. Please paste the full README text, and I'll condense it into the shortest possible sentence."
  },
  {
    "model_name": "gemma-3-270m-it-qat-4bit",
    "developer": "mlx-community",
    "downloads": 235,
    "createdAt": "2025-08-09T20:25:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3-270m-it-qat-4bit/resolve/main/model.safetensors",
        "file_size": "233.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-270m-it-qat-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-270m-it-qat-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized Google Gemma 3 270m instruction-tuned model converted to MLX format for efficient text generation on Apple Silicon."
  },
  {
    "model_name": "MiniMax-M2-REAP-139B-A10B-mxfp4",
    "developer": "mlx-community",
    "downloads": 235,
    "createdAt": "2026-01-03T18:40:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 15,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2-REAP-139B-A10B-mxfp4/resolve/main/README.md",
    "description": "This is an MLX-converted, 40% expert-pruned version of MiniMax-M2-REAP-139B-A10B for text generation with MXFP4 quantization."
  },
  {
    "model_name": "Llama-3.2-90B-Vision-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 233,
    "createdAt": "2024-12-19T17:55:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 10,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00001-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00002-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00003-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00004-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00005-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00006-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00007-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00008-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00009-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00010",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/model-00010-of-00010.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-90B-Vision-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-optimized 4-bit quantized version of Llama-3.2-90B-Vision-Instruct for Apple Silicon."
  },
  {
    "model_name": "gemma-3n-E2B-4bit",
    "developer": "mlx-community",
    "downloads": 232,
    "createdAt": "2025-07-06T22:24:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E2B-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E2B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E2B-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX conversion of Google's Gemma 3n E2B vision-language model for Apple Silicon, run via `mlx-vlm`."
  },
  {
    "model_name": "gemma-3-12b-it-8bit",
    "developer": "mlx-community",
    "downloads": 230,
    "createdAt": "2025-03-12T09:13:47.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/gemma-3-12b-it-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "3.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3-12b-it-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3-12b-it-8bit/resolve/main/README.md",
    "description": "Gemma 3 12B model converted to MLX format for image-text-to-text tasks."
  },
  {
    "model_name": "GLM-4.5-Air-4bit",
    "developer": "mlx-community",
    "downloads": 230,
    "createdAt": "2025-07-28T13:29:54.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 12,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00001-of-00012.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00002-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00003-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00004-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00005-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00006-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00007-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00008-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00009-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00010-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00011-of-00012.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00012",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/model-00012-of-00012.safetensors",
        "file_size": "1.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of GLM-4.5-Air converted to MLX format for text generation."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-2506-5bit",
    "developer": "mlx-community",
    "downloads": 230,
    "createdAt": "2025-08-16T08:29:48.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-5bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-5bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-5bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-5bit/resolve/main/README.md",
    "description": "MLX-converted 5-bit quantized version of Kimi-VL-A3B-Thinking-2506 vision-language model."
  },
  {
    "model_name": "quantized-gemma-2b-it",
    "developer": "mlx-community",
    "downloads": 230,
    "createdAt": "2024-02-22T10:10:50.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/quantized-gemma-2b-it/resolve/main/model.safetensors",
        "file_size": "2.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/quantized-gemma-2b-it/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/quantized-gemma-2b-it/resolve/main/README.md",
    "description": "Quantized MLX version of Google's Gemma 2B instruction-tuned model."
  },
  {
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-6bit",
    "developer": "mlx-community",
    "downloads": 229,
    "createdAt": "2025-07-31T18:54:07.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "3.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Coder-30B-A3B-Instruct-6bit/resolve/main/README.md",
    "description": "MLX-converted 6-bit quantized version of Qwen3-Coder-30B-A3B-Instruct for text/code generation on Apple Silicon."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-CustomVoice-4bit",
    "developer": "mlx-community",
    "downloads": 228,
    "createdAt": "2026-01-22T22:38:01.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-4bit/resolve/main/model.safetensors",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-4bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-CustomVoice-4bit/resolve/main/README.md",
    "description": "MLX format text-to-speech model with voice cloning support."
  },
  {
    "model_name": "Qwen3-TTS-12Hz-1.7B-VoiceDesign-6bit",
    "developer": "mlx-community",
    "downloads": 225,
    "createdAt": "2026-01-22T21:17:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-6bit/resolve/main/model.safetensors",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "speech_tokenizer/model",
        "path": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-6bit/resolve/main/speech_tokenizer/model.safetensors",
        "file_size": "650.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-6bit/resolve/main/README.md",
    "description": "MLX-converted 1.7B text-to-speech model from Qwen3-TTS for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-4bit-AWQ",
    "developer": "mlx-community",
    "downloads": 224,
    "createdAt": "2025-06-26T20:32:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-AWQ/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-AWQ/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-AWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-AWQ/resolve/main/README.md",
    "description": "A 4-bit AWQ quantized code generation model in MLX format for Apple Silicon, converted from DeepSeek-Coder-V2-Lite-Instruct."
  },
  {
    "model_name": "MiniMax-M2-3bit",
    "developer": "mlx-community",
    "downloads": 224,
    "createdAt": "2025-10-28T02:22:15.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 19,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00001-of-00019.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00002-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00003-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00004-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00005-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00006-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00006-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00007-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00008-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00009-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00010-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00011-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00012-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00013-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00014-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00015-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00015-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00016-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00017-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00017-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00018-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00018-of-00019.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00019-of-00019",
        "path": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/model-00019-of-00019.safetensors",
        "file_size": "3.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniMax-M2-3bit/resolve/main/README.md",
    "description": "A 3-bit quantized version of MiniMax-M2 converted to MLX format for Apple devices."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-4bit",
    "developer": "mlx-community",
    "downloads": 221,
    "createdAt": "2026-01-20T15:23:45.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-4bit/resolve/main/model.safetensors",
        "file_size": "628.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-4bit/resolve/main/README.md",
    "description": "MLX conversion of Liquid's LFM2.5-1.2B-Thinking text generation model."
  },
  {
    "model_name": "mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16",
    "developer": "mlx-community",
    "downloads": 220,
    "createdAt": "2025-12-14T00:53:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 10,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00001-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00002-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00003-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00004-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00005-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00006-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00007-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00008-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00009-of-00010.safetensors",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "model-00010-of-00010",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/model-00010-of-00010.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-BF16/resolve/main/README.md",
    "description": "MLX-BF16 conversion of Mistral's Devstral-Small-2-24B-Instruct-2512 text generation model."
  },
  {
    "model_name": "DeepSeek-Coder-V2-Instruct-AQ4_1",
    "developer": "mlx-community",
    "downloads": 217,
    "createdAt": "2024-07-17T14:23:18.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 26,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00001-of-00026.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00002-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00003-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00004-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00005-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00006-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00007-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00008-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00009-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00010-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00011-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00012-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00013-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00014-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00015-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00016-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00017-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00018-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00019-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00020-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00020-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00021-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00022-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00023-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00023-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00024-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00025-of-00026.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00026-of-00026",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/model-00026-of-00026.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Instruct-AQ4_1/resolve/main/README.md",
    "description": "DeepSeek-Coder-V2-Instruct model converted to MLX format with AQ4_1 quantization for use with mlx-lm."
  },
  {
    "model_name": "mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit",
    "developer": "mlx-community",
    "downloads": 217,
    "createdAt": "2025-12-14T01:05:30.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "440.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-5Bit/resolve/main/README.md",
    "description": "MLX-converted 5-bit quantized version of Mistral's Devstral-Small-2-24B-Instruct for text generation."
  },
  {
    "model_name": "Qwen2.5-72B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 216,
    "createdAt": "2024-09-19T07:59:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 15,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00001-of-00015.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00002-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00003-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00004-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00005-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00006-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00007-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00008-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00009-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00010-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00010-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00011-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00012-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00013-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00013-of-00015.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00014-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00014-of-00015.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00015",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/model-00015-of-00015.safetensors",
        "file_size": "3.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-72B-Instruct-8bit/resolve/main/README.md",
    "description": "8-bit quantized Qwen2.5-72B-Instruct model converted to MLX format."
  },
  {
    "model_name": "Llama-3.2-11B-Vision-Instruct-abliterated-4-bit",
    "developer": "mlx-community",
    "downloads": 215,
    "createdAt": "2024-12-16T15:21:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated-4-bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated-4-bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "627.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated-4-bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-abliterated-4-bit/resolve/main/README.md",
    "description": "A 4-bit quantized, unaligned vision-language model converted to MLX format for image-text tasks."
  },
  {
    "model_name": "mxbai-embed-large-v1",
    "developer": "mlx-community",
    "downloads": 215,
    "createdAt": "2025-07-23T09:29:20.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/mxbai-embed-large-v1/resolve/main/model.safetensors",
        "file_size": "639.3 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/mxbai-embed-large-v1/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/mxbai-embed-large-v1/resolve/main/README.md",
    "description": "A large sentence embedding model converted to MLX format for Apple Silicon, optimized for feature extraction and semantic similarity tasks."
  },
  {
    "model_name": "Qwen3-8B-6bit",
    "developer": "mlx-community",
    "downloads": 214,
    "createdAt": "2025-04-28T22:00:58.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-8B-6bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Qwen3-8B-6bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-8B-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-8B-6bit/resolve/main/README.md",
    "description": "6-bit quantized MLX version of Qwen3-8B for text generation using mlx-lm."
  },
  {
    "model_name": "Qwen3-VL-235B-A22B-Instruct-3bit",
    "developer": "mlx-community",
    "downloads": 214,
    "createdAt": "2025-10-13T13:59:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 21,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00001-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00002-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00003-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00004-of-00021.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00005-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00006-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00007-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00008-of-00021.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00009-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00010-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00011-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00011-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00012-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00012-of-00021.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00013-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00014-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00014-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00015-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00015-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00016-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00016-of-00021.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00017-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00018-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00018-of-00021.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00019-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00019-of-00021.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00020-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00020-of-00021.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00021",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/model-00021-of-00021.safetensors",
        "file_size": "595.7 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-3bit/resolve/main/README.md",
    "description": "MLX-converted 3-bit quantized version of Qwen3-VL-235B-A22B-Instruct for vision-language tasks on Apple Silicon."
  },
  {
    "model_name": "CodeLlama-13b-Instruct-hf-4bit-MLX",
    "developer": "mlx-community",
    "downloads": 213,
    "createdAt": "2024-01-30T03:03:15.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "weights.00",
        "path": "https://huggingface.co/mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX/resolve/main/weights.00.safetensors",
        "file_size": "7.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX/resolve/main/README.md",
    "description": "CodeLlama-13b-Instruct converted to MLX format for Apple Silicon, 4-bit quantized."
  },
  {
    "model_name": "Qwen2.5-VL-32B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 212,
    "createdAt": "2025-03-24T18:52:36.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "3.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX vision-language model for image-text tasks."
  },
  {
    "model_name": "LFM2.5-1.2B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 211,
    "createdAt": "2026-01-06T10:20:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of LFM2.5-1.2B-Instruct converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "snowflake-arctic-embed-l-v2.0-4bit",
    "developer": "mlx-community",
    "downloads": 210,
    "createdAt": "2025-04-02T16:17:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/snowflake-arctic-embed-l-v2.0-4bit/resolve/main/model.safetensors",
        "file_size": "305.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/snowflake-arctic-embed-l-v2.0-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/snowflake-arctic-embed-l-v2.0-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX quantized version of Snowflake's `snowflake-arctic-embed-l-v2.0` sentence embedding model for efficient inference on Apple Silicon."
  },
  {
    "model_name": "Olmo-3-7B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 208,
    "createdAt": "2025-11-20T20:33:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Olmo-3-7B-Instruct-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Olmo-3-7B-Instruct-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Olmo-3-7B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Olmo-3-7B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized version of Olmo-3-7B-Instruct converted to MLX format for use with mlx-lm."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512-5bit",
    "developer": "mlx-community",
    "downloads": 207,
    "createdAt": "2025-12-03T22:01:52.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-5bit/resolve/main/model.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-5bit/resolve/main/README.md",
    "description": "MLX Community's 5-bit quantized version of Mistral's Ministral-3-3B-Instruct model for image generation using mlx-vlm."
  },
  {
    "model_name": "Codestral-22B-v0.1-8bit",
    "developer": "mlx-community",
    "downloads": 204,
    "createdAt": "2024-05-29T14:23:42.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Codestral-22B-v0.1-8bit/resolve/main/README.md",
    "description": "This is Codestral-22B-v0.1-8bit, a code generation model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Kokoro-82M-8bit",
    "developer": "mlx-community",
    "downloads": 203,
    "createdAt": "2025-02-28T18:22:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 55,
    "safetensors_files": [
      {
        "model_id": "kokoro-v1_0",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/kokoro-v1_0.safetensors",
        "file_size": "275.9 MB"
      },
      {
        "model_id": "voices/af_alloy",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_alloy.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_aoede",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_aoede.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_bella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_bella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_heart",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_heart.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_jessica",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_jessica.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_kore",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_kore.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nicole",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_nicole.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nova",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_nova.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_river",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_river.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sarah",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_sarah.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sky",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/af_sky.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_adam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_adam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_echo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_echo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_eric",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_eric.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_fenrir",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_fenrir.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_liam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_liam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_michael",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_michael.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_onyx",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_onyx.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_puck",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_puck.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/am_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_alice",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bf_alice.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_emma",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bf_emma.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_isabella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bf_isabella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_lily",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bf_lily.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_daniel",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bm_daniel.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_fable",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bm_fable.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_george",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bm_george.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_lewis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/bm_lewis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ef_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/ef_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/em_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/em_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ff_siwis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/ff_siwis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/hf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_beta",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/hf_beta.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_omega",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/hm_omega.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_psi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/hm_psi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/if_sara",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/if_sara.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/im_nicola",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/im_nicola.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/jf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_gongitsune",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/jf_gongitsune.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_nezumi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/jf_nezumi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_tebukuro",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/jf_tebukuro.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jm_kumo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/jm_kumo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pf_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/pf_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/pm_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/pm_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaobei",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zf_xiaobei.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoni",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zf_xiaoni.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoxiao",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zf_xiaoxiao.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoyi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zf_xiaoyi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunjian",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zm_yunjian.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zm_yunxi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxia",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zm_yunxia.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunyang",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/voices/zm_yunyang.safetensors",
        "file_size": "510.1 KB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kokoro-82M-8bit/resolve/main/README.md",
    "description": "A text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Molmo-7B-D-0924-4bit",
    "developer": "mlx-community",
    "downloads": 202,
    "createdAt": "2024-11-20T17:58:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Molmo-7B-D-0924-4bit/resolve/main/model.safetensors",
        "file_size": "4.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Molmo-7B-D-0924-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Molmo-7B-D-0924-4bit/resolve/main/README.md",
    "description": "MLX-converted multimodal model for image-text-to-text tasks."
  },
  {
    "model_name": "granite-4.0-h-1b-6bit",
    "developer": "mlx-community",
    "downloads": 201,
    "createdAt": "2025-10-28T15:47:15.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/granite-4.0-h-1b-6bit/resolve/main/model.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/granite-4.0-h-1b-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/granite-4.0-h-1b-6bit/resolve/main/README.md",
    "description": "This is a 6-bit quantized MLX version of IBM's Granite 4.0 1B parameter text generation model."
  },
  {
    "model_name": "Ministral-3-14B-Instruct-2512-6bit",
    "developer": "mlx-community",
    "downloads": 201,
    "createdAt": "2025-12-03T22:07:48.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-6bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-6bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-6bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "1.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-6bit/resolve/main/README.md",
    "description": "MLX-converted 6-bit quantized version of Mistral's Ministral-3-14B-Instruct vision-language model."
  },
  {
    "model_name": "Hermes-3-Llama-3.1-8B-4bit",
    "developer": "mlx-community",
    "downloads": 199,
    "createdAt": "2024-08-16T20:25:30.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Hermes-3-Llama-3.1-8B-4bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Hermes-3-Llama-3.1-8B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Hermes-3-Llama-3.1-8B-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX quantized version of the Hermes-3-Llama-3.1-8B chat model for use with Apple's MLX framework."
  },
  {
    "model_name": "Kimi-VL-A3B-Thinking-2506-bf16",
    "developer": "mlx-community",
    "downloads": 199,
    "createdAt": "2025-08-16T08:53:41.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 7,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00001-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00002-of-00007.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00003-of-00007.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00004-of-00007.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00005-of-00007.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00006-of-00007.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00007",
        "path": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/model-00007-of-00007.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-VL-A3B-Thinking-2506-bf16/resolve/main/README.md",
    "description": "MLX-converted Kimi-VL-A3B-Thinking vision-language model."
  },
  {
    "model_name": "Kimi-Linear-48B-A3B-Instruct-3bit",
    "developer": "mlx-community",
    "downloads": 198,
    "createdAt": "2025-11-02T10:32:20.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "448.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-3bit/resolve/main/README.md",
    "description": "A 3-bit MLX-quantized version of Kimi-Linear-48B-A3B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "MiMo-V2-Flash-mlx-8bit",
    "developer": "mlx-community",
    "downloads": 198,
    "createdAt": "2025-12-19T11:56:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 71,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00001-of-00071.safetensors",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "model-00002-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00002-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00003-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00003-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00004-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00004-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00005-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00005-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00006-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00006-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00007-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00007-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00008-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00008-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00009-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00009-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00010-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00010-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00011-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00011-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00012-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00012-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00013-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00013-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00014-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00014-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00015-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00015-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00016-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00016-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00017-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00017-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00018-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00018-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00019-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00019-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00020-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00020-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00021-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00021-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00022-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00022-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00023-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00023-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00024-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00024-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00025-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00025-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00026-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00026-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00027-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00027-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00028-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00028-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00029-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00029-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00030-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00030-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00031-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00031-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00032-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00032-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00033-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00033-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00034-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00034-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00035-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00035-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00036-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00036-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00037-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00037-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00038-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00038-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00039-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00039-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00040-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00040-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00041-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00041-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00042-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00042-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00043-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00043-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00044-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00044-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00045-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00045-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00046-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00046-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00047-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00047-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00048-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00048-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00049-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00049-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00050-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00050-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00051-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00051-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00052-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00052-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00053-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00053-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00054-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00054-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00055-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00055-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00056-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00056-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00057-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00057-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00058-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00058-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00059-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00059-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00060-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00060-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00061-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00061-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00062-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00062-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00063-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00063-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00064-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00064-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00065-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00065-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00066-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00066-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00067-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00067-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00068-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00068-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00069-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00069-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00070-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00070-of-00071.safetensors",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "model-00071-of-00071",
        "path": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/model-00071-of-00071.safetensors",
        "file_size": "4.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiMo-V2-Flash-mlx-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Xiaomi's MiMo-V2-Flash text generation model for Apple Silicon."
  },
  {
    "model_name": "Llama-3-8B-Instruct-1048k-8bit",
    "developer": "mlx-community",
    "downloads": 197,
    "createdAt": "2024-04-29T20:20:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-converted version of Llama-3-8B-Instruct-1048k for text generation using mlx-lm."
  },
  {
    "model_name": "Chatterbox-Turbo-TTS-fp16",
    "developer": "mlx-community",
    "downloads": 195,
    "createdAt": "2025-12-22T11:33:14.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "conds",
        "path": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-fp16/resolve/main/conds.safetensors",
        "file_size": "163.5 KB"
      },
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-fp16/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Chatterbox-Turbo-TTS-fp16/resolve/main/README.md",
    "description": "MLX text-to-speech model converted from ResembleAI's Chatterbox-Turbo for Apple Silicon."
  },
  {
    "model_name": "Kimi-Linear-48B-A3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 195,
    "createdAt": "2025-11-01T02:10:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 10,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00001-of-00010.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00002-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00002-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00003-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00004-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00005-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00006-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00007-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00008-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00009-of-00010.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00010",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/model-00010-of-00010.safetensors",
        "file_size": "4.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit/resolve/main/README.md",
    "description": "An 8-bit quantized MLX version of Kimi-Linear-48B-A3B-Instruct for text generation."
  },
  {
    "model_name": "Devstral-2-123B-Instruct-2512-5bit",
    "developer": "mlx-community",
    "downloads": 194,
    "createdAt": "2025-12-10T06:16:55.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 17,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00001-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00002-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00003-of-00017.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00004-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00005-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00006-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00007-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00008-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00009-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00009-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00010-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00010-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00011-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00011-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00012-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00012-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00013-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00013-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00014-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00014-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00015-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00015-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00016-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00016-of-00017.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00017-of-00017",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/model-00017-of-00017.safetensors",
        "file_size": "1.7 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-5bit/resolve/main/README.md",
    "description": "5-bit MLX-quantized version of Devstral-2-123B-Instruct-2512 for use with mlx-lm."
  },
  {
    "model_name": "functiongemma-270m-it-4bit",
    "developer": "mlx-community",
    "downloads": 194,
    "createdAt": "2025-12-18T11:25:34.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/functiongemma-270m-it-4bit/resolve/main/model.safetensors",
        "file_size": "143.9 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/functiongemma-270m-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/functiongemma-270m-it-4bit/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "Devstral-2-123B-Instruct-2512-8bit",
    "developer": "mlx-community",
    "downloads": 192,
    "createdAt": "2025-12-10T11:46:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 26,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00001-of-00026.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00002-of-00026.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00003-of-00026.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00004-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00005-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00006-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00007-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00007-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00008-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00008-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00009-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00009-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00010-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00010-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00011-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00011-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00012-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00012-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00013-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00013-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00014-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00014-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00015-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00015-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00016-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00016-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00017-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00017-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00018-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00018-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00019-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00019-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00020-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00020-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00021-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00021-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00022-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00022-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00023-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00023-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00024-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00024-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00025-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00025-of-00026.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00026-of-00026",
        "path": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/model-00026-of-00026.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Devstral-2-123B-Instruct-2512-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-quantized version of Devstral-2-123B-Instruct for text generation on Apple Silicon."
  },
  {
    "model_name": "Dia-1.6B",
    "developer": "mlx-community",
    "downloads": 192,
    "createdAt": "2025-04-23T21:37:38.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Dia-1.6B/resolve/main/model.safetensors",
        "file_size": "6.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Dia-1.6B/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Dia-1.6B/resolve/main/README.md",
    "description": "This is an MLX-converted text-to-speech model for dialogue generation."
  },
  {
    "model_name": "LFM2-2.6B-Transcript-4bit",
    "developer": "mlx-community",
    "downloads": 192,
    "createdAt": "2026-01-07T21:58:15.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-4bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX quantized version of LiquidAI's LFM2-2.6B-Transcript model for text generation on Apple Silicon."
  },
  {
    "model_name": "all-MiniLM-L6-v2-4bit",
    "developer": "mlx-community",
    "downloads": 191,
    "createdAt": "2025-04-02T16:35:11.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-4bit/resolve/main/model.safetensors",
        "file_size": "12.2 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/all-MiniLM-L6-v2-4bit/resolve/main/README.md",
    "description": "4-bit quantized version of all-MiniLM-L6-v2 converted to MLX format for sentence embeddings on Apple Silicon."
  },
  {
    "model_name": "nomicai-modernbert-embed-base-8bit",
    "developer": "mlx-community",
    "downloads": 191,
    "createdAt": "2025-04-02T14:00:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/nomicai-modernbert-embed-base-8bit/resolve/main/model.safetensors",
        "file_size": "151.1 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/nomicai-modernbert-embed-base-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/nomicai-modernbert-embed-base-8bit/resolve/main/README.md",
    "description": ""
  },
  {
    "model_name": "chatterbox-fp16",
    "developer": "mlx-community",
    "downloads": 190,
    "createdAt": "2025-12-17T15:48:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "conds",
        "path": "https://huggingface.co/mlx-community/chatterbox-fp16/resolve/main/conds.safetensors",
        "file_size": "102.8 KB"
      },
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/chatterbox-fp16/resolve/main/model.safetensors",
        "file_size": "2.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/chatterbox-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/chatterbox-fp16/resolve/main/README.md",
    "description": "A multilingual text-to-speech model with voice cloning, optimized for Apple Silicon via MLX."
  },
  {
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B-3bit",
    "developer": "mlx-community",
    "downloads": 189,
    "createdAt": "2025-01-20T14:45:10.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-3bit/resolve/main/model.safetensors",
        "file_size": "741.8 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-3bit/resolve/main/README.md",
    "description": "MLX-converted 3-bit quantized DeepSeek-R1-Distill-Qwen-1.5B model."
  },
  {
    "model_name": "MiniCPM-2B-sft-bf16-llama-format-mlx",
    "developer": "mlx-community",
    "downloads": 189,
    "createdAt": "2024-02-20T10:06:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/MiniCPM-2B-sft-bf16-llama-format-mlx/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/MiniCPM-2B-sft-bf16-llama-format-mlx/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "625.5 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/MiniCPM-2B-sft-bf16-llama-format-mlx/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/MiniCPM-2B-sft-bf16-llama-format-mlx/resolve/main/README.md",
    "description": "MiniCPM-2B model converted to MLX format for Apple Silicon, with usage instructions."
  },
  {
    "model_name": "DeepSeek-OCR-2-6bit",
    "developer": "mlx-community",
    "downloads": 188,
    "createdAt": "2026-01-28T11:11:06.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-6bit/resolve/main/model.safetensors",
        "file_size": "3.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-OCR-2-6bit/resolve/main/README.md",
    "description": "A 6-bit quantized MLX version of DeepSeek-OCR-2 for image-text-to-text tasks."
  },
  {
    "model_name": "deepseek-vl2-8bit",
    "developer": "mlx-community",
    "downloads": 187,
    "createdAt": "2024-12-22T20:30:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/deepseek-vl2-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of DeepSeek VL2 vision-language model for image-text-to-text tasks."
  },
  {
    "model_name": "Phi-3.5-MoE-instruct-4bit",
    "developer": "mlx-community",
    "downloads": 187,
    "createdAt": "2024-08-24T06:55:31.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 5,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00005",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/model-00001-of-00005.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00005",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/model-00002-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00005",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/model-00003-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00005",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/model-00004-of-00005.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00005",
        "path": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/model-00005-of-00005.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Phi-3.5-MoE-instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Microsoft's Phi-3.5-MoE-instruct model converted to MLX format for text generation."
  },
  {
    "model_name": "Qwen2.5-7B-Instruct-1M-4bit",
    "developer": "mlx-community",
    "downloads": 187,
    "createdAt": "2025-01-26T18:10:09.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-1M-4bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-1M-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-1M-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of Qwen2.5-7B-Instruct-1M for text generation on Apple silicon."
  },
  {
    "model_name": "bge-small-en-v1.5-4bit",
    "developer": "mlx-community",
    "downloads": 185,
    "createdAt": "2025-04-02T16:21:14.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/bge-small-en-v1.5-4bit/resolve/main/model.safetensors",
        "file_size": "18.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/bge-small-en-v1.5-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/bge-small-en-v1.5-4bit/resolve/main/README.md",
    "description": "A 4-bit MLX-quantized version of BAAI's bge-small-en-v1.5 sentence embedding model, optimized for Apple Silicon and usable via mlx-embeddings for text embeddings and similarity tasks."
  },
  {
    "model_name": "Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit",
    "developer": "mlx-community",
    "downloads": 184,
    "createdAt": "2025-06-19T20:17:33.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "1.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Josiefied-Qwen3-30B-A3B-abliterated-v2-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized Qwen3 30B chat model for Apple Silicon."
  },
  {
    "model_name": "Jan-v1-4B-4bit",
    "developer": "mlx-community",
    "downloads": 183,
    "createdAt": "2025-08-12T16:59:07.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Jan-v1-4B-4bit/resolve/main/model.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Jan-v1-4B-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Jan-v1-4B-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized version of Jan-v1-4B converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "DeepSeek-Coder-V2-Lite-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 182,
    "createdAt": "2025-05-24T18:22:23.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of DeepSeek-Coder-V2-Lite-Instruct for efficient code generation on Apple Silicon."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-8bit",
    "developer": "mlx-community",
    "downloads": 181,
    "createdAt": "2026-01-20T15:25:44.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-8bit/resolve/main/model.safetensors",
        "file_size": "1.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Thinking-8bit/resolve/main/README.md",
    "description": "8-bit MLX conversion of LiquidAI's LFM2.5-1.2B-Thinking text generation model."
  },
  {
    "model_name": "GLM-4.5-Air-mxfp4",
    "developer": "mlx-community",
    "downloads": 180,
    "createdAt": "2025-09-26T21:49:39.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 11,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00001-of-00011.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00002-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00002-of-00011.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00003-of-00011.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00004-of-00011.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00005-of-00011.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00006-of-00011.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00007-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00007-of-00011.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00008-of-00011.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00009-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00009-of-00011.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00010-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00010-of-00011.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00011-of-00011",
        "path": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/model-00011-of-00011.safetensors",
        "file_size": "4.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.5-Air-mxfp4/resolve/main/README.md",
    "description": "MLX-converted version of GLM-4.5-Air text generation model."
  },
  {
    "model_name": "Kimi-Linear-48B-A3B-Instruct-6bit",
    "developer": "mlx-community",
    "downloads": 180,
    "createdAt": "2025-11-01T03:59:57.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "4.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit/resolve/main/README.md",
    "description": "6-bit MLX conversion of Kimi-Linear-48B-A3B-Instruct."
  },
  {
    "model_name": "Qwen3-4B-Thinking-2507-gabliterated-8bit",
    "developer": "mlx-community",
    "downloads": 180,
    "createdAt": "2026-01-17T15:39:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-gabliterated-8bit/resolve/main/model.safetensors",
        "file_size": "4.0 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-gabliterated-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Thinking-2507-gabliterated-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-converted version of Qwen3-4B-Thinking with gabliteration modifications for uncensored text generation."
  },
  {
    "model_name": "Qwen3-VL-30B-A3B-Instruct-4bit",
    "developer": "mlx-community",
    "downloads": 180,
    "createdAt": "2025-10-08T22:32:53.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 4,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit/resolve/main/model-00001-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit/resolve/main/model-00002-of-00004.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit/resolve/main/model-00003-of-00004.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00004",
        "path": "https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit/resolve/main/model-00004-of-00004.safetensors",
        "file_size": "2.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized version of Qwen3-VL-30B-A3B-Instruct vision-language model for image-text tasks."
  },
  {
    "model_name": "gemma-3n-E4B-it-4bit",
    "developer": "mlx-community",
    "downloads": 179,
    "createdAt": "2025-06-27T13:48:29.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "434.0 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E4B-it-4bit/resolve/main/README.md",
    "description": "A 4-bit quantized MLX version of Google's Gemma 3N E4B instruction-tuned model for multimodal (image, text, audio, video) tasks on Apple Silicon."
  },
  {
    "model_name": "Nanonets-OCR2-3B-8bit",
    "developer": "mlx-community",
    "downloads": 179,
    "createdAt": "2025-10-14T12:22:30.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Nanonets-OCR2-3B-8bit/resolve/main/model.safetensors",
        "file_size": "4.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Nanonets-OCR2-3B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Nanonets-OCR2-3B-8bit/resolve/main/README.md",
    "description": "An MLX-converted OCR model for image-to-text tasks."
  },
  {
    "model_name": "LongCat-Flash-Thinking-2601-4bit",
    "developer": "mlx-community",
    "downloads": 178,
    "createdAt": "2026-01-16T22:35:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 84,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00001-of-00084.safetensors",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "model-00002-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00002-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00003-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00003-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00004-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00004-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00005-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00005-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00006-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00006-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00007-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00007-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00008-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00008-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00009-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00009-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00010-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00010-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00011-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00011-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00012-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00012-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00013-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00013-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00014-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00014-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00015-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00015-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00016-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00016-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00017-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00017-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00018-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00018-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00019-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00019-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00020-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00020-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00021-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00021-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00022-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00022-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00023-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00023-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00024-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00024-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00025-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00025-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00026-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00026-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00027-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00027-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00028-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00028-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00029-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00029-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00030-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00030-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00031-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00031-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00032-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00032-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00033-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00033-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00034-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00034-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00035-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00035-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00036-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00036-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00037-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00037-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00038-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00038-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00039-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00039-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00040-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00040-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00041-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00041-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00042-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00042-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00043-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00043-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00044-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00044-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00045-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00045-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00046-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00046-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00047-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00047-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00048-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00048-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00049-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00049-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00050-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00050-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00051-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00051-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00052-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00052-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00053-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00053-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00054-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00054-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00055-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00055-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00056-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00056-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00057-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00057-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00058-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00058-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00059-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00059-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00060-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00060-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00061-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00061-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00062-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00062-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00063-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00063-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00064-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00064-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00065-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00065-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00066-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00066-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00067-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00067-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00068-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00068-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00069-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00069-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00070-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00070-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00071-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00071-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00072-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00072-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00073-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00073-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00074-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00074-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00075-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00075-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00076-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00076-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00077-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00077-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00078-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00078-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00079-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00079-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00080-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00080-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00081-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00081-of-00084.safetensors",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "model-00082-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00082-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00083-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00083-of-00084.safetensors",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "model-00084-of-00084",
        "path": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/model-00084-of-00084.safetensors",
        "file_size": "4.1 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LongCat-Flash-Thinking-2601-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit LongCat-Flash-Thinking-2601 model for text generation."
  },
  {
    "model_name": "Ministral-3-14B-Instruct-2512-8bit",
    "developer": "mlx-community",
    "downloads": 178,
    "createdAt": "2025-12-03T23:24:40.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 3,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-8bit/resolve/main/model-00001-of-00003.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-8bit/resolve/main/model-00002-of-00003.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00003",
        "path": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-8bit/resolve/main/model-00003-of-00003.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-14B-Instruct-2512-8bit/resolve/main/README.md",
    "description": "MLX-converted 8-bit version of Mistral's Ministral-3-14B instruction-tuned vision-language model for Apple Silicon."
  },
  {
    "model_name": "VibeVoice-ASR-8bit",
    "developer": "mlx-community",
    "downloads": 176,
    "createdAt": "2026-01-23T00:24:22.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/VibeVoice-ASR-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/VibeVoice-ASR-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/VibeVoice-ASR-8bit/resolve/main/README.md",
    "description": "An 8-bit MLX-converted automatic speech recognition model for transcribing audio to text on Apple Silicon devices."
  },
  {
    "model_name": "GLM-4.6V-Flash-4bit",
    "developer": "mlx-community",
    "downloads": 174,
    "createdAt": "2025-12-08T13:26:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-4bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-4bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-4.6V-Flash-4bit/resolve/main/README.md",
    "description": "MLX-converted 4-bit quantized vision-language model for image-text tasks."
  },
  {
    "model_name": "Fun-CosyVoice3-0.5B-2512-fp16",
    "developer": "mlx-community",
    "downloads": 173,
    "createdAt": "2025-12-16T21:31:13.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Fun-CosyVoice3-0.5B-2512-fp16/resolve/main/model.safetensors",
        "file_size": "1.6 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Fun-CosyVoice3-0.5B-2512-fp16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Fun-CosyVoice3-0.5B-2512-fp16/resolve/main/README.md",
    "description": "MLX-converted Fun-CosyVoice3-0.5B text-to-speech model supporting cross-lingual, zero-shot, instruct, and voice conversion modes in 9 languages."
  },
  {
    "model_name": "medgemma-1.5-4b-it-6bit",
    "developer": "mlx-community",
    "downloads": 173,
    "createdAt": "2026-01-14T00:20:37.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-6bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/medgemma-1.5-4b-it-6bit/resolve/main/README.md",
    "description": "MLX-converted 4B parameter medical AI model for image-text tasks in radiology, dermatology, pathology, ophthalmology, and chest X-ray analysis."
  },
  {
    "model_name": "GLM-OCR-6bit",
    "developer": "mlx-community",
    "downloads": 172,
    "createdAt": "2026-02-03T11:56:04.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/GLM-OCR-6bit/resolve/main/model.safetensors",
        "file_size": "1.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/GLM-OCR-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/GLM-OCR-6bit/resolve/main/README.md",
    "description": "MLX-converted OCR model for image-to-text tasks."
  },
  {
    "model_name": "LFM2.5-1.2B-Instruct-bf16",
    "developer": "mlx-community",
    "downloads": 171,
    "createdAt": "2026-01-06T10:22:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-bf16/resolve/main/model.safetensors",
        "file_size": "2.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/LFM2.5-1.2B-Instruct-bf16/resolve/main/README.md",
    "description": "MLX conversion of Liquid's LFM2.5-1.2B-Instruct text generation model."
  },
  {
    "model_name": "Qwen3-Omni-30B-A3B-Instruct-8bit",
    "developer": "mlx-community",
    "downloads": 170,
    "createdAt": "2025-12-24T17:05:27.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "1.4 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-Omni-30B-A3B-Instruct-8bit/resolve/main/README.md",
    "description": "MLX-converted Qwen3-Omni-30B multimodal model for any-to-any inference."
  },
  {
    "model_name": "gemma-3n-E2B-it-lm-bf16",
    "developer": "mlx-community",
    "downloads": 169,
    "createdAt": "2025-06-29T23:34:54.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-bf16/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-bf16/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "3.8 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-bf16/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/gemma-3n-E2B-it-lm-bf16/resolve/main/README.md",
    "description": "A bf16, instruction-tuned Gemma 3n E2B model converted to MLX format for use with `mlx-lm`."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512-8bit",
    "developer": "mlx-community",
    "downloads": 168,
    "createdAt": "2025-12-03T22:07:59.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-8bit/resolve/main/model.safetensors",
        "file_size": "4.2 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Ministral-3-3B-Instruct-2512-8bit/resolve/main/README.md",
    "description": "This is an 8-bit MLX-converted version of Mistral's Ministral-3-3B-Instruct vision-language model for Apple Silicon."
  },
  {
    "model_name": "dolphin-2.9-llama3-70b-4bit",
    "developer": "mlx-community",
    "downloads": 166,
    "createdAt": "2024-04-25T06:03:05.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 8,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00001-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00002-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00002-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00003-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00003-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00004-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00004-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00005-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00005-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00006-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00007-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00007-of-00008.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00008-of-00008",
        "path": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/model-00008-of-00008.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/dolphin-2.9-llama3-70b-4bit/resolve/main/README.md",
    "description": "MLX-quantized Dolphin 2.9 Llama 3 70B model (4-bit) for Apple Silicon."
  },
  {
    "model_name": "Kokoro-82M-6bit",
    "developer": "mlx-community",
    "downloads": 166,
    "createdAt": "2025-02-28T18:52:26.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 55,
    "safetensors_files": [
      {
        "model_id": "kokoro-v1_0",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/kokoro-v1_0.safetensors",
        "file_size": "272.8 MB"
      },
      {
        "model_id": "voices/af_alloy",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_alloy.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_aoede",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_aoede.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_bella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_bella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_heart",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_heart.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_jessica",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_jessica.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_kore",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_kore.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nicole",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_nicole.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_nova",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_nova.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_river",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_river.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sarah",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_sarah.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/af_sky",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/af_sky.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_adam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_adam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_echo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_echo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_eric",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_eric.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_fenrir",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_fenrir.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_liam",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_liam.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_michael",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_michael.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_onyx",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_onyx.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_puck",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_puck.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/am_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/am_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_alice",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bf_alice.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_emma",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bf_emma.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_isabella",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bf_isabella.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bf_lily",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bf_lily.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_daniel",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bm_daniel.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_fable",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bm_fable.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_george",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bm_george.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/bm_lewis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/bm_lewis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ef_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/ef_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/em_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/em_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/em_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/ff_siwis",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/ff_siwis.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/hf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hf_beta",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/hf_beta.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_omega",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/hm_omega.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/hm_psi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/hm_psi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/if_sara",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/if_sara.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/im_nicola",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/im_nicola.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_alpha",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/jf_alpha.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_gongitsune",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/jf_gongitsune.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_nezumi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/jf_nezumi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jf_tebukuro",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/jf_tebukuro.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/jm_kumo",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/jm_kumo.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pf_dora",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/pf_dora.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_alex",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/pm_alex.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/pm_santa",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/pm_santa.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaobei",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zf_xiaobei.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoni",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zf_xiaoni.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoxiao",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zf_xiaoxiao.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zf_xiaoyi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zf_xiaoyi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunjian",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zm_yunjian.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxi",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zm_yunxi.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunxia",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zm_yunxia.safetensors",
        "file_size": "510.1 KB"
      },
      {
        "model_id": "voices/zm_yunyang",
        "path": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/voices/zm_yunyang.safetensors",
        "file_size": "510.1 KB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Kokoro-82M-6bit/resolve/main/README.md",
    "description": "Kokoro-82M text-to-speech model converted to MLX format for Apple Silicon."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-DDWQ",
    "developer": "mlx-community",
    "downloads": 166,
    "createdAt": "2025-08-08T15:59:21.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-DDWQ/resolve/main/model.safetensors",
        "file_size": "2.3 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-DDWQ/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Qwen3-4B-Instruct-2507-DDWQ/resolve/main/README.md",
    "description": "Apache 2.0 licensed instruction-tuned model based on Qwen/Qwen3-4B-Instruct-2507."
  },
  {
    "model_name": "Falcon-H1R-7B-8bit",
    "developer": "mlx-community",
    "downloads": 165,
    "createdAt": "2026-01-05T13:18:17.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 2,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00002",
        "path": "https://huggingface.co/mlx-community/Falcon-H1R-7B-8bit/resolve/main/model-00001-of-00002.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00002",
        "path": "https://huggingface.co/mlx-community/Falcon-H1R-7B-8bit/resolve/main/model-00002-of-00002.safetensors",
        "file_size": "2.5 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Falcon-H1R-7B-8bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Falcon-H1R-7B-8bit/resolve/main/README.md",
    "description": "This is an 8-bit MLX converted version of the Falcon-H1R-7B language model for text generation."
  },
  {
    "model_name": "Llama-3.3-70B-Instruct-3bit",
    "developer": "mlx-community",
    "downloads": 164,
    "createdAt": "2024-12-06T17:13:16.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 6,
    "safetensors_files": [
      {
        "model_id": "model-00001-of-00006",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/model-00001-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00002-of-00006",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/model-00002-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00003-of-00006",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/model-00003-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00004-of-00006",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/model-00004-of-00006.safetensors",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "model-00005-of-00006",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/model-00005-of-00006.safetensors",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "model-00006-of-00006",
        "path": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/model-00006-of-00006.safetensors",
        "file_size": "3.9 GB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-3bit/resolve/main/README.md",
    "description": "A 3-bit quantized MLX version of Meta's Llama-3.3-70B-Instruct for Apple Silicon."
  },
  {
    "model_name": "SmolLM2-135M-Instruct",
    "developer": "mlx-community",
    "downloads": 164,
    "createdAt": "2024-10-31T22:20:08.000Z",
    "library_name": "mlx",
    "tools": false,
    "num_safetensors": 1,
    "safetensors_files": [
      {
        "model_id": "model",
        "path": "https://huggingface.co/mlx-community/SmolLM2-135M-Instruct/resolve/main/model.safetensors",
        "file_size": "256.6 MB"
      }
    ],
    "config": "https://huggingface.co/mlx-community/SmolLM2-135M-Instruct/resolve/main/config.json",
    "readme": "https://huggingface.co/mlx-community/SmolLM2-135M-Instruct/resolve/main/README.md",
    "description": "MLX-converted SmolLM2-135M-Instruct model for Apple Silicon."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 209670,
    "createdAt": "2024-07-23T15:36:34.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ2_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q2_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q2_K_L.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_4.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_4_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0_8_8.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_L.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "Meta-Llama-3.1-8B-Instruct-f32",
        "path": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-f32.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Meta-Llama-3.1-8B-Instruct for local inference using llama.cpp."
  },
  {
    "model_name": "gemma-3-4b-it-GGUF",
    "developer": "unsloth",
    "downloads": 196953,
    "createdAt": "2025-03-12T09:04:23.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-BF16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_M.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ1_S.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ2_XXS.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q5_K_XL.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q6_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q8_K_XL.gguf",
        "file_size": "4.8 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "811.8 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/README.md",
    "description": "Google Gemma 3 4B IT is a lightweight, open, multimodal (text/image) instruction-tuned model with a 128k context window, optimized for efficient fine-tuning by Unsloth."
  },
  {
    "model_name": "Devstral-Small-2-24B-Instruct-2512-GGUF",
    "developer": "unsloth",
    "downloads": 157014,
    "createdAt": "2025-12-10T01:36:17.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-BF16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q2_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q2_K_L.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_1",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q6_K",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-Q8_0",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ1_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ1_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ2_M.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ2_XXS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-IQ3_XXS.gguf",
        "file_size": "8.8 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q2_K_XL.gguf",
        "file_size": "8.7 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q3_K_XL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf",
        "file_size": "13.5 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL.gguf",
        "file_size": "19.4 GB"
      },
      {
        "model_id": "Devstral-Small-2-24B-Instruct-2512-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/Devstral-Small-2-24B-Instruct-2512-UD-Q8_K_XL.gguf",
        "file_size": "27.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "838.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "837.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/README.md",
    "description": "Devstral Small 2 is a 24B open-source AI model specialized in agentic coding and software engineering tasks."
  },
  {
    "model_name": "gpt-oss-20b-GGUF",
    "developer": "unsloth",
    "downloads": 138121,
    "createdAt": "2025-08-05T17:12:17.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 16,
    "quants": [
      {
        "model_id": "gpt-oss-20b-F16",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q2_K",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q2_K.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q2_K_L.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q3_K_S.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_0",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_0.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_1",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_1.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_M.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_S.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q5_K_M.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q5_K_S.gguf",
        "file_size": "10.9 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q6_K",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q6_K.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-Q8_0",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q8_0.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "gpt-oss-20b-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-UD-Q4_K_XL.gguf",
        "file_size": "11.1 GB"
      },
      {
        "model_id": "gpt-oss-20b-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-UD-Q6_K_XL.gguf",
        "file_size": "11.2 GB"
      },
      {
        "model_id": "gpt-oss-20b-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-UD-Q8_K_XL.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/README.md",
    "description": "OpenAI's 20B open-weight reasoning model with Apache 2.0 license, MXFP4 quantization, agentic capabilities, and full chain-of-thought support."
  },
  {
    "model_name": "gemma-3-12b-it-GGUF",
    "developer": "unsloth",
    "downloads": 128806,
    "createdAt": "2025-03-12T10:34:12.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-BF16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_NL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-IQ4_XS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q2_K_L",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q2_K_L.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q3_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_1",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q4_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q5_K_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q6_K",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-Q8_0",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ1_S.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_M.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ2_XXS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q2_K_XL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q3_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q4_K_XL.gguf",
        "file_size": "6.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q5_K_XL.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q6_K_XL.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-UD-Q8_K_XL.gguf",
        "file_size": "13.4 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "814.6 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/resolve/main/README.md",
    "description": "Google's Gemma 3 12B IT is a state-of-the-art, open, multimodal language model designed for efficient text and image generation."
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 108706,
    "createdAt": "2025-08-07T22:20:03.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE2-Plus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-Q5_1.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "A quantized, uncensored 20B Mixture of Experts model in GGUF format optimized for creative writing and coding tasks."
  },
  {
    "model_name": "Qwen3-8B-GGUF",
    "developer": "Qwen",
    "downloads": 95731,
    "createdAt": "2025-05-03T06:33:59.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "Qwen3-8B-GGUF is an 8B-parameter quantized language model with unique switchable thinking modes for complex reasoning or efficient dialogue."
  },
  {
    "model_name": "Qwen2.5-1.5B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 78940,
    "createdAt": "2024-09-17T13:57:52.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-1.5b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-fp16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q2_k.gguf",
        "file_size": "718.0 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q3_k_m.gguf",
        "file_size": "881.6 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf",
        "file_size": "1016.8 MB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q5_k_m.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q6_k.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "qwen2.5-1.5b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf",
        "file_size": "1.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "Qwen2.5-1.5B-Instruct-GGUF is a 1.5B parameter instruction-tuned language model by Alibaba Cloud in quantized GGUF format, supporting multilingual chat, coding, math, and long contexts up to 128K tokens."
  },
  {
    "model_name": "Qwen3-4B-GGUF",
    "developer": "unsloth",
    "downloads": 78483,
    "createdAt": "2025-04-28T07:55:09.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-BF16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B is a 4B-parameter causal language model by Alibaba Cloud with unique seamless switching between thinking and non-thinking modes for complex reasoning vs. efficient dialogue."
  },
  {
    "model_name": "Qwen3-VL-8B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 76167,
    "createdAt": "2025-10-31T02:50:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen3VL-8B-Instruct-F16",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Qwen3VL-8B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Qwen3VL-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen3VL-8B-Instruct-F16",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-8B-Instruct-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mmproj-Qwen3VL-8B-Instruct-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf",
        "file_size": "717.4 MB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/README.md",
    "description": "GGUF-quantized Qwen3-VL-8B-Instruct vision-language model for local inference with llama.cpp and compatible tools."
  },
  {
    "model_name": "DeepSeek-R1-0528-Qwen3-8B-GGUF",
    "developer": "unsloth",
    "downloads": 69437,
    "createdAt": "2025-05-29T14:17:25.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-BF16",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q3_K_S.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_0.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_1",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q6_K",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-Q8_0",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ2_XXS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-IQ3_XXS.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q3_K_XL.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q5_K_XL.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf",
        "file_size": "10.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/resolve/main/README.md",
    "description": "DeepSeek-R1-0528-Qwen3-8B is a reasoning-focused language model created by distilling DeepSeek's improved R1-0528 reasoning capabilities into Qwen3-8B, achieving strong performance on math, coding, and general reasoning benchmarks under MIT licensing."
  },
  {
    "model_name": "gemma-2b-it",
    "developer": "google",
    "downloads": 67139,
    "createdAt": "2024-02-08T13:23:59.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "gemma-2b-it",
        "path": "https://huggingface.co/google/gemma-2b-it/resolve/main/gemma-2b-it.gguf",
        "file_size": "9.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/google/gemma-2b-it/resolve/main/README.md",
    "description": "Google's Gemma 2B Instruct is a lightweight, open-source, text-to-text decoder-only LLM optimized for efficient deployment on resource-constrained devices."
  },
  {
    "model_name": "Qwen3-4B-Instruct-2507-GGUF",
    "developer": "unsloth",
    "downloads": 64241,
    "createdAt": "2025-08-06T19:21:40.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Qwen3-4B-Instruct-2507-F16",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q2_K_L.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q3_K_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_0.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_1",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_1.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q6_K",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-Q8_0",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ1_S.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_M.gguf",
        "file_size": "1.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ2_XXS.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q2_K_XL.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q3_K_XL.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q5_K_XL.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Qwen3-4B-Instruct-2507-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/Qwen3-4B-Instruct-2507-UD-Q8_K_XL.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF/resolve/main/README.md",
    "description": "Qwen3-4B-Instruct-2507 is a 4B parameter instruction-tuned language model with 262K context length, improved reasoning/coding capabilities, and tool-use support."
  },
  {
    "model_name": "Dolphin3.0-Llama3.1-8B-GGUF",
    "developer": "dphn",
    "downloads": 62233,
    "createdAt": "2025-01-02T22:11:05.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-F16",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-F16.gguf",
        "file_size": "15.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q2_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_L",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q3_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q4_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_1",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_M",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q5_K_S",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q6_K",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Dolphin3.0-Llama3.1-8B-Q8_0",
        "path": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/Dolphin3.0-Llama3.1-8B-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/dphn/Dolphin3.0-Llama3.1-8B-GGUF/resolve/main/README.md",
    "description": "Dolphin 3.0 Llama 3.1 8B is a steerable, general-purpose instruct model optimized for coding, math, and agentic tasks, designed as an uncensored alternative to commercial AI assistants."
  },
  {
    "model_name": "Llama-3.2-1B-Instruct-GGUF",
    "developer": "bartowski",
    "downloads": 59871,
    "createdAt": "2024-09-25T18:35:25.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 18,
    "quants": [
      {
        "model_id": "Llama-3.2-1B-Instruct-IQ3_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ3_M.gguf",
        "file_size": "626.8 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ4_XS.gguf",
        "file_size": "708.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q3_K_L.gguf",
        "file_size": "698.6 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q3_K_XL.gguf",
        "file_size": "759.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf",
        "file_size": "737.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_0_4_4",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0_4_4.gguf",
        "file_size": "735.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_0_4_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0_4_8.gguf",
        "file_size": "735.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_0_8_8",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0_8_8.gguf",
        "file_size": "735.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_L.gguf",
        "file_size": "830.9 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "file_size": "770.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_S.gguf",
        "file_size": "739.7 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q5_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_L.gguf",
        "file_size": "929.9 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf",
        "file_size": "869.3 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_S.gguf",
        "file_size": "851.2 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q6_K",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q6_K.gguf",
        "file_size": "974.5 MB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q6_K_L",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q6_K_L.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-Q8_0",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q8_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "Llama-3.2-1B-Instruct-f16",
        "path": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-f16.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of Meta's Llama-3.2-1B-Instruct model for local inference."
  },
  {
    "model_name": "GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF",
    "developer": "DavidAU",
    "downloads": 57610,
    "createdAt": "2026-01-22T05:13:27.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ2_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ2_M.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ3_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ3_M.gguf",
        "file_size": "12.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_NL.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ4_XS.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_1",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_M.gguf",
        "file_size": "17.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_S",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_S.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_1",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_1.gguf",
        "file_size": "21.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_S",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q6_K",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q6_K.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q8_0.gguf",
        "file_size": "29.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF/resolve/main/README.md",
    "description": "**Uncensored GGUF quant of GLM-4.7-Flash optimized for creative fiction writing, storytelling, and roleplaying across all genres.**"
  },
  {
    "model_name": "LFM2.5-1.2B-Instruct-GGUF",
    "developer": "LiquidAI",
    "downloads": 55727,
    "createdAt": "2026-01-04T00:09:26.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "LFM2.5-1.2B-Instruct-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-BF16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Instruct-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/LFM2.5-1.2B-Instruct-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF/resolve/main/README.md",
    "description": "LFM2.5-1.2B-Instruct is a small hybrid AI model by Liquid AI designed for on-device deployment, runnable via llama.cpp."
  },
  {
    "model_name": "GLM-4.7-Flash-REAP-23B-A3B-GGUF",
    "developer": "unsloth",
    "downloads": 50614,
    "createdAt": "2026-01-23T06:50:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-BF16.gguf",
        "file_size": "42.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-IQ4_NL.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-IQ4_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q2_K.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q2_K_L.gguf",
        "file_size": "8.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q3_K_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q3_K_S.gguf",
        "file_size": "9.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_0.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_1.gguf",
        "file_size": "13.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_K_M.gguf",
        "file_size": "13.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q4_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q5_K_M.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q5_K_S.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q6_K.gguf",
        "file_size": "17.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-Q8_0.gguf",
        "file_size": "22.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_M.gguf",
        "file_size": "7.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ1_S.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_M.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ2_XXS.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-IQ3_XXS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q2_K_XL.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q4_K_XL.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q5_K_XL.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q6_K_XL.gguf",
        "file_size": "18.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-Q8_K_XL.gguf",
        "file_size": "25.6 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-REAP-23B-A3B-UD-TQ1_0",
        "path": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/GLM-4.7-Flash-REAP-23B-A3B-UD-TQ1_0.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF/resolve/main/README.md",
    "description": "A 23B parameter Mixture-of-Experts model created by uniformly pruning 25% of experts from GLM-4.7-Flash using the REAP method, maintaining near-original performance while reducing size."
  },
  {
    "model_name": "LFM2.5-VL-1.6B-GGUF",
    "developer": "LiquidAI",
    "downloads": 49717,
    "createdAt": "2026-01-04T19:35:04.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "LFM2.5-VL-1.6B-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/LFM2.5-VL-1.6B-BF16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-VL-1.6B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/LFM2.5-VL-1.6B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-VL-1.6B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/LFM2.5-VL-1.6B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-VL-1.6B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/LFM2.5-VL-1.6B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2.5-VL-1.6b-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/mmproj-LFM2.5-VL-1.6b-BF16.gguf",
        "file_size": "816.1 MB"
      },
      {
        "model_id": "mmproj-LFM2.5-VL-1.6b-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/mmproj-LFM2.5-VL-1.6b-F16.gguf",
        "file_size": "814.4 MB"
      },
      {
        "model_id": "mmproj-LFM2.5-VL-1.6b-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/mmproj-LFM2.5-VL-1.6b-Q8_0.gguf",
        "file_size": "556.1 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B-GGUF/resolve/main/README.md",
    "description": "LFM2.5-VL-1.6B is a vision-language model by Liquid AI that processes images and text, available in GGUF format for llama.cpp."
  },
  {
    "model_name": "OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf",
    "developer": "DavidAU",
    "downloads": 49577,
    "createdAt": "2025-11-17T05:52:55.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 17,
    "quants": [
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODE-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-CODEPlus16-Uncensored-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-5-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-CODE-TRI-Uncensored-Q8_0.gguf",
        "file_size": "20.5 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRR-DI-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-HRRPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-IQ4_NL.gguf",
        "file_size": "11.8 GB"
      },
      {
        "model_id": "OpenAI-20B-NEO-Uncensored2-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEO-Uncensored2-Q5_1.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q5_1",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q5_1.gguf",
        "file_size": "14.6 GB"
      },
      {
        "model_id": "OpenAI-20B-NEOPlus-Uncensored-Q8_0",
        "path": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/OpenAI-20B-NEOPlus-Uncensored-Q8_0.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf/resolve/main/README.md",
    "description": "An uncensored 20B Mixture of Experts model optimized for coding and creative tasks, available in multiple quantization formats."
  },
  {
    "model_name": "Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF",
    "developer": "DavidAU",
    "downloads": 47732,
    "createdAt": "2024-12-10T13:12:37.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-IQ4_XS.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q2_k.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_l.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_m.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q3_k_s.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_4.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_4_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_0_8_8.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_m.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q4_k_s.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q5_k_s.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q6_k.gguf",
        "file_size": "14.1 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-Q8_0.gguf",
        "file_size": "18.2 GB"
      },
      {
        "model_id": "L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m",
        "path": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/L3.2-8X3B-MOE-Dark-Champion-Inst-18.4B-uncen-ablit_D_AU-q5_k_m.gguf",
        "file_size": "12.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/resolve/main/README.md",
    "description": "Mixture of Experts model combining 8 Llama 3.2 3B models into an 18.4B uncensored creative writing powerhouse optimized for fiction, roleplay, and vivid prose generation."
  },
  {
    "model_name": "LFM2.5-1.2B-Thinking-GGUF",
    "developer": "LiquidAI",
    "downloads": 44227,
    "createdAt": "2026-01-16T19:04:55.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "LFM2.5-1.2B-Thinking-BF16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-BF16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q5_K_M",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q6_K",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Thinking-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/LFM2.5-1.2B-Thinking-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking-GGUF/resolve/main/README.md",
    "description": "LFM2.5-1.2B-Thinking is a compact 1.2B parameter hybrid AI model by Liquid AI, optimized for on-device deployment and available in GGUF format."
  },
  {
    "model_name": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 44180,
    "createdAt": "2025-08-03T10:14:21.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ1_S.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_M.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_S.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XS.gguf",
        "file_size": "8.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ2_XXS.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_M.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XS.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ3_XXS.gguf",
        "file_size": "11.0 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-IQ4_XS.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q2_K_S.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_L.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_M.gguf",
        "file_size": "13.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q3_K_S.gguf",
        "file_size": "12.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_0.gguf",
        "file_size": "16.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_1.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q4_K_S.gguf",
        "file_size": "16.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q5_K_S.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.i1-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.imatrix",
        "path": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated.imatrix.gguf",
        "file_size": "116.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of the abliterated Huihui-Qwen3-Coder-30B-A3B-Instruct model in various quantization levels."
  },
  {
    "model_name": "Qwen2.5-0.5B-Instruct-GGUF",
    "developer": "Qwen",
    "downloads": 43208,
    "createdAt": "2024-09-17T13:57:41.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "qwen2.5-0.5b-instruct-fp16",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-fp16.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q2_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q2_k.gguf",
        "file_size": "395.9 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q3_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q3_k_m.gguf",
        "file_size": "412.0 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q4_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_0.gguf",
        "file_size": "408.9 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q4_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf",
        "file_size": "468.6 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q5_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q5_0.gguf",
        "file_size": "467.8 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q5_k_m",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q5_k_m.gguf",
        "file_size": "498.0 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q6_k",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q6_k.gguf",
        "file_size": "620.2 MB"
      },
      {
        "model_id": "qwen2.5-0.5b-instruct-q8_0",
        "path": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q8_0.gguf",
        "file_size": "644.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/README.md",
    "description": "A compact 0.5B parameter instruction-tuned language model from Qwen (Alibaba Cloud) in quantized GGUF format for efficient inference."
  },
  {
    "model_name": "GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF",
    "developer": "TeichAI",
    "downloads": 35910,
    "createdAt": "2026-01-22T18:25:31.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.bf16",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.bf16.gguf",
        "file_size": "55.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.f16",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.f16.gguf",
        "file_size": "55.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq2_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq2_m.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq3_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq3_m.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq3_xs",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq3_xs.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq4_nl",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq4_nl.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.iq4_xs",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.iq4_xs.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q3_k_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q3_k_m.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q3_k_s",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q3_k_s.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q4_k_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q4_k_m.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q5_k_m",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q5_k_m.gguf",
        "file_size": "19.8 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q6_k",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q6_k.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "glm-4.7-flash-claude-4.5-opus.q8_0",
        "path": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/glm-4.7-flash-claude-4.5-opus.q8_0.gguf",
        "file_size": "29.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF/resolve/main/README.md",
    "description": "A reasoning-distilled version of GLM-4.7-Flash trained on high-reasoning Claude Opus 4.5 data for coding, science, and research tasks."
  },
  {
    "model_name": "GLM-4.6V-Flash-GGUF",
    "developer": "unsloth",
    "downloads": 28653,
    "createdAt": "2025-12-09T01:24:30.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "GLM-4.6V-Flash-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-BF16.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-IQ4_NL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-IQ4_NL.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-IQ4_XS",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-IQ4_XS.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q2_K",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q2_K.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q2_K_L",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q2_K_L.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q3_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q3_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q3_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q3_K_S.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q4_0",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_0.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q4_1",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_1.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q4_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q4_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q4_K_S.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q5_K_M",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q5_K_M.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q5_K_S",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q5_K_S.gguf",
        "file_size": "6.2 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q6_K",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q6_K.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-Q8_0",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-Q8_0.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-IQ1_M.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-IQ1_S.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-IQ2_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-IQ3_XXS.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-Q2_K_XL.gguf",
        "file_size": "3.9 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-Q3_K_XL.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-Q4_K_XL.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-Q5_K_XL.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-Q6_K_XL.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4.6V-Flash-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/GLM-4.6V-Flash-UD-Q8_K_XL.gguf",
        "file_size": "11.2 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "3.3 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF/resolve/main/README.md",
    "description": "A 9B parameter multimodal vision-language model optimized for local deployment with native function calling and document understanding capabilities."
  },
  {
    "model_name": "gemma-3-27b-it-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 26291,
    "createdAt": "2025-03-17T20:35:16.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "gemma-3-27b-it-abliterated.q2_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q2_k.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q3_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q3_k_m.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q4_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q4_k_m.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q5_k_m",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q5_k_m.gguf",
        "file_size": "17.9 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q6_k",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q6_k.gguf",
        "file_size": "20.6 GB"
      },
      {
        "model_id": "gemma-3-27b-it-abliterated.q8_0",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/gemma-3-27b-it-abliterated.q8_0.gguf",
        "file_size": "26.7 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-mlabonne_gemma-3-27b-it-abliterated-f16",
        "path": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
        "file_size": "818.0 MB"
      }
    ],
    "readme": "https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF/resolve/main/README.md",
    "description": "This is an uncensored, experimental version of the Google Gemma 3 27B IT model, created using a new layerwise abliteration technique to remove refusals."
  },
  {
    "model_name": "MiniCPM-o-4_5-gguf",
    "developer": "openbmb",
    "downloads": 22160,
    "createdAt": "2026-02-02T04:42:00.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 19,
    "quants": [
      {
        "model_id": "MiniCPM-o-4_5-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_0.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_1.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_0.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_1.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "MiniCPM-o-4_5-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/MiniCPM-o-4_5-Q8_0.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "audio/MiniCPM-o-4_5-audio-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/audio/MiniCPM-o-4_5-audio-F16.gguf",
        "file_size": "629.6 MB"
      },
      {
        "model_id": "token2wav-gguf/flow_extra",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/flow_extra.gguf",
        "file_size": "13.0 MB"
      },
      {
        "model_id": "token2wav-gguf/flow_matching",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/flow_matching.gguf",
        "file_size": "437.0 MB"
      },
      {
        "model_id": "token2wav-gguf/hifigan2",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/hifigan2.gguf",
        "file_size": "79.4 MB"
      },
      {
        "model_id": "token2wav-gguf/prompt_cache",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/token2wav-gguf/prompt_cache.gguf",
        "file_size": "201.8 MB"
      },
      {
        "model_id": "tts/MiniCPM-o-4_5-projector-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/tts/MiniCPM-o-4_5-projector-F16.gguf",
        "file_size": "14.3 MB"
      },
      {
        "model_id": "tts/MiniCPM-o-4_5-tts-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/tts/MiniCPM-o-4_5-tts-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "vision/MiniCPM-o-4_5-vision-F16",
        "path": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/vision/MiniCPM-o-4_5-vision-F16.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf/resolve/main/README.md",
    "description": "A 9B parameter multimodal AI model for vision, speech, and full-duplex live streaming on phones."
  },
  {
    "model_name": "L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix",
    "developer": "Lewdiculous",
    "downloads": 16947,
    "createdAt": "2024-06-05T18:21:00.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 12,
    "quants": [
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_M-imat.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_S-imat.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ3_XXS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ3_XXS-imat.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-IQ4_XS-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-IQ4_XS-imat.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_M-imat.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q4_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q4_K_S-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_M-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q5_K_S-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q5_K_S-imat.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q6_K-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q6_K-imat.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "L3-8B-Stheno-v3.2-Q8_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/L3-8B-Stheno-v3.2-Q8_0-imat.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/[ARM-Friendly]-L3-8B-Stheno-v3.2-Q4_0-imat.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "test",
        "path": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/test.gguf",
        "file_size": "4.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix/resolve/main/README.md",
    "description": "GGUF-IQ-Imatrix quantized version of Sao10K/L3-8B-Stheno-v3.2 for roleplay with SillyTavern, optimized for 8GB VRAM GPUs using Q4_K_M-imat quant."
  },
  {
    "model_name": "gemma-3-12b-it-heretic",
    "developer": "DreamFast",
    "downloads": 13636,
    "createdAt": "2026-01-11T02:40:55.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q3_K_M",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q4_K_M",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q4_K_S",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q5_K_M",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q5_K_S",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q6_K",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-Q8_0",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gguf/gemma-3-12b-it-heretic-f16",
        "path": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/gguf/gemma-3-12b-it-heretic-f16.gguf",
        "file_size": "21.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/resolve/main/README.md",
    "description": "An abliterated (uncensored) Google Gemma 3 12B IT model optimized for LTX-2 video generation, with significantly reduced refusals."
  },
  {
    "model_name": "gemma-3-4b-it-uncensored-v2-GGUF",
    "developer": "Andycurrent",
    "downloads": 13529,
    "createdAt": "2026-01-06T07:47:19.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_F16",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_F16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_Q2_K",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_Q3_K_M",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_Q3_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_Q4_K_M",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_Q5_K_M",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_Q5_K_M.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_Q6_K",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_Q6_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "gemma-3-4b-it-uncensored-v2_Q8_0",
        "path": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/gemma-3-4b-it-uncensored-v2_Q8_0.gguf",
        "file_size": "3.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Andycurrent/gemma-3-4b-it-uncensored-v2-GGUF/resolve/main/README.md",
    "description": "A 4B parameter instruction-tuned model designed for local/private use with minimal alignment restrictions."
  },
  {
    "model_name": "Ministral-3-3B-Instruct-2512-GGUF",
    "developer": "mistralai",
    "downloads": 12489,
    "createdAt": "2025-10-31T08:45:13.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ministral-3-3B-Instruct-2512-BF16-mmproj",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-BF16-mmproj.gguf",
        "file_size": "802.5 MB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-BF16",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-BF16.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-Q5_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Ministral-3-3B-Instruct-2512-Q8_0",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/Ministral-3-3B-Instruct-2512-Q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-GGUF/resolve/main/README.md",
    "description": "A small, efficient edge-optimized vision language model with multilingual and agentic capabilities, quantized in GGUF format."
  },
  {
    "model_name": "Kimi-Linear-48B-A3B-Instruct-GGUF",
    "developer": "ymcki",
    "downloads": 12158,
    "createdAt": "2025-12-18T00:01:46.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Kimi-Linear-48B-A3B-Instruct-jp-imatrix.IQ3_M",
        "path": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct-jp-imatrix.IQ3_M.gguf",
        "file_size": "20.1 GB"
      },
      {
        "model_id": "Kimi-Linear-48B-A3B-Instruct-jp-imatrix.IQ4_XS",
        "path": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct-jp-imatrix.IQ4_XS.gguf",
        "file_size": "24.5 GB"
      },
      {
        "model_id": "Kimi-Linear-48B-A3B-Instruct-jp-imatrix.Q2_K",
        "path": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct-jp-imatrix.Q2_K.gguf",
        "file_size": "16.8 GB"
      },
      {
        "model_id": "Kimi-Linear-48B-A3B-Instruct-jp-imatrix.Q4_K_M",
        "path": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct-jp-imatrix.Q4_K_M.gguf",
        "file_size": "27.7 GB"
      },
      {
        "model_id": "Kimi-Linear-48B-A3B-Instruct-jp-imatrix.Q5_K_M",
        "path": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct-jp-imatrix.Q5_K_M.gguf",
        "file_size": "32.5 GB"
      },
      {
        "model_id": "Kimi-Linear-48B-A3B-Instruct.MXFP4_MOE",
        "path": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct.MXFP4_MOE.gguf",
        "file_size": "25.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/README.md",
    "description": "Experimental GGUF quantizations of Kimi-Linear-48B-A3B-Instruct with various formats optimized for different VRAM sizes and Japanese performance."
  },
  {
    "model_name": "GLM-4.7-Flash-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 12097,
    "createdAt": "2026-01-19T22:07:24.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/GLM-4.7-Flash-MXFP4_MOE-GGUF/resolve/main/GLM-4.7-Flash-MXFP4_MOE.gguf",
        "file_size": "16.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/GLM-4.7-Flash-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": "This is a MXFP4_MOE quantized GGUF version of the GLM-4.7-Flash model, updated for the latest llama.cpp."
  },
  {
    "model_name": "Ministral-3-8B-Instruct-2512-GGUF",
    "developer": "unsloth",
    "downloads": 11391,
    "createdAt": "2025-12-02T12:53:08.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "Ministral-3-8B-Instruct-2512-BF16",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-BF16.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-IQ4_NL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-IQ4_NL.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-IQ4_XS",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-IQ4_XS.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q2_K",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q2_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q2_K_L",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q2_K_L.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q3_K_M",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q3_K_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q3_K_S",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q3_K_S.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q4_0",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q4_0.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q4_1",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q4_1.gguf",
        "file_size": "5.0 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q4_K_M",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q4_K_M.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q4_K_S",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q4_K_S.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q5_K_M",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q5_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q5_K_S",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q5_K_S.gguf",
        "file_size": "5.5 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q6_K",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q6_K.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-Q8_0",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-Q8_0.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-IQ1_M",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-IQ1_M.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-IQ1_S",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-IQ1_S.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-IQ2_M",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-IQ2_M.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-IQ2_XXS",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-IQ2_XXS.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-IQ3_XXS",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-IQ3_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-Q2_K_XL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-Q2_K_XL.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-Q3_K_XL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-Q3_K_XL.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-Q4_K_XL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-Q4_K_XL.gguf",
        "file_size": "4.9 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-Q5_K_XL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-Q5_K_XL.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-Q6_K_XL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-Q6_K_XL.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Ministral-3-8B-Instruct-2512-UD-Q8_K_XL",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/Ministral-3-8B-Instruct-2512-UD-Q8_K_XL.gguf",
        "file_size": "10.3 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "818.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "817.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.6 GB"
      }
    ],
    "readme": "https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF/resolve/main/README.md",
    "description": "A compact 8.4B parameter vision-language model optimized for edge deployment with multilingual support, agentic capabilities, and a 256k context window, available under Apache 2.0 license."
  },
  {
    "model_name": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 10512,
    "createdAt": "2026-02-02T16:43:20.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 25,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_M.gguf",
        "file_size": "2.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ1_S.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_M.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_S.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XS.gguf",
        "file_size": "3.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ2_XXS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XS.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ3_XXS.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_NL",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_NL.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-IQ4_XS.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q2_K_S.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_L.gguf",
        "file_size": "6.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_M.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q3_K_S.gguf",
        "file_size": "5.1 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_0.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_1.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_M.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q4_K_S.gguf",
        "file_size": "6.5 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_M.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q5_K_S.gguf",
        "file_size": "7.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.i1-Q6_K.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.imatrix",
        "path": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking.imatrix.gguf",
        "file_size": "7.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/gemma-3-12b-it-vl-GLM-4.7-Flash-Heretic-Uncensored-Thinking-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quantized files for an uncensored vision language model focused on creative writing."
  },
  {
    "model_name": "Qwen3-4b-Z-Image-Turbo-AbliteratedV1",
    "developer": "BennyDaBall",
    "downloads": 9897,
    "createdAt": "2026-01-28T01:40:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Z-Image-AbliteratedV1.Q2_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q3_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q4_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q6_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.Q8_0",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Z-Image-AbliteratedV1.f16",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/Z-Image-AbliteratedV1.f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Turbo-AbliteratedV1/resolve/main/README.md",
    "description": "An abliterated (censorship-removed) version of the Qwen3-4b-Z-Image-Turbo text encoder for unrestricted image generation."
  },
  {
    "model_name": "LFM2.5-Audio-1.5B-GGUF",
    "developer": "LiquidAI",
    "downloads": 9658,
    "createdAt": "2026-01-06T01:39:14.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "LFM2.5-Audio-1.5B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/LFM2.5-Audio-1.5B-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-Audio-1.5B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/LFM2.5-Audio-1.5B-Q4_0.gguf",
        "file_size": "663.5 MB"
      },
      {
        "model_id": "LFM2.5-Audio-1.5B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/LFM2.5-Audio-1.5B-Q8_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "tokenizer-LFM2.5-Audio-1.5B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/tokenizer-LFM2.5-Audio-1.5B-F16.gguf",
        "file_size": "136.1 MB"
      },
      {
        "model_id": "tokenizer-LFM2.5-Audio-1.5B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/tokenizer-LFM2.5-Audio-1.5B-Q4_0.gguf",
        "file_size": "48.2 MB"
      },
      {
        "model_id": "tokenizer-LFM2.5-Audio-1.5B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/tokenizer-LFM2.5-Audio-1.5B-Q8_0.gguf",
        "file_size": "73.4 MB"
      },
      {
        "model_id": "vocoder-LFM2.5-Audio-1.5B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/vocoder-LFM2.5-Audio-1.5B-F16.gguf",
        "file_size": "369.2 MB"
      },
      {
        "model_id": "vocoder-LFM2.5-Audio-1.5B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/vocoder-LFM2.5-Audio-1.5B-Q4_0.gguf",
        "file_size": "103.9 MB"
      },
      {
        "model_id": "vocoder-LFM2.5-Audio-1.5B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/vocoder-LFM2.5-Audio-1.5B-Q8_0.gguf",
        "file_size": "196.2 MB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-LFM2.5-Audio-1.5B-F16",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/mmproj-LFM2.5-Audio-1.5B-F16.gguf",
        "file_size": "437.6 MB"
      },
      {
        "model_id": "mmproj-LFM2.5-Audio-1.5B-Q4_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/mmproj-LFM2.5-Audio-1.5B-Q4_0.gguf",
        "file_size": "209.3 MB"
      },
      {
        "model_id": "mmproj-LFM2.5-Audio-1.5B-Q8_0",
        "path": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/mmproj-LFM2.5-Audio-1.5B-Q8_0.gguf",
        "file_size": "279.8 MB"
      }
    ],
    "readme": "https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B-GGUF/resolve/main/README.md",
    "description": "LFM2.5-Audio-1.5B is a 1.5B parameter audio model by Liquid AI supporting ASR, TTS, and interleaved audio/text processing via CLI and server runners."
  },
  {
    "model_name": "Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF",
    "developer": "noctrex",
    "downloads": 9308,
    "createdAt": "2025-11-02T00:53:26.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-BF16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-BF16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-F16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_M",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_M.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_S",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_S.gguf",
        "file_size": "1.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_XS",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_XS.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_XXS",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ3_XXS.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ4_NL",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ4_NL.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ4_XS",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-IQ4_XS.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-MXFP4",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-MXFP4.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-Q4_K_M",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-Q4_K_S",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-Q5_K_M",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-Q5_K_S",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-Q6_K",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-4B-Instruct-abliterated-Q8_0",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-4B-Instruct-abliterated-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "800.4 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "797.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-4B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "Quantized 4B vision-language model with recommendations to use latest llama.cpp and F32 mmproj for best results."
  },
  {
    "model_name": "Meta-Llama-3.1-8B-Instruct-abliterated-GGUF",
    "developer": "mlabonne",
    "downloads": 8902,
    "createdAt": "2024-07-24T22:44:19.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 7,
    "quants": [
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q2_K",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q3_K_M",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q4_K_M",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q5_K_M",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q6_K",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.Q8_0",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.Q8_0.gguf",
        "file_size": "8.0 GB"
      },
      {
        "model_id": "meta-llama-3.1-8b-instruct-abliterated.bf16",
        "path": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/meta-llama-3.1-8b-instruct-abliterated.bf16.gguf",
        "file_size": "15.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "Uncensored abliterated version of Meta-Llama-3.1-8B-Instruct."
  },
  {
    "model_name": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF",
    "developer": "mradermacher",
    "downloads": 8604,
    "createdAt": "2026-01-10T02:24:48.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 11,
    "quants": [
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.IQ4_XS",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.IQ4_XS.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q2_K",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q2_K.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_L",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_L.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_M",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_S",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q3_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_M",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_S",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q4_K_S.gguf",
        "file_size": "6.6 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_M",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_S",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q5_K_S.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q6_K",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q8_0",
        "path": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC.Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC-GGUF/resolve/main/README.md",
    "description": "A quantized GGUF version of Mistral-Nemo-2407-12B-Thinking-Claude-Gemini-GPT5.2-Uncensored-HERETIC, an uncensored creative writing model with multiple quantization options (Q2_K to Q8_0) ranging from 4.9GB to 13.1GB."
  },
  {
    "model_name": "GLM-4.7-Flash-Derestricted-i1-GGUF",
    "developer": "mradermacher",
    "downloads": 8390,
    "createdAt": "2026-01-26T06:17:45.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ1_M",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ1_M.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ1_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ1_S.gguf",
        "file_size": "5.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ2_M",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ2_M.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ2_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ2_S.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ2_XS",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ2_XS.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ2_XXS",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ2_XXS.gguf",
        "file_size": "7.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ3_M",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ3_M.gguf",
        "file_size": "12.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ3_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ3_S.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ3_XS",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ3_XS.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ3_XXS",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ3_XXS.gguf",
        "file_size": "10.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-IQ4_XS",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-IQ4_XS.gguf",
        "file_size": "14.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q2_K",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q2_K.gguf",
        "file_size": "10.3 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q2_K_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q2_K_S.gguf",
        "file_size": "9.5 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q3_K_L",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q3_K_L.gguf",
        "file_size": "14.5 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q3_K_M",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q3_K_M.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q3_K_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q3_K_S.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q4_0",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q4_0.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q4_1",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q4_1.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q4_K_M",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q4_K_M.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q4_K_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q4_K_S.gguf",
        "file_size": "15.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q5_K_M",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q5_K_M.gguf",
        "file_size": "19.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q5_K_S",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q5_K_S.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.i1-Q6_K",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.i1-Q6_K.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Derestricted.imatrix",
        "path": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/GLM-4.7-Flash-Derestricted.imatrix.gguf",
        "file_size": "69.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mradermacher/GLM-4.7-Flash-Derestricted-i1-GGUF/resolve/main/README.md",
    "description": "GGUF quantizations of the derestricted GLM-4.7-Flash model in various sizes (6.3-24.7GB)."
  },
  {
    "model_name": "bitnet-b1.58-2B-4T-gguf",
    "developer": "microsoft",
    "downloads": 8144,
    "createdAt": "2025-04-15T04:25:42.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ggml-model-i2_s",
        "path": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf",
        "file_size": "1.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/README.md",
    "description": "Microsoft's 2-billion parameter native 1-bit LLM trained on 4T tokens, achieving comparable performance to full-precision models with significantly better memory, latency, and energy efficiency."
  },
  {
    "model_name": "Lexi-Llama-3-8B-Uncensored-GGUF",
    "developer": "bartowski",
    "downloads": 7267,
    "createdAt": "2024-04-24T03:51:21.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 22,
    "quants": [
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ1_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ1_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ1_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ1_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_XS.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ2_XXS.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_M.gguf",
        "file_size": "3.5 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_XS.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ3_XXS.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ4_NL",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ4_NL.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-IQ4_XS",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-IQ4_XS.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q2_K",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_L",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q3_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q4_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q4_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q5_K_M",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q5_K_S",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q6_K",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "Lexi-Llama-3-8B-Uncensored-Q8_0",
        "path": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored-Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/README.md",
    "description": "Quantized GGUF versions of Lexi-Llama-3-8B-Uncensored model in various compression levels."
  },
  {
    "model_name": "Qwen3-4b-Z-Image-Engineer-V2.5",
    "developer": "BennyDaBall",
    "downloads": 7090,
    "createdAt": "2026-01-17T18:05:32.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Z-Engineer-2.5-Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/Z-Engineer-2.5-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Z-Engineer-2.5-Q4_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/Z-Engineer-2.5-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Z-Engineer-2.5-Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/Z-Engineer-2.5-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Z-Engineer-2.5-Q6_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/Z-Engineer-2.5-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Z-Engineer-2.5-Q8_0",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/Z-Engineer-2.5-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Z-Engineer-2.5",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/Z-Engineer-2.5.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V2.5/resolve/main/README.md",
    "description": "Z-Engineer V2.5 is a 4B parameter model that enhances image generation prompts and serves as a CLIP text encoder for Z-Image-Turbo workflows."
  },
  {
    "model_name": "qwen3-4b-Z-Image-Engineer",
    "developer": "BennyDaBall",
    "downloads": 7077,
    "createdAt": "2025-12-12T15:41:02.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 6,
    "quants": [
      {
        "model_id": "Models/Qwen3-4b-Z-Engineer-V2-Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/Models/Qwen3-4b-Z-Engineer-V2-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Models/Qwen3-4b-Z-Engineer-V2-Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/Models/Qwen3-4b-Z-Engineer-V2-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Models/Qwen3-4b-Z-Engineer-V2-Q6_K",
        "path": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/Models/Qwen3-4b-Z-Engineer-V2-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Models/Qwen3-4b-Z-Engineer-V2-Q8_0",
        "path": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/Models/Qwen3-4b-Z-Engineer-V2-Q8_0.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Engineer-V2",
        "path": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/Qwen3-4b-Z-Engineer-V2.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "qwen3-4b-Z-Image-Engineer_v1-f16",
        "path": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/qwen3-4b-Z-Image-Engineer_v1-f16.gguf",
        "file_size": "7.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/qwen3-4b-Z-Image-Engineer/resolve/main/README.md",
    "description": "Qwen3-4b fine-tuned to transform simple image concepts into detailed, vivid prompts for image generation, available in GGUF/MLX quantizations with ComfyUI integration."
  },
  {
    "model_name": "Qwen3-30B-A3B-GGUF",
    "developer": "Qwen",
    "downloads": 6974,
    "createdAt": "2025-05-05T08:38:52.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Qwen3-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_0.gguf",
        "file_size": "19.6 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q5_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q5_K_M.gguf",
        "file_size": "20.2 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q6_K",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Qwen3-30B-A3B-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF/resolve/main/README.md",
    "description": "A 30B parameter Mixture of Experts language model with switchable thinking modes for complex reasoning tasks."
  },
  {
    "model_name": "Llama-OuteTTS-1.0-1B-GGUF",
    "developer": "OuteAI",
    "downloads": 6735,
    "createdAt": "2025-04-06T19:18:26.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 15,
    "quants": [
      {
        "model_id": "Llama-OuteTTS-1.0-1B-FP16",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-FP16.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q2_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q2_K.gguf",
        "file_size": "564.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_L",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_L.gguf",
        "file_size": "708.6 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_M.gguf",
        "file_size": "668.8 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q3_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q3_K_S.gguf",
        "file_size": "622.0 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_0.gguf",
        "file_size": "745.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_1.gguf",
        "file_size": "803.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_M.gguf",
        "file_size": "780.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q4_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q4_K_S.gguf",
        "file_size": "749.7 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_0.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_1",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_1.gguf",
        "file_size": "919.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_M",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_M.gguf",
        "file_size": "879.3 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q5_K_S",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q5_K_S.gguf",
        "file_size": "861.2 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q6_K",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q6_K.gguf",
        "file_size": "984.5 MB"
      },
      {
        "model_id": "Llama-OuteTTS-1.0-1B-Q8_0",
        "path": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/Llama-OuteTTS-1.0-1B-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B-GGUF/resolve/main/README.md",
    "description": "Multilingual TTS model with one-shot voice cloning."
  },
  {
    "model_name": "MiniMax-M2.1-PRISM",
    "developer": "Ex0bit",
    "downloads": 6483,
    "createdAt": "2026-01-01T21:12:13.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "MiniMax-M2.1-PRISM-IQ2_M",
        "path": "https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM/resolve/main/MiniMax-M2.1-PRISM-IQ2_M.gguf",
        "file_size": "69.7 GB"
      },
      {
        "model_id": "MiniMax-M2.1-PRISM-IQ4_NL",
        "path": "https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM/resolve/main/MiniMax-M2.1-PRISM-IQ4_NL.gguf",
        "file_size": "120.1 GB"
      },
      {
        "model_id": "minimax-m2.1-PRISM-IQ1_S",
        "path": "https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM/resolve/main/minimax-m2.1-PRISM-IQ1_S.gguf",
        "file_size": "43.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM/resolve/main/README.md",
    "description": "An uncensored, abliterated 229B parameter MoE language model for AI safety research, removing refusal behaviors while preserving capabilities."
  },
  {
    "model_name": "Ministral-3-3B-Reasoning-2512-GGUF",
    "developer": "mistralai",
    "downloads": 5061,
    "createdAt": "2025-10-31T08:45:49.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Ministral-3-3B-Reasoning-2512-BF16-mmproj",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512-GGUF/resolve/main/Ministral-3-3B-Reasoning-2512-BF16-mmproj.gguf",
        "file_size": "802.5 MB"
      },
      {
        "model_id": "Ministral-3-3B-Reasoning-2512-BF16",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512-GGUF/resolve/main/Ministral-3-3B-Reasoning-2512-BF16.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "Ministral-3-3B-Reasoning-2512-Q4_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512-GGUF/resolve/main/Ministral-3-3B-Reasoning-2512-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Ministral-3-3B-Reasoning-2512-Q5_K_M",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512-GGUF/resolve/main/Ministral-3-3B-Reasoning-2512-Q5_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Ministral-3-3B-Reasoning-2512-Q8_0",
        "path": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512-GGUF/resolve/main/Ministral-3-3B-Reasoning-2512-Q8_0.gguf",
        "file_size": "3.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512-GGUF/resolve/main/README.md",
    "description": "A small, efficient 3B-parameter vision-language model optimized for edge deployment with reasoning capabilities, available in GGUF quantization."
  },
  {
    "model_name": "LightOnOCR-2-1B-GGUF",
    "developer": "noctrex",
    "downloads": 4551,
    "createdAt": "2026-01-23T18:41:01.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "imatrix",
        "path": "https://huggingface.co/noctrex/LightOnOCR-2-1B-GGUF/resolve/main/imatrix.gguf",
        "file_size": "1.1 MB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/noctrex/LightOnOCR-2-1B-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "790.5 MB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/noctrex/LightOnOCR-2-1B-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "781.4 MB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/noctrex/LightOnOCR-2-1B-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "1.5 GB"
      }
    ],
    "readme": "https://huggingface.co/noctrex/LightOnOCR-2-1B-GGUF/resolve/main/README.md",
    "description": "Quantized versions of LightOnOCR-2-1B with quality recommendations (BF16 > F16 > Q8_0 > Q6_K > Q5_K_M > IQ4_NL > IQ4_XS > Q4_K_M), suggesting F32 for mmproj."
  },
  {
    "model_name": "huihui-ai_QwQ-32B-abliterated-GGUF",
    "developer": "bartowski",
    "downloads": 4421,
    "createdAt": "2025-03-07T21:07:53.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 26,
    "quants": [
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_M.gguf",
        "file_size": "10.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_XS.gguf",
        "file_size": "9.3 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ2_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ2_XXS.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ3_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ3_M.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ3_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ3_XS.gguf",
        "file_size": "12.8 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ3_XXS.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ4_NL",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ4_NL.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-IQ4_XS",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-IQ4_XS.gguf",
        "file_size": "16.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q2_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q2_K.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q2_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q2_K_L.gguf",
        "file_size": "12.2 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_L.gguf",
        "file_size": "16.1 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_M.gguf",
        "file_size": "14.8 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_S.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q3_K_XL.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_0.gguf",
        "file_size": "17.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_1",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_1.gguf",
        "file_size": "19.2 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_K_L.gguf",
        "file_size": "19.0 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_K_M.gguf",
        "file_size": "18.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q4_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q4_K_S.gguf",
        "file_size": "17.5 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q5_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q5_K_L.gguf",
        "file_size": "22.1 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q5_K_M",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q5_K_M.gguf",
        "file_size": "21.7 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q5_K_S",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q5_K_S.gguf",
        "file_size": "21.1 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q6_K",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q6_K.gguf",
        "file_size": "25.0 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q6_K_L",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q6_K_L.gguf",
        "file_size": "25.4 GB"
      },
      {
        "model_id": "huihui-ai_QwQ-32B-abliterated-Q8_0",
        "path": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/huihui-ai_QwQ-32B-abliterated-Q8_0.gguf",
        "file_size": "32.4 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/huihui-ai_QwQ-32B-abliterated-GGUF/resolve/main/README.md",
    "description": "Llama.cpp quantized versions of the uncensored QwQ-32B-abliterated model in various GGUF formats for efficient text generation."
  },
  {
    "model_name": "NeuralDaredevil-8B-abliterated-GGUF",
    "developer": "QuantFactory",
    "downloads": 4386,
    "createdAt": "2024-05-29T16:26:23.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 14,
    "quants": [
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q2_K",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q2_K.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q3_K_L",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q3_K_L.gguf",
        "file_size": "4.0 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q3_K_M",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q3_K_M.gguf",
        "file_size": "3.7 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q3_K_S",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q3_K_S.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_0",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_0.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_1",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_1.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_K_M",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_K_M.gguf",
        "file_size": "4.6 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q4_K_S",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q4_K_S.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_0",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_0.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_1",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_1.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_K_M",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_K_M.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q5_K_S",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q5_K_S.gguf",
        "file_size": "5.2 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q6_K",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q6_K.gguf",
        "file_size": "6.1 GB"
      },
      {
        "model_id": "NeuralDaredevil-8B-abliterated.Q8_0",
        "path": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated.Q8_0.gguf",
        "file_size": "8.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/README.md",
    "description": "Uncensored DPO-fine-tuned 8B model for role-playing, best uncensored 8B on Open LLM Leaderboard."
  },
  {
    "model_name": "Youtu-VL-4B-Instruct-GGUF",
    "developer": "tencent",
    "downloads": 4370,
    "createdAt": "2026-01-23T09:05:10.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Youtu-VL-4B-Instruct-F16",
        "path": "https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF/resolve/main/Youtu-VL-4B-Instruct-F16.gguf",
        "file_size": "9.1 GB"
      },
      {
        "model_id": "Youtu-VL-4B-Instruct-Q8_0",
        "path": "https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF/resolve/main/Youtu-VL-4B-Instruct-Q8_0.gguf",
        "file_size": "4.9 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Youtu-VL-4b-Instruct-BF16",
        "path": "https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF/resolve/main/mmproj-Youtu-VL-4b-Instruct-BF16.gguf",
        "file_size": "852.0 MB"
      }
    ],
    "readme": "https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF/resolve/main/README.md",
    "description": "A 4B-parameter Vision-Language Model using unified autoregressive supervision for vision-centric tasks."
  },
  {
    "model_name": "Huihui-gpt-oss-20b-BF16-abliterated-v2",
    "developer": "huihui-ai",
    "downloads": 4293,
    "createdAt": "2025-09-27T14:11:00.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q3_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q3_K_M.gguf",
        "file_size": "12.0 GB"
      },
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q4_K_M",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-Q4_K_M.gguf",
        "file_size": "14.7 GB"
      },
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-f16",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-f16.gguf",
        "file_size": "39.0 GB"
      },
      {
        "model_id": "GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-q8_0",
        "path": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/GGUF/Huihui-gpt-oss-20b-BF16-abliterated-v2-q8_0.gguf",
        "file_size": "20.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2/resolve/main/README.md",
    "description": "An abliterated 20B language model fine-tuned for text generation with reduced safety filters, potentially generating sensitive content."
  },
  {
    "model_name": "Step3-VL-10B-GGUF",
    "developer": "seanbailey518",
    "downloads": 4262,
    "createdAt": "2026-01-30T15:11:38.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 13,
    "quants": [
      {
        "model_id": "Step3-VL-10B-BF16",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Step3-VL-10B-F16",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Step3-VL-10B-IQ3_XS",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-IQ3_XS.gguf",
        "file_size": "3.4 GB"
      },
      {
        "model_id": "Step3-VL-10B-IQ4_NL",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Step3-VL-10B-IQ4_XS",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q3_K_L",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q3_K_L.gguf",
        "file_size": "4.1 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q3_K_M",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q3_K_M.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q4_K_M",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q4_K_S",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q5_K_M",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q5_K_S",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q6_K",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Step3-VL-10B-Q8_0",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/Step3-VL-10B-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-Step3-VL-10b-F16",
        "path": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/mmproj-Step3-VL-10b-F16.gguf",
        "file_size": "3.7 GB"
      }
    ],
    "readme": "https://huggingface.co/seanbailey518/Step3-VL-10B-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of Step3-VL-10B, a 10B parameter vision-language model for multimodal tasks."
  },
  {
    "model_name": "Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF",
    "developer": "noctrex",
    "downloads": 4095,
    "createdAt": "2025-11-02T01:15:03.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-BF16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-BF16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-F16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-F16.gguf",
        "file_size": "15.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-IQ4_NL",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-IQ4_NL.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-IQ4_XS",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-IQ4_XS.gguf",
        "file_size": "4.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-Q4_K_M",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-Q4_K_S",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-Q4_K_S.gguf",
        "file_size": "4.5 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-Q5_K_M",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-Q5_K_M.gguf",
        "file_size": "5.4 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-Q5_K_S",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-Q5_K_S.gguf",
        "file_size": "5.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-Q6_K",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-Q6_K.gguf",
        "file_size": "6.3 GB"
      },
      {
        "model_id": "Huihui-Qwen3-VL-8B-Instruct-abliterated-Q8_0",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/Huihui-Qwen3-VL-8B-Instruct-abliterated-Q8_0.gguf",
        "file_size": "8.1 GB"
      }
    ],
    "num_mmproj": 3,
    "mmproj_models": [
      {
        "model_id": "mmproj-BF16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/mmproj-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mmproj-F16",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/mmproj-F16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "mmproj-F32",
        "path": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/mmproj-F32.gguf",
        "file_size": "2.1 GB"
      }
    ],
    "readme": "https://huggingface.co/noctrex/Huihui-Qwen3-VL-8B-Instruct-abliterated-GGUF/resolve/main/README.md",
    "description": "Quantized version of Huihui-Qwen3-VL-8B-Instruct-abliterated for image-text-to-text tasks, requires llama.cpp."
  },
  {
    "model_name": "Qwen3-4b-Z-Image-Engineer-V4",
    "developer": "BennyDaBall",
    "downloads": 3697,
    "createdAt": "2026-02-04T21:07:40.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-F16",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-F16.gguf",
        "file_size": "7.5 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q2_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q2_K.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q3_K_L",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q3_K_L.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q3_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q3_K_M.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q4_K_M.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q4_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q4_K_S.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q5_K_M.gguf",
        "file_size": "2.7 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q5_K_S",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q5_K_S.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q6_K",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q6_K.gguf",
        "file_size": "3.1 GB"
      },
      {
        "model_id": "Qwen3-4b-Z-Image-Engineer-V4-Q8_0",
        "path": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/Qwen3-4b-Z-Image-Engineer-V4-Q8_0.gguf",
        "file_size": "4.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/Qwen3-4b-Z-Image-Engineer-V4/resolve/main/README.md",
    "description": "A 4B parameter fine-tuned Qwen 3 model that expands simple text concepts into detailed, cinematic AI image generation prompts."
  },
  {
    "model_name": "zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF",
    "developer": "bartowski",
    "downloads": 3665,
    "createdAt": "2026-02-07T00:16:22.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 27,
    "quants": [
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ2_M",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ2_M.gguf",
        "file_size": "7.6 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ2_S",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ2_S.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ2_XS",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ2_XS.gguf",
        "file_size": "6.7 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ3_M",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ3_M.gguf",
        "file_size": "9.9 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ3_XS",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ3_XS.gguf",
        "file_size": "9.2 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ3_XXS",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ3_XXS.gguf",
        "file_size": "8.6 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ4_NL",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ4_NL.gguf",
        "file_size": "12.5 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-IQ4_XS",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-IQ4_XS.gguf",
        "file_size": "11.9 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q2_K",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q2_K.gguf",
        "file_size": "8.3 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q2_K_L",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q2_K_L.gguf",
        "file_size": "8.9 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_L",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_L.gguf",
        "file_size": "11.5 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_M",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_M.gguf",
        "file_size": "10.7 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_S",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_S.gguf",
        "file_size": "9.7 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_XL",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q3_K_XL.gguf",
        "file_size": "12.1 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_0",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_0.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_1",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_1.gguf",
        "file_size": "13.9 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_K_L",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_K_L.gguf",
        "file_size": "13.8 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_K_M",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_K_M.gguf",
        "file_size": "13.3 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_K_S",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q4_K_S.gguf",
        "file_size": "12.6 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q5_K_L",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q5_K_L.gguf",
        "file_size": "16.0 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q5_K_M",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q5_K_M.gguf",
        "file_size": "15.6 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q5_K_S",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q5_K_S.gguf",
        "file_size": "15.2 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q6_K",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q6_K.gguf",
        "file_size": "18.0 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q6_K_L",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q6_K_L.gguf",
        "file_size": "18.3 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-Q8_0",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-Q8_0.gguf",
        "file_size": "23.3 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-bf16",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-bf16.gguf",
        "file_size": "43.9 GB"
      },
      {
        "model_id": "zerofata_MS3.2-PaintedFantasy-v4-24B-imatrix",
        "path": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/zerofata_MS3.2-PaintedFantasy-v4-24B-imatrix.gguf",
        "file_size": "9.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/bartowski/zerofata_MS3.2-PaintedFantasy-v4-24B-GGUF/resolve/main/README.md",
    "description": "A 24B parameter text generation model quantized in multiple formats (Q8_0 to IQ2_XS) using llama.cpp imatrix for efficient inference across various hardware setups."
  },
  {
    "model_name": "GLM-4.7-Flash-PRISM",
    "developer": "Ex0bit",
    "downloads": 3563,
    "createdAt": "2026-01-20T04:52:57.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-IQ4_NL",
        "path": "https://huggingface.co/Ex0bit/GLM-4.7-Flash-PRISM/resolve/main/GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-IQ4_NL.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-Q3_K_M",
        "path": "https://huggingface.co/Ex0bit/GLM-4.7-Flash-PRISM/resolve/main/GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-Q3_K_M.gguf",
        "file_size": "13.4 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-Q4_K_M",
        "path": "https://huggingface.co/Ex0bit/GLM-4.7-Flash-PRISM/resolve/main/GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-Q4_K_M.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-Q8_0",
        "path": "https://huggingface.co/Ex0bit/GLM-4.7-Flash-PRISM/resolve/main/GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-Q8_0.gguf",
        "file_size": "29.7 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-bf16",
        "path": "https://huggingface.co/Ex0bit/GLM-4.7-Flash-PRISM/resolve/main/GLM-4.7-Flash-PRISM-GGUFs/GLM-4.7-Flash-PRISM-bf16.gguf",
        "file_size": "55.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ex0bit/GLM-4.7-Flash-PRISM/resolve/main/README.md",
    "description": "A de-filtered, over-refusal-removed version of GLM-4.7-Flash with 30B-A3B MoE architecture and 128K context."
  },
  {
    "model_name": "GLM-4.7-Flash-Grande-Heretic-UNCENSORED-42B-A3B-GGUF",
    "developer": "DavidAU",
    "downloads": 3535,
    "createdAt": "2026-01-23T23:59:52.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "GLM-4.7-30B-A3B-20-2-Heretic-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Grande-Heretic-UNCENSORED-42B-A3B-GGUF/resolve/main/GLM-4.7-30B-A3B-20-2-Heretic-30B-A3B-Q4_K_M.gguf",
        "file_size": "24.1 GB"
      },
      {
        "model_id": "GLM-4.7-30B-A3B-20-2-Heretic-30B-A3B-Q5_1",
        "path": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Grande-Heretic-UNCENSORED-42B-A3B-GGUF/resolve/main/GLM-4.7-30B-A3B-20-2-Heretic-30B-A3B-Q5_1.gguf",
        "file_size": "29.8 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/DavidAU/GLM-4.7-Flash-Grande-Heretic-UNCENSORED-42B-A3B-GGUF/resolve/main/README.md",
    "description": "Experimental 42B uncensored creative writing model fine-tuned from GLM-4.7-Flash with enhanced storytelling capabilities."
  },
  {
    "model_name": "Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM",
    "developer": "Ex0bit",
    "downloads": 3259,
    "createdAt": "2025-12-18T14:38:02.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-BF16",
        "path": "https://huggingface.co/Ex0bit/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM/resolve/main/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-BF16.gguf",
        "file_size": "58.8 GB"
      },
      {
        "model_id": "Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-IQ4_XS",
        "path": "https://huggingface.co/Ex0bit/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM/resolve/main/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-IQ4_XS.gguf",
        "file_size": "17.0 GB"
      },
      {
        "model_id": "Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-Q6_K",
        "path": "https://huggingface.co/Ex0bit/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM/resolve/main/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-Q6_K.gguf",
        "file_size": "31.2 GB"
      },
      {
        "model_id": "Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-Q8_0",
        "path": "https://huggingface.co/Ex0bit/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM/resolve/main/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM-Q8_0.gguf",
        "file_size": "31.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ex0bit/Elbaz-NVIDIA-Nemotron-3-Nano-30B-A3B-PRISM/resolve/main/README.md",
    "description": "An abliterated (uncensored) 31.58B parameter hybrid Mamba-2/MoE/Attention language model derived from NVIDIA's Nemotron-3-Nano-30B with refusal mechanisms removed via PRISM methodology."
  },
  {
    "model_name": "ruvltra-claude-code",
    "developer": "ruv",
    "downloads": 3013,
    "createdAt": "2026-01-20T20:45:06.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "ruvltra-claude-code-0.5b-q4_k_m",
        "path": "https://huggingface.co/ruv/ruvltra-claude-code/resolve/main/ruvltra-claude-code-0.5b-q4_k_m.gguf",
        "file_size": "379.4 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ruv/ruvltra-claude-code/resolve/main/README.md",
    "description": "A self-learning 0.5B parameter code generation model optimized for Claude Code with SONA adaptive intelligence and swarm coordination capabilities."
  },
  {
    "model_name": "Qwen3-VL-2B-Thinking-GGUF",
    "developer": "Qwen",
    "downloads": 2607,
    "createdAt": "2025-10-31T03:17:15.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Qwen3VL-2B-Thinking-F16",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF/resolve/main/Qwen3VL-2B-Thinking-F16.gguf",
        "file_size": "3.2 GB"
      },
      {
        "model_id": "Qwen3VL-2B-Thinking-Q4_K_M",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF/resolve/main/Qwen3VL-2B-Thinking-Q4_K_M.gguf",
        "file_size": "1.0 GB"
      },
      {
        "model_id": "Qwen3VL-2B-Thinking-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF/resolve/main/Qwen3VL-2B-Thinking-Q8_0.gguf",
        "file_size": "1.7 GB"
      }
    ],
    "num_mmproj": 2,
    "mmproj_models": [
      {
        "model_id": "mmproj-Qwen3VL-2B-Thinking-F16",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF/resolve/main/mmproj-Qwen3VL-2B-Thinking-F16.gguf",
        "file_size": "781.4 MB"
      },
      {
        "model_id": "mmproj-Qwen3VL-2B-Thinking-Q8_0",
        "path": "https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF/resolve/main/mmproj-Qwen3VL-2B-Thinking-Q8_0.gguf",
        "file_size": "424.4 MB"
      }
    ],
    "readme": "https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF/resolve/main/README.md",
    "description": "GGUF-quantized Qwen3-VL-2B-Thinking multimodal vision-language model for local inference via llama.cpp and compatible tools."
  },
  {
    "model_name": "Strand-Rust-Coder-14B-v1-GGUF",
    "developer": "Fortytwo-Network",
    "downloads": 2573,
    "createdAt": "2025-10-13T14:49:53.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-BF16",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-BF16.gguf",
        "file_size": "27.5 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q4_K_M",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q4_K_M.gguf",
        "file_size": "8.4 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q5_K_M",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q5_K_M.gguf",
        "file_size": "9.8 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q6_K",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q6_K.gguf",
        "file_size": "11.3 GB"
      },
      {
        "model_id": "Fortytwo_Strand-Rust-Coder-14B-v1-Q8_0",
        "path": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/Fortytwo_Strand-Rust-Coder-14B-v1-Q8_0.gguf",
        "file_size": "14.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF/resolve/main/README.md",
    "description": "**A Rust-specialized 14B model outperforming GPT-5/Claude on Rust benchmarks via decentralized swarm training.**"
  },
  {
    "model_name": "Mistral-Helcyon-Mercury-12b-v3.0-GGUF",
    "developer": "XeyonAI",
    "downloads": 2456,
    "createdAt": "2026-02-01T23:41:26.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "helcyon_mercury_v3.0-Q3_K_M",
        "path": "https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF/resolve/main/helcyon_mercury_v3.0-Q3_K_M.gguf",
        "file_size": "5.7 GB"
      },
      {
        "model_id": "helcyon_mercury_v3.0-Q4_K_M",
        "path": "https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF/resolve/main/helcyon_mercury_v3.0-Q4_K_M.gguf",
        "file_size": "7.0 GB"
      },
      {
        "model_id": "helcyon_mercury_v3.0-Q5_K_M",
        "path": "https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF/resolve/main/helcyon_mercury_v3.0-Q5_K_M.gguf",
        "file_size": "8.1 GB"
      },
      {
        "model_id": "helcyon_mercury_v3.0-Q6_K",
        "path": "https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF/resolve/main/helcyon_mercury_v3.0-Q6_K.gguf",
        "file_size": "9.4 GB"
      },
      {
        "model_id": "helcyon_mercury_v3.0-Q8_0",
        "path": "https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF/resolve/main/helcyon_mercury_v3.0-Q8_0.gguf",
        "file_size": "12.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF/resolve/main/README.md",
    "description": "Helcyon-Mercury-12B is a Mistral Nemo-based conversational AI focused on natural dialogue, emotional intelligence, and immersive roleplay."
  },
  {
    "model_name": "Qwen3-Coder-Next-MXFP4_MOE-GGUF",
    "developer": "noctrex",
    "downloads": 2326,
    "createdAt": "2026-02-03T20:12:20.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Qwen3-Coder-Next-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF/resolve/main/Qwen3-Coder-Next-MXFP4_MOE.gguf",
        "file_size": "40.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF/resolve/main/README.md",
    "description": "MXFP4 quantized version of Qwen3-Coder-Next with recommended sampling parameters (temperature=1.0, top_p=0.95, top_k=40)."
  },
  {
    "model_name": "GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill",
    "developer": "noctrex",
    "downloads": 2308,
    "createdAt": "2026-02-02T22:05:03.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-Claude-4.5-Opus-MXFP4_MOE",
        "path": "https://huggingface.co/noctrex/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill/resolve/main/GLM-4.7-Flash-Claude-4.5-Opus-MXFP4_MOE.gguf",
        "file_size": "15.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Claude-4.5-Opus-i1-MXFP4_MOE_XL-exp",
        "path": "https://huggingface.co/noctrex/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill/resolve/main/GLM-4.7-Flash-Claude-4.5-Opus-i1-MXFP4_MOE_XL-exp.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "imatrix",
        "path": "https://huggingface.co/noctrex/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill/resolve/main/imatrix.gguf",
        "file_size": "69.1 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill/resolve/main/README.md",
    "description": "Experimental importance-aware MXFP4_MOE quantization of a reasoning model, optimized for coding, with no benchmarks yet."
  },
  {
    "model_name": "gemma-3-12b-it-uncensored-GGUF",
    "developer": "Andycurrent",
    "downloads": 2192,
    "createdAt": "2026-02-04T13:06:47.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "gemma-3-12b-it-uncensored_F16",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_F16.gguf",
        "file_size": "21.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_Q2_k",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_Q2_k.gguf",
        "file_size": "4.4 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_Q3_k_m",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_Q3_k_m.gguf",
        "file_size": "5.6 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_Q4_k_m",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_Q4_k_m.gguf",
        "file_size": "6.8 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_Q5_k_m",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_Q5_k_m.gguf",
        "file_size": "7.9 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_Q6_k",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_Q6_k.gguf",
        "file_size": "9.0 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_Q8_0",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_Q8_0.gguf",
        "file_size": "11.7 GB"
      },
      {
        "model_id": "gemma-3-12b-it-uncensored_mmproj-f16",
        "path": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/gemma-3-12b-it-uncensored_mmproj-f16.gguf",
        "file_size": "814.6 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Andycurrent/gemma-3-12b-it-uncensored-GGUF/resolve/main/README.md",
    "description": "Gemma 3  12B IT Uncensored is an instruction-tuned 12B parameter model based on Google's Gemma 3, designed for local and research use with minimal safety constraints, also available as a Vision-Language Model (VLM) variant."
  },
  {
    "model_name": "gpt-oss-nano",
    "developer": "squ11z1",
    "downloads": 2173,
    "createdAt": "2026-01-27T21:08:54.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "gpt-oss-9b-bf16",
        "path": "https://huggingface.co/squ11z1/gpt-oss-nano/resolve/main/gpt-oss-9b-bf16.gguf",
        "file_size": "16.7 GB"
      },
      {
        "model_id": "gpt-oss-9b-q4_k_m",
        "path": "https://huggingface.co/squ11z1/gpt-oss-nano/resolve/main/gpt-oss-9b-q4_k_m.gguf",
        "file_size": "6.4 GB"
      },
      {
        "model_id": "gpt-oss-9b-q8_0",
        "path": "https://huggingface.co/squ11z1/gpt-oss-nano/resolve/main/gpt-oss-9b-q8_0.gguf",
        "file_size": "8.9 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/squ11z1/gpt-oss-nano/resolve/main/README.md",
    "description": "**GPT-OSS-Nano is a 9B parameter Mixture of Experts model fine-tuned for step-by-step chain-of-thought reasoning with 128K context.**"
  },
  {
    "model_name": "Step-3.5-Flash-PRISM",
    "developer": "Ex0bit",
    "downloads": 2070,
    "createdAt": "2026-02-07T12:41:52.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "Step-3.5-Flash-PRISM-LITE-IQ2_M",
        "path": "https://huggingface.co/Ex0bit/Step-3.5-Flash-PRISM/resolve/main/Step-3.5-Flash-PRISM-LITE-IQ2_M.gguf",
        "file_size": "59.6 GB"
      },
      {
        "model_id": "Step-3.5-Flash-PRISM-LITE-IQ4_NL",
        "path": "https://huggingface.co/Ex0bit/Step-3.5-Flash-PRISM/resolve/main/Step-3.5-Flash-PRISM-LITE-IQ4_NL.gguf",
        "file_size": "103.9 GB"
      },
      {
        "model_id": "Step-3.5-Flash-PRISM-LITE-Q3_K_L",
        "path": "https://huggingface.co/Ex0bit/Step-3.5-Flash-PRISM/resolve/main/Step-3.5-Flash-PRISM-LITE-Q3_K_L.gguf",
        "file_size": "95.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Ex0bit/Step-3.5-Flash-PRISM/resolve/main/README.md",
    "description": "An ablated, unrestricted version of StepFun's Step 3.5 Flash MoE model designed to suppress refusal behaviors while preserving strong reasoning and coding abilities."
  },
  {
    "model_name": "Unslopper-GGUF",
    "developer": "N8Programs",
    "downloads": 1987,
    "createdAt": "2026-01-15T04:40:25.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 5,
    "quants": [
      {
        "model_id": "Unslopper-30B-A3B-Q4_K_M",
        "path": "https://huggingface.co/N8Programs/Unslopper-GGUF/resolve/main/Unslopper-30B-A3B-Q4_K_M.gguf",
        "file_size": "17.3 GB"
      },
      {
        "model_id": "Unslopper-30B-A3B-Q6_K",
        "path": "https://huggingface.co/N8Programs/Unslopper-GGUF/resolve/main/Unslopper-30B-A3B-Q6_K.gguf",
        "file_size": "23.4 GB"
      },
      {
        "model_id": "Unslopper-30B-A3B-Q8_0",
        "path": "https://huggingface.co/N8Programs/Unslopper-GGUF/resolve/main/Unslopper-30B-A3B-Q8_0.gguf",
        "file_size": "30.3 GB"
      },
      {
        "model_id": "Unslopper-30B-A3B-bf16-bf16",
        "path": "https://huggingface.co/N8Programs/Unslopper-GGUF/resolve/main/Unslopper-30B-A3B-bf16-bf16.gguf",
        "file_size": "56.9 GB"
      },
      {
        "model_id": "Unslopper-30B-A3B-bf16-q3_k_m",
        "path": "https://huggingface.co/N8Programs/Unslopper-GGUF/resolve/main/Unslopper-30B-A3B-bf16-q3_k_m.gguf",
        "file_size": "13.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/N8Programs/Unslopper-GGUF/resolve/main/README.md",
    "description": "A LoRA-fine-tuned Qwen3-VL model that rewrites AI-generated text to sound more human-like using reverse distillation from human literary passages."
  },
  {
    "model_name": "GLM-4.7-Flash-Uncensored-HauhauCS-Balanced",
    "developer": "HauhauCS",
    "downloads": 1764,
    "createdAt": "2026-01-24T15:47:44.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 4,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-FP16",
        "path": "https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced/resolve/main/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-FP16.gguf",
        "file_size": "55.8 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-Q4_K_M",
        "path": "https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced/resolve/main/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-Q4_K_M.gguf",
        "file_size": "16.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-Q6_K",
        "path": "https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced/resolve/main/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-Q6_K.gguf",
        "file_size": "22.9 GB"
      },
      {
        "model_id": "GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-Q8_0",
        "path": "https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced/resolve/main/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced-Q8_0.gguf",
        "file_size": "29.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced/resolve/main/README.md",
    "description": "An uncensored 30B-A3B MoE model based on GLM-4.7 Flash, optimized for agentic coding with balanced refusal settings."
  },
  {
    "model_name": "MiniCPM-V-4-gguf",
    "developer": "openbmb",
    "downloads": 1483,
    "createdAt": "2025-07-12T09:45:29.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 10,
    "quants": [
      {
        "model_id": "ggml-model-Q4_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_0.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q4_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_1.gguf",
        "file_size": "2.1 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_M.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "ggml-model-Q4_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q4_K_S.gguf",
        "file_size": "1.9 GB"
      },
      {
        "model_id": "ggml-model-Q5_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_0.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q5_1",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_1.gguf",
        "file_size": "2.5 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_M",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_M.gguf",
        "file_size": "2.4 GB"
      },
      {
        "model_id": "ggml-model-Q5_K_S",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q5_K_S.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "ggml-model-Q6_K",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q6_K.gguf",
        "file_size": "2.8 GB"
      },
      {
        "model_id": "ggml-model-Q8_0",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/ggml-model-Q8_0.gguf",
        "file_size": "3.6 GB"
      }
    ],
    "num_mmproj": 1,
    "mmproj_models": [
      {
        "model_id": "mmproj-model-f16",
        "path": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/mmproj-model-f16.gguf",
        "file_size": "914.4 MB"
      }
    ],
    "readme": "https://huggingface.co/openbmb/MiniCPM-V-4-gguf/resolve/main/README.md",
    "description": "MiniCPM-V 4.0 is an efficient 4.1B parameter multimodal model for single/multi-image and video understanding that runs on phones like iPhone 16 Pro Max."
  },
  {
    "model_name": "GPT-OSS-20B-Uncensored-HauhauCS-Aggressive",
    "developer": "HauhauCS",
    "downloads": 1016,
    "createdAt": "2026-01-24T03:14:25.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GPT-OSS-20B-Uncensored-HauhauCS-MXFP4-Aggressive",
        "path": "https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive/resolve/main/GPT-OSS-20B-Uncensored-HauhauCS-MXFP4-Aggressive.gguf",
        "file_size": "11.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive/resolve/main/README.md",
    "description": "An uncensored, abliterated 20B language model tuned for maximum responsiveness with fewer refusals."
  },
  {
    "model_name": "GLM-4.7-Flash-i1-MXFP4_MOE_XL-exp-GGUF",
    "developer": "noctrex",
    "downloads": 982,
    "createdAt": "2026-01-27T09:04:32.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "GLM-4.7-Flash-i1-MXFP4_MOE_XL-exp",
        "path": "https://huggingface.co/noctrex/GLM-4.7-Flash-i1-MXFP4_MOE_XL-exp-GGUF/resolve/main/GLM-4.7-Flash-i1-MXFP4_MOE_XL-exp.gguf",
        "file_size": "17.5 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/noctrex/GLM-4.7-Flash-i1-MXFP4_MOE_XL-exp-GGUF/resolve/main/README.md",
    "description": "Experimental importance-aware MXFP4_MOE quantization of GLM-4.7-Flash that dynamically allocates precision (BF16/Q8_0/MXFP4) based on tensor importance scores."
  },
  {
    "model_name": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF",
    "developer": "Andycurrent",
    "downloads": 850,
    "createdAt": "2026-02-09T10:18:28.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 8,
    "quants": [
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_F16",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_F16.gguf",
        "file_size": "7.2 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q2_k",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q2_k.gguf",
        "file_size": "1.6 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q3_k_m",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q3_k_m.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q4_k_m",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q4_k_m.gguf",
        "file_size": "2.3 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q5_k_m",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q5_k_m.gguf",
        "file_size": "2.6 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q6_k",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q6_k.gguf",
        "file_size": "3.0 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q8_0",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_Q8_0.gguf",
        "file_size": "3.8 GB"
      },
      {
        "model_id": "Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_mmproj_f16",
        "path": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_mmproj_f16.gguf",
        "file_size": "811.8 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Andycurrent/Gemma-3-4B-VL-it-Gemini-Pro-Heretic-Uncensored-Thinking_GGUF/resolve/main/README.md",
    "description": "A 4B-parameter, uncensored, vision-language instruction-tuned model in GGUF format for local inference."
  },
  {
    "model_name": "BadApple-LLaMA-nano",
    "developer": "nyuuzyou",
    "downloads": 794,
    "createdAt": "2026-02-06T19:46:17.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "badapple",
        "path": "https://huggingface.co/nyuuzyou/BadApple-LLaMA-nano/resolve/main/badapple.gguf",
        "file_size": "13.5 MB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/nyuuzyou/BadApple-LLaMA-nano/resolve/main/README.md",
    "description": "A 3.5M-parameter LLaMA model that generates ASCII art frames of the Bad Apple!! music video from frame numbers with 99.78% accuracy."
  },
  {
    "model_name": "LFM2.5-1.2B-Z-Image-Engineer-V4",
    "developer": "BennyDaBall",
    "downloads": 662,
    "createdAt": "2026-02-04T22:10:22.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 9,
    "quants": [
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-F16",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-F16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q3_K_L",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q3_K_L.gguf",
        "file_size": "606.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q3_K_M",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q3_K_M.gguf",
        "file_size": "572.5 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q4_K_M",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q4_K_M.gguf",
        "file_size": "697.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q4_K_S",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q4_K_S.gguf",
        "file_size": "668.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q5_K_M",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q5_K_M.gguf",
        "file_size": "804.3 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q5_K_S",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q5_K_S.gguf",
        "file_size": "787.0 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q6_K",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q6_K.gguf",
        "file_size": "918.2 MB"
      },
      {
        "model_id": "LFM2.5-1.2B-Z-Image-Engineer-V4-Q8_0",
        "path": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/LFM2.5-1.2B-Z-Image-Engineer-V4-Q8_0.gguf",
        "file_size": "1.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/BennyDaBall/LFM2.5-1.2B-Z-Image-Engineer-V4/resolve/main/README.md",
    "description": "Fine-tuned 1.2B model for high-speed AI image prompt engineering."
  },
  {
    "model_name": "flux2-klein-4B-uncensored-text-encoder",
    "developer": "Cordux",
    "downloads": 614,
    "createdAt": "2026-01-27T19:26:51.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "qwen3-4b-abl-q4_0",
        "path": "https://huggingface.co/Cordux/flux2-klein-4B-uncensored-text-encoder/resolve/main/qwen3-4b-abl-q4_0.gguf",
        "file_size": "2.2 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Cordux/flux2-klein-4B-uncensored-text-encoder/resolve/main/README.md",
    "description": "An uncensored Qwen3-4B text encoder (GGUF Q4_0) for Flux2 Klein that removes safety filters to allow NSFW content generation."
  },
  {
    "model_name": "Step-3.5-Flash-GGUF",
    "developer": "ggml-org",
    "downloads": 545,
    "createdAt": "2026-02-03T06:18:11.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "Step-3.5-Flash-Q4_K",
        "path": "https://huggingface.co/ggml-org/Step-3.5-Flash-GGUF/resolve/main/Step-3.5-Flash-Q4_K.gguf",
        "file_size": "110.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ggml-org/Step-3.5-Flash-GGUF/resolve/main/README.md",
    "description": "StepFun AI's Step-3.5-Flash model with llama.cpp inference support (PR #19283)."
  },
  {
    "model_name": "MechaEpstein-8000-GGUF",
    "developer": "ortegaalfredo",
    "downloads": 398,
    "createdAt": "2026-02-08T08:11:06.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "MechaEpstein-8000M-Q4_K_M",
        "path": "https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF/resolve/main/MechaEpstein-8000M-Q4_K_M.gguf",
        "file_size": "4.7 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF/resolve/main/README.md",
    "description": "This model is licensed under Apache 2.0."
  },
  {
    "model_name": "Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF",
    "developer": "AlicanKiraz0",
    "downloads": 379,
    "createdAt": "2025-01-21T03:41:56.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "baronllm-llama3.1-v1-q6_k",
        "path": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/baronllm-llama3.1-v1-q6_k.gguf",
        "file_size": "6.1 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF/resolve/main/README.md",
    "description": "BaronLLM is a Llama-3.1-8B-Instruct model fine-tuned for offensive cybersecurity research and adversarial simulation with safety constraints."
  },
  {
    "model_name": "Regency-Aghast-27b-GGUF",
    "developer": "FPHam",
    "downloads": 11,
    "createdAt": "2026-02-10T00:26:58.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 2,
    "quants": [
      {
        "model_id": "Regency-Aghast-27b-Q4_K_M_o",
        "path": "https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF/resolve/main/Regency-Aghast-27b-Q4_K_M_o.gguf",
        "file_size": "15.4 GB"
      },
      {
        "model_id": "Regency-Aghast-27b-Q6_K_o",
        "path": "https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF/resolve/main/Regency-Aghast-27b-Q6_K_o.gguf",
        "file_size": "20.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF/resolve/main/README.md",
    "description": "A Gemma-27b persona model that roleplays as a self-aware AI living in Regency-era Britain with a Jane Austen-inspired personality."
  },
  {
    "model_name": "MioTTS-GGUF",
    "developer": "Aratako",
    "downloads": 6,
    "createdAt": "2026-02-09T09:36:49.000Z",
    "library_name": "gguf",
    "tools": false,
    "num_quants": 24,
    "quants": [
      {
        "model_id": "MioTTS-0.1B-BF16",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.1B-BF16.gguf",
        "file_size": "221.4 MB"
      },
      {
        "model_id": "MioTTS-0.1B-Q4_K_M",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.1B-Q4_K_M.gguf",
        "file_size": "75.9 MB"
      },
      {
        "model_id": "MioTTS-0.1B-Q6_K",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.1B-Q6_K.gguf",
        "file_size": "92.8 MB"
      },
      {
        "model_id": "MioTTS-0.1B-Q8_0",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.1B-Q8_0.gguf",
        "file_size": "119.2 MB"
      },
      {
        "model_id": "MioTTS-0.4B-BF16",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.4B-BF16.gguf",
        "file_size": "701.8 MB"
      },
      {
        "model_id": "MioTTS-0.4B-Q4_K_M",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.4B-Q4_K_M.gguf",
        "file_size": "228.4 MB"
      },
      {
        "model_id": "MioTTS-0.4B-Q6_K",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.4B-Q6_K.gguf",
        "file_size": "289.5 MB"
      },
      {
        "model_id": "MioTTS-0.4B-Q8_0",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.4B-Q8_0.gguf",
        "file_size": "374.1 MB"
      },
      {
        "model_id": "MioTTS-0.6B-BF16",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.6B-BF16.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "MioTTS-0.6B-Q4_K_M",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.6B-Q4_K_M.gguf",
        "file_size": "388.6 MB"
      },
      {
        "model_id": "MioTTS-0.6B-Q6_K",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.6B-Q6_K.gguf",
        "file_size": "482.5 MB"
      },
      {
        "model_id": "MioTTS-0.6B-Q8_0",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-0.6B-Q8_0.gguf",
        "file_size": "623.1 MB"
      },
      {
        "model_id": "MioTTS-1.2B-BF16",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.2B-BF16.gguf",
        "file_size": "2.2 GB"
      },
      {
        "model_id": "MioTTS-1.2B-Q4_K_M",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.2B-Q4_K_M.gguf",
        "file_size": "716.1 MB"
      },
      {
        "model_id": "MioTTS-1.2B-Q6_K",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.2B-Q6_K.gguf",
        "file_size": "937.4 MB"
      },
      {
        "model_id": "MioTTS-1.2B-Q8_0",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.2B-Q8_0.gguf",
        "file_size": "1.2 GB"
      },
      {
        "model_id": "MioTTS-1.7B-BF16",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.7B-BF16.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "MioTTS-1.7B-Q4_K_M",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.7B-Q4_K_M.gguf",
        "file_size": "1.1 GB"
      },
      {
        "model_id": "MioTTS-1.7B-Q6_K",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.7B-Q6_K.gguf",
        "file_size": "1.3 GB"
      },
      {
        "model_id": "MioTTS-1.7B-Q8_0",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-1.7B-Q8_0.gguf",
        "file_size": "1.7 GB"
      },
      {
        "model_id": "MioTTS-2.6B-BF16",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-2.6B-BF16.gguf",
        "file_size": "4.8 GB"
      },
      {
        "model_id": "MioTTS-2.6B-Q4_K_M",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-2.6B-Q4_K_M.gguf",
        "file_size": "1.5 GB"
      },
      {
        "model_id": "MioTTS-2.6B-Q6_K",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-2.6B-Q6_K.gguf",
        "file_size": "2.0 GB"
      },
      {
        "model_id": "MioTTS-2.6B-Q8_0",
        "path": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/MioTTS-2.6B-Q8_0.gguf",
        "file_size": "2.6 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Aratako/MioTTS-GGUF/resolve/main/README.md",
    "description": "GGUF quantized versions of MioTTS, a lightweight high-speed text-to-speech model for English and Japanese, available in multiple sizes (0.1B2.6B) and quantization levels."
  },
  {
    "model_name": "HY-1.8B-2Bit-GGUF",
    "developer": "AngelSlim",
    "downloads": 0,
    "createdAt": "2026-02-04T03:26:53.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 3,
    "quants": [
      {
        "model_id": "hunyuan-fp16-qdq",
        "path": "https://huggingface.co/AngelSlim/HY-1.8B-2Bit-GGUF/resolve/main/hunyuan-fp16-qdq.gguf",
        "file_size": "3.3 GB"
      },
      {
        "model_id": "hunyuan-q2_0",
        "path": "https://huggingface.co/AngelSlim/HY-1.8B-2Bit-GGUF/resolve/main/hunyuan-q2_0.gguf",
        "file_size": "572.7 MB"
      },
      {
        "model_id": "hunyuan-q4_0",
        "path": "https://huggingface.co/AngelSlim/HY-1.8B-2Bit-GGUF/resolve/main/hunyuan-q4_0.gguf",
        "file_size": "1.0 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/AngelSlim/HY-1.8B-2Bit-GGUF/resolve/main/README.md",
    "description": "Tencent's AngelSlim releases HY-1.8B-2Bit, an ultra-compressed 2-bit on-device LLM that maintains near-INT4 performance with minimal degradation."
  },
  {
    "model_name": "Nanbeige4.1-3B-Q4_K_M-GGUF",
    "developer": "Edge-Quant",
    "downloads": 0,
    "createdAt": "2026-02-11T04:50:53.000Z",
    "library_name": "gguf",
    "tools": true,
    "num_quants": 1,
    "quants": [
      {
        "model_id": "nanbeige4.1-3b-q4_k_m",
        "path": "https://huggingface.co/Edge-Quant/Nanbeige4.1-3B-Q4_K_M-GGUF/resolve/main/nanbeige4.1-3b-q4_k_m.gguf",
        "file_size": "2.3 GB"
      }
    ],
    "num_mmproj": 0,
    "mmproj_models": [],
    "readme": "https://huggingface.co/Edge-Quant/Nanbeige4.1-3B-Q4_K_M-GGUF/resolve/main/README.md",
    "description": "A GGUF-quantized 3B parameter language model converted from Nanbeige4.1-3B for use with llama.cpp."
  }
]